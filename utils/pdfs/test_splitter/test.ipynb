{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/root/autodl-tmp/One_Month_Paper')\n",
    "from utils.pdfs.mainPdfReader import mainPdfReader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = r\"/root/autodl-tmp/One_Month_Paper/tmp/2401.00006.pdf\"\n",
    "\n",
    "reader = mainPdfReader(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unknown widths : \n",
      "[0, IndirectObject(576, 0, 140499258408912)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(578, 0, 140499258408912)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(580, 0, 140499258408912)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(582, 0, 140499258408912)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(746, 0, 140499258408912)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(748, 0, 140499258408912)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(750, 0, 140499258408912)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(752, 0, 140499258408912)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(754, 0, 140499258408912)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(756, 0, 140499258408912)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(758, 0, 140499258408912)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(760, 0, 140499258408912)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(763, 0, 140499258408912)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(765, 0, 140499258408912)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(767, 0, 140499258408912)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(769, 0, 140499258408912)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(771, 0, 140499258408912)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(773, 0, 140499258408912)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the piece 0\n",
      " \n",
      "Building Open-Ended Embodied Agent via Language-PolicyBidirectional AdaptationShaopeng Zhai* 1Jie Wang* 1Tianyi Zhang* 1Fuxian Huang* 1Qi Zhang* 1Ming Zhou* 1Jing Hou* 2 1Yu Qiao1Yu Liu1AbstractBuilding embodied agents on integrating LargeLanguage Models (LLMs) and ReinforcementLearning (RL) have revolutionized human-AI in-teraction: researchers can now leverage languageinstructions to plan decision-making for open-ended tasks.。However, existing research faceschallenges in meeting the requirement of open-endedness.. \n",
      "the piece 1\n",
      " \n",
      "However, existing research faceschallenges in meeting the requirement of open-endedness.。They typically either train LLM/RLmodels to adapt to a fixed counterpart, limiting ex-ploration of novel skills and hindering the efficacyof human-AI interaction.。To this end, we presentOpenPAL, a co-training framework comprisingtwo stages: (1) fine-tuning a pre-trained LLM totranslate human instructions into goals for plan-ning, and goal-conditioned training a policy fordecision-making; (2) co-training to align the LLMand policy, achieving instruction open-endedness.We conducted experiments using Contra, an open-ended FPS game, demonstrating that an agenttrained with OpenPAL not only comprehends ar-bitrary instructions but also exhibits efficient exe-cution.. \n",
      "the piece 2\n",
      " \n",
      "hey typically either train LLM/RLmodels to adapt to a fixed counterpart, limiting ex-ploration of novel skills and hindering the efficacyof human-AI interaction.。To this end, we presentOpenPAL, a co-training framework comprisingtwo stages: (1) fine-tuning a pre-trained LLM totranslate human instructions into goals for plan-ning, and goal-conditioned training a policy fordecision-making; (2) co-training to align the LLMand policy, achieving instruction open-endedness.We conducted experiments using Contra, an open-ended FPS game, demonstrating that an agenttrained with OpenPAL not only comprehends ar-bitrary instructions but also exhibits efficient exe-cution.。These results suggest that OpenPAL holdsthe potential to construct open-ended embodiedagents in practical scenarios.1.. \n",
      "the piece 3\n",
      " \n",
      " this end, we presentOpenPAL, a co-training framework comprisingtwo stages: (1) fine-tuning a pre-trained LLM totranslate human instructions into goals for plan-ning, and goal-conditioned training a policy fordecision-making; (2) co-training to align the LLMand policy, achieving instruction open-endedness.We conducted experiments using Contra, an open-ended FPS game, demonstrating that an agenttrained with OpenPAL not only comprehends ar-bitrary instructions but also exhibits efficient exe-cution.。These results suggest that OpenPAL holdsthe potential to construct open-ended embodiedagents in practical scenarios.1.。IntroductionWith the increasing prevalence of LLMs such as ChatGPT,researchers have progressively shifted their focus towardsLLM-centered principles, building embodied agents thatinteract with humans to tackle open-ended tasks (Khandel-wal et al., 2022; Huang et al., 2023).. \n",
      "the piece 4\n",
      " \n",
      "se results suggest that OpenPAL holdsthe potential to construct open-ended embodiedagents in practical scenarios.1.。IntroductionWith the increasing prevalence of LLMs such as ChatGPT,researchers have progressively shifted their focus towardsLLM-centered principles, building embodied agents thatinteract with humans to tackle open-ended tasks (Khandel-wal et al., 2022; Huang et al., 2023).。To achieve this target,we need to resolve the challenge of developing AI agentswith the ability to continuously learn new skills, which isrelated to a domain commonly referred to as open-endedlearning that is broadly categorized into two main factions:*Equal contribution1Shanghai AI Laboratory2Tongji University.. \n",
      "the piece 5\n",
      " \n",
      "oductionWith the increasing prevalence of LLMs such as ChatGPT,researchers have progressively shifted their focus towardsLLM-centered principles, building embodied agents thatinteract with humans to tackle open-ended tasks (Khandel-wal et al., 2022; Huang et al., 2023).。To achieve this target,we need to resolve the challenge of developing AI agentswith the ability to continuously learn new skills, which isrelated to a domain commonly referred to as open-endedlearning that is broadly categorized into two main factions:*Equal contribution1Shanghai AI Laboratory2Tongji University.。Correspondence to: Shaopeng Zhai<zhaishaopeng@pjlab.org.cn>, Yu Liu <liuyu@pjlab.org.cn>.Copyright 2024 by the author(s).. \n",
      "the piece 6\n",
      " \n",
      "hieve this target,we need to resolve the challenge of developing AI agentswith the ability to continuously learn new skills, which isrelated to a domain commonly referred to as open-endedlearning that is broadly categorized into two main factions:*Equal contribution1Shanghai AI Laboratory2Tongji University.。Correspondence to: Shaopeng Zhai<zhaishaopeng@pjlab.org.cn>, Yu Liu <liuyu@pjlab.org.cn>.Copyright 2024 by the author(s).。In progress.。(1) pre-training LLMs to translate human-instructions intosub-tasks, for open-ended planning (Wang et al., 2023a;Ouyang et al., 2022), and (2) curriculum RL for open-endedcontrol (Team et al., 2021; Balduzzi et al., 2019).For pre-trained LLMs, particularly those with closed sourcearchitectures, focus on resolving planning with generalknowledge acquired during the pre-training stage (Wanget al., 2023a).. \n",
      "the piece 7\n",
      " \n",
      "pondence to: Shaopeng Zhai<zhaishaopeng@pjlab.org.cn>, Yu Liu <liuyu@pjlab.org.cn>.Copyright 2024 by the author(s).。In progress.。(1) pre-training LLMs to translate human-instructions intosub-tasks, for open-ended planning (Wang et al., 2023a;Ouyang et al., 2022), and (2) curriculum RL for open-endedcontrol (Team et al., 2021; Balduzzi et al., 2019).For pre-trained LLMs, particularly those with closed sourcearchitectures, focus on resolving planning with generalknowledge acquired during the pre-training stage (Wanget al., 2023a).。However, they share shortcomings like re-lying on task-oriented and hand-crafted prompting, strug-gling to comprehend interactions in special contexts suchas games and be incompetent for high real-time require-ments due to inefficient model computation.. \n",
      "the piece 8\n",
      " \n",
      "ress.。(1) pre-training LLMs to translate human-instructions intosub-tasks, for open-ended planning (Wang et al., 2023a;Ouyang et al., 2022), and (2) curriculum RL for open-endedcontrol (Team et al., 2021; Balduzzi et al., 2019).For pre-trained LLMs, particularly those with closed sourcearchitectures, focus on resolving planning with generalknowledge acquired during the pre-training stage (Wanget al., 2023a).。However, they share shortcomings like re-lying on task-oriented and hand-crafted prompting, strug-gling to comprehend interactions in special contexts suchas games and be incompetent for high real-time require-ments due to inefficient model computation.。In contrast,curriculum RL conducts open-ended learning in an end-to-end manner, developing in diverse methodologies such aspopulation-based RL (Team et al., 2021), goal-conditionedRL (GCRL) (Liu et al., 2022) and etc.. \n",
      "the piece 9\n",
      " \n",
      "training LLMs to translate human-instructions intosub-tasks, for open-ended planning (Wang et al., 2023a;Ouyang et al., 2022), and (2) curriculum RL for open-endedcontrol (Team et al., 2021; Balduzzi et al., 2019).For pre-trained LLMs, particularly those with closed sourcearchitectures, focus on resolving planning with generalknowledge acquired during the pre-training stage (Wanget al., 2023a).。However, they share shortcomings like re-lying on task-oriented and hand-crafted prompting, strug-gling to comprehend interactions in special contexts suchas games and be incompetent for high real-time require-ments due to inefficient model computation.。In contrast,curriculum RL conducts open-ended learning in an end-to-end manner, developing in diverse methodologies such aspopulation-based RL (Team et al., 2021), goal-conditionedRL (GCRL) (Liu et al., 2022) and etc.。Despite RL excelsin learning novel skills compared to rule-based control, itlacks the direct capability for interaction with humans.. \n",
      "the piece 10\n",
      " \n",
      "they share shortcomings like re-lying on task-oriented and hand-crafted prompting, strug-gling to comprehend interactions in special contexts suchas games and be incompetent for high real-time require-ments due to inefficient model computation.。In contrast,curriculum RL conducts open-ended learning in an end-to-end manner, developing in diverse methodologies such aspopulation-based RL (Team et al., 2021), goal-conditionedRL (GCRL) (Liu et al., 2022) and etc.。Despite RL excelsin learning novel skills compared to rule-based control, itlacks the direct capability for interaction with humans.。Toleverage advantages from both sides, i.e., being capable ofinteracting human and AI in solving real-time tasks towardsopen-endedness, an ideal implementation is to integratingLLM for planning and RL policy for decision making.. \n",
      "the piece 11\n",
      " \n",
      "t,curriculum RL conducts open-ended learning in an end-to-end manner, developing in diverse methodologies such aspopulation-based RL (Team et al., 2021), goal-conditionedRL (GCRL) (Liu et al., 2022) and etc.。Despite RL excelsin learning novel skills compared to rule-based control, itlacks the direct capability for interaction with humans.。Toleverage advantages from both sides, i.e., being capable ofinteracting human and AI in solving real-time tasks towardsopen-endedness, an ideal implementation is to integratingLLM for planning and RL policy for decision making.。How-ever, existing studies in this domain have focused on im-proving training efficiency or reducing interaction costs byeither independently training the RL policy or LLM (Huet al., 2023; Du et al., 2023) to adapt the other, resulting inoverfitting and failing to explore novel skills in specific envi-ronments that necessitate specialized knowledge and fallingshort of achieving true open-endedness.. \n",
      "the piece 12\n",
      " \n",
      "excelsin learning novel skills compared to rule-based control, itlacks the direct capability for interaction with humans.。Toleverage advantages from both sides, i.e., being capable ofinteracting human and AI in solving real-time tasks towardsopen-endedness, an ideal implementation is to integratingLLM for planning and RL policy for decision making.。How-ever, existing studies in this domain have focused on im-proving training efficiency or reducing interaction costs byeither independently training the RL policy or LLM (Huet al., 2023; Du et al., 2023) to adapt the other, resulting inoverfitting and failing to explore novel skills in specific envi-ronments that necessitate specialized knowledge and fallingshort of achieving true open-endedness.。Despite previouswork resolve this issue with LLM-based re-planning (Wanget al., 2023a;b), it is inefficient for high-dimensional tasksand the re-planning is still in the existing range of strength.To address the above challenge, we propose a co-trainingframework, OpenPAL, strunctured as a two-stage learningprocess to implemente bi-directional adaptation.. \n",
      "the piece 13\n",
      " \n",
      "dvantages from both sides, i.e., being capable ofinteracting human and AI in solving real-time tasks towardsopen-endedness, an ideal implementation is to integratingLLM for planning and RL policy for decision making.。How-ever, existing studies in this domain have focused on im-proving training efficiency or reducing interaction costs byeither independently training the RL policy or LLM (Huet al., 2023; Du et al., 2023) to adapt the other, resulting inoverfitting and failing to explore novel skills in specific envi-ronments that necessitate specialized knowledge and fallingshort of achieving true open-endedness.。Despite previouswork resolve this issue with LLM-based re-planning (Wanget al., 2023a;b), it is inefficient for high-dimensional tasksand the re-planning is still in the existing range of strength.To address the above challenge, we propose a co-trainingframework, OpenPAL, strunctured as a two-stage learningprocess to implemente bi-directional adaptation.。This de-sign enables the RL policy continuously explore novel skillswhiling align the LLM and the policy towards achievinginstruction open-endedness.. \n",
      "the piece 14\n",
      " \n",
      "sting studies in this domain have focused on im-proving training efficiency or reducing interaction costs byeither independently training the RL policy or LLM (Huet al., 2023; Du et al., 2023) to adapt the other, resulting inoverfitting and failing to explore novel skills in specific envi-ronments that necessitate specialized knowledge and fallingshort of achieving true open-endedness.。Despite previouswork resolve this issue with LLM-based re-planning (Wanget al., 2023a;b), it is inefficient for high-dimensional tasksand the re-planning is still in the existing range of strength.To address the above challenge, we propose a co-trainingframework, OpenPAL, strunctured as a two-stage learningprocess to implemente bi-directional adaptation.。This de-sign enables the RL policy continuously explore novel skillswhiling align the LLM and the policy towards achievinginstruction open-endedness.。In the first stage, we sepa-rately train a ChatGLM-6B (Du et al., 2022) as a planner(or goal generator) Gllmand policy πg, where Gllmgen-erates goals with given instructions and environment con-1arXiv:2401.00006v3  [cs.AI]  6 Feb 2024Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationOpen-ended EnvironmentsLLM Policy       Co-trainingMulti-step Fine-tuning Goal-conditioned RL“Find Them Out !”/gid00175Prepare to Ambush.”/gid00175Be Careful !”Human InstructionsObservations Agent Actions/gid00003/gid00036/gid00045/gid00031/gid00174/gid00046/gid00183/gid00032/gid00052/gid00032/gid00183/gid00049/gid00036/gid00032/gid00050/gid00006/gid00041/gid00049/gid00036/gid00045/gid00042/gid00041/gid00040/gid00032/gid00041/gid00047 /gid00002/gid00034/gid00032/gid00041/gid00047 /gid00016/gid00047/gid00035/gid00032/gid00045/gid00001/gid00043/gid00039/gid00028/gid00052/gid00032/gid00045/gid00046Goals PlanningRewards FeedbackGoal RewardsFigure 1: Overview of co-training in OpenPAL.. \n",
      "the piece 15\n",
      " \n",
      "uswork resolve this issue with LLM-based re-planning (Wanget al., 2023a;b), it is inefficient for high-dimensional tasksand the re-planning is still in the existing range of strength.To address the above challenge, we propose a co-trainingframework, OpenPAL, strunctured as a two-stage learningprocess to implemente bi-directional adaptation.。This de-sign enables the RL policy continuously explore novel skillswhiling align the LLM and the policy towards achievinginstruction open-endedness.。In the first stage, we sepa-rately train a ChatGLM-6B (Du et al., 2022) as a planner(or goal generator) Gllmand policy πg, where Gllmgen-erates goals with given instructions and environment con-1arXiv:2401.00006v3  [cs.AI]  6 Feb 2024Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationOpen-ended EnvironmentsLLM Policy       Co-trainingMulti-step Fine-tuning Goal-conditioned RL“Find Them Out !”/gid00175Prepare to Ambush.”/gid00175Be Careful !”Human InstructionsObservations Agent Actions/gid00003/gid00036/gid00045/gid00031/gid00174/gid00046/gid00183/gid00032/gid00052/gid00032/gid00183/gid00049/gid00036/gid00032/gid00050/gid00006/gid00041/gid00049/gid00036/gid00045/gid00042/gid00041/gid00040/gid00032/gid00041/gid00047 /gid00002/gid00034/gid00032/gid00041/gid00047 /gid00016/gid00047/gid00035/gid00032/gid00045/gid00001/gid00043/gid00039/gid00028/gid00052/gid00032/gid00045/gid00046Goals PlanningRewards FeedbackGoal RewardsFigure 1: Overview of co-training in OpenPAL.。The Policy and LLM is pre-trained with multi-step fine-tuning and goal-conditioned RL, respectively.. \n",
      "the piece 16\n",
      " \n",
      "ables the RL policy continuously explore novel skillswhiling align the LLM and the policy towards achievinginstruction open-endedness.。In the first stage, we sepa-rately train a ChatGLM-6B (Du et al., 2022) as a planner(or goal generator) Gllmand policy πg, where Gllmgen-erates goals with given instructions and environment con-1arXiv:2401.00006v3  [cs.AI]  6 Feb 2024Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationOpen-ended EnvironmentsLLM Policy       Co-trainingMulti-step Fine-tuning Goal-conditioned RL“Find Them Out !”/gid00175Prepare to Ambush.”/gid00175Be Careful !”Human InstructionsObservations Agent Actions/gid00003/gid00036/gid00045/gid00031/gid00174/gid00046/gid00183/gid00032/gid00052/gid00032/gid00183/gid00049/gid00036/gid00032/gid00050/gid00006/gid00041/gid00049/gid00036/gid00045/gid00042/gid00041/gid00040/gid00032/gid00041/gid00047 /gid00002/gid00034/gid00032/gid00041/gid00047 /gid00016/gid00047/gid00035/gid00032/gid00045/gid00001/gid00043/gid00039/gid00028/gid00052/gid00032/gid00045/gid00046Goals PlanningRewards FeedbackGoal RewardsFigure 1: Overview of co-training in OpenPAL.。The Policy and LLM is pre-trained with multi-step fine-tuning and goal-conditioned RL, respectively.。Then, the co-training aligns them towards achieving instruction open-endedness.text, and πglearns to execute goals.. \n",
      "the piece 17\n",
      " \n",
      "ge, we sepa-rately train a ChatGLM-6B (Du et al., 2022) as a planner(or goal generator) Gllmand policy πg, where Gllmgen-erates goals with given instructions and environment con-1arXiv:2401.00006v3  [cs.AI]  6 Feb 2024Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationOpen-ended EnvironmentsLLM Policy       Co-trainingMulti-step Fine-tuning Goal-conditioned RL“Find Them Out !”/gid00175Prepare to Ambush.”/gid00175Be Careful !”Human InstructionsObservations Agent Actions/gid00003/gid00036/gid00045/gid00031/gid00174/gid00046/gid00183/gid00032/gid00052/gid00032/gid00183/gid00049/gid00036/gid00032/gid00050/gid00006/gid00041/gid00049/gid00036/gid00045/gid00042/gid00041/gid00040/gid00032/gid00041/gid00047 /gid00002/gid00034/gid00032/gid00041/gid00047 /gid00016/gid00047/gid00035/gid00032/gid00045/gid00001/gid00043/gid00039/gid00028/gid00052/gid00032/gid00045/gid00046Goals PlanningRewards FeedbackGoal RewardsFigure 1: Overview of co-training in OpenPAL.。The Policy and LLM is pre-trained with multi-step fine-tuning and goal-conditioned RL, respectively.。Then, the co-training aligns them towards achieving instruction open-endedness.text, and πglearns to execute goals.。To achieve that, wepropose multi-step fine-tuning a pre-trained LLM with GPT-4-generated instructions and goals, and open-ended goalgeneration to learn a goal-conditioned policy.. \n",
      "the piece 18\n",
      " \n",
      "M is pre-trained with multi-step fine-tuning and goal-conditioned RL, respectively.。Then, the co-training aligns them towards achieving instruction open-endedness.text, and πglearns to execute goals.。To achieve that, wepropose multi-step fine-tuning a pre-trained LLM with GPT-4-generated instructions and goals, and open-ended goalgeneration to learn a goal-conditioned policy.。In the secondstage, we implement co-training to align Gllm(planning)andπg(decision-making), as illustrated in Figure 1.。Thisaims to achieve instruction open-endedness, aligning theinstruction space with the open-ended goal space that theagent achieved.. \n",
      "the piece 19\n",
      " \n",
      "ing aligns them towards achieving instruction open-endedness.text, and πglearns to execute goals.。To achieve that, wepropose multi-step fine-tuning a pre-trained LLM with GPT-4-generated instructions and goals, and open-ended goalgeneration to learn a goal-conditioned policy.。In the secondstage, we implement co-training to align Gllm(planning)andπg(decision-making), as illustrated in Figure 1.。Thisaims to achieve instruction open-endedness, aligning theinstruction space with the open-ended goal space that theagent achieved.。Specifically, we implement the co-trainingas an interleaved execution of (1) Reinforcement Learn-ing with Agent Feedback (RLAF) for Gllmand (2) GCRLforπgwith goals generated by Gllm, where RLAF cen-ters around rewarding Gllmwith agent feedback and goalexecution.. \n",
      "the piece 20\n",
      " \n",
      "propose multi-step fine-tuning a pre-trained LLM with GPT-4-generated instructions and goals, and open-ended goalgeneration to learn a goal-conditioned policy.。In the secondstage, we implement co-training to align Gllm(planning)andπg(decision-making), as illustrated in Figure 1.。Thisaims to achieve instruction open-endedness, aligning theinstruction space with the open-ended goal space that theagent achieved.。Specifically, we implement the co-trainingas an interleaved execution of (1) Reinforcement Learn-ing with Agent Feedback (RLAF) for Gllmand (2) GCRLforπgwith goals generated by Gllm, where RLAF cen-ters around rewarding Gllmwith agent feedback and goalexecution.。This two-staged approach optimizes the LLMfor comprehending environment context under the consid-eration of decision-making, while concurrently enhancingdecision-making for goals aligned with human instructions.For evaluation, we employ Contra, an open-ended FPS game.The results demonstrate that OpenPAL achieves a high goalcompletion ratio for open-ended human-AI interaction.2.. \n",
      "the piece 21\n",
      " \n",
      "we implement co-training to align Gllm(planning)andπg(decision-making), as illustrated in Figure 1.。Thisaims to achieve instruction open-endedness, aligning theinstruction space with the open-ended goal space that theagent achieved.。Specifically, we implement the co-trainingas an interleaved execution of (1) Reinforcement Learn-ing with Agent Feedback (RLAF) for Gllmand (2) GCRLforπgwith goals generated by Gllm, where RLAF cen-ters around rewarding Gllmwith agent feedback and goalexecution.。This two-staged approach optimizes the LLMfor comprehending environment context under the consid-eration of decision-making, while concurrently enhancingdecision-making for goals aligned with human instructions.For evaluation, we employ Contra, an open-ended FPS game.The results demonstrate that OpenPAL achieves a high goalcompletion ratio for open-ended human-AI interaction.2.。BackgroundGoal-conditioned Reinforcement Learning.. \n",
      "the piece 22\n",
      " \n",
      "nstruction open-endedness, aligning theinstruction space with the open-ended goal space that theagent achieved.。Specifically, we implement the co-trainingas an interleaved execution of (1) Reinforcement Learn-ing with Agent Feedback (RLAF) for Gllmand (2) GCRLforπgwith goals generated by Gllm, where RLAF cen-ters around rewarding Gllmwith agent feedback and goalexecution.。This two-staged approach optimizes the LLMfor comprehending environment context under the consid-eration of decision-making, while concurrently enhancingdecision-making for goals aligned with human instructions.For evaluation, we employ Contra, an open-ended FPS game.The results demonstrate that OpenPAL achieves a high goalcompletion ratio for open-ended human-AI interaction.2.。BackgroundGoal-conditioned Reinforcement Learning.。Formally,GCRL could be formulated as a goal-augmented MarkovDecision Process M(Liu et al., 2022).. \n",
      "the piece 23\n",
      " \n",
      "ment the co-trainingas an interleaved execution of (1) Reinforcement Learn-ing with Agent Feedback (RLAF) for Gllmand (2) GCRLforπgwith goals generated by Gllm, where RLAF cen-ters around rewarding Gllmwith agent feedback and goalexecution.。This two-staged approach optimizes the LLMfor comprehending environment context under the consid-eration of decision-making, while concurrently enhancingdecision-making for goals aligned with human instructions.For evaluation, we employ Contra, an open-ended FPS game.The results demonstrate that OpenPAL achieves a high goalcompletion ratio for open-ended human-AI interaction.2.。BackgroundGoal-conditioned Reinforcement Learning.。Formally,GCRL could be formulated as a goal-augmented MarkovDecision Process M(Liu et al., 2022).。Denoting Matuple⟨S,A,G,P,R, γ⟩, where S,A,Gthe state, actionand goal spaces, respectively.. \n",
      "the piece 24\n",
      " \n",
      "h optimizes the LLMfor comprehending environment context under the consid-eration of decision-making, while concurrently enhancingdecision-making for goals aligned with human instructions.For evaluation, we employ Contra, an open-ended FPS game.The results demonstrate that OpenPAL achieves a high goalcompletion ratio for open-ended human-AI interaction.2.。BackgroundGoal-conditioned Reinforcement Learning.。Formally,GCRL could be formulated as a goal-augmented MarkovDecision Process M(Liu et al., 2022).。Denoting Matuple⟨S,A,G,P,R, γ⟩, where S,A,Gthe state, actionand goal spaces, respectively.。In general, Gis a projectionofS, i.e.,G=PROJ(S).Pdefines the state transitionprobabilities, i.e., P:S × A → ∆(S), where ∆(·)adistribution.. \n",
      "the piece 25\n",
      " \n",
      "ed Reinforcement Learning.。Formally,GCRL could be formulated as a goal-augmented MarkovDecision Process M(Liu et al., 2022).。Denoting Matuple⟨S,A,G,P,R, γ⟩, where S,A,Gthe state, actionand goal spaces, respectively.。In general, Gis a projectionofS, i.e.,G=PROJ(S).Pdefines the state transitionprobabilities, i.e., P:S × A → ∆(S), where ∆(·)adistribution.。R:S × A × G → Rdefines the rewardfunction r(s, a, g ).。At the beginning of an trajectory τ, agoalgis sampled from a distribution Pg, which generallydefines a task for the agent to execute.. \n",
      "the piece 26\n",
      " \n",
      "rmulated as a goal-augmented MarkovDecision Process M(Liu et al., 2022).。Denoting Matuple⟨S,A,G,P,R, γ⟩, where S,A,Gthe state, actionand goal spaces, respectively.。In general, Gis a projectionofS, i.e.,G=PROJ(S).Pdefines the state transitionprobabilities, i.e., P:S × A → ∆(S), where ∆(·)adistribution.。R:S × A × G → Rdefines the rewardfunction r(s, a, g ).。At the beginning of an trajectory τ, agoalgis sampled from a distribution Pg, which generallydefines a task for the agent to execute.。As for decisionmaking, πdenotes a policy as π:S × G → ∆(A), whichis a distribution over the action space.. \n",
      "the piece 27\n",
      " \n",
      ", γ⟩, where S,A,Gthe state, actionand goal spaces, respectively.。In general, Gis a projectionofS, i.e.,G=PROJ(S).Pdefines the state transitionprobabilities, i.e., P:S × A → ∆(S), where ∆(·)adistribution.。R:S × A × G → Rdefines the rewardfunction r(s, a, g ).。At the beginning of an trajectory τ, agoalgis sampled from a distribution Pg, which generallydefines a task for the agent to execute.。As for decisionmaking, πdenotes a policy as π:S × G → ∆(A), whichis a distribution over the action space.。To solve a M, orachieve open-endedness in other words, an agent with policyπneeds to maximize its accumulative reward over the goalspace as Eat∼π(·|st,g),(st,at)∼τ,g∼PghPTt=0γtr(st, at, g)i,where γ∈[0,1)discounts a reward at each time step toensure the convergence.. \n",
      "the piece 28\n",
      " \n",
      "nofS, i.e.,G=PROJ(S).Pdefines the state transitionprobabilities, i.e., P:S × A → ∆(S), where ∆(·)adistribution.。R:S × A × G → Rdefines the rewardfunction r(s, a, g ).。At the beginning of an trajectory τ, agoalgis sampled from a distribution Pg, which generallydefines a task for the agent to execute.。As for decisionmaking, πdenotes a policy as π:S × G → ∆(A), whichis a distribution over the action space.。To solve a M, orachieve open-endedness in other words, an agent with policyπneeds to maximize its accumulative reward over the goalspace as Eat∼π(·|st,g),(st,at)∼τ,g∼PghPTt=0γtr(st, at, g)i,where γ∈[0,1)discounts a reward at each time step toensure the convergence.。Normally, r(st, at, g)is binary asr(st, at, g) =(1PROJ(st+1) =g0otherwise.. \n",
      "the piece 29\n",
      " \n",
      "ewardfunction r(s, a, g ).。At the beginning of an trajectory τ, agoalgis sampled from a distribution Pg, which generallydefines a task for the agent to execute.。As for decisionmaking, πdenotes a policy as π:S × G → ∆(A), whichis a distribution over the action space.。To solve a M, orachieve open-endedness in other words, an agent with policyπneeds to maximize its accumulative reward over the goalspace as Eat∼π(·|st,g),(st,at)∼τ,g∼PghPTt=0γtr(st, at, g)i,where γ∈[0,1)discounts a reward at each time step toensure the convergence.。Normally, r(st, at, g)is binary asr(st, at, g) =(1PROJ(st+1) =g0otherwise.。(1)To approximatePTt=0γtr(st, at, g), GRL suggests us-ing the Universal Value Function Approximator (UVFA)V(s, g).. \n",
      "the piece 30\n",
      " \n",
      "tory τ, agoalgis sampled from a distribution Pg, which generallydefines a task for the agent to execute.。As for decisionmaking, πdenotes a policy as π:S × G → ∆(A), whichis a distribution over the action space.。To solve a M, orachieve open-endedness in other words, an agent with policyπneeds to maximize its accumulative reward over the goalspace as Eat∼π(·|st,g),(st,at)∼τ,g∼PghPTt=0γtr(st, at, g)i,where γ∈[0,1)discounts a reward at each time step toensure the convergence.。Normally, r(st, at, g)is binary asr(st, at, g) =(1PROJ(st+1) =g0otherwise.。(1)To approximatePTt=0γtr(st, at, g), GRL suggests us-ing the Universal Value Function Approximator (UVFA)V(s, g).。As for the solving of open-endedness, there arecritical challenges that need to be resolved, including butnot limited to: (1) Pgis agnostic to an agent, and (2) Gisuncountable and continuous.. \n",
      "the piece 31\n",
      " \n",
      "s a policy as π:S × G → ∆(A), whichis a distribution over the action space.。To solve a M, orachieve open-endedness in other words, an agent with policyπneeds to maximize its accumulative reward over the goalspace as Eat∼π(·|st,g),(st,at)∼τ,g∼PghPTt=0γtr(st, at, g)i,where γ∈[0,1)discounts a reward at each time step toensure the convergence.。Normally, r(st, at, g)is binary asr(st, at, g) =(1PROJ(st+1) =g0otherwise.。(1)To approximatePTt=0γtr(st, at, g), GRL suggests us-ing the Universal Value Function Approximator (UVFA)V(s, g).。As for the solving of open-endedness, there arecritical challenges that need to be resolved, including butnot limited to: (1) Pgis agnostic to an agent, and (2) Gisuncountable and continuous.。To resolve these issues, exist-ing research is centered on curriculum RL (Weng, 2020),i.e., automatically discovering novel goals from past learn-ing.. \n",
      "the piece 32\n",
      " \n",
      "dedness in other words, an agent with policyπneeds to maximize its accumulative reward over the goalspace as Eat∼π(·|st,g),(st,at)∼τ,g∼PghPTt=0γtr(st, at, g)i,where γ∈[0,1)discounts a reward at each time step toensure the convergence.。Normally, r(st, at, g)is binary asr(st, at, g) =(1PROJ(st+1) =g0otherwise.。(1)To approximatePTt=0γtr(st, at, g), GRL suggests us-ing the Universal Value Function Approximator (UVFA)V(s, g).。As for the solving of open-endedness, there arecritical challenges that need to be resolved, including butnot limited to: (1) Pgis agnostic to an agent, and (2) Gisuncountable and continuous.。To resolve these issues, exist-ing research is centered on curriculum RL (Weng, 2020),i.e., automatically discovering novel goals from past learn-ing.。Hindsight goal relabelling (Andrychowicz et al., 2017;Fang et al., 2019; Zheng et al., 2022; Schaul et al., 2015)implicitly implements curriculum learning by introducinga prioritized replay mechanism and performs high data ef-ficiency.. \n",
      "the piece 33\n",
      " \n",
      "asr(st, at, g) =(1PROJ(st+1) =g0otherwise.。(1)To approximatePTt=0γtr(st, at, g), GRL suggests us-ing the Universal Value Function Approximator (UVFA)V(s, g).。As for the solving of open-endedness, there arecritical challenges that need to be resolved, including butnot limited to: (1) Pgis agnostic to an agent, and (2) Gisuncountable and continuous.。To resolve these issues, exist-ing research is centered on curriculum RL (Weng, 2020),i.e., automatically discovering novel goals from past learn-ing.。Hindsight goal relabelling (Andrychowicz et al., 2017;Fang et al., 2019; Zheng et al., 2022; Schaul et al., 2015)implicitly implements curriculum learning by introducinga prioritized replay mechanism and performs high data ef-ficiency.。Despite numerous curriculum approaches, thesample inefficiency due to the setting of binary reward sig-nals (Equation (4)) hinders policy learning.. \n",
      "the piece 34\n",
      " \n",
      " g), GRL suggests us-ing the Universal Value Function Approximator (UVFA)V(s, g).。As for the solving of open-endedness, there arecritical challenges that need to be resolved, including butnot limited to: (1) Pgis agnostic to an agent, and (2) Gisuncountable and continuous.。To resolve these issues, exist-ing research is centered on curriculum RL (Weng, 2020),i.e., automatically discovering novel goals from past learn-ing.。Hindsight goal relabelling (Andrychowicz et al., 2017;Fang et al., 2019; Zheng et al., 2022; Schaul et al., 2015)implicitly implements curriculum learning by introducinga prioritized replay mechanism and performs high data ef-ficiency.。Despite numerous curriculum approaches, thesample inefficiency due to the setting of binary reward sig-nals (Equation (4)) hinders policy learning.。As a solution,existing research leverages reward shaping (Ng et al., 1999;Ecoffet et al., 2021; Ding et al., 2023; Trott et al., 2019)which is a straightforward and efficient idea.LLMs and Human-AI Interaction.. \n",
      "the piece 35\n",
      " \n",
      "ss, there arecritical challenges that need to be resolved, including butnot limited to: (1) Pgis agnostic to an agent, and (2) Gisuncountable and continuous.。To resolve these issues, exist-ing research is centered on curriculum RL (Weng, 2020),i.e., automatically discovering novel goals from past learn-ing.。Hindsight goal relabelling (Andrychowicz et al., 2017;Fang et al., 2019; Zheng et al., 2022; Schaul et al., 2015)implicitly implements curriculum learning by introducinga prioritized replay mechanism and performs high data ef-ficiency.。Despite numerous curriculum approaches, thesample inefficiency due to the setting of binary reward sig-nals (Equation (4)) hinders policy learning.。As a solution,existing research leverages reward shaping (Ng et al., 1999;Ecoffet et al., 2021; Ding et al., 2023; Trott et al., 2019)which is a straightforward and efficient idea.LLMs and Human-AI Interaction.。LLMs (Brown et al.,2020; OpenAI, 2023) a class of neural networks that ex-Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationecute in auto-regressive for text generation.. \n",
      "the piece 36\n",
      " \n",
      "research is centered on curriculum RL (Weng, 2020),i.e., automatically discovering novel goals from past learn-ing.。Hindsight goal relabelling (Andrychowicz et al., 2017;Fang et al., 2019; Zheng et al., 2022; Schaul et al., 2015)implicitly implements curriculum learning by introducinga prioritized replay mechanism and performs high data ef-ficiency.。Despite numerous curriculum approaches, thesample inefficiency due to the setting of binary reward sig-nals (Equation (4)) hinders policy learning.。As a solution,existing research leverages reward shaping (Ng et al., 1999;Ecoffet et al., 2021; Ding et al., 2023; Trott et al., 2019)which is a straightforward and efficient idea.LLMs and Human-AI Interaction.。LLMs (Brown et al.,2020; OpenAI, 2023) a class of neural networks that ex-Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationecute in auto-regressive for text generation.。Given a se-quence of text tokens with length tasx1:t= (x1, .. \n",
      "the piece 37\n",
      " \n",
      "wicz et al., 2017;Fang et al., 2019; Zheng et al., 2022; Schaul et al., 2015)implicitly implements curriculum learning by introducinga prioritized replay mechanism and performs high data ef-ficiency.。Despite numerous curriculum approaches, thesample inefficiency due to the setting of binary reward sig-nals (Equation (4)) hinders policy learning.。As a solution,existing research leverages reward shaping (Ng et al., 1999;Ecoffet et al., 2021; Ding et al., 2023; Trott et al., 2019)which is a straightforward and efficient idea.LLMs and Human-AI Interaction.。LLMs (Brown et al.,2020; OpenAI, 2023) a class of neural networks that ex-Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationecute in auto-regressive for text generation.。Given a se-quence of text tokens with length tasx1:t= (x1, .。.. \n",
      "the piece 38\n",
      " \n",
      "s, thesample inefficiency due to the setting of binary reward sig-nals (Equation (4)) hinders policy learning.。As a solution,existing research leverages reward shaping (Ng et al., 1999;Ecoffet et al., 2021; Ding et al., 2023; Trott et al., 2019)which is a straightforward and efficient idea.LLMs and Human-AI Interaction.。LLMs (Brown et al.,2020; OpenAI, 2023) a class of neural networks that ex-Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationecute in auto-regressive for text generation.。Given a se-quence of text tokens with length tasx1:t= (x1, .。.。.. \n",
      "the piece 39\n",
      " \n",
      "ges reward shaping (Ng et al., 1999;Ecoffet et al., 2021; Ding et al., 2023; Trott et al., 2019)which is a straightforward and efficient idea.LLMs and Human-AI Interaction.。LLMs (Brown et al.,2020; OpenAI, 2023) a class of neural networks that ex-Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationecute in auto-regressive for text generation.。Given a se-quence of text tokens with length tasx1:t= (x1, .。.。.。, x t),the generation of a next token xt+1could be formulatedas sampling from a probabilistic model P(·|x1:t).. \n",
      "the piece 40\n",
      " \n",
      "a class of neural networks that ex-Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationecute in auto-regressive for text generation.。Given a se-quence of text tokens with length tasx1:t= (x1, .。.。.。, x t),the generation of a next token xt+1could be formulatedas sampling from a probabilistic model P(·|x1:t).。As forthe training of LLMs, the target is equivalently to find aparameter set θLLMwhich satisfies the optimal generation,i.e.,θLLM= arg max θEx1:t,xt+1P(xt+1|x1:t;θ).。Beyondthe research of LLMs, it is attractive to leverage LLMsas an interface to interact human with agents (Jiang et al.,2023; Hermann et al., 2017).. \n",
      "the piece 41\n",
      " \n",
      "ngth tasx1:t= (x1, .。.。.。, x t),the generation of a next token xt+1could be formulatedas sampling from a probabilistic model P(·|x1:t).。As forthe training of LLMs, the target is equivalently to find aparameter set θLLMwhich satisfies the optimal generation,i.e.,θLLM= arg max θEx1:t,xt+1P(xt+1|x1:t;θ).。Beyondthe research of LLMs, it is attractive to leverage LLMsas an interface to interact human with agents (Jiang et al.,2023; Hermann et al., 2017).。We can roughly reformulatethe generation as xt+1∼P(·|ι, x1:t)for human-AI inter-action, where ιthe language instruction as a prompt, x1:tthe context.. \n",
      "the piece 42\n",
      " \n",
      " xt+1could be formulatedas sampling from a probabilistic model P(·|x1:t).。As forthe training of LLMs, the target is equivalently to find aparameter set θLLMwhich satisfies the optimal generation,i.e.,θLLM= arg max θEx1:t,xt+1P(xt+1|x1:t;θ).。Beyondthe research of LLMs, it is attractive to leverage LLMsas an interface to interact human with agents (Jiang et al.,2023; Hermann et al., 2017).。We can roughly reformulatethe generation as xt+1∼P(·|ι, x1:t)for human-AI inter-action, where ιthe language instruction as a prompt, x1:tthe context.。For the cases have low real-time requirements,xt+1is a control signal for decision making (Brohan et al.,2022; Zitkovich et al., 2023; Nakano et al., 2021).. \n",
      "the piece 43\n",
      " \n",
      "+1could be formulatedas sampling from a probabilistic model P(·|x1:t).。As forthe training of LLMs, the target is equivalently to find aparameter set θLLMwhich satisfies the optimal generation,i.e.,θLLM= arg max θEx1:t,xt+1P(xt+1|x1:t;θ).。Beyondthe research of LLMs, it is attractive to leverage LLMsas an interface to interact human with agents (Jiang et al.,2023; Hermann et al., 2017).。We can roughly reformulatethe generation as xt+1∼P(·|ι, x1:t)for human-AI inter-action, where ιthe language instruction as a prompt, x1:tthe context.。For the cases have low real-time requirements,xt+1is a control signal for decision making (Brohan et al.,2022; Zitkovich et al., 2023; Nakano et al., 2021).。Whilefor the cases have high real-time requirements, xt+1is agoal and will be fed to a controller to guide the decisionmaking (Wang et al., 2023b;a).. \n",
      "the piece 44\n",
      " \n",
      "ould be formulatedas sampling from a probabilistic model P(·|x1:t).。As forthe training of LLMs, the target is equivalently to find aparameter set θLLMwhich satisfies the optimal generation,i.e.,θLLM= arg max θEx1:t,xt+1P(xt+1|x1:t;θ).。Beyondthe research of LLMs, it is attractive to leverage LLMsas an interface to interact human with agents (Jiang et al.,2023; Hermann et al., 2017).。We can roughly reformulatethe generation as xt+1∼P(·|ι, x1:t)for human-AI inter-action, where ιthe language instruction as a prompt, x1:tthe context.。For the cases have low real-time requirements,xt+1is a control signal for decision making (Brohan et al.,2022; Zitkovich et al., 2023; Nakano et al., 2021).。Whilefor the cases have high real-time requirements, xt+1is agoal and will be fed to a controller to guide the decisionmaking (Wang et al., 2023b;a).。Our study falls within thelatter situation, building open-endeded embodied agents inthe cases with high real-time requirements.3.. \n",
      "the piece 45\n",
      " \n",
      "uivalently to find aparameter set θLLMwhich satisfies the optimal generation,i.e.,θLLM= arg max θEx1:t,xt+1P(xt+1|x1:t;θ).。Beyondthe research of LLMs, it is attractive to leverage LLMsas an interface to interact human with agents (Jiang et al.,2023; Hermann et al., 2017).。We can roughly reformulatethe generation as xt+1∼P(·|ι, x1:t)for human-AI inter-action, where ιthe language instruction as a prompt, x1:tthe context.。For the cases have low real-time requirements,xt+1is a control signal for decision making (Brohan et al.,2022; Zitkovich et al., 2023; Nakano et al., 2021).。Whilefor the cases have high real-time requirements, xt+1is agoal and will be fed to a controller to guide the decisionmaking (Wang et al., 2023b;a).。Our study falls within thelatter situation, building open-endeded embodied agents inthe cases with high real-time requirements.3.。The Contra: A Battle Royale FPS GameContra seamlessly merges the last-man-standing gameplaydynamics with the survival, exploration, and scavengingelements inherent in first-person shooting games (Gautamet al., 2021).. \n",
      "the piece 46\n",
      " \n",
      "to leverage LLMsas an interface to interact human with agents (Jiang et al.,2023; Hermann et al., 2017).。We can roughly reformulatethe generation as xt+1∼P(·|ι, x1:t)for human-AI inter-action, where ιthe language instruction as a prompt, x1:tthe context.。For the cases have low real-time requirements,xt+1is a control signal for decision making (Brohan et al.,2022; Zitkovich et al., 2023; Nakano et al., 2021).。Whilefor the cases have high real-time requirements, xt+1is agoal and will be fed to a controller to guide the decisionmaking (Wang et al., 2023b;a).。Our study falls within thelatter situation, building open-endeded embodied agents inthe cases with high real-time requirements.3.。The Contra: A Battle Royale FPS GameContra seamlessly merges the last-man-standing gameplaydynamics with the survival, exploration, and scavengingelements inherent in first-person shooting games (Gautamet al., 2021).。It unfolds with multiple hostile teams, neces-sitating players to collaborate with teammates, withstandadversaries, and strive to outlast others in the ever-changingarena.. \n",
      "the piece 47\n",
      " \n",
      "+1∼P(·|ι, x1:t)for human-AI inter-action, where ιthe language instruction as a prompt, x1:tthe context.。For the cases have low real-time requirements,xt+1is a control signal for decision making (Brohan et al.,2022; Zitkovich et al., 2023; Nakano et al., 2021).。Whilefor the cases have high real-time requirements, xt+1is agoal and will be fed to a controller to guide the decisionmaking (Wang et al., 2023b;a).。Our study falls within thelatter situation, building open-endeded embodied agents inthe cases with high real-time requirements.3.。The Contra: A Battle Royale FPS GameContra seamlessly merges the last-man-standing gameplaydynamics with the survival, exploration, and scavengingelements inherent in first-person shooting games (Gautamet al., 2021).。It unfolds with multiple hostile teams, neces-sitating players to collaborate with teammates, withstandadversaries, and strive to outlast others in the ever-changingarena.。The agent’s objectives encompass individual survivaland the elimination of encountered enemies.. \n",
      "the piece 48\n",
      " \n",
      "t+1is a control signal for decision making (Brohan et al.,2022; Zitkovich et al., 2023; Nakano et al., 2021).。Whilefor the cases have high real-time requirements, xt+1is agoal and will be fed to a controller to guide the decisionmaking (Wang et al., 2023b;a).。Our study falls within thelatter situation, building open-endeded embodied agents inthe cases with high real-time requirements.3.。The Contra: A Battle Royale FPS GameContra seamlessly merges the last-man-standing gameplaydynamics with the survival, exploration, and scavengingelements inherent in first-person shooting games (Gautamet al., 2021).。It unfolds with multiple hostile teams, neces-sitating players to collaborate with teammates, withstandadversaries, and strive to outlast others in the ever-changingarena.。The agent’s objectives encompass individual survivaland the elimination of encountered enemies.。An agent inContra mandates a sequential acquisition of skills, startingfrom fundamental abilities like running and item collec-tion.. \n",
      "the piece 49\n",
      " \n",
      "nts, xt+1is agoal and will be fed to a controller to guide the decisionmaking (Wang et al., 2023b;a).。Our study falls within thelatter situation, building open-endeded embodied agents inthe cases with high real-time requirements.3.。The Contra: A Battle Royale FPS GameContra seamlessly merges the last-man-standing gameplaydynamics with the survival, exploration, and scavengingelements inherent in first-person shooting games (Gautamet al., 2021).。It unfolds with multiple hostile teams, neces-sitating players to collaborate with teammates, withstandadversaries, and strive to outlast others in the ever-changingarena.。The agent’s objectives encompass individual survivaland the elimination of encountered enemies.。An agent inContra mandates a sequential acquisition of skills, startingfrom fundamental abilities like running and item collec-tion.。As the learning proceeds, an agent must master moreintricate skills such as evading enemy projectiles and co-ordinating tactics with teammates, an open-ended learningprocess.. \n",
      "the piece 50\n",
      " \n",
      "ing open-endeded embodied agents inthe cases with high real-time requirements.3.。The Contra: A Battle Royale FPS GameContra seamlessly merges the last-man-standing gameplaydynamics with the survival, exploration, and scavengingelements inherent in first-person shooting games (Gautamet al., 2021).。It unfolds with multiple hostile teams, neces-sitating players to collaborate with teammates, withstandadversaries, and strive to outlast others in the ever-changingarena.。The agent’s objectives encompass individual survivaland the elimination of encountered enemies.。An agent inContra mandates a sequential acquisition of skills, startingfrom fundamental abilities like running and item collec-tion.。As the learning proceeds, an agent must master moreintricate skills such as evading enemy projectiles and co-ordinating tactics with teammates, an open-ended learningprocess.。The primary rationale behind choosing Contra asour testbed lies in its possession of proprietary knowledgenot encompassed in general knowledge repositories.. \n",
      "the piece 51\n",
      " \n",
      "sly merges the last-man-standing gameplaydynamics with the survival, exploration, and scavengingelements inherent in first-person shooting games (Gautamet al., 2021).。It unfolds with multiple hostile teams, neces-sitating players to collaborate with teammates, withstandadversaries, and strive to outlast others in the ever-changingarena.。The agent’s objectives encompass individual survivaland the elimination of encountered enemies.。An agent inContra mandates a sequential acquisition of skills, startingfrom fundamental abilities like running and item collec-tion.。As the learning proceeds, an agent must master moreintricate skills such as evading enemy projectiles and co-ordinating tactics with teammates, an open-ended learningprocess.。The primary rationale behind choosing Contra asour testbed lies in its possession of proprietary knowledgenot encompassed in general knowledge repositories.。Con-sequently, we employ Reinforcement Learning (RL) forknowledge discovery, and co-training to align the LanguageModel (LLM) and RL in comprehending the environment.4.. \n",
      "the piece 52\n",
      " \n",
      "ing players to collaborate with teammates, withstandadversaries, and strive to outlast others in the ever-changingarena.。The agent’s objectives encompass individual survivaland the elimination of encountered enemies.。An agent inContra mandates a sequential acquisition of skills, startingfrom fundamental abilities like running and item collec-tion.。As the learning proceeds, an agent must master moreintricate skills such as evading enemy projectiles and co-ordinating tactics with teammates, an open-ended learningprocess.。The primary rationale behind choosing Contra asour testbed lies in its possession of proprietary knowledgenot encompassed in general knowledge repositories.。Con-sequently, we employ Reinforcement Learning (RL) forknowledge discovery, and co-training to align the LanguageModel (LLM) and RL in comprehending the environment.4.。A Co-training Framework: OpenPALConsidering the training in the context of open-ended learn-ing is extensive data-thirsty, we first introduce two criticalengineering designs to enhance training efficiency.. \n",
      "the piece 53\n",
      " \n",
      "and the elimination of encountered enemies.。An agent inContra mandates a sequential acquisition of skills, startingfrom fundamental abilities like running and item collec-tion.。As the learning proceeds, an agent must master moreintricate skills such as evading enemy projectiles and co-ordinating tactics with teammates, an open-ended learningprocess.。The primary rationale behind choosing Contra asour testbed lies in its possession of proprietary knowledgenot encompassed in general knowledge repositories.。Con-sequently, we employ Reinforcement Learning (RL) forknowledge discovery, and co-training to align the LanguageModel (LLM) and RL in comprehending the environment.4.。A Co-training Framework: OpenPALConsidering the training in the context of open-ended learn-ing is extensive data-thirsty, we first introduce two criticalengineering designs to enhance training efficiency.。Specif-ically, OpenPAL incorporates a distributed RL frameworkinspired by AlphaStar (Vinyals et al., 2019) with modi-fications, resulting in the formation of the Actor-League-Learner architecture.. \n",
      "the piece 54\n",
      " \n",
      "f skills, startingfrom fundamental abilities like running and item collec-tion.。As the learning proceeds, an agent must master moreintricate skills such as evading enemy projectiles and co-ordinating tactics with teammates, an open-ended learningprocess.。The primary rationale behind choosing Contra asour testbed lies in its possession of proprietary knowledgenot encompassed in general knowledge repositories.。Con-sequently, we employ Reinforcement Learning (RL) forknowledge discovery, and co-training to align the LanguageModel (LLM) and RL in comprehending the environment.4.。A Co-training Framework: OpenPALConsidering the training in the context of open-ended learn-ing is extensive data-thirsty, we first introduce two criticalengineering designs to enhance training efficiency.。Specif-ically, OpenPAL incorporates a distributed RL frameworkinspired by AlphaStar (Vinyals et al., 2019) with modi-fications, resulting in the formation of the Actor-League-Learner architecture.。In this architecture, the League is re-sponsible for distributing rollout tasks to a cluster of Actors(CPU nodes) for data collection and evaluation, while opti-mization tasks are delegated to the Learner (GPU node) forpolicy updates.. \n",
      "the piece 55\n",
      " \n",
      "ricate skills such as evading enemy projectiles and co-ordinating tactics with teammates, an open-ended learningprocess.。The primary rationale behind choosing Contra asour testbed lies in its possession of proprietary knowledgenot encompassed in general knowledge repositories.。Con-sequently, we employ Reinforcement Learning (RL) forknowledge discovery, and co-training to align the LanguageModel (LLM) and RL in comprehending the environment.4.。A Co-training Framework: OpenPALConsidering the training in the context of open-ended learn-ing is extensive data-thirsty, we first introduce two criticalengineering designs to enhance training efficiency.。Specif-ically, OpenPAL incorporates a distributed RL frameworkinspired by AlphaStar (Vinyals et al., 2019) with modi-fications, resulting in the formation of the Actor-League-Learner architecture.。In this architecture, the League is re-sponsible for distributing rollout tasks to a cluster of Actors(CPU nodes) for data collection and evaluation, while opti-mization tasks are delegated to the Learner (GPU node) forpolicy updates.。This distributed approach significantly en-hances rollout throughput, thereby improving overall train-ing efficiency.. \n",
      "the piece 56\n",
      " \n",
      "bed lies in its possession of proprietary knowledgenot encompassed in general knowledge repositories.。Con-sequently, we employ Reinforcement Learning (RL) forknowledge discovery, and co-training to align the LanguageModel (LLM) and RL in comprehending the environment.4.。A Co-training Framework: OpenPALConsidering the training in the context of open-ended learn-ing is extensive data-thirsty, we first introduce two criticalengineering designs to enhance training efficiency.。Specif-ically, OpenPAL incorporates a distributed RL frameworkinspired by AlphaStar (Vinyals et al., 2019) with modi-fications, resulting in the formation of the Actor-League-Learner architecture.。In this architecture, the League is re-sponsible for distributing rollout tasks to a cluster of Actors(CPU nodes) for data collection and evaluation, while opti-mization tasks are delegated to the Learner (GPU node) forpolicy updates.。This distributed approach significantly en-hances rollout throughput, thereby improving overall train-ing efficiency.。Another efficiency challenge stems from theiterative development of Contra.. \n",
      "the piece 57\n",
      " \n",
      "knowledge discovery, and co-training to align the LanguageModel (LLM) and RL in comprehending the environment.4.。A Co-training Framework: OpenPALConsidering the training in the context of open-ended learn-ing is extensive data-thirsty, we first introduce two criticalengineering designs to enhance training efficiency.。Specif-ically, OpenPAL incorporates a distributed RL frameworkinspired by AlphaStar (Vinyals et al., 2019) with modi-fications, resulting in the formation of the Actor-League-Learner architecture.。In this architecture, the League is re-sponsible for distributing rollout tasks to a cluster of Actors(CPU nodes) for data collection and evaluation, while opti-mization tasks are delegated to the Learner (GPU node) forpolicy updates.。This distributed approach significantly en-hances rollout throughput, thereby improving overall train-ing efficiency.。Another efficiency challenge stems from theiterative development of Contra.。During the period of ourresearch, Contra’s environmental attributes continuouslychange as the programming development.. \n",
      "the piece 58\n",
      " \n",
      "in the context of open-ended learn-ing is extensive data-thirsty, we first introduce two criticalengineering designs to enhance training efficiency.。Specif-ically, OpenPAL incorporates a distributed RL frameworkinspired by AlphaStar (Vinyals et al., 2019) with modi-fications, resulting in the formation of the Actor-League-Learner architecture.。In this architecture, the League is re-sponsible for distributing rollout tasks to a cluster of Actors(CPU nodes) for data collection and evaluation, while opti-mization tasks are delegated to the Learner (GPU node) forpolicy updates.。This distributed approach significantly en-hances rollout throughput, thereby improving overall train-ing efficiency.。Another efficiency challenge stems from theiterative development of Contra.。During the period of ourresearch, Contra’s environmental attributes continuouslychange as the programming development.。Thus, policyretraining would be necessary if there is no explicit inter-vention.. \n",
      "the piece 59\n",
      " \n",
      "workinspired by AlphaStar (Vinyals et al., 2019) with modi-fications, resulting in the formation of the Actor-League-Learner architecture.。In this architecture, the League is re-sponsible for distributing rollout tasks to a cluster of Actors(CPU nodes) for data collection and evaluation, while opti-mization tasks are delegated to the Learner (GPU node) forpolicy updates.。This distributed approach significantly en-hances rollout throughput, thereby improving overall train-ing efficiency.。Another efficiency challenge stems from theiterative development of Contra.。During the period of ourresearch, Contra’s environmental attributes continuouslychange as the programming development.。Thus, policyretraining would be necessary if there is no explicit inter-vention.。To reduce such an extra computation burden, weemploy surgery (Berner et al., 2019) to retain learned skillsat the lowest training cost, enabling adaptation to a changingobservation/goal space while ensuring compatibility withnetwork inputs.. \n",
      "the piece 60\n",
      " \n",
      "buting rollout tasks to a cluster of Actors(CPU nodes) for data collection and evaluation, while opti-mization tasks are delegated to the Learner (GPU node) forpolicy updates.。This distributed approach significantly en-hances rollout throughput, thereby improving overall train-ing efficiency.。Another efficiency challenge stems from theiterative development of Contra.。During the period of ourresearch, Contra’s environmental attributes continuouslychange as the programming development.。Thus, policyretraining would be necessary if there is no explicit inter-vention.。To reduce such an extra computation burden, weemploy surgery (Berner et al., 2019) to retain learned skillsat the lowest training cost, enabling adaptation to a changingobservation/goal space while ensuring compatibility withnetwork inputs.。Detailed information on the distributed RLframework can be found in Appendix F, and version changesare listed in Table 8.. \n",
      "the piece 61\n",
      " \n",
      "roughput, thereby improving overall train-ing efficiency.。Another efficiency challenge stems from theiterative development of Contra.。During the period of ourresearch, Contra’s environmental attributes continuouslychange as the programming development.。Thus, policyretraining would be necessary if there is no explicit inter-vention.。To reduce such an extra computation burden, weemploy surgery (Berner et al., 2019) to retain learned skillsat the lowest training cost, enabling adaptation to a changingobservation/goal space while ensuring compatibility withnetwork inputs.。Detailed information on the distributed RLframework can be found in Appendix F, and version changesare listed in Table 8.。In the following content, we will in-troduce OpenPAL in two stages, including the independenttraining at stage I (Section 4.1 ∼4.3) and the co-training atstage II (Section 4.4).4.1.. \n",
      "the piece 62\n",
      " \n",
      "ent of Contra.。During the period of ourresearch, Contra’s environmental attributes continuouslychange as the programming development.。Thus, policyretraining would be necessary if there is no explicit inter-vention.。To reduce such an extra computation burden, weemploy surgery (Berner et al., 2019) to retain learned skillsat the lowest training cost, enabling adaptation to a changingobservation/goal space while ensuring compatibility withnetwork inputs.。Detailed information on the distributed RLframework can be found in Appendix F, and version changesare listed in Table 8.。In the following content, we will in-troduce OpenPAL in two stages, including the independenttraining at stage I (Section 4.1 ∼4.3) and the co-training atstage II (Section 4.4).4.1.。Exploring Basic Skills via Non-goal RLIn the realm of GCRL, the prevalent approach involvescurriculum learning a goal-conditioned policy from scratch,learning goal execution while exploring goals.. \n",
      "the piece 63\n",
      " \n",
      "butes continuouslychange as the programming development.。Thus, policyretraining would be necessary if there is no explicit inter-vention.。To reduce such an extra computation burden, weemploy surgery (Berner et al., 2019) to retain learned skillsat the lowest training cost, enabling adaptation to a changingobservation/goal space while ensuring compatibility withnetwork inputs.。Detailed information on the distributed RLframework can be found in Appendix F, and version changesare listed in Table 8.。In the following content, we will in-troduce OpenPAL in two stages, including the independenttraining at stage I (Section 4.1 ∼4.3) and the co-training atstage II (Section 4.4).4.1.。Exploring Basic Skills via Non-goal RLIn the realm of GCRL, the prevalent approach involvescurriculum learning a goal-conditioned policy from scratch,learning goal execution while exploring goals.。However,it maybe inefficient for an agent to explore the whole goalspace when there is a lack of prior knowledge of the goalspace.. \n",
      "the piece 64\n",
      " \n",
      "it inter-vention.。To reduce such an extra computation burden, weemploy surgery (Berner et al., 2019) to retain learned skillsat the lowest training cost, enabling adaptation to a changingobservation/goal space while ensuring compatibility withnetwork inputs.。Detailed information on the distributed RLframework can be found in Appendix F, and version changesare listed in Table 8.。In the following content, we will in-troduce OpenPAL in two stages, including the independenttraining at stage I (Section 4.1 ∼4.3) and the co-training atstage II (Section 4.4).4.1.。Exploring Basic Skills via Non-goal RLIn the realm of GCRL, the prevalent approach involvescurriculum learning a goal-conditioned policy from scratch,learning goal execution while exploring goals.。However,it maybe inefficient for an agent to explore the whole goalspace when there is a lack of prior knowledge of the goalspace.。Thus, we opt for leveraging non-goal RL for basicskill learning before goal-conditioned learning.. \n",
      "the piece 65\n",
      " \n",
      "rner et al., 2019) to retain learned skillsat the lowest training cost, enabling adaptation to a changingobservation/goal space while ensuring compatibility withnetwork inputs.。Detailed information on the distributed RLframework can be found in Appendix F, and version changesare listed in Table 8.。In the following content, we will in-troduce OpenPAL in two stages, including the independenttraining at stage I (Section 4.1 ∼4.3) and the co-training atstage II (Section 4.4).4.1.。Exploring Basic Skills via Non-goal RLIn the realm of GCRL, the prevalent approach involvescurriculum learning a goal-conditioned policy from scratch,learning goal execution while exploring goals.。However,it maybe inefficient for an agent to explore the whole goalspace when there is a lack of prior knowledge of the goalspace.。Thus, we opt for leveraging non-goal RL for basicskill learning before goal-conditioned learning.。For theimplementation, we employ Proximal Policy Optimization(PPO) (Schulman et al., 2017) with fine-grained rewardshaping asr(s, a) =λ1rb(s, a) +λ2roa(s, a), (2)where rbfor skill learning which focuses on targeting theagent towards wining and surviving as long as possible, is alinear combination of diverse behavior factors, roaencour-ages the agent to avoid obstacles due to the agent is notsensitive to obstacles when navigating, and λ1,λ2the fac-tors to weight the contribution of each item.. \n",
      "the piece 66\n",
      " \n",
      "in Appendix F, and version changesare listed in Table 8.。In the following content, we will in-troduce OpenPAL in two stages, including the independenttraining at stage I (Section 4.1 ∼4.3) and the co-training atstage II (Section 4.4).4.1.。Exploring Basic Skills via Non-goal RLIn the realm of GCRL, the prevalent approach involvescurriculum learning a goal-conditioned policy from scratch,learning goal execution while exploring goals.。However,it maybe inefficient for an agent to explore the whole goalspace when there is a lack of prior knowledge of the goalspace.。Thus, we opt for leveraging non-goal RL for basicskill learning before goal-conditioned learning.。For theimplementation, we employ Proximal Policy Optimization(PPO) (Schulman et al., 2017) with fine-grained rewardshaping asr(s, a) =λ1rb(s, a) +λ2roa(s, a), (2)where rbfor skill learning which focuses on targeting theagent towards wining and surviving as long as possible, is alinear combination of diverse behavior factors, roaencour-ages the agent to avoid obstacles due to the agent is notsensitive to obstacles when navigating, and λ1,λ2the fac-tors to weight the contribution of each item.。The details ofthe reward construction are included in Table 5, Appendix A.Then, the value function for estimatingPTl=tγl−tr(sl, al)is implemented as a multi-head network and shares the back-bone of policy, i.e., V(st) =λ1Vb(st)+λ2Voa(st), whereVb(st)andVoa(st)approximatePTl=tγl−trb(sl, al)andPTl=tγl−troa(sl, al), respectively.4.2.. \n",
      "the piece 67\n",
      " \n",
      ", including the independenttraining at stage I (Section 4.1 ∼4.3) and the co-training atstage II (Section 4.4).4.1.。Exploring Basic Skills via Non-goal RLIn the realm of GCRL, the prevalent approach involvescurriculum learning a goal-conditioned policy from scratch,learning goal execution while exploring goals.。However,it maybe inefficient for an agent to explore the whole goalspace when there is a lack of prior knowledge of the goalspace.。Thus, we opt for leveraging non-goal RL for basicskill learning before goal-conditioned learning.。For theimplementation, we employ Proximal Policy Optimization(PPO) (Schulman et al., 2017) with fine-grained rewardshaping asr(s, a) =λ1rb(s, a) +λ2roa(s, a), (2)where rbfor skill learning which focuses on targeting theagent towards wining and surviving as long as possible, is alinear combination of diverse behavior factors, roaencour-ages the agent to avoid obstacles due to the agent is notsensitive to obstacles when navigating, and λ1,λ2the fac-tors to weight the contribution of each item.。The details ofthe reward construction are included in Table 5, Appendix A.Then, the value function for estimatingPTl=tγl−tr(sl, al)is implemented as a multi-head network and shares the back-bone of policy, i.e., V(st) =λ1Vb(st)+λ2Voa(st), whereVb(st)andVoa(st)approximatePTl=tγl−trb(sl, al)andPTl=tγl−troa(sl, al), respectively.4.2.。Learning a Goal-conditioned PolicyWe construct the goal space using various state attributeswhich can be determined and effected by interaction.. \n",
      "the piece 68\n",
      " \n",
      "valent approach involvescurriculum learning a goal-conditioned policy from scratch,learning goal execution while exploring goals.。However,it maybe inefficient for an agent to explore the whole goalspace when there is a lack of prior knowledge of the goalspace.。Thus, we opt for leveraging non-goal RL for basicskill learning before goal-conditioned learning.。For theimplementation, we employ Proximal Policy Optimization(PPO) (Schulman et al., 2017) with fine-grained rewardshaping asr(s, a) =λ1rb(s, a) +λ2roa(s, a), (2)where rbfor skill learning which focuses on targeting theagent towards wining and surviving as long as possible, is alinear combination of diverse behavior factors, roaencour-ages the agent to avoid obstacles due to the agent is notsensitive to obstacles when navigating, and λ1,λ2the fac-tors to weight the contribution of each item.。The details ofthe reward construction are included in Table 5, Appendix A.Then, the value function for estimatingPTl=tγl−tr(sl, al)is implemented as a multi-head network and shares the back-bone of policy, i.e., V(st) =λ1Vb(st)+λ2Voa(st), whereVb(st)andVoa(st)approximatePTl=tγl−trb(sl, al)andPTl=tγl−troa(sl, al), respectively.4.2.。Learning a Goal-conditioned PolicyWe construct the goal space using various state attributeswhich can be determined and effected by interaction.。Indetail, they are (1) agent private states that can be directlychanged by the agent or other players, such as firing, walk-ing, etc.. \n",
      "the piece 69\n",
      " \n",
      "pace when there is a lack of prior knowledge of the goalspace.。Thus, we opt for leveraging non-goal RL for basicskill learning before goal-conditioned learning.。For theimplementation, we employ Proximal Policy Optimization(PPO) (Schulman et al., 2017) with fine-grained rewardshaping asr(s, a) =λ1rb(s, a) +λ2roa(s, a), (2)where rbfor skill learning which focuses on targeting theagent towards wining and surviving as long as possible, is alinear combination of diverse behavior factors, roaencour-ages the agent to avoid obstacles due to the agent is notsensitive to obstacles when navigating, and λ1,λ2the fac-tors to weight the contribution of each item.。The details ofthe reward construction are included in Table 5, Appendix A.Then, the value function for estimatingPTl=tγl−tr(sl, al)is implemented as a multi-head network and shares the back-bone of policy, i.e., V(st) =λ1Vb(st)+λ2Voa(st), whereVb(st)andVoa(st)approximatePTl=tγl−trb(sl, al)andPTl=tγl−troa(sl, al), respectively.4.2.。Learning a Goal-conditioned PolicyWe construct the goal space using various state attributeswhich can be determined and effected by interaction.。Indetail, they are (1) agent private states that can be directlychanged by the agent or other players, such as firing, walk-ing, etc.。(2) enemies states that can be effected throughthe agent interactions, such as knock down an enemies; andBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(3)teammates states that can be effected by the interactionbetween the agent and its teammates.. \n",
      "the piece 70\n",
      " \n",
      "e goal-conditioned learning.。For theimplementation, we employ Proximal Policy Optimization(PPO) (Schulman et al., 2017) with fine-grained rewardshaping asr(s, a) =λ1rb(s, a) +λ2roa(s, a), (2)where rbfor skill learning which focuses on targeting theagent towards wining and surviving as long as possible, is alinear combination of diverse behavior factors, roaencour-ages the agent to avoid obstacles due to the agent is notsensitive to obstacles when navigating, and λ1,λ2the fac-tors to weight the contribution of each item.。The details ofthe reward construction are included in Table 5, Appendix A.Then, the value function for estimatingPTl=tγl−tr(sl, al)is implemented as a multi-head network and shares the back-bone of policy, i.e., V(st) =λ1Vb(st)+λ2Voa(st), whereVb(st)andVoa(st)approximatePTl=tγl−trb(sl, al)andPTl=tγl−troa(sl, al), respectively.4.2.。Learning a Goal-conditioned PolicyWe construct the goal space using various state attributeswhich can be determined and effected by interaction.。Indetail, they are (1) agent private states that can be directlychanged by the agent or other players, such as firing, walk-ing, etc.。(2) enemies states that can be effected throughthe agent interactions, such as knock down an enemies; andBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(3)teammates states that can be effected by the interactionbetween the agent and its teammates.。We summarize themin Table 4.. \n",
      "the piece 71\n",
      " \n",
      "hulman et al., 2017) with fine-grained rewardshaping asr(s, a) =λ1rb(s, a) +λ2roa(s, a), (2)where rbfor skill learning which focuses on targeting theagent towards wining and surviving as long as possible, is alinear combination of diverse behavior factors, roaencour-ages the agent to avoid obstacles due to the agent is notsensitive to obstacles when navigating, and λ1,λ2the fac-tors to weight the contribution of each item.。The details ofthe reward construction are included in Table 5, Appendix A.Then, the value function for estimatingPTl=tγl−tr(sl, al)is implemented as a multi-head network and shares the back-bone of policy, i.e., V(st) =λ1Vb(st)+λ2Voa(st), whereVb(st)andVoa(st)approximatePTl=tγl−trb(sl, al)andPTl=tγl−troa(sl, al), respectively.4.2.。Learning a Goal-conditioned PolicyWe construct the goal space using various state attributeswhich can be determined and effected by interaction.。Indetail, they are (1) agent private states that can be directlychanged by the agent or other players, such as firing, walk-ing, etc.。(2) enemies states that can be effected throughthe agent interactions, such as knock down an enemies; andBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(3)teammates states that can be effected by the interactionbetween the agent and its teammates.。We summarize themin Table 4.。With the above consideration, we further modeleach attribute as a sub-goal space Giwith multiple candi-dates that can be expressed as a set of normalized indices{j|Gi||j= 0, .. \n",
      "the piece 72\n",
      " \n",
      " A.Then, the value function for estimatingPTl=tγl−tr(sl, al)is implemented as a multi-head network and shares the back-bone of policy, i.e., V(st) =λ1Vb(st)+λ2Voa(st), whereVb(st)andVoa(st)approximatePTl=tγl−trb(sl, al)andPTl=tγl−troa(sl, al), respectively.4.2.。Learning a Goal-conditioned PolicyWe construct the goal space using various state attributeswhich can be determined and effected by interaction.。Indetail, they are (1) agent private states that can be directlychanged by the agent or other players, such as firing, walk-ing, etc.。(2) enemies states that can be effected throughthe agent interactions, such as knock down an enemies; andBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(3)teammates states that can be effected by the interactionbetween the agent and its teammates.。We summarize themin Table 4.。With the above consideration, we further modeleach attribute as a sub-goal space Giwith multiple candi-dates that can be expressed as a set of normalized indices{j|Gi||j= 0, .。.. \n",
      "the piece 73\n",
      " \n",
      "ous state attributeswhich can be determined and effected by interaction.。Indetail, they are (1) agent private states that can be directlychanged by the agent or other players, such as firing, walk-ing, etc.。(2) enemies states that can be effected throughthe agent interactions, such as knock down an enemies; andBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(3)teammates states that can be effected by the interactionbetween the agent and its teammates.。We summarize themin Table 4.。With the above consideration, we further modeleach attribute as a sub-goal space Giwith multiple candi-dates that can be expressed as a set of normalized indices{j|Gi||j= 0, .。.。.. \n",
      "the piece 74\n",
      " \n",
      "y the agent or other players, such as firing, walk-ing, etc.。(2) enemies states that can be effected throughthe agent interactions, such as knock down an enemies; andBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(3)teammates states that can be effected by the interactionbetween the agent and its teammates.。We summarize themin Table 4.。With the above consideration, we further modeleach attribute as a sub-goal space Giwith multiple candi-dates that can be expressed as a set of normalized indices{j|Gi||j= 0, .。.。.。,|Gi|}, where 0 an invalid attribute valueindicates the corresponding attribute not be selected.. \n",
      "the piece 75\n",
      " \n",
      "h as knock down an enemies; andBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(3)teammates states that can be effected by the interactionbetween the agent and its teammates.。We summarize themin Table 4.。With the above consideration, we further modeleach attribute as a sub-goal space Giwith multiple candi-dates that can be expressed as a set of normalized indices{j|Gi||j= 0, .。.。.。,|Gi|}, where 0 an invalid attribute valueindicates the corresponding attribute not be selected.。Forthe goal space, there are 68 sub-goal spaces that shape it asG= Π68i=1Gi.. \n",
      "the piece 76\n",
      " \n",
      "each attribute as a sub-goal space Giwith multiple candi-dates that can be expressed as a set of normalized indices{j|Gi||j= 0, .。.。.。,|Gi|}, where 0 an invalid attribute valueindicates the corresponding attribute not be selected.。Forthe goal space, there are 68 sub-goal spaces that shape it asG= Π68i=1Gi.。Obviously, gwhich comprises of more validsub-goals, the more difficult to complete for the policy.Open-ended Goal Generation.。Among existingGCRL research, hindsight goal relabelling and genera-tion (Andrychowicz et al., 2017; Ren et al., 2019) areeffective goal-conditioned learning methods that advantagefrom a free of goal prior, compared to explicit curriculum.However, there is a limitation of in-distribution goalexploration (Bai et al., 2019), i.e., policy learning andgoal exploration shares the same training dataset, whichis inefficient in exploration as the range of goals arelimited by the scale of samples.. \n",
      "the piece 77\n",
      " \n",
      "pace Giwith multiple candi-dates that can be expressed as a set of normalized indices{j|Gi||j= 0, .。.。.。,|Gi|}, where 0 an invalid attribute valueindicates the corresponding attribute not be selected.。Forthe goal space, there are 68 sub-goal spaces that shape it asG= Π68i=1Gi.。Obviously, gwhich comprises of more validsub-goals, the more difficult to complete for the policy.Open-ended Goal Generation.。Among existingGCRL research, hindsight goal relabelling and genera-tion (Andrychowicz et al., 2017; Ren et al., 2019) areeffective goal-conditioned learning methods that advantagefrom a free of goal prior, compared to explicit curriculum.However, there is a limitation of in-distribution goalexploration (Bai et al., 2019), i.e., policy learning andgoal exploration shares the same training dataset, whichis inefficient in exploration as the range of goals arelimited by the scale of samples.。Comparatively, if wecan model the goal distribution, we can not only achievedata efficiency akin to that of hindsight goal generation,but also progressively attain an open-ended goal spaceby adjusting the goal distribution.. \n",
      "the piece 78\n",
      " \n",
      "ribute not be selected.。Forthe goal space, there are 68 sub-goal spaces that shape it asG= Π68i=1Gi.。Obviously, gwhich comprises of more validsub-goals, the more difficult to complete for the policy.Open-ended Goal Generation.。Among existingGCRL research, hindsight goal relabelling and genera-tion (Andrychowicz et al., 2017; Ren et al., 2019) areeffective goal-conditioned learning methods that advantagefrom a free of goal prior, compared to explicit curriculum.However, there is a limitation of in-distribution goalexploration (Bai et al., 2019), i.e., policy learning andgoal exploration shares the same training dataset, whichis inefficient in exploration as the range of goals arelimited by the scale of samples.。Comparatively, if wecan model the goal distribution, we can not only achievedata efficiency akin to that of hindsight goal generation,but also progressively attain an open-ended goal spaceby adjusting the goal distribution.。Therefore, we train aneural-based goal generator Gopover a dataset of trajectorysegments Dτ={τ}explored by the well-trained policy π⋆from Section 4.1 as it is trained for exploring basic skills.We assume that a goal gcorresponding to a given initialstatescan be represented by a 3-tuple x= (s,∆t,∆V),where ∆tthe time slot required to achieve gstarting froms, and ∆Va vector of state values from stogwith theconsideration of representation.. \n",
      "the piece 79\n",
      " \n",
      "ute not be selected.。Forthe goal space, there are 68 sub-goal spaces that shape it asG= Π68i=1Gi.。Obviously, gwhich comprises of more validsub-goals, the more difficult to complete for the policy.Open-ended Goal Generation.。Among existingGCRL research, hindsight goal relabelling and genera-tion (Andrychowicz et al., 2017; Ren et al., 2019) areeffective goal-conditioned learning methods that advantagefrom a free of goal prior, compared to explicit curriculum.However, there is a limitation of in-distribution goalexploration (Bai et al., 2019), i.e., policy learning andgoal exploration shares the same training dataset, whichis inefficient in exploration as the range of goals arelimited by the scale of samples.。Comparatively, if wecan model the goal distribution, we can not only achievedata efficiency akin to that of hindsight goal generation,but also progressively attain an open-ended goal spaceby adjusting the goal distribution.。Therefore, we train aneural-based goal generator Gopover a dataset of trajectorysegments Dτ={τ}explored by the well-trained policy π⋆from Section 4.1 as it is trained for exploring basic skills.We assume that a goal gcorresponding to a given initialstatescan be represented by a 3-tuple x= (s,∆t,∆V),where ∆tthe time slot required to achieve gstarting froms, and ∆Va vector of state values from stogwith theconsideration of representation.。As a result, we train thegoal generator Gopto take input in the form of x, therebyallowing variations in ∆tand∆Vto yield different goalsfor a given state s. For an implementation, we firstlyconstruct a dataset Dx,g={(x, g)}fromDτ, where eachitem in Dx,gsatisfies:s∼UNIFORM (τ:150), (3)g=PROJ(s′), s′∼P(s′) =Vb(s′|s′∈τ−20:)Ps′∈τ−20:Vb(s′),∆t=the time slot from stos′,∆V=\u0002Vb(s), Vb(s′)\u0003.τ:150 the first 150 states of τ,τ−20:the last 20states.. \n",
      "the piece 80\n",
      " \n",
      " not be selected.。Forthe goal space, there are 68 sub-goal spaces that shape it asG= Π68i=1Gi.。Obviously, gwhich comprises of more validsub-goals, the more difficult to complete for the policy.Open-ended Goal Generation.。Among existingGCRL research, hindsight goal relabelling and genera-tion (Andrychowicz et al., 2017; Ren et al., 2019) areeffective goal-conditioned learning methods that advantagefrom a free of goal prior, compared to explicit curriculum.However, there is a limitation of in-distribution goalexploration (Bai et al., 2019), i.e., policy learning andgoal exploration shares the same training dataset, whichis inefficient in exploration as the range of goals arelimited by the scale of samples.。Comparatively, if wecan model the goal distribution, we can not only achievedata efficiency akin to that of hindsight goal generation,but also progressively attain an open-ended goal spaceby adjusting the goal distribution.。Therefore, we train aneural-based goal generator Gopover a dataset of trajectorysegments Dτ={τ}explored by the well-trained policy π⋆from Section 4.1 as it is trained for exploring basic skills.We assume that a goal gcorresponding to a given initialstatescan be represented by a 3-tuple x= (s,∆t,∆V),where ∆tthe time slot required to achieve gstarting froms, and ∆Va vector of state values from stogwith theconsideration of representation.。As a result, we train thegoal generator Gopto take input in the form of x, therebyallowing variations in ∆tand∆Vto yield different goalsfor a given state s. For an implementation, we firstlyconstruct a dataset Dx,g={(x, g)}fromDτ, where eachitem in Dx,gsatisfies:s∼UNIFORM (τ:150), (3)g=PROJ(s′), s′∼P(s′) =Vb(s′|s′∈τ−20:)Ps′∈τ−20:Vb(s′),∆t=the time slot from stos′,∆V=\u0002Vb(s), Vb(s′)\u0003.τ:150 the first 150 states of τ,τ−20:the last 20states.。Then we train Gopwith a MSE loss asminGopEDx,g[∥Gop(x)−g∥2].. \n",
      "the piece 81\n",
      " \n",
      "iously, gwhich comprises of more validsub-goals, the more difficult to complete for the policy.Open-ended Goal Generation.。Among existingGCRL research, hindsight goal relabelling and genera-tion (Andrychowicz et al., 2017; Ren et al., 2019) areeffective goal-conditioned learning methods that advantagefrom a free of goal prior, compared to explicit curriculum.However, there is a limitation of in-distribution goalexploration (Bai et al., 2019), i.e., policy learning andgoal exploration shares the same training dataset, whichis inefficient in exploration as the range of goals arelimited by the scale of samples.。Comparatively, if wecan model the goal distribution, we can not only achievedata efficiency akin to that of hindsight goal generation,but also progressively attain an open-ended goal spaceby adjusting the goal distribution.。Therefore, we train aneural-based goal generator Gopover a dataset of trajectorysegments Dτ={τ}explored by the well-trained policy π⋆from Section 4.1 as it is trained for exploring basic skills.We assume that a goal gcorresponding to a given initialstatescan be represented by a 3-tuple x= (s,∆t,∆V),where ∆tthe time slot required to achieve gstarting froms, and ∆Va vector of state values from stogwith theconsideration of representation.。As a result, we train thegoal generator Gopto take input in the form of x, therebyallowing variations in ∆tand∆Vto yield different goalsfor a given state s. For an implementation, we firstlyconstruct a dataset Dx,g={(x, g)}fromDτ, where eachitem in Dx,gsatisfies:s∼UNIFORM (τ:150), (3)g=PROJ(s′), s′∼P(s′) =Vb(s′|s′∈τ−20:)Ps′∈τ−20:Vb(s′),∆t=the time slot from stos′,∆V=\u0002Vb(s), Vb(s′)\u0003.τ:150 the first 150 states of τ,τ−20:the last 20states.。Then we train Gopwith a MSE loss asminGopEDx,g[∥Gop(x)−g∥2].。While varying ∆tand∆Vproduces diverse goals, it remains challenging to compre-hensively cover the entire goal space corresponding toa given state.. \n",
      "the piece 82\n",
      " \n",
      "e for the policy.Open-ended Goal Generation.。Among existingGCRL research, hindsight goal relabelling and genera-tion (Andrychowicz et al., 2017; Ren et al., 2019) areeffective goal-conditioned learning methods that advantagefrom a free of goal prior, compared to explicit curriculum.However, there is a limitation of in-distribution goalexploration (Bai et al., 2019), i.e., policy learning andgoal exploration shares the same training dataset, whichis inefficient in exploration as the range of goals arelimited by the scale of samples.。Comparatively, if wecan model the goal distribution, we can not only achievedata efficiency akin to that of hindsight goal generation,but also progressively attain an open-ended goal spaceby adjusting the goal distribution.。Therefore, we train aneural-based goal generator Gopover a dataset of trajectorysegments Dτ={τ}explored by the well-trained policy π⋆from Section 4.1 as it is trained for exploring basic skills.We assume that a goal gcorresponding to a given initialstatescan be represented by a 3-tuple x= (s,∆t,∆V),where ∆tthe time slot required to achieve gstarting froms, and ∆Va vector of state values from stogwith theconsideration of representation.。As a result, we train thegoal generator Gopto take input in the form of x, therebyallowing variations in ∆tand∆Vto yield different goalsfor a given state s. For an implementation, we firstlyconstruct a dataset Dx,g={(x, g)}fromDτ, where eachitem in Dx,gsatisfies:s∼UNIFORM (τ:150), (3)g=PROJ(s′), s′∼P(s′) =Vb(s′|s′∈τ−20:)Ps′∈τ−20:Vb(s′),∆t=the time slot from stos′,∆V=\u0002Vb(s), Vb(s′)\u0003.τ:150 the first 150 states of τ,τ−20:the last 20states.。Then we train Gopwith a MSE loss asminGopEDx,g[∥Gop(x)−g∥2].。While varying ∆tand∆Vproduces diverse goals, it remains challenging to compre-hensively cover the entire goal space corresponding toa given state.。As a supplement, we propose integratinggoal generation with a uniform sampler, denoted as Grnd,which randomly samples goals from the goal space Gus-ingGrnd= Π68i=1UNIFORM (Gi).. \n",
      "the piece 83\n",
      " \n",
      "icz et al., 2017; Ren et al., 2019) areeffective goal-conditioned learning methods that advantagefrom a free of goal prior, compared to explicit curriculum.However, there is a limitation of in-distribution goalexploration (Bai et al., 2019), i.e., policy learning andgoal exploration shares the same training dataset, whichis inefficient in exploration as the range of goals arelimited by the scale of samples.。Comparatively, if wecan model the goal distribution, we can not only achievedata efficiency akin to that of hindsight goal generation,but also progressively attain an open-ended goal spaceby adjusting the goal distribution.。Therefore, we train aneural-based goal generator Gopover a dataset of trajectorysegments Dτ={τ}explored by the well-trained policy π⋆from Section 4.1 as it is trained for exploring basic skills.We assume that a goal gcorresponding to a given initialstatescan be represented by a 3-tuple x= (s,∆t,∆V),where ∆tthe time slot required to achieve gstarting froms, and ∆Va vector of state values from stogwith theconsideration of representation.。As a result, we train thegoal generator Gopto take input in the form of x, therebyallowing variations in ∆tand∆Vto yield different goalsfor a given state s. For an implementation, we firstlyconstruct a dataset Dx,g={(x, g)}fromDτ, where eachitem in Dx,gsatisfies:s∼UNIFORM (τ:150), (3)g=PROJ(s′), s′∼P(s′) =Vb(s′|s′∈τ−20:)Ps′∈τ−20:Vb(s′),∆t=the time slot from stos′,∆V=\u0002Vb(s), Vb(s′)\u0003.τ:150 the first 150 states of τ,τ−20:the last 20states.。Then we train Gopwith a MSE loss asminGopEDx,g[∥Gop(x)−g∥2].。While varying ∆tand∆Vproduces diverse goals, it remains challenging to compre-hensively cover the entire goal space corresponding toa given state.。As a supplement, we propose integratinggoal generation with a uniform sampler, denoted as Grnd,which randomly samples goals from the goal space Gus-ingGrnd= Π68i=1UNIFORM (Gi).。This results in a goalgeneration g∼Grnd∪Gop.Intrinsic Reward Shaping.. \n",
      "the piece 84\n",
      " \n",
      "ficiency akin to that of hindsight goal generation,but also progressively attain an open-ended goal spaceby adjusting the goal distribution.。Therefore, we train aneural-based goal generator Gopover a dataset of trajectorysegments Dτ={τ}explored by the well-trained policy π⋆from Section 4.1 as it is trained for exploring basic skills.We assume that a goal gcorresponding to a given initialstatescan be represented by a 3-tuple x= (s,∆t,∆V),where ∆tthe time slot required to achieve gstarting froms, and ∆Va vector of state values from stogwith theconsideration of representation.。As a result, we train thegoal generator Gopto take input in the form of x, therebyallowing variations in ∆tand∆Vto yield different goalsfor a given state s. For an implementation, we firstlyconstruct a dataset Dx,g={(x, g)}fromDτ, where eachitem in Dx,gsatisfies:s∼UNIFORM (τ:150), (3)g=PROJ(s′), s′∼P(s′) =Vb(s′|s′∈τ−20:)Ps′∈τ−20:Vb(s′),∆t=the time slot from stos′,∆V=\u0002Vb(s), Vb(s′)\u0003.τ:150 the first 150 states of τ,τ−20:the last 20states.。Then we train Gopwith a MSE loss asminGopEDx,g[∥Gop(x)−g∥2].。While varying ∆tand∆Vproduces diverse goals, it remains challenging to compre-hensively cover the entire goal space corresponding toa given state.。As a supplement, we propose integratinggoal generation with a uniform sampler, denoted as Grnd,which randomly samples goals from the goal space Gus-ingGrnd= Π68i=1UNIFORM (Gi).。This results in a goalgeneration g∼Grnd∪Gop.Intrinsic Reward Shaping.。As introduced in the afore-mentioned, a critical challenge hinders the goal comple-tion is sparse rewarding.. \n",
      "the piece 85\n",
      " \n",
      "ents Dτ={τ}explored by the well-trained policy π⋆from Section 4.1 as it is trained for exploring basic skills.We assume that a goal gcorresponding to a given initialstatescan be represented by a 3-tuple x= (s,∆t,∆V),where ∆tthe time slot required to achieve gstarting froms, and ∆Va vector of state values from stogwith theconsideration of representation.。As a result, we train thegoal generator Gopto take input in the form of x, therebyallowing variations in ∆tand∆Vto yield different goalsfor a given state s. For an implementation, we firstlyconstruct a dataset Dx,g={(x, g)}fromDτ, where eachitem in Dx,gsatisfies:s∼UNIFORM (τ:150), (3)g=PROJ(s′), s′∼P(s′) =Vb(s′|s′∈τ−20:)Ps′∈τ−20:Vb(s′),∆t=the time slot from stos′,∆V=\u0002Vb(s), Vb(s′)\u0003.τ:150 the first 150 states of τ,τ−20:the last 20states.。Then we train Gopwith a MSE loss asminGopEDx,g[∥Gop(x)−g∥2].。While varying ∆tand∆Vproduces diverse goals, it remains challenging to compre-hensively cover the entire goal space corresponding toa given state.。As a supplement, we propose integratinggoal generation with a uniform sampler, denoted as Grnd,which randomly samples goals from the goal space Gus-ingGrnd= Π68i=1UNIFORM (Gi).。This results in a goalgeneration g∼Grnd∪Gop.Intrinsic Reward Shaping.。As introduced in the afore-mentioned, a critical challenge hinders the goal comple-tion is sparse rewarding.。To mitigate this, we extend thereward function in Equation (2) with an intrinsic rewardrg(st, at, g)that evaluates the degree of goal completion.Equation (4) shows the calculation of rg(st, at, g)as theEuclidean norm difference between two consecutive statesand a goal asrg(st, at, g) =∥g−PROJ(st−1)∥p−∥g−PROJ(st)∥p,(4)where ∥ · ∥pindicates the p-norm.. \n",
      "the piece 86\n",
      " \n",
      "owing variations in ∆tand∆Vto yield different goalsfor a given state s. For an implementation, we firstlyconstruct a dataset Dx,g={(x, g)}fromDτ, where eachitem in Dx,gsatisfies:s∼UNIFORM (τ:150), (3)g=PROJ(s′), s′∼P(s′) =Vb(s′|s′∈τ−20:)Ps′∈τ−20:Vb(s′),∆t=the time slot from stos′,∆V=\u0002Vb(s), Vb(s′)\u0003.τ:150 the first 150 states of τ,τ−20:the last 20states.。Then we train Gopwith a MSE loss asminGopEDx,g[∥Gop(x)−g∥2].。While varying ∆tand∆Vproduces diverse goals, it remains challenging to compre-hensively cover the entire goal space corresponding toa given state.。As a supplement, we propose integratinggoal generation with a uniform sampler, denoted as Grnd,which randomly samples goals from the goal space Gus-ingGrnd= Π68i=1UNIFORM (Gi).。This results in a goalgeneration g∼Grnd∪Gop.Intrinsic Reward Shaping.。As introduced in the afore-mentioned, a critical challenge hinders the goal comple-tion is sparse rewarding.。To mitigate this, we extend thereward function in Equation (2) with an intrinsic rewardrg(st, at, g)that evaluates the degree of goal completion.Equation (4) shows the calculation of rg(st, at, g)as theEuclidean norm difference between two consecutive statesand a goal asrg(st, at, g) =∥g−PROJ(st−1)∥p−∥g−PROJ(st)∥p,(4)where ∥ · ∥pindicates the p-norm.。This reward provides adenser reward signal at each time step to the agent about itsproximity to the goal, offering more nuanced informationthan a binary signal indicating whether it has reached thegoal or not.. \n",
      "the piece 87\n",
      " \n",
      "uces diverse goals, it remains challenging to compre-hensively cover the entire goal space corresponding toa given state.。As a supplement, we propose integratinggoal generation with a uniform sampler, denoted as Grnd,which randomly samples goals from the goal space Gus-ingGrnd= Π68i=1UNIFORM (Gi).。This results in a goalgeneration g∼Grnd∪Gop.Intrinsic Reward Shaping.。As introduced in the afore-mentioned, a critical challenge hinders the goal comple-tion is sparse rewarding.。To mitigate this, we extend thereward function in Equation (2) with an intrinsic rewardrg(st, at, g)that evaluates the degree of goal completion.Equation (4) shows the calculation of rg(st, at, g)as theEuclidean norm difference between two consecutive statesand a goal asrg(st, at, g) =∥g−PROJ(st−1)∥p−∥g−PROJ(st)∥p,(4)where ∥ · ∥pindicates the p-norm.。This reward provides adenser reward signal at each time step to the agent about itsproximity to the goal, offering more nuanced informationthan a binary signal indicating whether it has reached thegoal or not.。In our current implementation, we set p= 1.Thus, the reward function for GCRL is formulated asr(s, a, g ) =r(s, a) +λ3rg(s, a, g ), (5)where r(s, a)comes from Equation (2).. \n",
      "the piece 88\n",
      " \n",
      " cover the entire goal space corresponding toa given state.。As a supplement, we propose integratinggoal generation with a uniform sampler, denoted as Grnd,which randomly samples goals from the goal space Gus-ingGrnd= Π68i=1UNIFORM (Gi).。This results in a goalgeneration g∼Grnd∪Gop.Intrinsic Reward Shaping.。As introduced in the afore-mentioned, a critical challenge hinders the goal comple-tion is sparse rewarding.。To mitigate this, we extend thereward function in Equation (2) with an intrinsic rewardrg(st, at, g)that evaluates the degree of goal completion.Equation (4) shows the calculation of rg(st, at, g)as theEuclidean norm difference between two consecutive statesand a goal asrg(st, at, g) =∥g−PROJ(st−1)∥p−∥g−PROJ(st)∥p,(4)where ∥ · ∥pindicates the p-norm.。This reward provides adenser reward signal at each time step to the agent about itsproximity to the goal, offering more nuanced informationthan a binary signal indicating whether it has reached thegoal or not.。In our current implementation, we set p= 1.Thus, the reward function for GCRL is formulated asr(s, a, g ) =r(s, a) +λ3rg(s, a, g ), (5)where r(s, a)comes from Equation (2).。And for thevalue function corresponds to Equation (5), we extendthe multi-head V(st)with a new value head Vg(st, g)asV(st, g) =V(s) +λ3Vg(st, g), where Vg(st, g)approxi-matesPTi=tγi−trg(st, at, g).Avoiding Policy Degeneration.. \n",
      "the piece 89\n",
      " \n",
      "s Grnd,which randomly samples goals from the goal space Gus-ingGrnd= Π68i=1UNIFORM (Gi).。This results in a goalgeneration g∼Grnd∪Gop.Intrinsic Reward Shaping.。As introduced in the afore-mentioned, a critical challenge hinders the goal comple-tion is sparse rewarding.。To mitigate this, we extend thereward function in Equation (2) with an intrinsic rewardrg(st, at, g)that evaluates the degree of goal completion.Equation (4) shows the calculation of rg(st, at, g)as theEuclidean norm difference between two consecutive statesand a goal asrg(st, at, g) =∥g−PROJ(st−1)∥p−∥g−PROJ(st)∥p,(4)where ∥ · ∥pindicates the p-norm.。This reward provides adenser reward signal at each time step to the agent about itsproximity to the goal, offering more nuanced informationthan a binary signal indicating whether it has reached thegoal or not.。In our current implementation, we set p= 1.Thus, the reward function for GCRL is formulated asr(s, a, g ) =r(s, a) +λ3rg(s, a, g ), (5)where r(s, a)comes from Equation (2).。And for thevalue function corresponds to Equation (5), we extendthe multi-head V(st)with a new value head Vg(st, g)asV(st, g) =V(s) +λ3Vg(st, g), where Vg(st, g)approxi-matesPTi=tγi−trg(st, at, g).Avoiding Policy Degeneration.。Letπ⋆denote the well-trained policy from the non-goal reinforcement learningstep.. \n",
      "the piece 90\n",
      " \n",
      "e afore-mentioned, a critical challenge hinders the goal comple-tion is sparse rewarding.。To mitigate this, we extend thereward function in Equation (2) with an intrinsic rewardrg(st, at, g)that evaluates the degree of goal completion.Equation (4) shows the calculation of rg(st, at, g)as theEuclidean norm difference between two consecutive statesand a goal asrg(st, at, g) =∥g−PROJ(st−1)∥p−∥g−PROJ(st)∥p,(4)where ∥ · ∥pindicates the p-norm.。This reward provides adenser reward signal at each time step to the agent about itsproximity to the goal, offering more nuanced informationthan a binary signal indicating whether it has reached thegoal or not.。In our current implementation, we set p= 1.Thus, the reward function for GCRL is formulated asr(s, a, g ) =r(s, a) +λ3rg(s, a, g ), (5)where r(s, a)comes from Equation (2).。And for thevalue function corresponds to Equation (5), we extendthe multi-head V(st)with a new value head Vg(st, g)asV(st, g) =V(s) +λ3Vg(st, g), where Vg(st, g)approxi-matesPTi=tγi−trg(st, at, g).Avoiding Policy Degeneration.。Letπ⋆denote the well-trained policy from the non-goal reinforcement learningstep.。However, we have observed a performance degenera-tion on basic skill execution when continuing the trainingof the goal-conditioned policy πg,θstarting from π⋆.. \n",
      "the piece 91\n",
      " \n",
      " sparse rewarding.。To mitigate this, we extend thereward function in Equation (2) with an intrinsic rewardrg(st, at, g)that evaluates the degree of goal completion.Equation (4) shows the calculation of rg(st, at, g)as theEuclidean norm difference between two consecutive statesand a goal asrg(st, at, g) =∥g−PROJ(st−1)∥p−∥g−PROJ(st)∥p,(4)where ∥ · ∥pindicates the p-norm.。This reward provides adenser reward signal at each time step to the agent about itsproximity to the goal, offering more nuanced informationthan a binary signal indicating whether it has reached thegoal or not.。In our current implementation, we set p= 1.Thus, the reward function for GCRL is formulated asr(s, a, g ) =r(s, a) +λ3rg(s, a, g ), (5)where r(s, a)comes from Equation (2).。And for thevalue function corresponds to Equation (5), we extendthe multi-head V(st)with a new value head Vg(st, g)asV(st, g) =V(s) +λ3Vg(st, g), where Vg(st, g)approxi-matesPTi=tγi−trg(st, at, g).Avoiding Policy Degeneration.。Letπ⋆denote the well-trained policy from the non-goal reinforcement learningstep.。However, we have observed a performance degenera-tion on basic skill execution when continuing the trainingof the goal-conditioned policy πg,θstarting from π⋆.。Thisis attributed to two aspects: (1) catastrophic forgetting onthe basic skills as the goal-conditioned learning continues;(2) a change in the input of the policy network from sto(s, g), where gintroduces interference in decision-making,as the policy has not encountered goal inputs during non-goal-conditioned training.. \n",
      "the piece 92\n",
      " \n",
      "t, at, g)that evaluates the degree of goal completion.Equation (4) shows the calculation of rg(st, at, g)as theEuclidean norm difference between two consecutive statesand a goal asrg(st, at, g) =∥g−PROJ(st−1)∥p−∥g−PROJ(st)∥p,(4)where ∥ · ∥pindicates the p-norm.。This reward provides adenser reward signal at each time step to the agent about itsproximity to the goal, offering more nuanced informationthan a binary signal indicating whether it has reached thegoal or not.。In our current implementation, we set p= 1.Thus, the reward function for GCRL is formulated asr(s, a, g ) =r(s, a) +λ3rg(s, a, g ), (5)where r(s, a)comes from Equation (2).。And for thevalue function corresponds to Equation (5), we extendthe multi-head V(st)with a new value head Vg(st, g)asV(st, g) =V(s) +λ3Vg(st, g), where Vg(st, g)approxi-matesPTi=tγi−trg(st, at, g).Avoiding Policy Degeneration.。Letπ⋆denote the well-trained policy from the non-goal reinforcement learningstep.。However, we have observed a performance degenera-tion on basic skill execution when continuing the trainingof the goal-conditioned policy πg,θstarting from π⋆.。Thisis attributed to two aspects: (1) catastrophic forgetting onthe basic skills as the goal-conditioned learning continues;(2) a change in the input of the policy network from sto(s, g), where gintroduces interference in decision-making,as the policy has not encountered goal inputs during non-goal-conditioned training.。To address thes issues, we pro-pose a modification to the goal-conditioned policy learningobjective by introducing a KL-divergence regularizer, andintroduce 20% workers for non-goal policy learning to avoidcatastrophic forgetting.. \n",
      "the piece 93\n",
      " \n",
      " to the goal, offering more nuanced informationthan a binary signal indicating whether it has reached thegoal or not.。In our current implementation, we set p= 1.Thus, the reward function for GCRL is formulated asr(s, a, g ) =r(s, a) +λ3rg(s, a, g ), (5)where r(s, a)comes from Equation (2).。And for thevalue function corresponds to Equation (5), we extendthe multi-head V(st)with a new value head Vg(st, g)asV(st, g) =V(s) +λ3Vg(st, g), where Vg(st, g)approxi-matesPTi=tγi−trg(st, at, g).Avoiding Policy Degeneration.。Letπ⋆denote the well-trained policy from the non-goal reinforcement learningstep.。However, we have observed a performance degenera-tion on basic skill execution when continuing the trainingof the goal-conditioned policy πg,θstarting from π⋆.。Thisis attributed to two aspects: (1) catastrophic forgetting onthe basic skills as the goal-conditioned learning continues;(2) a change in the input of the policy network from sto(s, g), where gintroduces interference in decision-making,as the policy has not encountered goal inputs during non-goal-conditioned training.。To address thes issues, we pro-pose a modification to the goal-conditioned policy learningobjective by introducing a KL-divergence regularizer, andintroduce 20% workers for non-goal policy learning to avoidcatastrophic forgetting.。This regularizer quantifies the dis-tance between π⋆andπg,θwhen πg,θconditioned on g=∅as it is equivalently to non-goal policy:maxθE(s,g)\u0002J(πg,θ)−1g=∅·DKL(π⋆∥πg,θ)\u0003.. \n",
      "the piece 94\n",
      " \n",
      "sr(s, a, g ) =r(s, a) +λ3rg(s, a, g ), (5)where r(s, a)comes from Equation (2).。And for thevalue function corresponds to Equation (5), we extendthe multi-head V(st)with a new value head Vg(st, g)asV(st, g) =V(s) +λ3Vg(st, g), where Vg(st, g)approxi-matesPTi=tγi−trg(st, at, g).Avoiding Policy Degeneration.。Letπ⋆denote the well-trained policy from the non-goal reinforcement learningstep.。However, we have observed a performance degenera-tion on basic skill execution when continuing the trainingof the goal-conditioned policy πg,θstarting from π⋆.。Thisis attributed to two aspects: (1) catastrophic forgetting onthe basic skills as the goal-conditioned learning continues;(2) a change in the input of the policy network from sto(s, g), where gintroduces interference in decision-making,as the policy has not encountered goal inputs during non-goal-conditioned training.。To address thes issues, we pro-pose a modification to the goal-conditioned policy learningobjective by introducing a KL-divergence regularizer, andintroduce 20% workers for non-goal policy learning to avoidcatastrophic forgetting.。This regularizer quantifies the dis-tance between π⋆andπg,θwhen πg,θconditioned on g=∅as it is equivalently to non-goal policy:maxθE(s,g)\u0002J(πg,θ)−1g=∅·DKL(π⋆∥πg,θ)\u0003.。(6)J(πg,θ)the policy loss in PPO, and 1g=∅indicates that theKL-divergence term is only activated when an empty goalinput for πg,θ.. \n",
      "the piece 95\n",
      " \n",
      " value head Vg(st, g)asV(st, g) =V(s) +λ3Vg(st, g), where Vg(st, g)approxi-matesPTi=tγi−trg(st, at, g).Avoiding Policy Degeneration.。Letπ⋆denote the well-trained policy from the non-goal reinforcement learningstep.。However, we have observed a performance degenera-tion on basic skill execution when continuing the trainingof the goal-conditioned policy πg,θstarting from π⋆.。Thisis attributed to two aspects: (1) catastrophic forgetting onthe basic skills as the goal-conditioned learning continues;(2) a change in the input of the policy network from sto(s, g), where gintroduces interference in decision-making,as the policy has not encountered goal inputs during non-goal-conditioned training.。To address thes issues, we pro-pose a modification to the goal-conditioned policy learningobjective by introducing a KL-divergence regularizer, andintroduce 20% workers for non-goal policy learning to avoidcatastrophic forgetting.。This regularizer quantifies the dis-tance between π⋆andπg,θwhen πg,θconditioned on g=∅as it is equivalently to non-goal policy:maxθE(s,g)\u0002J(πg,θ)−1g=∅·DKL(π⋆∥πg,θ)\u0003.。(6)J(πg,θ)the policy loss in PPO, and 1g=∅indicates that theKL-divergence term is only activated when an empty goalinput for πg,θ.。Algorithm 3, Appendix H summarizes thelearning process.. \n",
      "the piece 96\n",
      " \n",
      "ave observed a performance degenera-tion on basic skill execution when continuing the trainingof the goal-conditioned policy πg,θstarting from π⋆.。Thisis attributed to two aspects: (1) catastrophic forgetting onthe basic skills as the goal-conditioned learning continues;(2) a change in the input of the policy network from sto(s, g), where gintroduces interference in decision-making,as the policy has not encountered goal inputs during non-goal-conditioned training.。To address thes issues, we pro-pose a modification to the goal-conditioned policy learningobjective by introducing a KL-divergence regularizer, andintroduce 20% workers for non-goal policy learning to avoidcatastrophic forgetting.。This regularizer quantifies the dis-tance between π⋆andπg,θwhen πg,θconditioned on g=∅as it is equivalently to non-goal policy:maxθE(s,g)\u0002J(πg,θ)−1g=∅·DKL(π⋆∥πg,θ)\u0003.。(6)J(πg,θ)the policy loss in PPO, and 1g=∅indicates that theKL-divergence term is only activated when an empty goalinput for πg,θ.。Algorithm 3, Appendix H summarizes thelearning process.。Furthermore, we observed that occasion-ally sampling experience from π⋆to train πg,θcan alsorelieve the degeneration.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation4.3.. \n",
      "the piece 97\n",
      " \n",
      "he trainingof the goal-conditioned policy πg,θstarting from π⋆.。Thisis attributed to two aspects: (1) catastrophic forgetting onthe basic skills as the goal-conditioned learning continues;(2) a change in the input of the policy network from sto(s, g), where gintroduces interference in decision-making,as the policy has not encountered goal inputs during non-goal-conditioned training.。To address thes issues, we pro-pose a modification to the goal-conditioned policy learningobjective by introducing a KL-divergence regularizer, andintroduce 20% workers for non-goal policy learning to avoidcatastrophic forgetting.。This regularizer quantifies the dis-tance between π⋆andπg,θwhen πg,θconditioned on g=∅as it is equivalently to non-goal policy:maxθE(s,g)\u0002J(πg,θ)−1g=∅·DKL(π⋆∥πg,θ)\u0003.。(6)J(πg,θ)the policy loss in PPO, and 1g=∅indicates that theKL-divergence term is only activated when an empty goalinput for πg,θ.。Algorithm 3, Appendix H summarizes thelearning process.。Furthermore, we observed that occasion-ally sampling experience from π⋆to train πg,θcan alsorelieve the degeneration.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation4.3.。Fine-tuning a LLM-based Goal GeneratorLetIrepresent the set of natural language instructions, andOthe set of abstracted environment states in text.. \n",
      "the piece 98\n",
      " \n",
      "itioned learning continues;(2) a change in the input of the policy network from sto(s, g), where gintroduces interference in decision-making,as the policy has not encountered goal inputs during non-goal-conditioned training.。To address thes issues, we pro-pose a modification to the goal-conditioned policy learningobjective by introducing a KL-divergence regularizer, andintroduce 20% workers for non-goal policy learning to avoidcatastrophic forgetting.。This regularizer quantifies the dis-tance between π⋆andπg,θwhen πg,θconditioned on g=∅as it is equivalently to non-goal policy:maxθE(s,g)\u0002J(πg,θ)−1g=∅·DKL(π⋆∥πg,θ)\u0003.。(6)J(πg,θ)the policy loss in PPO, and 1g=∅indicates that theKL-divergence term is only activated when an empty goalinput for πg,θ.。Algorithm 3, Appendix H summarizes thelearning process.。Furthermore, we observed that occasion-ally sampling experience from π⋆to train πg,θcan alsorelieve the degeneration.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation4.3.。Fine-tuning a LLM-based Goal GeneratorLetIrepresent the set of natural language instructions, andOthe set of abstracted environment states in text.。Our ob-jective is to fine-tune a pre-trained LLM as a goal generator,denoted as Gllm:O×I → G , which means Gllmgeneratesa goal relevant to a given instruction with the considerationof current environment context, i.e., a state abstraction.Dataset Construction.. \n",
      "the piece 99\n",
      " \n",
      "e by introducing a KL-divergence regularizer, andintroduce 20% workers for non-goal policy learning to avoidcatastrophic forgetting.。This regularizer quantifies the dis-tance between π⋆andπg,θwhen πg,θconditioned on g=∅as it is equivalently to non-goal policy:maxθE(s,g)\u0002J(πg,θ)−1g=∅·DKL(π⋆∥πg,θ)\u0003.。(6)J(πg,θ)the policy loss in PPO, and 1g=∅indicates that theKL-divergence term is only activated when an empty goalinput for πg,θ.。Algorithm 3, Appendix H summarizes thelearning process.。Furthermore, we observed that occasion-ally sampling experience from π⋆to train πg,θcan alsorelieve the degeneration.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation4.3.。Fine-tuning a LLM-based Goal GeneratorLetIrepresent the set of natural language instructions, andOthe set of abstracted environment states in text.。Our ob-jective is to fine-tune a pre-trained LLM as a goal generator,denoted as Gllm:O×I → G , which means Gllmgeneratesa goal relevant to a given instruction with the considerationof current environment context, i.e., a state abstraction.Dataset Construction.。To achieve that, we first constructOusing states collected by the πg,θ.. \n",
      "the piece 100\n",
      " \n",
      "valently to non-goal policy:maxθE(s,g)\u0002J(πg,θ)−1g=∅·DKL(π⋆∥πg,θ)\u0003.。(6)J(πg,θ)the policy loss in PPO, and 1g=∅indicates that theKL-divergence term is only activated when an empty goalinput for πg,θ.。Algorithm 3, Appendix H summarizes thelearning process.。Furthermore, we observed that occasion-ally sampling experience from π⋆to train πg,θcan alsorelieve the degeneration.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation4.3.。Fine-tuning a LLM-based Goal GeneratorLetIrepresent the set of natural language instructions, andOthe set of abstracted environment states in text.。Our ob-jective is to fine-tune a pre-trained LLM as a goal generator,denoted as Gllm:O×I → G , which means Gllmgeneratesa goal relevant to a given instruction with the considerationof current environment context, i.e., a state abstraction.Dataset Construction.。To achieve that, we first constructOusing states collected by the πg,θ.。Each abstraction o∈ Oencapsulates essential state features of its correspondingstates, and the extraction rules are outlined in Appendix B.For the creation of I, we leverage various instruction gener-ation to ensure its diversity and scalability, aligning with ouroverarching goal of achieving open-endedness in the instruc-tion space.. \n",
      "the piece 101\n",
      " \n",
      "n an empty goalinput for πg,θ.。Algorithm 3, Appendix H summarizes thelearning process.。Furthermore, we observed that occasion-ally sampling experience from π⋆to train πg,θcan alsorelieve the degeneration.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation4.3.。Fine-tuning a LLM-based Goal GeneratorLetIrepresent the set of natural language instructions, andOthe set of abstracted environment states in text.。Our ob-jective is to fine-tune a pre-trained LLM as a goal generator,denoted as Gllm:O×I → G , which means Gllmgeneratesa goal relevant to a given instruction with the considerationof current environment context, i.e., a state abstraction.Dataset Construction.。To achieve that, we first constructOusing states collected by the πg,θ.。Each abstraction o∈ Oencapsulates essential state features of its correspondingstates, and the extraction rules are outlined in Appendix B.For the creation of I, we leverage various instruction gener-ation to ensure its diversity and scalability, aligning with ouroverarching goal of achieving open-endedness in the instruc-tion space.。Specifically, Iis derived from four types.. \n",
      "the piece 102\n",
      " \n",
      "ampling experience from π⋆to train πg,θcan alsorelieve the degeneration.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation4.3.。Fine-tuning a LLM-based Goal GeneratorLetIrepresent the set of natural language instructions, andOthe set of abstracted environment states in text.。Our ob-jective is to fine-tune a pre-trained LLM as a goal generator,denoted as Gllm:O×I → G , which means Gllmgeneratesa goal relevant to a given instruction with the considerationof current environment context, i.e., a state abstraction.Dataset Construction.。To achieve that, we first constructOusing states collected by the πg,θ.。Each abstraction o∈ Oencapsulates essential state features of its correspondingstates, and the extraction rules are outlined in Appendix B.For the creation of I, we leverage various instruction gener-ation to ensure its diversity and scalability, aligning with ouroverarching goal of achieving open-endedness in the instruc-tion space.。Specifically, Iis derived from four types.。Mostof these are formulated through a tuple of an initial state anda target state/trajectory collected by πg, which aims to alignGllmandπgat environmental comprehension.. \n",
      "the piece 103\n",
      " \n",
      "e degeneration.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation4.3.。Fine-tuning a LLM-based Goal GeneratorLetIrepresent the set of natural language instructions, andOthe set of abstracted environment states in text.。Our ob-jective is to fine-tune a pre-trained LLM as a goal generator,denoted as Gllm:O×I → G , which means Gllmgeneratesa goal relevant to a given instruction with the considerationof current environment context, i.e., a state abstraction.Dataset Construction.。To achieve that, we first constructOusing states collected by the πg,θ.。Each abstraction o∈ Oencapsulates essential state features of its correspondingstates, and the extraction rules are outlined in Appendix B.For the creation of I, we leverage various instruction gener-ation to ensure its diversity and scalability, aligning with ouroverarching goal of achieving open-endedness in the instruc-tion space.。Specifically, Iis derived from four types.。Mostof these are formulated through a tuple of an initial state anda target state/trajectory collected by πg, which aims to alignGllmandπgat environmental comprehension.。Then, weleverage this data and GPT-4 (OpenAI, 2023) to generateappropriate instruction.. \n",
      "the piece 104\n",
      " \n",
      "et of abstracted environment states in text.。Our ob-jective is to fine-tune a pre-trained LLM as a goal generator,denoted as Gllm:O×I → G , which means Gllmgeneratesa goal relevant to a given instruction with the considerationof current environment context, i.e., a state abstraction.Dataset Construction.。To achieve that, we first constructOusing states collected by the πg,θ.。Each abstraction o∈ Oencapsulates essential state features of its correspondingstates, and the extraction rules are outlined in Appendix B.For the creation of I, we leverage various instruction gener-ation to ensure its diversity and scalability, aligning with ouroverarching goal of achieving open-endedness in the instruc-tion space.。Specifically, Iis derived from four types.。Mostof these are formulated through a tuple of an initial state anda target state/trajectory collected by πg, which aims to alignGllmandπgat environmental comprehension.。Then, weleverage this data and GPT-4 (OpenAI, 2023) to generateappropriate instruction.。This instruction aims to direct fromthe specified initial state to the intended target state, andCoT (Wei et al., 2023) is deployed to enhance performance.Specifically, the four types of instruction generation are (1)IH(Human Instructions, HI) : human-annotated instruc-tions; (2) IS(State Instructions, SI) : GPT-4-generatedinstructions by giving a tuple of states (s, s′)where the sthe initial state that sampled from agent trajectories and s′the target state that is manually constructed by modifyingfeatures of the s; (3)IA(Agent Instructions, AI) : GPT-4-generated instructions by giving a pair of (s, τ)where sthe initial state, τthe agent trajectory; and (4) IR(RandomInstructions, RI) : a mixture of the above three instructionsets to form a supplementary dataset.. \n",
      "the piece 105\n",
      " \n",
      "ns Gllmgeneratesa goal relevant to a given instruction with the considerationof current environment context, i.e., a state abstraction.Dataset Construction.。To achieve that, we first constructOusing states collected by the πg,θ.。Each abstraction o∈ Oencapsulates essential state features of its correspondingstates, and the extraction rules are outlined in Appendix B.For the creation of I, we leverage various instruction gener-ation to ensure its diversity and scalability, aligning with ouroverarching goal of achieving open-endedness in the instruc-tion space.。Specifically, Iis derived from four types.。Mostof these are formulated through a tuple of an initial state anda target state/trajectory collected by πg, which aims to alignGllmandπgat environmental comprehension.。Then, weleverage this data and GPT-4 (OpenAI, 2023) to generateappropriate instruction.。This instruction aims to direct fromthe specified initial state to the intended target state, andCoT (Wei et al., 2023) is deployed to enhance performance.Specifically, the four types of instruction generation are (1)IH(Human Instructions, HI) : human-annotated instruc-tions; (2) IS(State Instructions, SI) : GPT-4-generatedinstructions by giving a tuple of states (s, s′)where the sthe initial state that sampled from agent trajectories and s′the target state that is manually constructed by modifyingfeatures of the s; (3)IA(Agent Instructions, AI) : GPT-4-generated instructions by giving a pair of (s, τ)where sthe initial state, τthe agent trajectory; and (4) IR(RandomInstructions, RI) : a mixture of the above three instructionsets to form a supplementary dataset.。By accompanying OwithI, we further construct Dx={(o, ι)|(o, ι)∈ O × I} .Subsequently, we employ GPT-4 to generate appropriategoals ˆGusingDxas labeled data for training Gllm, result-ing in a dataset D={(o, ι, g )|(o, ι, g )∈ O × I × ˆG}.. \n",
      "the piece 106\n",
      " \n",
      " essential state features of its correspondingstates, and the extraction rules are outlined in Appendix B.For the creation of I, we leverage various instruction gener-ation to ensure its diversity and scalability, aligning with ouroverarching goal of achieving open-endedness in the instruc-tion space.。Specifically, Iis derived from four types.。Mostof these are formulated through a tuple of an initial state anda target state/trajectory collected by πg, which aims to alignGllmandπgat environmental comprehension.。Then, weleverage this data and GPT-4 (OpenAI, 2023) to generateappropriate instruction.。This instruction aims to direct fromthe specified initial state to the intended target state, andCoT (Wei et al., 2023) is deployed to enhance performance.Specifically, the four types of instruction generation are (1)IH(Human Instructions, HI) : human-annotated instruc-tions; (2) IS(State Instructions, SI) : GPT-4-generatedinstructions by giving a tuple of states (s, s′)where the sthe initial state that sampled from agent trajectories and s′the target state that is manually constructed by modifyingfeatures of the s; (3)IA(Agent Instructions, AI) : GPT-4-generated instructions by giving a pair of (s, τ)where sthe initial state, τthe agent trajectory; and (4) IR(RandomInstructions, RI) : a mixture of the above three instructionsets to form a supplementary dataset.。By accompanying OwithI, we further construct Dx={(o, ι)|(o, ι)∈ O × I} .Subsequently, we employ GPT-4 to generate appropriategoals ˆGusingDxas labeled data for training Gllm, result-ing in a dataset D={(o, ι, g )|(o, ι, g )∈ O × I × ˆG}.。Toensure that the goals generated by GPT-4 conform to theformat we want, a comprehensive prompt engineering en-deavor was conducted to establish a set of predeterminedrules for GPT-4.. \n",
      "the piece 107\n",
      " \n",
      "rules are outlined in Appendix B.For the creation of I, we leverage various instruction gener-ation to ensure its diversity and scalability, aligning with ouroverarching goal of achieving open-endedness in the instruc-tion space.。Specifically, Iis derived from four types.。Mostof these are formulated through a tuple of an initial state anda target state/trajectory collected by πg, which aims to alignGllmandπgat environmental comprehension.。Then, weleverage this data and GPT-4 (OpenAI, 2023) to generateappropriate instruction.。This instruction aims to direct fromthe specified initial state to the intended target state, andCoT (Wei et al., 2023) is deployed to enhance performance.Specifically, the four types of instruction generation are (1)IH(Human Instructions, HI) : human-annotated instruc-tions; (2) IS(State Instructions, SI) : GPT-4-generatedinstructions by giving a tuple of states (s, s′)where the sthe initial state that sampled from agent trajectories and s′the target state that is manually constructed by modifyingfeatures of the s; (3)IA(Agent Instructions, AI) : GPT-4-generated instructions by giving a pair of (s, τ)where sthe initial state, τthe agent trajectory; and (4) IR(RandomInstructions, RI) : a mixture of the above three instructionsets to form a supplementary dataset.。By accompanying OwithI, we further construct Dx={(o, ι)|(o, ι)∈ O × I} .Subsequently, we employ GPT-4 to generate appropriategoals ˆGusingDxas labeled data for training Gllm, result-ing in a dataset D={(o, ι, g )|(o, ι, g )∈ O × I × ˆG}.。Toensure that the goals generated by GPT-4 conform to theformat we want, a comprehensive prompt engineering en-deavor was conducted to establish a set of predeterminedrules for GPT-4.。The rule-based prompts that guide GPT-4’sresponses are documented in Table 10, with examples ofprompts for generation provided in Table 17.Multi-step Fine-tuning.. \n",
      "the piece 108\n",
      " \n",
      "anda target state/trajectory collected by πg, which aims to alignGllmandπgat environmental comprehension.。Then, weleverage this data and GPT-4 (OpenAI, 2023) to generateappropriate instruction.。This instruction aims to direct fromthe specified initial state to the intended target state, andCoT (Wei et al., 2023) is deployed to enhance performance.Specifically, the four types of instruction generation are (1)IH(Human Instructions, HI) : human-annotated instruc-tions; (2) IS(State Instructions, SI) : GPT-4-generatedinstructions by giving a tuple of states (s, s′)where the sthe initial state that sampled from agent trajectories and s′the target state that is manually constructed by modifyingfeatures of the s; (3)IA(Agent Instructions, AI) : GPT-4-generated instructions by giving a pair of (s, τ)where sthe initial state, τthe agent trajectory; and (4) IR(RandomInstructions, RI) : a mixture of the above three instructionsets to form a supplementary dataset.。By accompanying OwithI, we further construct Dx={(o, ι)|(o, ι)∈ O × I} .Subsequently, we employ GPT-4 to generate appropriategoals ˆGusingDxas labeled data for training Gllm, result-ing in a dataset D={(o, ι, g )|(o, ι, g )∈ O × I × ˆG}.。Toensure that the goals generated by GPT-4 conform to theformat we want, a comprehensive prompt engineering en-deavor was conducted to establish a set of predeterminedrules for GPT-4.。The rule-based prompts that guide GPT-4’sresponses are documented in Table 10, with examples ofprompts for generation provided in Table 17.Multi-step Fine-tuning.。We fine-tune ChatGLM-6B withLoRA (Hu et al., 2021) in three steps, as illustrated inFigure 8.. \n",
      "the piece 109\n",
      " \n",
      ", which aims to alignGllmandπgat environmental comprehension.。Then, weleverage this data and GPT-4 (OpenAI, 2023) to generateappropriate instruction.。This instruction aims to direct fromthe specified initial state to the intended target state, andCoT (Wei et al., 2023) is deployed to enhance performance.Specifically, the four types of instruction generation are (1)IH(Human Instructions, HI) : human-annotated instruc-tions; (2) IS(State Instructions, SI) : GPT-4-generatedinstructions by giving a tuple of states (s, s′)where the sthe initial state that sampled from agent trajectories and s′the target state that is manually constructed by modifyingfeatures of the s; (3)IA(Agent Instructions, AI) : GPT-4-generated instructions by giving a pair of (s, τ)where sthe initial state, τthe agent trajectory; and (4) IR(RandomInstructions, RI) : a mixture of the above three instructionsets to form a supplementary dataset.。By accompanying OwithI, we further construct Dx={(o, ι)|(o, ι)∈ O × I} .Subsequently, we employ GPT-4 to generate appropriategoals ˆGusingDxas labeled data for training Gllm, result-ing in a dataset D={(o, ι, g )|(o, ι, g )∈ O × I × ˆG}.。Toensure that the goals generated by GPT-4 conform to theformat we want, a comprehensive prompt engineering en-deavor was conducted to establish a set of predeterminedrules for GPT-4.。The rule-based prompts that guide GPT-4’sresponses are documented in Table 10, with examples ofprompts for generation provided in Table 17.Multi-step Fine-tuning.。We fine-tune ChatGLM-6B withLoRA (Hu et al., 2021) in three steps, as illustrated inFigure 8.。The steps include (1) CoT-assisted fine-tuning(CoFT) : we split the CoT steps of building Iinto indepen-dent training data, aiming to expand the volume of trainingdata as well as enhance the goal generator’s reasoning andunderstanding to Dx; (2) Supervised Fine-tuning (SFT) :strictly formatting the LLM-generated goals and furtherimproving the accuracy; and (3) Ensemble Fine-tuning(EFT) : multiple checkpoints of Gllmare utilized to gen-erate goal candidates for each (o, ι)∈ Dx, then sub-goalswith highest counts are reconstructed as a ground goal tofine-tune the model to enhance the generation.4.4.. \n",
      "the piece 110\n",
      " \n",
      " to direct fromthe specified initial state to the intended target state, andCoT (Wei et al., 2023) is deployed to enhance performance.Specifically, the four types of instruction generation are (1)IH(Human Instructions, HI) : human-annotated instruc-tions; (2) IS(State Instructions, SI) : GPT-4-generatedinstructions by giving a tuple of states (s, s′)where the sthe initial state that sampled from agent trajectories and s′the target state that is manually constructed by modifyingfeatures of the s; (3)IA(Agent Instructions, AI) : GPT-4-generated instructions by giving a pair of (s, τ)where sthe initial state, τthe agent trajectory; and (4) IR(RandomInstructions, RI) : a mixture of the above three instructionsets to form a supplementary dataset.。By accompanying OwithI, we further construct Dx={(o, ι)|(o, ι)∈ O × I} .Subsequently, we employ GPT-4 to generate appropriategoals ˆGusingDxas labeled data for training Gllm, result-ing in a dataset D={(o, ι, g )|(o, ι, g )∈ O × I × ˆG}.。Toensure that the goals generated by GPT-4 conform to theformat we want, a comprehensive prompt engineering en-deavor was conducted to establish a set of predeterminedrules for GPT-4.。The rule-based prompts that guide GPT-4’sresponses are documented in Table 10, with examples ofprompts for generation provided in Table 17.Multi-step Fine-tuning.。We fine-tune ChatGLM-6B withLoRA (Hu et al., 2021) in three steps, as illustrated inFigure 8.。The steps include (1) CoT-assisted fine-tuning(CoFT) : we split the CoT steps of building Iinto indepen-dent training data, aiming to expand the volume of trainingdata as well as enhance the goal generator’s reasoning andunderstanding to Dx; (2) Supervised Fine-tuning (SFT) :strictly formatting the LLM-generated goals and furtherimproving the accuracy; and (3) Ensemble Fine-tuning(EFT) : multiple checkpoints of Gllmare utilized to gen-erate goal candidates for each (o, ι)∈ Dx, then sub-goalswith highest counts are reconstructed as a ground goal tofine-tune the model to enhance the generation.4.4.。Collaborative TrainingAfter completing the above training steps, we obtained awell-trained goal generator Gllmand goal-conditioned pol-icyπgthat satisfactorily adhere to their respective goal distri-butions.. \n",
      "the piece 111\n",
      " \n",
      "l., 2023) is deployed to enhance performance.Specifically, the four types of instruction generation are (1)IH(Human Instructions, HI) : human-annotated instruc-tions; (2) IS(State Instructions, SI) : GPT-4-generatedinstructions by giving a tuple of states (s, s′)where the sthe initial state that sampled from agent trajectories and s′the target state that is manually constructed by modifyingfeatures of the s; (3)IA(Agent Instructions, AI) : GPT-4-generated instructions by giving a pair of (s, τ)where sthe initial state, τthe agent trajectory; and (4) IR(RandomInstructions, RI) : a mixture of the above three instructionsets to form a supplementary dataset.。By accompanying OwithI, we further construct Dx={(o, ι)|(o, ι)∈ O × I} .Subsequently, we employ GPT-4 to generate appropriategoals ˆGusingDxas labeled data for training Gllm, result-ing in a dataset D={(o, ι, g )|(o, ι, g )∈ O × I × ˆG}.。Toensure that the goals generated by GPT-4 conform to theformat we want, a comprehensive prompt engineering en-deavor was conducted to establish a set of predeterminedrules for GPT-4.。The rule-based prompts that guide GPT-4’sresponses are documented in Table 10, with examples ofprompts for generation provided in Table 17.Multi-step Fine-tuning.。We fine-tune ChatGLM-6B withLoRA (Hu et al., 2021) in three steps, as illustrated inFigure 8.。The steps include (1) CoT-assisted fine-tuning(CoFT) : we split the CoT steps of building Iinto indepen-dent training data, aiming to expand the volume of trainingdata as well as enhance the goal generator’s reasoning andunderstanding to Dx; (2) Supervised Fine-tuning (SFT) :strictly formatting the LLM-generated goals and furtherimproving the accuracy; and (3) Ensemble Fine-tuning(EFT) : multiple checkpoints of Gllmare utilized to gen-erate goal candidates for each (o, ι)∈ Dx, then sub-goalswith highest counts are reconstructed as a ground goal tofine-tune the model to enhance the generation.4.4.。Collaborative TrainingAfter completing the above training steps, we obtained awell-trained goal generator Gllmand goal-conditioned pol-icyπgthat satisfactorily adhere to their respective goal distri-butions.。However, an inconsistency persists between Gllmandπgstemming from their independent training objectives,where Gllmaimed to generate goals that satisfy given in-structions, and πgfocused on exploring goals.. \n",
      "the piece 112\n",
      " \n",
      "te appropriategoals ˆGusingDxas labeled data for training Gllm, result-ing in a dataset D={(o, ι, g )|(o, ι, g )∈ O × I × ˆG}.。Toensure that the goals generated by GPT-4 conform to theformat we want, a comprehensive prompt engineering en-deavor was conducted to establish a set of predeterminedrules for GPT-4.。The rule-based prompts that guide GPT-4’sresponses are documented in Table 10, with examples ofprompts for generation provided in Table 17.Multi-step Fine-tuning.。We fine-tune ChatGLM-6B withLoRA (Hu et al., 2021) in three steps, as illustrated inFigure 8.。The steps include (1) CoT-assisted fine-tuning(CoFT) : we split the CoT steps of building Iinto indepen-dent training data, aiming to expand the volume of trainingdata as well as enhance the goal generator’s reasoning andunderstanding to Dx; (2) Supervised Fine-tuning (SFT) :strictly formatting the LLM-generated goals and furtherimproving the accuracy; and (3) Ensemble Fine-tuning(EFT) : multiple checkpoints of Gllmare utilized to gen-erate goal candidates for each (o, ι)∈ Dx, then sub-goalswith highest counts are reconstructed as a ground goal tofine-tune the model to enhance the generation.4.4.。Collaborative TrainingAfter completing the above training steps, we obtained awell-trained goal generator Gllmand goal-conditioned pol-icyπgthat satisfactorily adhere to their respective goal distri-butions.。However, an inconsistency persists between Gllmandπgstemming from their independent training objectives,where Gllmaimed to generate goals that satisfy given in-structions, and πgfocused on exploring goals.。Therefore,we introduce co-training to address the aforementioned is-sue ensuring that the goals generated by Gllmare not onlylinguistically sound but also aligned with the capabilities ofπg.. \n",
      "the piece 113\n",
      " \n",
      "eavor was conducted to establish a set of predeterminedrules for GPT-4.。The rule-based prompts that guide GPT-4’sresponses are documented in Table 10, with examples ofprompts for generation provided in Table 17.Multi-step Fine-tuning.。We fine-tune ChatGLM-6B withLoRA (Hu et al., 2021) in three steps, as illustrated inFigure 8.。The steps include (1) CoT-assisted fine-tuning(CoFT) : we split the CoT steps of building Iinto indepen-dent training data, aiming to expand the volume of trainingdata as well as enhance the goal generator’s reasoning andunderstanding to Dx; (2) Supervised Fine-tuning (SFT) :strictly formatting the LLM-generated goals and furtherimproving the accuracy; and (3) Ensemble Fine-tuning(EFT) : multiple checkpoints of Gllmare utilized to gen-erate goal candidates for each (o, ι)∈ Dx, then sub-goalswith highest counts are reconstructed as a ground goal tofine-tune the model to enhance the generation.4.4.。Collaborative TrainingAfter completing the above training steps, we obtained awell-trained goal generator Gllmand goal-conditioned pol-icyπgthat satisfactorily adhere to their respective goal distri-butions.。However, an inconsistency persists between Gllmandπgstemming from their independent training objectives,where Gllmaimed to generate goals that satisfy given in-structions, and πgfocused on exploring goals.。Therefore,we introduce co-training to address the aforementioned is-sue ensuring that the goals generated by Gllmare not onlylinguistically sound but also aligned with the capabilities ofπg.。We formulate the co-training as follows:(πg = arg max πgEg∼PG|Gllm\u0002Vπg(s, g)\u0003PG|Gllm = arg max PG|GllmEg∼PG|Gllm[VGllm(s, g)],(7)where PG|Gllmthe goal distribution conditioned by Gllm,V(s, g)denotes an approximate evaluation for πgorGllm,in general, a state value function.. \n",
      "the piece 114\n",
      " \n",
      "tion provided in Table 17.Multi-step Fine-tuning.。We fine-tune ChatGLM-6B withLoRA (Hu et al., 2021) in three steps, as illustrated inFigure 8.。The steps include (1) CoT-assisted fine-tuning(CoFT) : we split the CoT steps of building Iinto indepen-dent training data, aiming to expand the volume of trainingdata as well as enhance the goal generator’s reasoning andunderstanding to Dx; (2) Supervised Fine-tuning (SFT) :strictly formatting the LLM-generated goals and furtherimproving the accuracy; and (3) Ensemble Fine-tuning(EFT) : multiple checkpoints of Gllmare utilized to gen-erate goal candidates for each (o, ι)∈ Dx, then sub-goalswith highest counts are reconstructed as a ground goal tofine-tune the model to enhance the generation.4.4.。Collaborative TrainingAfter completing the above training steps, we obtained awell-trained goal generator Gllmand goal-conditioned pol-icyπgthat satisfactorily adhere to their respective goal distri-butions.。However, an inconsistency persists between Gllmandπgstemming from their independent training objectives,where Gllmaimed to generate goals that satisfy given in-structions, and πgfocused on exploring goals.。Therefore,we introduce co-training to address the aforementioned is-sue ensuring that the goals generated by Gllmare not onlylinguistically sound but also aligned with the capabilities ofπg.。We formulate the co-training as follows:(πg = arg max πgEg∼PG|Gllm\u0002Vπg(s, g)\u0003PG|Gllm = arg max PG|GllmEg∼PG|Gllm[VGllm(s, g)],(7)where PG|Gllmthe goal distribution conditioned by Gllm,V(s, g)denotes an approximate evaluation for πgorGllm,in general, a state value function.。It is noteworthy that ourco-training framework is close to a hierarchical reinforce-ment learning framework (HRL) (Vezhnevets et al., 2017),where the Manager (comparable to Gllm) plans goals for thelearning of the Worker (comparable to πg), with RL beingperformed for each.. \n",
      "the piece 115\n",
      " \n",
      ") CoT-assisted fine-tuning(CoFT) : we split the CoT steps of building Iinto indepen-dent training data, aiming to expand the volume of trainingdata as well as enhance the goal generator’s reasoning andunderstanding to Dx; (2) Supervised Fine-tuning (SFT) :strictly formatting the LLM-generated goals and furtherimproving the accuracy; and (3) Ensemble Fine-tuning(EFT) : multiple checkpoints of Gllmare utilized to gen-erate goal candidates for each (o, ι)∈ Dx, then sub-goalswith highest counts are reconstructed as a ground goal tofine-tune the model to enhance the generation.4.4.。Collaborative TrainingAfter completing the above training steps, we obtained awell-trained goal generator Gllmand goal-conditioned pol-icyπgthat satisfactorily adhere to their respective goal distri-butions.。However, an inconsistency persists between Gllmandπgstemming from their independent training objectives,where Gllmaimed to generate goals that satisfy given in-structions, and πgfocused on exploring goals.。Therefore,we introduce co-training to address the aforementioned is-sue ensuring that the goals generated by Gllmare not onlylinguistically sound but also aligned with the capabilities ofπg.。We formulate the co-training as follows:(πg = arg max πgEg∼PG|Gllm\u0002Vπg(s, g)\u0003PG|Gllm = arg max PG|GllmEg∼PG|Gllm[VGllm(s, g)],(7)where PG|Gllmthe goal distribution conditioned by Gllm,V(s, g)denotes an approximate evaluation for πgorGllm,in general, a state value function.。It is noteworthy that ourco-training framework is close to a hierarchical reinforce-ment learning framework (HRL) (Vezhnevets et al., 2017),where the Manager (comparable to Gllm) plans goals for thelearning of the Worker (comparable to πg), with RL beingperformed for each.。Inspired by HRL, we implement co-training by integrating the goal-conditioned training of πgand Reinforcement Learning with Agent Feedback (RLAF)forGllm.. \n",
      "the piece 116\n",
      " \n",
      "ng data, aiming to expand the volume of trainingdata as well as enhance the goal generator’s reasoning andunderstanding to Dx; (2) Supervised Fine-tuning (SFT) :strictly formatting the LLM-generated goals and furtherimproving the accuracy; and (3) Ensemble Fine-tuning(EFT) : multiple checkpoints of Gllmare utilized to gen-erate goal candidates for each (o, ι)∈ Dx, then sub-goalswith highest counts are reconstructed as a ground goal tofine-tune the model to enhance the generation.4.4.。Collaborative TrainingAfter completing the above training steps, we obtained awell-trained goal generator Gllmand goal-conditioned pol-icyπgthat satisfactorily adhere to their respective goal distri-butions.。However, an inconsistency persists between Gllmandπgstemming from their independent training objectives,where Gllmaimed to generate goals that satisfy given in-structions, and πgfocused on exploring goals.。Therefore,we introduce co-training to address the aforementioned is-sue ensuring that the goals generated by Gllmare not onlylinguistically sound but also aligned with the capabilities ofπg.。We formulate the co-training as follows:(πg = arg max πgEg∼PG|Gllm\u0002Vπg(s, g)\u0003PG|Gllm = arg max PG|GllmEg∼PG|Gllm[VGllm(s, g)],(7)where PG|Gllmthe goal distribution conditioned by Gllm,V(s, g)denotes an approximate evaluation for πgorGllm,in general, a state value function.。It is noteworthy that ourco-training framework is close to a hierarchical reinforce-ment learning framework (HRL) (Vezhnevets et al., 2017),where the Manager (comparable to Gllm) plans goals for thelearning of the Worker (comparable to πg), with RL beingperformed for each.。Inspired by HRL, we implement co-training by integrating the goal-conditioned training of πgand Reinforcement Learning with Agent Feedback (RLAF)forGllm.。RLAF is built upon PPO, with a reward shapingthat considers (1) Rfthe evaluation of goal completion,where a high reward indicates that a goal is completed orthe reachable probability from current state; (2) Retheevaluation of crucial sub-goal completion, which involvesexamining cases by pairing instructions in a batch with a setof essential sub-goals; (3) Rmthe evaluation of outputtingthe proper goal format, with the LLM being penalized basedon edit distance.. \n",
      "the piece 117\n",
      " \n",
      "al-conditioned pol-icyπgthat satisfactorily adhere to their respective goal distri-butions.。However, an inconsistency persists between Gllmandπgstemming from their independent training objectives,where Gllmaimed to generate goals that satisfy given in-structions, and πgfocused on exploring goals.。Therefore,we introduce co-training to address the aforementioned is-sue ensuring that the goals generated by Gllmare not onlylinguistically sound but also aligned with the capabilities ofπg.。We formulate the co-training as follows:(πg = arg max πgEg∼PG|Gllm\u0002Vπg(s, g)\u0003PG|Gllm = arg max PG|GllmEg∼PG|Gllm[VGllm(s, g)],(7)where PG|Gllmthe goal distribution conditioned by Gllm,V(s, g)denotes an approximate evaluation for πgorGllm,in general, a state value function.。It is noteworthy that ourco-training framework is close to a hierarchical reinforce-ment learning framework (HRL) (Vezhnevets et al., 2017),where the Manager (comparable to Gllm) plans goals for thelearning of the Worker (comparable to πg), with RL beingperformed for each.。Inspired by HRL, we implement co-training by integrating the goal-conditioned training of πgand Reinforcement Learning with Agent Feedback (RLAF)forGllm.。RLAF is built upon PPO, with a reward shapingthat considers (1) Rfthe evaluation of goal completion,where a high reward indicates that a goal is completed orthe reachable probability from current state; (2) Retheevaluation of crucial sub-goal completion, which involvesexamining cases by pairing instructions in a batch with a setof essential sub-goals; (3) Rmthe evaluation of outputtingthe proper goal format, with the LLM being penalized basedon edit distance.。Then, we can express the reward func-tion as R(s, ι, g ) =Rf(s, g) +Re(s, g) +Rm(s, g)andAppendix K includes more details.. \n",
      "the piece 118\n",
      " \n",
      "ed to generate goals that satisfy given in-structions, and πgfocused on exploring goals.。Therefore,we introduce co-training to address the aforementioned is-sue ensuring that the goals generated by Gllmare not onlylinguistically sound but also aligned with the capabilities ofπg.。We formulate the co-training as follows:(πg = arg max πgEg∼PG|Gllm\u0002Vπg(s, g)\u0003PG|Gllm = arg max PG|GllmEg∼PG|Gllm[VGllm(s, g)],(7)where PG|Gllmthe goal distribution conditioned by Gllm,V(s, g)denotes an approximate evaluation for πgorGllm,in general, a state value function.。It is noteworthy that ourco-training framework is close to a hierarchical reinforce-ment learning framework (HRL) (Vezhnevets et al., 2017),where the Manager (comparable to Gllm) plans goals for thelearning of the Worker (comparable to πg), with RL beingperformed for each.。Inspired by HRL, we implement co-training by integrating the goal-conditioned training of πgand Reinforcement Learning with Agent Feedback (RLAF)forGllm.。RLAF is built upon PPO, with a reward shapingthat considers (1) Rfthe evaluation of goal completion,where a high reward indicates that a goal is completed orthe reachable probability from current state; (2) Retheevaluation of crucial sub-goal completion, which involvesexamining cases by pairing instructions in a batch with a setof essential sub-goals; (3) Rmthe evaluation of outputtingthe proper goal format, with the LLM being penalized basedon edit distance.。Then, we can express the reward func-tion as R(s, ι, g ) =Rf(s, g) +Re(s, g) +Rm(s, g)andAppendix K includes more details.。We observed the train-ing will lead Gllmandπgcompromise to a local optimal,i.e.,Gllmcomforts a high completion ratio for πgbut ne-glect consistency with instructions, and πgsimultaneouslyrewards Gllmwith a high completion ratio.. \n",
      "the piece 119\n",
      " \n",
      "ot onlylinguistically sound but also aligned with the capabilities ofπg.。We formulate the co-training as follows:(πg = arg max πgEg∼PG|Gllm\u0002Vπg(s, g)\u0003PG|Gllm = arg max PG|GllmEg∼PG|Gllm[VGllm(s, g)],(7)where PG|Gllmthe goal distribution conditioned by Gllm,V(s, g)denotes an approximate evaluation for πgorGllm,in general, a state value function.。It is noteworthy that ourco-training framework is close to a hierarchical reinforce-ment learning framework (HRL) (Vezhnevets et al., 2017),where the Manager (comparable to Gllm) plans goals for thelearning of the Worker (comparable to πg), with RL beingperformed for each.。Inspired by HRL, we implement co-training by integrating the goal-conditioned training of πgand Reinforcement Learning with Agent Feedback (RLAF)forGllm.。RLAF is built upon PPO, with a reward shapingthat considers (1) Rfthe evaluation of goal completion,where a high reward indicates that a goal is completed orthe reachable probability from current state; (2) Retheevaluation of crucial sub-goal completion, which involvesexamining cases by pairing instructions in a batch with a setof essential sub-goals; (3) Rmthe evaluation of outputtingthe proper goal format, with the LLM being penalized basedon edit distance.。Then, we can express the reward func-tion as R(s, ι, g ) =Rf(s, g) +Re(s, g) +Rm(s, g)andAppendix K includes more details.。We observed the train-ing will lead Gllmandπgcompromise to a local optimal,i.e.,Gllmcomforts a high completion ratio for πgbut ne-glect consistency with instructions, and πgsimultaneouslyrewards Gllmwith a high completion ratio.。Furthermore,as the policy training continuing, the evaluation for goalgeneration is out-date.. \n",
      "the piece 120\n",
      " \n",
      "s, g)],(7)where PG|Gllmthe goal distribution conditioned by Gllm,V(s, g)denotes an approximate evaluation for πgorGllm,in general, a state value function.。It is noteworthy that ourco-training framework is close to a hierarchical reinforce-ment learning framework (HRL) (Vezhnevets et al., 2017),where the Manager (comparable to Gllm) plans goals for thelearning of the Worker (comparable to πg), with RL beingperformed for each.。Inspired by HRL, we implement co-training by integrating the goal-conditioned training of πgand Reinforcement Learning with Agent Feedback (RLAF)forGllm.。RLAF is built upon PPO, with a reward shapingthat considers (1) Rfthe evaluation of goal completion,where a high reward indicates that a goal is completed orthe reachable probability from current state; (2) Retheevaluation of crucial sub-goal completion, which involvesexamining cases by pairing instructions in a batch with a setof essential sub-goals; (3) Rmthe evaluation of outputtingthe proper goal format, with the LLM being penalized basedon edit distance.。Then, we can express the reward func-tion as R(s, ι, g ) =Rf(s, g) +Re(s, g) +Rm(s, g)andAppendix K includes more details.。We observed the train-ing will lead Gllmandπgcompromise to a local optimal,i.e.,Gllmcomforts a high completion ratio for πgbut ne-glect consistency with instructions, and πgsimultaneouslyrewards Gllmwith a high completion ratio.。Furthermore,as the policy training continuing, the evaluation for goalgeneration is out-date.。To fix this issue, we propose a pe-riodic reset for the RLAF, i.e., the parameters of the Gllmwill be reset for every set number of steps to avoid beingtrapped in a local convergence, achieving enhanced goalcompletion, and keeping goals consistent with human in-Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationstructions.. \n",
      "the piece 121\n",
      " \n",
      "evets et al., 2017),where the Manager (comparable to Gllm) plans goals for thelearning of the Worker (comparable to πg), with RL beingperformed for each.。Inspired by HRL, we implement co-training by integrating the goal-conditioned training of πgand Reinforcement Learning with Agent Feedback (RLAF)forGllm.。RLAF is built upon PPO, with a reward shapingthat considers (1) Rfthe evaluation of goal completion,where a high reward indicates that a goal is completed orthe reachable probability from current state; (2) Retheevaluation of crucial sub-goal completion, which involvesexamining cases by pairing instructions in a batch with a setof essential sub-goals; (3) Rmthe evaluation of outputtingthe proper goal format, with the LLM being penalized basedon edit distance.。Then, we can express the reward func-tion as R(s, ι, g ) =Rf(s, g) +Re(s, g) +Rm(s, g)andAppendix K includes more details.。We observed the train-ing will lead Gllmandπgcompromise to a local optimal,i.e.,Gllmcomforts a high completion ratio for πgbut ne-glect consistency with instructions, and πgsimultaneouslyrewards Gllmwith a high completion ratio.。Furthermore,as the policy training continuing, the evaluation for goalgeneration is out-date.。To fix this issue, we propose a pe-riodic reset for the RLAF, i.e., the parameters of the Gllmwill be reset for every set number of steps to avoid beingtrapped in a local convergence, achieving enhanced goalcompletion, and keeping goals consistent with human in-Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationstructions.。Considering the training efficiency, we conductLoRA (Hu et al., 2021) to update the model weights forGllm.. \n",
      "the piece 122\n",
      " \n",
      "th Agent Feedback (RLAF)forGllm.。RLAF is built upon PPO, with a reward shapingthat considers (1) Rfthe evaluation of goal completion,where a high reward indicates that a goal is completed orthe reachable probability from current state; (2) Retheevaluation of crucial sub-goal completion, which involvesexamining cases by pairing instructions in a batch with a setof essential sub-goals; (3) Rmthe evaluation of outputtingthe proper goal format, with the LLM being penalized basedon edit distance.。Then, we can express the reward func-tion as R(s, ι, g ) =Rf(s, g) +Re(s, g) +Rm(s, g)andAppendix K includes more details.。We observed the train-ing will lead Gllmandπgcompromise to a local optimal,i.e.,Gllmcomforts a high completion ratio for πgbut ne-glect consistency with instructions, and πgsimultaneouslyrewards Gllmwith a high completion ratio.。Furthermore,as the policy training continuing, the evaluation for goalgeneration is out-date.。To fix this issue, we propose a pe-riodic reset for the RLAF, i.e., the parameters of the Gllmwill be reset for every set number of steps to avoid beingtrapped in a local convergence, achieving enhanced goalcompletion, and keeping goals consistent with human in-Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationstructions.。Considering the training efficiency, we conductLoRA (Hu et al., 2021) to update the model weights forGllm.。Figure 8 illustrates the whole training process, andAlgorithm 1 summarizes the corresponding pseudo-code.5.. \n",
      "the piece 123\n",
      " \n",
      "dicates that a goal is completed orthe reachable probability from current state; (2) Retheevaluation of crucial sub-goal completion, which involvesexamining cases by pairing instructions in a batch with a setof essential sub-goals; (3) Rmthe evaluation of outputtingthe proper goal format, with the LLM being penalized basedon edit distance.。Then, we can express the reward func-tion as R(s, ι, g ) =Rf(s, g) +Re(s, g) +Rm(s, g)andAppendix K includes more details.。We observed the train-ing will lead Gllmandπgcompromise to a local optimal,i.e.,Gllmcomforts a high completion ratio for πgbut ne-glect consistency with instructions, and πgsimultaneouslyrewards Gllmwith a high completion ratio.。Furthermore,as the policy training continuing, the evaluation for goalgeneration is out-date.。To fix this issue, we propose a pe-riodic reset for the RLAF, i.e., the parameters of the Gllmwill be reset for every set number of steps to avoid beingtrapped in a local convergence, achieving enhanced goalcompletion, and keeping goals consistent with human in-Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationstructions.。Considering the training efficiency, we conductLoRA (Hu et al., 2021) to update the model weights forGllm.。Figure 8 illustrates the whole training process, andAlgorithm 1 summarizes the corresponding pseudo-code.5.。ExperimentWe conduct empirical experiments to evaluate the efficacy ofboth stages of our proposed OpenPAL.. \n",
      "the piece 124\n",
      " \n",
      "We observed the train-ing will lead Gllmandπgcompromise to a local optimal,i.e.,Gllmcomforts a high completion ratio for πgbut ne-glect consistency with instructions, and πgsimultaneouslyrewards Gllmwith a high completion ratio.。Furthermore,as the policy training continuing, the evaluation for goalgeneration is out-date.。To fix this issue, we propose a pe-riodic reset for the RLAF, i.e., the parameters of the Gllmwill be reset for every set number of steps to avoid beingtrapped in a local convergence, achieving enhanced goalcompletion, and keeping goals consistent with human in-Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationstructions.。Considering the training efficiency, we conductLoRA (Hu et al., 2021) to update the model weights forGllm.。Figure 8 illustrates the whole training process, andAlgorithm 1 summarizes the corresponding pseudo-code.5.。ExperimentWe conduct empirical experiments to evaluate the efficacy ofboth stages of our proposed OpenPAL.。To make the Contrasatisfy the learning requirements, we give well-designedspaces and reward functions as follows.Observation Space.. \n",
      "the piece 125\n",
      " \n",
      "ut ne-glect consistency with instructions, and πgsimultaneouslyrewards Gllmwith a high completion ratio.。Furthermore,as the policy training continuing, the evaluation for goalgeneration is out-date.。To fix this issue, we propose a pe-riodic reset for the RLAF, i.e., the parameters of the Gllmwill be reset for every set number of steps to avoid beingtrapped in a local convergence, achieving enhanced goalcompletion, and keeping goals consistent with human in-Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationstructions.。Considering the training efficiency, we conductLoRA (Hu et al., 2021) to update the model weights forGllm.。Figure 8 illustrates the whole training process, andAlgorithm 1 summarizes the corresponding pseudo-code.5.。ExperimentWe conduct empirical experiments to evaluate the efficacy ofboth stages of our proposed OpenPAL.。To make the Contrasatisfy the learning requirements, we give well-designedspaces and reward functions as follows.Observation Space.。The observation space encompassesmany factors, such as unit features detailing the agent states,those of other players and environmental features captur-ing interaction events.. \n",
      "the piece 126\n",
      " \n",
      " pe-riodic reset for the RLAF, i.e., the parameters of the Gllmwill be reset for every set number of steps to avoid beingtrapped in a local convergence, achieving enhanced goalcompletion, and keeping goals consistent with human in-Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationstructions.。Considering the training efficiency, we conductLoRA (Hu et al., 2021) to update the model weights forGllm.。Figure 8 illustrates the whole training process, andAlgorithm 1 summarizes the corresponding pseudo-code.5.。ExperimentWe conduct empirical experiments to evaluate the efficacy ofboth stages of our proposed OpenPAL.。To make the Contrasatisfy the learning requirements, we give well-designedspaces and reward functions as follows.Observation Space.。The observation space encompassesmany factors, such as unit features detailing the agent states,those of other players and environmental features captur-ing interaction events.。Additionally, an agent-centric RGBbird’s-eye-view (BEV) of the local environment is consid-ered.. \n",
      "the piece 127\n",
      " \n",
      "er of steps to avoid beingtrapped in a local convergence, achieving enhanced goalcompletion, and keeping goals consistent with human in-Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationstructions.。Considering the training efficiency, we conductLoRA (Hu et al., 2021) to update the model weights forGllm.。Figure 8 illustrates the whole training process, andAlgorithm 1 summarizes the corresponding pseudo-code.5.。ExperimentWe conduct empirical experiments to evaluate the efficacy ofboth stages of our proposed OpenPAL.。To make the Contrasatisfy the learning requirements, we give well-designedspaces and reward functions as follows.Observation Space.。The observation space encompassesmany factors, such as unit features detailing the agent states,those of other players and environmental features captur-ing interaction events.。Additionally, an agent-centric RGBbird’s-eye-view (BEV) of the local environment is consid-ered.。Table 4 includes detailed information.Action Space.. \n",
      "the piece 128\n",
      " \n",
      " the whole training process, andAlgorithm 1 summarizes the corresponding pseudo-code.5.。ExperimentWe conduct empirical experiments to evaluate the efficacy ofboth stages of our proposed OpenPAL.。To make the Contrasatisfy the learning requirements, we give well-designedspaces and reward functions as follows.Observation Space.。The observation space encompassesmany factors, such as unit features detailing the agent states,those of other players and environmental features captur-ing interaction events.。Additionally, an agent-centric RGBbird’s-eye-view (BEV) of the local environment is consid-ered.。Table 4 includes detailed information.Action Space.。The action space is implemented on topof Contra’s micro-operation API, comprising a collectionof multi-grained actions.. \n",
      "the piece 129\n",
      " \n",
      " empirical experiments to evaluate the efficacy ofboth stages of our proposed OpenPAL.。To make the Contrasatisfy the learning requirements, we give well-designedspaces and reward functions as follows.Observation Space.。The observation space encompassesmany factors, such as unit features detailing the agent states,those of other players and environmental features captur-ing interaction events.。Additionally, an agent-centric RGBbird’s-eye-view (BEV) of the local environment is consid-ered.。Table 4 includes detailed information.Action Space.。The action space is implemented on topof Contra’s micro-operation API, comprising a collectionof multi-grained actions.。These actions range from fine-grained movements, such as six-degrees-of-freedom move-ment and weapon usage, to compound actions in coarse-grained categories, such as firing at a target, and each actionis executed over a duration of 200ms, hence the controlfrequency is 5Hz.. \n",
      "the piece 130\n",
      " \n",
      "sfy the learning requirements, we give well-designedspaces and reward functions as follows.Observation Space.。The observation space encompassesmany factors, such as unit features detailing the agent states,those of other players and environmental features captur-ing interaction events.。Additionally, an agent-centric RGBbird’s-eye-view (BEV) of the local environment is consid-ered.。Table 4 includes detailed information.Action Space.。The action space is implemented on topof Contra’s micro-operation API, comprising a collectionof multi-grained actions.。These actions range from fine-grained movements, such as six-degrees-of-freedom move-ment and weapon usage, to compound actions in coarse-grained categories, such as firing at a target, and each actionis executed over a duration of 200ms, hence the controlfrequency is 5Hz.。The total size of the action space is 54.Further details in Table 6.Reward Functions.. \n",
      "the piece 131\n",
      " \n",
      ".。The observation space encompassesmany factors, such as unit features detailing the agent states,those of other players and environmental features captur-ing interaction events.。Additionally, an agent-centric RGBbird’s-eye-view (BEV) of the local environment is consid-ered.。Table 4 includes detailed information.Action Space.。The action space is implemented on topof Contra’s micro-operation API, comprising a collectionof multi-grained actions.。These actions range from fine-grained movements, such as six-degrees-of-freedom move-ment and weapon usage, to compound actions in coarse-grained categories, such as firing at a target, and each actionis executed over a duration of 200ms, hence the controlfrequency is 5Hz.。The total size of the action space is 54.Further details in Table 6.Reward Functions.。A comprehensive representation isemployed for the reward function, considering various fac-tors contributing to goal-conditioned policy learning.. \n",
      "the piece 132\n",
      " \n",
      "ental features captur-ing interaction events.。Additionally, an agent-centric RGBbird’s-eye-view (BEV) of the local environment is consid-ered.。Table 4 includes detailed information.Action Space.。The action space is implemented on topof Contra’s micro-operation API, comprising a collectionof multi-grained actions.。These actions range from fine-grained movements, such as six-degrees-of-freedom move-ment and weapon usage, to compound actions in coarse-grained categories, such as firing at a target, and each actionis executed over a duration of 200ms, hence the controlfrequency is 5Hz.。The total size of the action space is 54.Further details in Table 6.Reward Functions.。A comprehensive representation isemployed for the reward function, considering various fac-tors contributing to goal-conditioned policy learning.。Thesefactors are organized as a linear combination to formulatethe reward function.. \n",
      "the piece 133\n",
      " \n",
      "on.Action Space.。The action space is implemented on topof Contra’s micro-operation API, comprising a collectionof multi-grained actions.。These actions range from fine-grained movements, such as six-degrees-of-freedom move-ment and weapon usage, to compound actions in coarse-grained categories, such as firing at a target, and each actionis executed over a duration of 200ms, hence the controlfrequency is 5Hz.。The total size of the action space is 54.Further details in Table 6.Reward Functions.。A comprehensive representation isemployed for the reward function, considering various fac-tors contributing to goal-conditioned policy learning.。Thesefactors are organized as a linear combination to formulatethe reward function.。Furthermore, we determine the weightsfor the combination with a two-fold principle: (1) assigningweights to reward items based on their scales and empha-sizing important factors; (2) dynamically adjusting weightsin response to learning feedback, such as decreasing or in-creasing the weights of corresponding factors.. \n",
      "the piece 134\n",
      " \n",
      " a collectionof multi-grained actions.。These actions range from fine-grained movements, such as six-degrees-of-freedom move-ment and weapon usage, to compound actions in coarse-grained categories, such as firing at a target, and each actionis executed over a duration of 200ms, hence the controlfrequency is 5Hz.。The total size of the action space is 54.Further details in Table 6.Reward Functions.。A comprehensive representation isemployed for the reward function, considering various fac-tors contributing to goal-conditioned policy learning.。Thesefactors are organized as a linear combination to formulatethe reward function.。Furthermore, we determine the weightsfor the combination with a two-fold principle: (1) assigningweights to reward items based on their scales and empha-sizing important factors; (2) dynamically adjusting weightsin response to learning feedback, such as decreasing or in-creasing the weights of corresponding factors.。Additionalinformation is available in Appendix A.5.1.. \n",
      "the piece 135\n",
      " \n",
      "range from fine-grained movements, such as six-degrees-of-freedom move-ment and weapon usage, to compound actions in coarse-grained categories, such as firing at a target, and each actionis executed over a duration of 200ms, hence the controlfrequency is 5Hz.。The total size of the action space is 54.Further details in Table 6.Reward Functions.。A comprehensive representation isemployed for the reward function, considering various fac-tors contributing to goal-conditioned policy learning.。Thesefactors are organized as a linear combination to formulatethe reward function.。Furthermore, we determine the weightsfor the combination with a two-fold principle: (1) assigningweights to reward items based on their scales and empha-sizing important factors; (2) dynamically adjusting weightsin response to learning feedback, such as decreasing or in-creasing the weights of corresponding factors.。Additionalinformation is available in Appendix A.5.1.。Evaluating Goal-conditioned RLWe evaluate the πgof stage I from three distinct perspectivesto verify the open-endedness achieved on G: (1) the comple-tion ratio, (2) generalization capability concerning unseengoals, and (3) robustness when integrating goal-conditionedlearning atop non-goal learning.. \n",
      "the piece 136\n",
      " \n",
      "se-grained categories, such as firing at a target, and each actionis executed over a duration of 200ms, hence the controlfrequency is 5Hz.。The total size of the action space is 54.Further details in Table 6.Reward Functions.。A comprehensive representation isemployed for the reward function, considering various fac-tors contributing to goal-conditioned policy learning.。Thesefactors are organized as a linear combination to formulatethe reward function.。Furthermore, we determine the weightsfor the combination with a two-fold principle: (1) assigningweights to reward items based on their scales and empha-sizing important factors; (2) dynamically adjusting weightsin response to learning feedback, such as decreasing or in-creasing the weights of corresponding factors.。Additionalinformation is available in Appendix A.5.1.。Evaluating Goal-conditioned RLWe evaluate the πgof stage I from three distinct perspectivesto verify the open-endedness achieved on G: (1) the comple-tion ratio, (2) generalization capability concerning unseengoals, and (3) robustness when integrating goal-conditionedlearning atop non-goal learning.。Given that GCRL in Open-PAL comprises random and hindsight stages, our evaluationinvolves a comparative analysis with a baseline, HER , i.e.,training the RL agent with hindsight goal generation.. \n",
      "the piece 137\n",
      " \n",
      "reward function, considering various fac-tors contributing to goal-conditioned policy learning.。Thesefactors are organized as a linear combination to formulatethe reward function.。Furthermore, we determine the weightsfor the combination with a two-fold principle: (1) assigningweights to reward items based on their scales and empha-sizing important factors; (2) dynamically adjusting weightsin response to learning feedback, such as decreasing or in-creasing the weights of corresponding factors.。Additionalinformation is available in Appendix A.5.1.。Evaluating Goal-conditioned RLWe evaluate the πgof stage I from three distinct perspectivesto verify the open-endedness achieved on G: (1) the comple-tion ratio, (2) generalization capability concerning unseengoals, and (3) robustness when integrating goal-conditionedlearning atop non-goal learning.。Given that GCRL in Open-PAL comprises random and hindsight stages, our evaluationinvolves a comparative analysis with a baseline, HER , i.e.,training the RL agent with hindsight goal generation.。Fig-ure 2(a) presents a comparison of the goal completion ratioacross different methods on a validation dataset where goalsare generated using GrndandGop.. \n",
      "the piece 138\n",
      " \n",
      "earning.。Thesefactors are organized as a linear combination to formulatethe reward function.。Furthermore, we determine the weightsfor the combination with a two-fold principle: (1) assigningweights to reward items based on their scales and empha-sizing important factors; (2) dynamically adjusting weightsin response to learning feedback, such as decreasing or in-creasing the weights of corresponding factors.。Additionalinformation is available in Appendix A.5.1.。Evaluating Goal-conditioned RLWe evaluate the πgof stage I from three distinct perspectivesto verify the open-endedness achieved on G: (1) the comple-tion ratio, (2) generalization capability concerning unseengoals, and (3) robustness when integrating goal-conditionedlearning atop non-goal learning.。Given that GCRL in Open-PAL comprises random and hindsight stages, our evaluationinvolves a comparative analysis with a baseline, HER , i.e.,training the RL agent with hindsight goal generation.。Fig-ure 2(a) presents a comparison of the goal completion ratioacross different methods on a validation dataset where goalsare generated using GrndandGop.。As depicted in Fig-ure 2(a), our method surpasses HER by ≈3.4%.. \n",
      "the piece 139\n",
      " \n",
      "on with a two-fold principle: (1) assigningweights to reward items based on their scales and empha-sizing important factors; (2) dynamically adjusting weightsin response to learning feedback, such as decreasing or in-creasing the weights of corresponding factors.。Additionalinformation is available in Appendix A.5.1.。Evaluating Goal-conditioned RLWe evaluate the πgof stage I from three distinct perspectivesto verify the open-endedness achieved on G: (1) the comple-tion ratio, (2) generalization capability concerning unseengoals, and (3) robustness when integrating goal-conditionedlearning atop non-goal learning.。Given that GCRL in Open-PAL comprises random and hindsight stages, our evaluationinvolves a comparative analysis with a baseline, HER , i.e.,training the RL agent with hindsight goal generation.。Fig-ure 2(a) presents a comparison of the goal completion ratioacross different methods on a validation dataset where goalsare generated using GrndandGop.。As depicted in Fig-ure 2(a), our method surpasses HER by ≈3.4%.。Figure 2(b)evaluates the generalization on unseen goals, addressing thesecond aspect mentioned earlier.. \n",
      "the piece 140\n",
      " \n",
      "les and empha-sizing important factors; (2) dynamically adjusting weightsin response to learning feedback, such as decreasing or in-creasing the weights of corresponding factors.。Additionalinformation is available in Appendix A.5.1.。Evaluating Goal-conditioned RLWe evaluate the πgof stage I from three distinct perspectivesto verify the open-endedness achieved on G: (1) the comple-tion ratio, (2) generalization capability concerning unseengoals, and (3) robustness when integrating goal-conditionedlearning atop non-goal learning.。Given that GCRL in Open-PAL comprises random and hindsight stages, our evaluationinvolves a comparative analysis with a baseline, HER , i.e.,training the RL agent with hindsight goal generation.。Fig-ure 2(a) presents a comparison of the goal completion ratioacross different methods on a validation dataset where goalsare generated using GrndandGop.。As depicted in Fig-ure 2(a), our method surpasses HER by ≈3.4%.。Figure 2(b)evaluates the generalization on unseen goals, addressing thesecond aspect mentioned earlier.。It is noteworthy that theunseen goals are re-combinations of goals obtained withHER and Gllm.. \n",
      "the piece 141\n",
      " \n",
      "tivesto verify the open-endedness achieved on G: (1) the comple-tion ratio, (2) generalization capability concerning unseengoals, and (3) robustness when integrating goal-conditionedlearning atop non-goal learning.。Given that GCRL in Open-PAL comprises random and hindsight stages, our evaluationinvolves a comparative analysis with a baseline, HER , i.e.,training the RL agent with hindsight goal generation.。Fig-ure 2(a) presents a comparison of the goal completion ratioacross different methods on a validation dataset where goalsare generated using GrndandGop.。As depicted in Fig-ure 2(a), our method surpasses HER by ≈3.4%.。Figure 2(b)evaluates the generalization on unseen goals, addressing thesecond aspect mentioned earlier.。It is noteworthy that theunseen goals are re-combinations of goals obtained withHER and Gllm.。As indicated in Figure 2(b), our methodexcels over the baseline in terms of completion ratio.. \n",
      "the piece 142\n",
      " \n",
      "e comple-tion ratio, (2) generalization capability concerning unseengoals, and (3) robustness when integrating goal-conditionedlearning atop non-goal learning.。Given that GCRL in Open-PAL comprises random and hindsight stages, our evaluationinvolves a comparative analysis with a baseline, HER , i.e.,training the RL agent with hindsight goal generation.。Fig-ure 2(a) presents a comparison of the goal completion ratioacross different methods on a validation dataset where goalsare generated using GrndandGop.。As depicted in Fig-ure 2(a), our method surpasses HER by ≈3.4%.。Figure 2(b)evaluates the generalization on unseen goals, addressing thesecond aspect mentioned earlier.。It is noteworthy that theunseen goals are re-combinations of goals obtained withHER and Gllm.。As indicated in Figure 2(b), our methodexcels over the baseline in terms of completion ratio.。Fig-ure 2(c) answers the third point by comparing the use ofKL-divergence regularizer for policy learning, consideringchanges in overall performance and the ability to eliminateenemies.. \n",
      "the piece 143\n",
      " \n",
      "raining the RL agent with hindsight goal generation.。Fig-ure 2(a) presents a comparison of the goal completion ratioacross different methods on a validation dataset where goalsare generated using GrndandGop.。As depicted in Fig-ure 2(a), our method surpasses HER by ≈3.4%.。Figure 2(b)evaluates the generalization on unseen goals, addressing thesecond aspect mentioned earlier.。It is noteworthy that theunseen goals are re-combinations of goals obtained withHER and Gllm.。As indicated in Figure 2(b), our methodexcels over the baseline in terms of completion ratio.。Fig-ure 2(c) answers the third point by comparing the use ofKL-divergence regularizer for policy learning, consideringchanges in overall performance and the ability to eliminateenemies.。Three metrics are designed for evaluation: (1)Mean basic reward per step, which indicates whether thecurrent policy degenerates in performing basic skills perstep against a well-trained non-goal policy, and intentionalto emphasize the agent’s immediate responsiveness overfinal results; (2) #Enemies killed, representing the averagenumber of enemies killed by the agent per episode; and (3)#Enemies knocked down, representing the average numberof enemies knocked down by the agent per episode.5.2.. \n",
      "the piece 144\n",
      " \n",
      "GrndandGop.。As depicted in Fig-ure 2(a), our method surpasses HER by ≈3.4%.。Figure 2(b)evaluates the generalization on unseen goals, addressing thesecond aspect mentioned earlier.。It is noteworthy that theunseen goals are re-combinations of goals obtained withHER and Gllm.。As indicated in Figure 2(b), our methodexcels over the baseline in terms of completion ratio.。Fig-ure 2(c) answers the third point by comparing the use ofKL-divergence regularizer for policy learning, consideringchanges in overall performance and the ability to eliminateenemies.。Three metrics are designed for evaluation: (1)Mean basic reward per step, which indicates whether thecurrent policy degenerates in performing basic skills perstep against a well-trained non-goal policy, and intentionalto emphasize the agent’s immediate responsiveness overfinal results; (2) #Enemies killed, representing the averagenumber of enemies killed by the agent per episode; and (3)#Enemies knocked down, representing the average numberof enemies knocked down by the agent per episode.5.2.。Evaluating LLM-based Goal GenerationWe conducted evaluation of Gllmthrough two compara-tive experiments on GPT-4-generated instruction datasets,aiming to investigate the impact of different instructiondatasets and fine-tuning paradigms.. \n",
      "the piece 145\n",
      " \n",
      "pect mentioned earlier.。It is noteworthy that theunseen goals are re-combinations of goals obtained withHER and Gllm.。As indicated in Figure 2(b), our methodexcels over the baseline in terms of completion ratio.。Fig-ure 2(c) answers the third point by comparing the use ofKL-divergence regularizer for policy learning, consideringchanges in overall performance and the ability to eliminateenemies.。Three metrics are designed for evaluation: (1)Mean basic reward per step, which indicates whether thecurrent policy degenerates in performing basic skills perstep against a well-trained non-goal policy, and intentionalto emphasize the agent’s immediate responsiveness overfinal results; (2) #Enemies killed, representing the averagenumber of enemies killed by the agent per episode; and (3)#Enemies knocked down, representing the average numberof enemies knocked down by the agent per episode.5.2.。Evaluating LLM-based Goal GenerationWe conducted evaluation of Gllmthrough two compara-tive experiments on GPT-4-generated instruction datasets,aiming to investigate the impact of different instructiondatasets and fine-tuning paradigms.。The evaluation metricsemployed encompass precision, recall, and F1 score.. \n",
      "the piece 146\n",
      " \n",
      " re-combinations of goals obtained withHER and Gllm.。As indicated in Figure 2(b), our methodexcels over the baseline in terms of completion ratio.。Fig-ure 2(c) answers the third point by comparing the use ofKL-divergence regularizer for policy learning, consideringchanges in overall performance and the ability to eliminateenemies.。Three metrics are designed for evaluation: (1)Mean basic reward per step, which indicates whether thecurrent policy degenerates in performing basic skills perstep against a well-trained non-goal policy, and intentionalto emphasize the agent’s immediate responsiveness overfinal results; (2) #Enemies killed, representing the averagenumber of enemies killed by the agent per episode; and (3)#Enemies knocked down, representing the average numberof enemies knocked down by the agent per episode.5.2.。Evaluating LLM-based Goal GenerationWe conducted evaluation of Gllmthrough two compara-tive experiments on GPT-4-generated instruction datasets,aiming to investigate the impact of different instructiondatasets and fine-tuning paradigms.。The evaluation metricsemployed encompass precision, recall, and F1 score.。It’sworth noting that a potential issue in determining the pre-cision of generating sub-goals that are close in semantics.For instance, associating the sub-goal “moving speed” val-ues “very fast” versus “fast” may be perceived as a negativeinstance under precision measurement.. \n",
      "the piece 147\n",
      " \n",
      "he baseline in terms of completion ratio.。Fig-ure 2(c) answers the third point by comparing the use ofKL-divergence regularizer for policy learning, consideringchanges in overall performance and the ability to eliminateenemies.。Three metrics are designed for evaluation: (1)Mean basic reward per step, which indicates whether thecurrent policy degenerates in performing basic skills perstep against a well-trained non-goal policy, and intentionalto emphasize the agent’s immediate responsiveness overfinal results; (2) #Enemies killed, representing the averagenumber of enemies killed by the agent per episode; and (3)#Enemies knocked down, representing the average numberof enemies knocked down by the agent per episode.5.2.。Evaluating LLM-based Goal GenerationWe conducted evaluation of Gllmthrough two compara-tive experiments on GPT-4-generated instruction datasets,aiming to investigate the impact of different instructiondatasets and fine-tuning paradigms.。The evaluation metricsemployed encompass precision, recall, and F1 score.。It’sworth noting that a potential issue in determining the pre-cision of generating sub-goals that are close in semantics.For instance, associating the sub-goal “moving speed” val-ues “very fast” versus “fast” may be perceived as a negativeinstance under precision measurement.。Consequently, weargue that the generation of such sub-goals should weighmore in choosing sub-goal than determining values.. \n",
      "the piece 148\n",
      " \n",
      " use ofKL-divergence regularizer for policy learning, consideringchanges in overall performance and the ability to eliminateenemies.。Three metrics are designed for evaluation: (1)Mean basic reward per step, which indicates whether thecurrent policy degenerates in performing basic skills perstep against a well-trained non-goal policy, and intentionalto emphasize the agent’s immediate responsiveness overfinal results; (2) #Enemies killed, representing the averagenumber of enemies killed by the agent per episode; and (3)#Enemies knocked down, representing the average numberof enemies knocked down by the agent per episode.5.2.。Evaluating LLM-based Goal GenerationWe conducted evaluation of Gllmthrough two compara-tive experiments on GPT-4-generated instruction datasets,aiming to investigate the impact of different instructiondatasets and fine-tuning paradigms.。The evaluation metricsemployed encompass precision, recall, and F1 score.。It’sworth noting that a potential issue in determining the pre-cision of generating sub-goals that are close in semantics.For instance, associating the sub-goal “moving speed” val-ues “very fast” versus “fast” may be perceived as a negativeinstance under precision measurement.。Consequently, weargue that the generation of such sub-goals should weighmore in choosing sub-goal than determining values.。Thus,we further propose three choice-based metrics: precision(choice), recall (choice), and F1 (choice).. \n",
      "the piece 149\n",
      " \n",
      " and the ability to eliminateenemies.。Three metrics are designed for evaluation: (1)Mean basic reward per step, which indicates whether thecurrent policy degenerates in performing basic skills perstep against a well-trained non-goal policy, and intentionalto emphasize the agent’s immediate responsiveness overfinal results; (2) #Enemies killed, representing the averagenumber of enemies killed by the agent per episode; and (3)#Enemies knocked down, representing the average numberof enemies knocked down by the agent per episode.5.2.。Evaluating LLM-based Goal GenerationWe conducted evaluation of Gllmthrough two compara-tive experiments on GPT-4-generated instruction datasets,aiming to investigate the impact of different instructiondatasets and fine-tuning paradigms.。The evaluation metricsemployed encompass precision, recall, and F1 score.。It’sworth noting that a potential issue in determining the pre-cision of generating sub-goals that are close in semantics.For instance, associating the sub-goal “moving speed” val-ues “very fast” versus “fast” may be perceived as a negativeinstance under precision measurement.。Consequently, weargue that the generation of such sub-goals should weighmore in choosing sub-goal than determining values.。Thus,we further propose three choice-based metrics: precision(choice), recall (choice), and F1 (choice).。Table 1 providesDataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)HI 0.435 0.611 0.361 0.517 0.395 0.560AI 0.474 0.611 0.419 0.532 0.445 0.569SI 0.444 0.601 0.413 0.539 0.428 0.568RI 0.499 0.633 0.414 0.526 0.453 0.574ALL 0.555 0.685 0.505 0.621 0.529 0.652Table 1: Evaluation on different datasets.. \n",
      "the piece 150\n",
      " \n",
      "kills perstep against a well-trained non-goal policy, and intentionalto emphasize the agent’s immediate responsiveness overfinal results; (2) #Enemies killed, representing the averagenumber of enemies killed by the agent per episode; and (3)#Enemies knocked down, representing the average numberof enemies knocked down by the agent per episode.5.2.。Evaluating LLM-based Goal GenerationWe conducted evaluation of Gllmthrough two compara-tive experiments on GPT-4-generated instruction datasets,aiming to investigate the impact of different instructiondatasets and fine-tuning paradigms.。The evaluation metricsemployed encompass precision, recall, and F1 score.。It’sworth noting that a potential issue in determining the pre-cision of generating sub-goals that are close in semantics.For instance, associating the sub-goal “moving speed” val-ues “very fast” versus “fast” may be perceived as a negativeinstance under precision measurement.。Consequently, weargue that the generation of such sub-goals should weighmore in choosing sub-goal than determining values.。Thus,we further propose three choice-based metrics: precision(choice), recall (choice), and F1 (choice).。Table 1 providesDataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)HI 0.435 0.611 0.361 0.517 0.395 0.560AI 0.474 0.611 0.419 0.532 0.445 0.569SI 0.444 0.601 0.413 0.539 0.428 0.568RI 0.499 0.633 0.414 0.526 0.453 0.574ALL 0.555 0.685 0.505 0.621 0.529 0.652Table 1: Evaluation on different datasets.。“ALL” the propor-tional mixture of the four base datasets.a comparison of five types of instruction datasets used in themulti-step fine-tuning process for Gllm.. \n",
      "the piece 151\n",
      " \n",
      " to investigate the impact of different instructiondatasets and fine-tuning paradigms.。The evaluation metricsemployed encompass precision, recall, and F1 score.。It’sworth noting that a potential issue in determining the pre-cision of generating sub-goals that are close in semantics.For instance, associating the sub-goal “moving speed” val-ues “very fast” versus “fast” may be perceived as a negativeinstance under precision measurement.。Consequently, weargue that the generation of such sub-goals should weighmore in choosing sub-goal than determining values.。Thus,we further propose three choice-based metrics: precision(choice), recall (choice), and F1 (choice).。Table 1 providesDataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)HI 0.435 0.611 0.361 0.517 0.395 0.560AI 0.474 0.611 0.419 0.532 0.445 0.569SI 0.444 0.601 0.413 0.539 0.428 0.568RI 0.499 0.633 0.414 0.526 0.453 0.574ALL 0.555 0.685 0.505 0.621 0.529 0.652Table 1: Evaluation on different datasets.。“ALL” the propor-tional mixture of the four base datasets.a comparison of five types of instruction datasets used in themulti-step fine-tuning process for Gllm.。The comparisonreveals that utilizing a mixture significantly outperformsindividual base datasets, which indicates a mixture aidsGllmin capturing human preferences and understandingthe implications of each abstracted state, thereby enhancinggoal generation.. \n",
      "the piece 152\n",
      " \n",
      "rating sub-goals that are close in semantics.For instance, associating the sub-goal “moving speed” val-ues “very fast” versus “fast” may be perceived as a negativeinstance under precision measurement.。Consequently, weargue that the generation of such sub-goals should weighmore in choosing sub-goal than determining values.。Thus,we further propose three choice-based metrics: precision(choice), recall (choice), and F1 (choice).。Table 1 providesDataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)HI 0.435 0.611 0.361 0.517 0.395 0.560AI 0.474 0.611 0.419 0.532 0.445 0.569SI 0.444 0.601 0.413 0.539 0.428 0.568RI 0.499 0.633 0.414 0.526 0.453 0.574ALL 0.555 0.685 0.505 0.621 0.529 0.652Table 1: Evaluation on different datasets.。“ALL” the propor-tional mixture of the four base datasets.a comparison of five types of instruction datasets used in themulti-step fine-tuning process for Gllm.。The comparisonreveals that utilizing a mixture significantly outperformsindividual base datasets, which indicates a mixture aidsGllmin capturing human preferences and understandingthe implications of each abstracted state, thereby enhancinggoal generation.。Table 2 compares four kinds of fine-tuningwith the proposed multi-step fine-tuning, including (1) SFT:Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 1 COLLABORATIVE TRAINING1:Input: θthe parameters of πg,θ;β={βllm, βLoRA}forGllm,β ,βllmthe pre-trained parameters of Gllm;βLoRA thefine-tuned LoRA parameters of Gllm;I=IH∪ IS∪ IA∪ IRthe instruction set2:Reloading θ,βllm, merging βLoRA intoβllm3:forloop=1, 2, ... do4: Initialize a new βLoRA andβ={βllm, βLoRA}5: foriteration=1, 2, ..., n do6: Agents from a batch of workers send states {sj|j= 1, .. \n",
      "the piece 153\n",
      " \n",
      "sub-goal “moving speed” val-ues “very fast” versus “fast” may be perceived as a negativeinstance under precision measurement.。Consequently, weargue that the generation of such sub-goals should weighmore in choosing sub-goal than determining values.。Thus,we further propose three choice-based metrics: precision(choice), recall (choice), and F1 (choice).。Table 1 providesDataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)HI 0.435 0.611 0.361 0.517 0.395 0.560AI 0.474 0.611 0.419 0.532 0.445 0.569SI 0.444 0.601 0.413 0.539 0.428 0.568RI 0.499 0.633 0.414 0.526 0.453 0.574ALL 0.555 0.685 0.505 0.621 0.529 0.652Table 1: Evaluation on different datasets.。“ALL” the propor-tional mixture of the four base datasets.a comparison of five types of instruction datasets used in themulti-step fine-tuning process for Gllm.。The comparisonreveals that utilizing a mixture significantly outperformsindividual base datasets, which indicates a mixture aidsGllmin capturing human preferences and understandingthe implications of each abstracted state, thereby enhancinggoal generation.。Table 2 compares four kinds of fine-tuningwith the proposed multi-step fine-tuning, including (1) SFT:Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 1 COLLABORATIVE TRAINING1:Input: θthe parameters of πg,θ;β={βllm, βLoRA}forGllm,β ,βllmthe pre-trained parameters of Gllm;βLoRA thefine-tuned LoRA parameters of Gllm;I=IH∪ IS∪ IA∪ IRthe instruction set2:Reloading θ,βllm, merging βLoRA intoβllm3:forloop=1, 2, ... do4: Initialize a new βLoRA andβ={βllm, βLoRA}5: foriteration=1, 2, ..., n do6: Agents from a batch of workers send states {sj|j= 1, .。.. \n",
      "the piece 154\n",
      " \n",
      "choice-based metrics: precision(choice), recall (choice), and F1 (choice).。Table 1 providesDataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)HI 0.435 0.611 0.361 0.517 0.395 0.560AI 0.474 0.611 0.419 0.532 0.445 0.569SI 0.444 0.601 0.413 0.539 0.428 0.568RI 0.499 0.633 0.414 0.526 0.453 0.574ALL 0.555 0.685 0.505 0.621 0.529 0.652Table 1: Evaluation on different datasets.。“ALL” the propor-tional mixture of the four base datasets.a comparison of five types of instruction datasets used in themulti-step fine-tuning process for Gllm.。The comparisonreveals that utilizing a mixture significantly outperformsindividual base datasets, which indicates a mixture aidsGllmin capturing human preferences and understandingthe implications of each abstracted state, thereby enhancinggoal generation.。Table 2 compares four kinds of fine-tuningwith the proposed multi-step fine-tuning, including (1) SFT:Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 1 COLLABORATIVE TRAINING1:Input: θthe parameters of πg,θ;β={βllm, βLoRA}forGllm,β ,βllmthe pre-trained parameters of Gllm;βLoRA thefine-tuned LoRA parameters of Gllm;I=IH∪ IS∪ IA∪ IRthe instruction set2:Reloading θ,βllm, merging βLoRA intoβllm3:forloop=1, 2, ... do4: Initialize a new βLoRA andβ={βllm, βLoRA}5: foriteration=1, 2, ..., n do6: Agents from a batch of workers send states {sj|j= 1, .。.。.. \n",
      "the piece 155\n",
      " \n",
      ")RecallRecall(Choice)F1F1(Choice)HI 0.435 0.611 0.361 0.517 0.395 0.560AI 0.474 0.611 0.419 0.532 0.445 0.569SI 0.444 0.601 0.413 0.539 0.428 0.568RI 0.499 0.633 0.414 0.526 0.453 0.574ALL 0.555 0.685 0.505 0.621 0.529 0.652Table 1: Evaluation on different datasets.。“ALL” the propor-tional mixture of the four base datasets.a comparison of five types of instruction datasets used in themulti-step fine-tuning process for Gllm.。The comparisonreveals that utilizing a mixture significantly outperformsindividual base datasets, which indicates a mixture aidsGllmin capturing human preferences and understandingthe implications of each abstracted state, thereby enhancinggoal generation.。Table 2 compares four kinds of fine-tuningwith the proposed multi-step fine-tuning, including (1) SFT:Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 1 COLLABORATIVE TRAINING1:Input: θthe parameters of πg,θ;β={βllm, βLoRA}forGllm,β ,βllmthe pre-trained parameters of Gllm;βLoRA thefine-tuned LoRA parameters of Gllm;I=IH∪ IS∪ IA∪ IRthe instruction set2:Reloading θ,βllm, merging βLoRA intoβllm3:forloop=1, 2, ... do4: Initialize a new βLoRA andβ={βllm, βLoRA}5: foriteration=1, 2, ..., n do6: Agents from a batch of workers send states {sj|j= 1, .。.。.。, m }toGllm7: Random sample a batch of instructions: Itrain ={ιj|j= 1, .. \n",
      "the piece 156\n",
      " \n",
      "569SI 0.444 0.601 0.413 0.539 0.428 0.568RI 0.499 0.633 0.414 0.526 0.453 0.574ALL 0.555 0.685 0.505 0.621 0.529 0.652Table 1: Evaluation on different datasets.。“ALL” the propor-tional mixture of the four base datasets.a comparison of five types of instruction datasets used in themulti-step fine-tuning process for Gllm.。The comparisonreveals that utilizing a mixture significantly outperformsindividual base datasets, which indicates a mixture aidsGllmin capturing human preferences and understandingthe implications of each abstracted state, thereby enhancinggoal generation.。Table 2 compares four kinds of fine-tuningwith the proposed multi-step fine-tuning, including (1) SFT:Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 1 COLLABORATIVE TRAINING1:Input: θthe parameters of πg,θ;β={βllm, βLoRA}forGllm,β ,βllmthe pre-trained parameters of Gllm;βLoRA thefine-tuned LoRA parameters of Gllm;I=IH∪ IS∪ IA∪ IRthe instruction set2:Reloading θ,βllm, merging βLoRA intoβllm3:forloop=1, 2, ... do4: Initialize a new βLoRA andβ={βllm, βLoRA}5: foriteration=1, 2, ..., n do6: Agents from a batch of workers send states {sj|j= 1, .。.。.。, m }toGllm7: Random sample a batch of instructions: Itrain ={ιj|j= 1, .。.. \n",
      "the piece 157\n",
      " \n",
      "llm.。The comparisonreveals that utilizing a mixture significantly outperformsindividual base datasets, which indicates a mixture aidsGllmin capturing human preferences and understandingthe implications of each abstracted state, thereby enhancinggoal generation.。Table 2 compares four kinds of fine-tuningwith the proposed multi-step fine-tuning, including (1) SFT:Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 1 COLLABORATIVE TRAINING1:Input: θthe parameters of πg,θ;β={βllm, βLoRA}forGllm,β ,βllmthe pre-trained parameters of Gllm;βLoRA thefine-tuned LoRA parameters of Gllm;I=IH∪ IS∪ IA∪ IRthe instruction set2:Reloading θ,βllm, merging βLoRA intoβllm3:forloop=1, 2, ... do4: Initialize a new βLoRA andβ={βllm, βLoRA}5: foriteration=1, 2, ..., n do6: Agents from a batch of workers send states {sj|j= 1, .。.。.。, m }toGllm7: Random sample a batch of instructions: Itrain ={ιj|j= 1, .。.。.. \n",
      "the piece 158\n",
      " \n",
      "ences and understandingthe implications of each abstracted state, thereby enhancinggoal generation.。Table 2 compares four kinds of fine-tuningwith the proposed multi-step fine-tuning, including (1) SFT:Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 1 COLLABORATIVE TRAINING1:Input: θthe parameters of πg,θ;β={βllm, βLoRA}forGllm,β ,βllmthe pre-trained parameters of Gllm;βLoRA thefine-tuned LoRA parameters of Gllm;I=IH∪ IS∪ IA∪ IRthe instruction set2:Reloading θ,βllm, merging βLoRA intoβllm3:forloop=1, 2, ... do4: Initialize a new βLoRA andβ={βllm, βLoRA}5: foriteration=1, 2, ..., n do6: Agents from a batch of workers send states {sj|j= 1, .。.。.。, m }toGllm7: Random sample a batch of instructions: Itrain ={ιj|j= 1, .。.。.。, m } ⊂ I8: Generate goals in string with LLM: Gs={gj∼Gllm,β(sj, ιj)|j= 1, .. \n",
      "the piece 159\n",
      " \n",
      "idirectional AdaptationAlgorithm 1 COLLABORATIVE TRAINING1:Input: θthe parameters of πg,θ;β={βllm, βLoRA}forGllm,β ,βllmthe pre-trained parameters of Gllm;βLoRA thefine-tuned LoRA parameters of Gllm;I=IH∪ IS∪ IA∪ IRthe instruction set2:Reloading θ,βllm, merging βLoRA intoβllm3:forloop=1, 2, ... do4: Initialize a new βLoRA andβ={βllm, βLoRA}5: foriteration=1, 2, ..., n do6: Agents from a batch of workers send states {sj|j= 1, .。.。.。, m }toGllm7: Random sample a batch of instructions: Itrain ={ιj|j= 1, .。.。.。, m } ⊂ I8: Generate goals in string with LLM: Gs={gj∼Gllm,β(sj, ιj)|j= 1, .。.. \n",
      "the piece 160\n",
      " \n",
      ".。.。, m }and parse Gsto formatted goals: G9: Distribute Gto agents πg,θ, then collect trajectories {τj}and returns {Rj}to form(τ, R) ={(τj, Rj)|j= 1, .。.。.。, m },where τj={gj, s1, a1, r1, .。.。.。, s Tj, aTj, rTj}, Rj=TjXt=1rt10: Update θwith Equation (6) and (τ, R)11: Filter completed goals GcfromG, extract Rthe set from (τ, R)as agent feedback rewards Rf12: Compute examination reward by evaluating crucial sub-goal completion as Re=REWARD e(Gc,IS∩ Itrain,G),and formatting reward: Rm=REWARD m(gs, g), refer to Appendix K13: Update βLoRA with PPO, and merge updated βLoRA intoβllm14: end for15:end forTuning PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)SFT 0.547 0.663 0.490 0.602 0.517 0.632CoTF 0.533 0.652 0.487 0.599 0.509 0.624CoTF→SFT 0.555 0.685 0.505 0.621 0.529 0.652CoTF→SFT→EFT 0.590 0.694 0.501 0.593 0.516 0.629Table 2: Evaluation on different tuning methods.only use the target prompt without CoT data to supervisedfine-tuning, which can be regarded as a baseline for a naiveSFT; (2) CoTF: only CoT-assisted fine-tuning; (3) CoTF→SFT: further SFT target prompt after CoTF; (4) CoTF→SFT→EFT: further ensemble fine-tuning target promptafter CoTF.. \n",
      "the piece 161\n",
      " \n",
      "。, m }and parse Gsto formatted goals: G9: Distribute Gto agents πg,θ, then collect trajectories {τj}and returns {Rj}to form(τ, R) ={(τj, Rj)|j= 1, .。.。.。, m },where τj={gj, s1, a1, r1, .。.。.。, s Tj, aTj, rTj}, Rj=TjXt=1rt10: Update θwith Equation (6) and (τ, R)11: Filter completed goals GcfromG, extract Rthe set from (τ, R)as agent feedback rewards Rf12: Compute examination reward by evaluating crucial sub-goal completion as Re=REWARD e(Gc,IS∩ Itrain,G),and formatting reward: Rm=REWARD m(gs, g), refer to Appendix K13: Update βLoRA with PPO, and merge updated βLoRA intoβllm14: end for15:end forTuning PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)SFT 0.547 0.663 0.490 0.602 0.517 0.632CoTF 0.533 0.652 0.487 0.599 0.509 0.624CoTF→SFT 0.555 0.685 0.505 0.621 0.529 0.652CoTF→SFT→EFT 0.590 0.694 0.501 0.593 0.516 0.629Table 2: Evaluation on different tuning methods.only use the target prompt without CoT data to supervisedfine-tuning, which can be regarded as a baseline for a naiveSFT; (2) CoTF: only CoT-assisted fine-tuning; (3) CoTF→SFT: further SFT target prompt after CoTF; (4) CoTF→SFT→EFT: further ensemble fine-tuning target promptafter CoTF.。With the comparison, we conclude that CoTFand SFT can improve each other and achieve better per-formance.. \n",
      "the piece 162\n",
      " \n",
      "m }and parse Gsto formatted goals: G9: Distribute Gto agents πg,θ, then collect trajectories {τj}and returns {Rj}to form(τ, R) ={(τj, Rj)|j= 1, .。.。.。, m },where τj={gj, s1, a1, r1, .。.。.。, s Tj, aTj, rTj}, Rj=TjXt=1rt10: Update θwith Equation (6) and (τ, R)11: Filter completed goals GcfromG, extract Rthe set from (τ, R)as agent feedback rewards Rf12: Compute examination reward by evaluating crucial sub-goal completion as Re=REWARD e(Gc,IS∩ Itrain,G),and formatting reward: Rm=REWARD m(gs, g), refer to Appendix K13: Update βLoRA with PPO, and merge updated βLoRA intoβllm14: end for15:end forTuning PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)SFT 0.547 0.663 0.490 0.602 0.517 0.632CoTF 0.533 0.652 0.487 0.599 0.509 0.624CoTF→SFT 0.555 0.685 0.505 0.621 0.529 0.652CoTF→SFT→EFT 0.590 0.694 0.501 0.593 0.516 0.629Table 2: Evaluation on different tuning methods.only use the target prompt without CoT data to supervisedfine-tuning, which can be regarded as a baseline for a naiveSFT; (2) CoTF: only CoT-assisted fine-tuning; (3) CoTF→SFT: further SFT target prompt after CoTF; (4) CoTF→SFT→EFT: further ensemble fine-tuning target promptafter CoTF.。With the comparison, we conclude that CoTFand SFT can improve each other and achieve better per-formance.。Furthermore, ensemble fine-tuning significantlyenhances precision while marginally decreasing recall, mak-ing it more suitable for generating accurate concise goals.5.3.. \n",
      "the piece 163\n",
      " \n",
      "llect trajectories {τj}and returns {Rj}to form(τ, R) ={(τj, Rj)|j= 1, .。.。.。, m },where τj={gj, s1, a1, r1, .。.。.。, s Tj, aTj, rTj}, Rj=TjXt=1rt10: Update θwith Equation (6) and (τ, R)11: Filter completed goals GcfromG, extract Rthe set from (τ, R)as agent feedback rewards Rf12: Compute examination reward by evaluating crucial sub-goal completion as Re=REWARD e(Gc,IS∩ Itrain,G),and formatting reward: Rm=REWARD m(gs, g), refer to Appendix K13: Update βLoRA with PPO, and merge updated βLoRA intoβllm14: end for15:end forTuning PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)SFT 0.547 0.663 0.490 0.602 0.517 0.632CoTF 0.533 0.652 0.487 0.599 0.509 0.624CoTF→SFT 0.555 0.685 0.505 0.621 0.529 0.652CoTF→SFT→EFT 0.590 0.694 0.501 0.593 0.516 0.629Table 2: Evaluation on different tuning methods.only use the target prompt without CoT data to supervisedfine-tuning, which can be regarded as a baseline for a naiveSFT; (2) CoTF: only CoT-assisted fine-tuning; (3) CoTF→SFT: further SFT target prompt after CoTF; (4) CoTF→SFT→EFT: further ensemble fine-tuning target promptafter CoTF.。With the comparison, we conclude that CoTFand SFT can improve each other and achieve better per-formance.。Furthermore, ensemble fine-tuning significantlyenhances precision while marginally decreasing recall, mak-ing it more suitable for generating accurate concise goals.5.3.。Evaluating Co-trainingWe conduct an analysis of the completion ratio correspond-ing to the number of valid sub-goals during the co-trainingprocess.. \n",
      "the piece 164\n",
      " \n",
      "ct trajectories {τj}and returns {Rj}to form(τ, R) ={(τj, Rj)|j= 1, .。.。.。, m },where τj={gj, s1, a1, r1, .。.。.。, s Tj, aTj, rTj}, Rj=TjXt=1rt10: Update θwith Equation (6) and (τ, R)11: Filter completed goals GcfromG, extract Rthe set from (τ, R)as agent feedback rewards Rf12: Compute examination reward by evaluating crucial sub-goal completion as Re=REWARD e(Gc,IS∩ Itrain,G),and formatting reward: Rm=REWARD m(gs, g), refer to Appendix K13: Update βLoRA with PPO, and merge updated βLoRA intoβllm14: end for15:end forTuning PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)SFT 0.547 0.663 0.490 0.602 0.517 0.632CoTF 0.533 0.652 0.487 0.599 0.509 0.624CoTF→SFT 0.555 0.685 0.505 0.621 0.529 0.652CoTF→SFT→EFT 0.590 0.694 0.501 0.593 0.516 0.629Table 2: Evaluation on different tuning methods.only use the target prompt without CoT data to supervisedfine-tuning, which can be regarded as a baseline for a naiveSFT; (2) CoTF: only CoT-assisted fine-tuning; (3) CoTF→SFT: further SFT target prompt after CoTF; (4) CoTF→SFT→EFT: further ensemble fine-tuning target promptafter CoTF.。With the comparison, we conclude that CoTFand SFT can improve each other and achieve better per-formance.。Furthermore, ensemble fine-tuning significantlyenhances precision while marginally decreasing recall, mak-ing it more suitable for generating accurate concise goals.5.3.。Evaluating Co-trainingWe conduct an analysis of the completion ratio correspond-ing to the number of valid sub-goals during the co-trainingprocess.。Though the dimension size of goal space achieves68, the number of sub-goals for valid goals predominantlyfalls within the range of 1 to 7.. \n",
      "the piece 165\n",
      " \n",
      "trajectories {τj}and returns {Rj}to form(τ, R) ={(τj, Rj)|j= 1, .。.。.。, m },where τj={gj, s1, a1, r1, .。.。.。, s Tj, aTj, rTj}, Rj=TjXt=1rt10: Update θwith Equation (6) and (τ, R)11: Filter completed goals GcfromG, extract Rthe set from (τ, R)as agent feedback rewards Rf12: Compute examination reward by evaluating crucial sub-goal completion as Re=REWARD e(Gc,IS∩ Itrain,G),and formatting reward: Rm=REWARD m(gs, g), refer to Appendix K13: Update βLoRA with PPO, and merge updated βLoRA intoβllm14: end for15:end forTuning PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)SFT 0.547 0.663 0.490 0.602 0.517 0.632CoTF 0.533 0.652 0.487 0.599 0.509 0.624CoTF→SFT 0.555 0.685 0.505 0.621 0.529 0.652CoTF→SFT→EFT 0.590 0.694 0.501 0.593 0.516 0.629Table 2: Evaluation on different tuning methods.only use the target prompt without CoT data to supervisedfine-tuning, which can be regarded as a baseline for a naiveSFT; (2) CoTF: only CoT-assisted fine-tuning; (3) CoTF→SFT: further SFT target prompt after CoTF; (4) CoTF→SFT→EFT: further ensemble fine-tuning target promptafter CoTF.。With the comparison, we conclude that CoTFand SFT can improve each other and achieve better per-formance.。Furthermore, ensemble fine-tuning significantlyenhances precision while marginally decreasing recall, mak-ing it more suitable for generating accurate concise goals.5.3.。Evaluating Co-trainingWe conduct an analysis of the completion ratio correspond-ing to the number of valid sub-goals during the co-trainingprocess.。Though the dimension size of goal space achieves68, the number of sub-goals for valid goals predominantlyfalls within the range of 1 to 7.。This is rational as completinga goal with an excessive number of sub-goals is exceedinglychallenging for a policy, even impossibility for human toachieve.. \n",
      "the piece 166\n",
      " \n",
      "ere τj={gj, s1, a1, r1, .。.。.。, s Tj, aTj, rTj}, Rj=TjXt=1rt10: Update θwith Equation (6) and (τ, R)11: Filter completed goals GcfromG, extract Rthe set from (τ, R)as agent feedback rewards Rf12: Compute examination reward by evaluating crucial sub-goal completion as Re=REWARD e(Gc,IS∩ Itrain,G),and formatting reward: Rm=REWARD m(gs, g), refer to Appendix K13: Update βLoRA with PPO, and merge updated βLoRA intoβllm14: end for15:end forTuning PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)SFT 0.547 0.663 0.490 0.602 0.517 0.632CoTF 0.533 0.652 0.487 0.599 0.509 0.624CoTF→SFT 0.555 0.685 0.505 0.621 0.529 0.652CoTF→SFT→EFT 0.590 0.694 0.501 0.593 0.516 0.629Table 2: Evaluation on different tuning methods.only use the target prompt without CoT data to supervisedfine-tuning, which can be regarded as a baseline for a naiveSFT; (2) CoTF: only CoT-assisted fine-tuning; (3) CoTF→SFT: further SFT target prompt after CoTF; (4) CoTF→SFT→EFT: further ensemble fine-tuning target promptafter CoTF.。With the comparison, we conclude that CoTFand SFT can improve each other and achieve better per-formance.。Furthermore, ensemble fine-tuning significantlyenhances precision while marginally decreasing recall, mak-ing it more suitable for generating accurate concise goals.5.3.。Evaluating Co-trainingWe conduct an analysis of the completion ratio correspond-ing to the number of valid sub-goals during the co-trainingprocess.。Though the dimension size of goal space achieves68, the number of sub-goals for valid goals predominantlyfalls within the range of 1 to 7.。This is rational as completinga goal with an excessive number of sub-goals is exceedinglychallenging for a policy, even impossibility for human toachieve.。Furthermore, Figure 3(a) shows that the improve-Instruction Goal(Before co-training) Goal(After co-training)Stop!Whether prone position: TrueAverage velocity: StaticAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementGet down,stay hidden.Whether prone position: TrueAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementAverage velocity: StaticWhether seen by enemy: FalseEnemy!Rush and fire.Whether hold a gun: TrueWhether have bullets: TrueHorizontal direction of view: SouthwestWhether seen enemy: TrueAverage velocity: FastLength of distance moved: longAverage velocity: FastWhether hold a gun: TrueHorizontal direction of movement: SouthwestWhether seen enemy: TrueDamage to enemy: HighHorizontal direction of view: SouthwestEnemies nearby,move to defendand avoid damage.Whether prone position: TrueAverage velocity: FastWhether hold a gun: TrueHealth level: FullWhether to restore health: TrueLength of distance moved: longAverage velocity: FastWhether prone position: FalseHorizontal direction of movement: North(Position of enemy: South)Whether hold a gun: TrueTable 3: Comparison of goal-generation.. \n",
      "the piece 167\n",
      " \n",
      " τj={gj, s1, a1, r1, .。.。.。, s Tj, aTj, rTj}, Rj=TjXt=1rt10: Update θwith Equation (6) and (τ, R)11: Filter completed goals GcfromG, extract Rthe set from (τ, R)as agent feedback rewards Rf12: Compute examination reward by evaluating crucial sub-goal completion as Re=REWARD e(Gc,IS∩ Itrain,G),and formatting reward: Rm=REWARD m(gs, g), refer to Appendix K13: Update βLoRA with PPO, and merge updated βLoRA intoβllm14: end for15:end forTuning PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)SFT 0.547 0.663 0.490 0.602 0.517 0.632CoTF 0.533 0.652 0.487 0.599 0.509 0.624CoTF→SFT 0.555 0.685 0.505 0.621 0.529 0.652CoTF→SFT→EFT 0.590 0.694 0.501 0.593 0.516 0.629Table 2: Evaluation on different tuning methods.only use the target prompt without CoT data to supervisedfine-tuning, which can be regarded as a baseline for a naiveSFT; (2) CoTF: only CoT-assisted fine-tuning; (3) CoTF→SFT: further SFT target prompt after CoTF; (4) CoTF→SFT→EFT: further ensemble fine-tuning target promptafter CoTF.。With the comparison, we conclude that CoTFand SFT can improve each other and achieve better per-formance.。Furthermore, ensemble fine-tuning significantlyenhances precision while marginally decreasing recall, mak-ing it more suitable for generating accurate concise goals.5.3.。Evaluating Co-trainingWe conduct an analysis of the completion ratio correspond-ing to the number of valid sub-goals during the co-trainingprocess.。Though the dimension size of goal space achieves68, the number of sub-goals for valid goals predominantlyfalls within the range of 1 to 7.。This is rational as completinga goal with an excessive number of sub-goals is exceedinglychallenging for a policy, even impossibility for human toachieve.。Furthermore, Figure 3(a) shows that the improve-Instruction Goal(Before co-training) Goal(After co-training)Stop!Whether prone position: TrueAverage velocity: StaticAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementGet down,stay hidden.Whether prone position: TrueAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementAverage velocity: StaticWhether seen by enemy: FalseEnemy!Rush and fire.Whether hold a gun: TrueWhether have bullets: TrueHorizontal direction of view: SouthwestWhether seen enemy: TrueAverage velocity: FastLength of distance moved: longAverage velocity: FastWhether hold a gun: TrueHorizontal direction of movement: SouthwestWhether seen enemy: TrueDamage to enemy: HighHorizontal direction of view: SouthwestEnemies nearby,move to defendand avoid damage.Whether prone position: TrueAverage velocity: FastWhether hold a gun: TrueHealth level: FullWhether to restore health: TrueLength of distance moved: longAverage velocity: FastWhether prone position: FalseHorizontal direction of movement: North(Position of enemy: South)Whether hold a gun: TrueTable 3: Comparison of goal-generation.。Cyan the helpful,pink the conflicting, and orange the critical sub-goals.. \n",
      "the piece 168\n",
      " \n",
      "={gj, s1, a1, r1, .。.。.。, s Tj, aTj, rTj}, Rj=TjXt=1rt10: Update θwith Equation (6) and (τ, R)11: Filter completed goals GcfromG, extract Rthe set from (τ, R)as agent feedback rewards Rf12: Compute examination reward by evaluating crucial sub-goal completion as Re=REWARD e(Gc,IS∩ Itrain,G),and formatting reward: Rm=REWARD m(gs, g), refer to Appendix K13: Update βLoRA with PPO, and merge updated βLoRA intoβllm14: end for15:end forTuning PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)SFT 0.547 0.663 0.490 0.602 0.517 0.632CoTF 0.533 0.652 0.487 0.599 0.509 0.624CoTF→SFT 0.555 0.685 0.505 0.621 0.529 0.652CoTF→SFT→EFT 0.590 0.694 0.501 0.593 0.516 0.629Table 2: Evaluation on different tuning methods.only use the target prompt without CoT data to supervisedfine-tuning, which can be regarded as a baseline for a naiveSFT; (2) CoTF: only CoT-assisted fine-tuning; (3) CoTF→SFT: further SFT target prompt after CoTF; (4) CoTF→SFT→EFT: further ensemble fine-tuning target promptafter CoTF.。With the comparison, we conclude that CoTFand SFT can improve each other and achieve better per-formance.。Furthermore, ensemble fine-tuning significantlyenhances precision while marginally decreasing recall, mak-ing it more suitable for generating accurate concise goals.5.3.。Evaluating Co-trainingWe conduct an analysis of the completion ratio correspond-ing to the number of valid sub-goals during the co-trainingprocess.。Though the dimension size of goal space achieves68, the number of sub-goals for valid goals predominantlyfalls within the range of 1 to 7.。This is rational as completinga goal with an excessive number of sub-goals is exceedinglychallenging for a policy, even impossibility for human toachieve.。Furthermore, Figure 3(a) shows that the improve-Instruction Goal(Before co-training) Goal(After co-training)Stop!Whether prone position: TrueAverage velocity: StaticAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementGet down,stay hidden.Whether prone position: TrueAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementAverage velocity: StaticWhether seen by enemy: FalseEnemy!Rush and fire.Whether hold a gun: TrueWhether have bullets: TrueHorizontal direction of view: SouthwestWhether seen enemy: TrueAverage velocity: FastLength of distance moved: longAverage velocity: FastWhether hold a gun: TrueHorizontal direction of movement: SouthwestWhether seen enemy: TrueDamage to enemy: HighHorizontal direction of view: SouthwestEnemies nearby,move to defendand avoid damage.Whether prone position: TrueAverage velocity: FastWhether hold a gun: TrueHealth level: FullWhether to restore health: TrueLength of distance moved: longAverage velocity: FastWhether prone position: FalseHorizontal direction of movement: North(Position of enemy: South)Whether hold a gun: TrueTable 3: Comparison of goal-generation.。Cyan the helpful,pink the conflicting, and orange the critical sub-goals.。Itis evident that co-training enables goal-generation to avoidconflicts of sub-goals and improves reasonability by includ-ing helpful and critical sub-goals.ments mainly lies in 2≤ |g| ≤4, because |g|= 1 is tooeasy while |g| ≥5is too hard to complete.. \n",
      "the piece 169\n",
      " \n",
      "om (τ, R)as agent feedback rewards Rf12: Compute examination reward by evaluating crucial sub-goal completion as Re=REWARD e(Gc,IS∩ Itrain,G),and formatting reward: Rm=REWARD m(gs, g), refer to Appendix K13: Update βLoRA with PPO, and merge updated βLoRA intoβllm14: end for15:end forTuning PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)SFT 0.547 0.663 0.490 0.602 0.517 0.632CoTF 0.533 0.652 0.487 0.599 0.509 0.624CoTF→SFT 0.555 0.685 0.505 0.621 0.529 0.652CoTF→SFT→EFT 0.590 0.694 0.501 0.593 0.516 0.629Table 2: Evaluation on different tuning methods.only use the target prompt without CoT data to supervisedfine-tuning, which can be regarded as a baseline for a naiveSFT; (2) CoTF: only CoT-assisted fine-tuning; (3) CoTF→SFT: further SFT target prompt after CoTF; (4) CoTF→SFT→EFT: further ensemble fine-tuning target promptafter CoTF.。With the comparison, we conclude that CoTFand SFT can improve each other and achieve better per-formance.。Furthermore, ensemble fine-tuning significantlyenhances precision while marginally decreasing recall, mak-ing it more suitable for generating accurate concise goals.5.3.。Evaluating Co-trainingWe conduct an analysis of the completion ratio correspond-ing to the number of valid sub-goals during the co-trainingprocess.。Though the dimension size of goal space achieves68, the number of sub-goals for valid goals predominantlyfalls within the range of 1 to 7.。This is rational as completinga goal with an excessive number of sub-goals is exceedinglychallenging for a policy, even impossibility for human toachieve.。Furthermore, Figure 3(a) shows that the improve-Instruction Goal(Before co-training) Goal(After co-training)Stop!Whether prone position: TrueAverage velocity: StaticAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementGet down,stay hidden.Whether prone position: TrueAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementAverage velocity: StaticWhether seen by enemy: FalseEnemy!Rush and fire.Whether hold a gun: TrueWhether have bullets: TrueHorizontal direction of view: SouthwestWhether seen enemy: TrueAverage velocity: FastLength of distance moved: longAverage velocity: FastWhether hold a gun: TrueHorizontal direction of movement: SouthwestWhether seen enemy: TrueDamage to enemy: HighHorizontal direction of view: SouthwestEnemies nearby,move to defendand avoid damage.Whether prone position: TrueAverage velocity: FastWhether hold a gun: TrueHealth level: FullWhether to restore health: TrueLength of distance moved: longAverage velocity: FastWhether prone position: FalseHorizontal direction of movement: North(Position of enemy: South)Whether hold a gun: TrueTable 3: Comparison of goal-generation.。Cyan the helpful,pink the conflicting, and orange the critical sub-goals.。Itis evident that co-training enables goal-generation to avoidconflicts of sub-goals and improves reasonability by includ-ing helpful and critical sub-goals.ments mainly lies in 2≤ |g| ≤4, because |g|= 1 is tooeasy while |g| ≥5is too hard to complete.。Figure 3(b)shows a case of |g|= 3 that co-training indeed improvesthe completion ratio as the green curve.. \n",
      "the piece 170\n",
      " \n",
      "(τ, R)as agent feedback rewards Rf12: Compute examination reward by evaluating crucial sub-goal completion as Re=REWARD e(Gc,IS∩ Itrain,G),and formatting reward: Rm=REWARD m(gs, g), refer to Appendix K13: Update βLoRA with PPO, and merge updated βLoRA intoβllm14: end for15:end forTuning PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)SFT 0.547 0.663 0.490 0.602 0.517 0.632CoTF 0.533 0.652 0.487 0.599 0.509 0.624CoTF→SFT 0.555 0.685 0.505 0.621 0.529 0.652CoTF→SFT→EFT 0.590 0.694 0.501 0.593 0.516 0.629Table 2: Evaluation on different tuning methods.only use the target prompt without CoT data to supervisedfine-tuning, which can be regarded as a baseline for a naiveSFT; (2) CoTF: only CoT-assisted fine-tuning; (3) CoTF→SFT: further SFT target prompt after CoTF; (4) CoTF→SFT→EFT: further ensemble fine-tuning target promptafter CoTF.。With the comparison, we conclude that CoTFand SFT can improve each other and achieve better per-formance.。Furthermore, ensemble fine-tuning significantlyenhances precision while marginally decreasing recall, mak-ing it more suitable for generating accurate concise goals.5.3.。Evaluating Co-trainingWe conduct an analysis of the completion ratio correspond-ing to the number of valid sub-goals during the co-trainingprocess.。Though the dimension size of goal space achieves68, the number of sub-goals for valid goals predominantlyfalls within the range of 1 to 7.。This is rational as completinga goal with an excessive number of sub-goals is exceedinglychallenging for a policy, even impossibility for human toachieve.。Furthermore, Figure 3(a) shows that the improve-Instruction Goal(Before co-training) Goal(After co-training)Stop!Whether prone position: TrueAverage velocity: StaticAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementGet down,stay hidden.Whether prone position: TrueAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementAverage velocity: StaticWhether seen by enemy: FalseEnemy!Rush and fire.Whether hold a gun: TrueWhether have bullets: TrueHorizontal direction of view: SouthwestWhether seen enemy: TrueAverage velocity: FastLength of distance moved: longAverage velocity: FastWhether hold a gun: TrueHorizontal direction of movement: SouthwestWhether seen enemy: TrueDamage to enemy: HighHorizontal direction of view: SouthwestEnemies nearby,move to defendand avoid damage.Whether prone position: TrueAverage velocity: FastWhether hold a gun: TrueHealth level: FullWhether to restore health: TrueLength of distance moved: longAverage velocity: FastWhether prone position: FalseHorizontal direction of movement: North(Position of enemy: South)Whether hold a gun: TrueTable 3: Comparison of goal-generation.。Cyan the helpful,pink the conflicting, and orange the critical sub-goals.。Itis evident that co-training enables goal-generation to avoidconflicts of sub-goals and improves reasonability by includ-ing helpful and critical sub-goals.ments mainly lies in 2≤ |g| ≤4, because |g|= 1 is tooeasy while |g| ≥5is too hard to complete.。Figure 3(b)shows a case of |g|= 3 that co-training indeed improvesthe completion ratio as the green curve.。It is noteworthythat the performance suddenly downgrades at each reset.This phenomenon is attributed to the reset of Gllmbreaksthe adaptation with πg, avoiding being trapped in local opti-mal.. \n",
      "the piece 171\n",
      " \n",
      " R)as agent feedback rewards Rf12: Compute examination reward by evaluating crucial sub-goal completion as Re=REWARD e(Gc,IS∩ Itrain,G),and formatting reward: Rm=REWARD m(gs, g), refer to Appendix K13: Update βLoRA with PPO, and merge updated βLoRA intoβllm14: end for15:end forTuning PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)SFT 0.547 0.663 0.490 0.602 0.517 0.632CoTF 0.533 0.652 0.487 0.599 0.509 0.624CoTF→SFT 0.555 0.685 0.505 0.621 0.529 0.652CoTF→SFT→EFT 0.590 0.694 0.501 0.593 0.516 0.629Table 2: Evaluation on different tuning methods.only use the target prompt without CoT data to supervisedfine-tuning, which can be regarded as a baseline for a naiveSFT; (2) CoTF: only CoT-assisted fine-tuning; (3) CoTF→SFT: further SFT target prompt after CoTF; (4) CoTF→SFT→EFT: further ensemble fine-tuning target promptafter CoTF.。With the comparison, we conclude that CoTFand SFT can improve each other and achieve better per-formance.。Furthermore, ensemble fine-tuning significantlyenhances precision while marginally decreasing recall, mak-ing it more suitable for generating accurate concise goals.5.3.。Evaluating Co-trainingWe conduct an analysis of the completion ratio correspond-ing to the number of valid sub-goals during the co-trainingprocess.。Though the dimension size of goal space achieves68, the number of sub-goals for valid goals predominantlyfalls within the range of 1 to 7.。This is rational as completinga goal with an excessive number of sub-goals is exceedinglychallenging for a policy, even impossibility for human toachieve.。Furthermore, Figure 3(a) shows that the improve-Instruction Goal(Before co-training) Goal(After co-training)Stop!Whether prone position: TrueAverage velocity: StaticAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementGet down,stay hidden.Whether prone position: TrueAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementAverage velocity: StaticWhether seen by enemy: FalseEnemy!Rush and fire.Whether hold a gun: TrueWhether have bullets: TrueHorizontal direction of view: SouthwestWhether seen enemy: TrueAverage velocity: FastLength of distance moved: longAverage velocity: FastWhether hold a gun: TrueHorizontal direction of movement: SouthwestWhether seen enemy: TrueDamage to enemy: HighHorizontal direction of view: SouthwestEnemies nearby,move to defendand avoid damage.Whether prone position: TrueAverage velocity: FastWhether hold a gun: TrueHealth level: FullWhether to restore health: TrueLength of distance moved: longAverage velocity: FastWhether prone position: FalseHorizontal direction of movement: North(Position of enemy: South)Whether hold a gun: TrueTable 3: Comparison of goal-generation.。Cyan the helpful,pink the conflicting, and orange the critical sub-goals.。Itis evident that co-training enables goal-generation to avoidconflicts of sub-goals and improves reasonability by includ-ing helpful and critical sub-goals.ments mainly lies in 2≤ |g| ≤4, because |g|= 1 is tooeasy while |g| ≥5is too hard to complete.。Figure 3(b)shows a case of |g|= 3 that co-training indeed improvesthe completion ratio as the green curve.。It is noteworthythat the performance suddenly downgrades at each reset.This phenomenon is attributed to the reset of Gllmbreaksthe adaptation with πg, avoiding being trapped in local opti-mal.。Meanwhile, the performance tends to converge, whichindicates the successor loops produce a better adaptationBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps24252627282930Goal Completion Ratio (%)OursHER(a)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps15.017.520.022.525.027.530.032.5Goal Completion Ratio (%)OursHER (b)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps0.100.080.060.040.020.00Reward0.30.40.50.60.70.8Game StatisticsMean basic reward per stepMean basic reward per step (No KL)#Enemies killed#Enemies knocked down#Enemies killed (No KL)#Enemies knocked down (No KL) (c)Figure 2: (a) The goal completion rate on training dataset; (b) The goal completion rate on unseen goals, i.e., the test dataset;(c) The evaluation of policy learning in cases of w/ and w/o KL-divergence regularizer.. \n",
      "the piece 172\n",
      " \n",
      "Compute examination reward by evaluating crucial sub-goal completion as Re=REWARD e(Gc,IS∩ Itrain,G),and formatting reward: Rm=REWARD m(gs, g), refer to Appendix K13: Update βLoRA with PPO, and merge updated βLoRA intoβllm14: end for15:end forTuning PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)SFT 0.547 0.663 0.490 0.602 0.517 0.632CoTF 0.533 0.652 0.487 0.599 0.509 0.624CoTF→SFT 0.555 0.685 0.505 0.621 0.529 0.652CoTF→SFT→EFT 0.590 0.694 0.501 0.593 0.516 0.629Table 2: Evaluation on different tuning methods.only use the target prompt without CoT data to supervisedfine-tuning, which can be regarded as a baseline for a naiveSFT; (2) CoTF: only CoT-assisted fine-tuning; (3) CoTF→SFT: further SFT target prompt after CoTF; (4) CoTF→SFT→EFT: further ensemble fine-tuning target promptafter CoTF.。With the comparison, we conclude that CoTFand SFT can improve each other and achieve better per-formance.。Furthermore, ensemble fine-tuning significantlyenhances precision while marginally decreasing recall, mak-ing it more suitable for generating accurate concise goals.5.3.。Evaluating Co-trainingWe conduct an analysis of the completion ratio correspond-ing to the number of valid sub-goals during the co-trainingprocess.。Though the dimension size of goal space achieves68, the number of sub-goals for valid goals predominantlyfalls within the range of 1 to 7.。This is rational as completinga goal with an excessive number of sub-goals is exceedinglychallenging for a policy, even impossibility for human toachieve.。Furthermore, Figure 3(a) shows that the improve-Instruction Goal(Before co-training) Goal(After co-training)Stop!Whether prone position: TrueAverage velocity: StaticAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementGet down,stay hidden.Whether prone position: TrueAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementAverage velocity: StaticWhether seen by enemy: FalseEnemy!Rush and fire.Whether hold a gun: TrueWhether have bullets: TrueHorizontal direction of view: SouthwestWhether seen enemy: TrueAverage velocity: FastLength of distance moved: longAverage velocity: FastWhether hold a gun: TrueHorizontal direction of movement: SouthwestWhether seen enemy: TrueDamage to enemy: HighHorizontal direction of view: SouthwestEnemies nearby,move to defendand avoid damage.Whether prone position: TrueAverage velocity: FastWhether hold a gun: TrueHealth level: FullWhether to restore health: TrueLength of distance moved: longAverage velocity: FastWhether prone position: FalseHorizontal direction of movement: North(Position of enemy: South)Whether hold a gun: TrueTable 3: Comparison of goal-generation.。Cyan the helpful,pink the conflicting, and orange the critical sub-goals.。Itis evident that co-training enables goal-generation to avoidconflicts of sub-goals and improves reasonability by includ-ing helpful and critical sub-goals.ments mainly lies in 2≤ |g| ≤4, because |g|= 1 is tooeasy while |g| ≥5is too hard to complete.。Figure 3(b)shows a case of |g|= 3 that co-training indeed improvesthe completion ratio as the green curve.。It is noteworthythat the performance suddenly downgrades at each reset.This phenomenon is attributed to the reset of Gllmbreaksthe adaptation with πg, avoiding being trapped in local opti-mal.。Meanwhile, the performance tends to converge, whichindicates the successor loops produce a better adaptationBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps24252627282930Goal Completion Ratio (%)OursHER(a)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps15.017.520.022.525.027.530.032.5Goal Completion Ratio (%)OursHER (b)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps0.100.080.060.040.020.00Reward0.30.40.50.60.70.8Game StatisticsMean basic reward per stepMean basic reward per step (No KL)#Enemies killed#Enemies knocked down#Enemies killed (No KL)#Enemies knocked down (No KL) (c)Figure 2: (a) The goal completion rate on training dataset; (b) The goal completion rate on unseen goals, i.e., the test dataset;(c) The evaluation of policy learning in cases of w/ and w/o KL-divergence regularizer.。(a) Goal completion rate ( 1≤ |g| ≤7)Trend of  p o l i c y  a b i l i t ySmoothe d  C o m p l e t i o n  R a t eOriginal C o m p l e t i o n  R a t eReset Lo o p sGoal Com p l e t i o n  R a t i o  ( % )(b) Goal completion rate ( |g|= 3)Sub goal        g4   g8   g12   g16   g20 Time in one loop0.0250.0500.0750.1000.1250.1500.1750.200Possibility (c) Sub-goals distributionFigure 3: (a) The completion ratio of goals with dimension size ranges from 1 to 7; (b) The goal completion ratio of goalsthat|g|= 3, the trend curve reflects the improving completion ratio; (c) The sub-goals distribution changes along thetraining in one loop of co-training, where the description of each giis included in Table 14.between LLM and policy than before.. \n",
      "the piece 173\n",
      " \n",
      "pute examination reward by evaluating crucial sub-goal completion as Re=REWARD e(Gc,IS∩ Itrain,G),and formatting reward: Rm=REWARD m(gs, g), refer to Appendix K13: Update βLoRA with PPO, and merge updated βLoRA intoβllm14: end for15:end forTuning PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)SFT 0.547 0.663 0.490 0.602 0.517 0.632CoTF 0.533 0.652 0.487 0.599 0.509 0.624CoTF→SFT 0.555 0.685 0.505 0.621 0.529 0.652CoTF→SFT→EFT 0.590 0.694 0.501 0.593 0.516 0.629Table 2: Evaluation on different tuning methods.only use the target prompt without CoT data to supervisedfine-tuning, which can be regarded as a baseline for a naiveSFT; (2) CoTF: only CoT-assisted fine-tuning; (3) CoTF→SFT: further SFT target prompt after CoTF; (4) CoTF→SFT→EFT: further ensemble fine-tuning target promptafter CoTF.。With the comparison, we conclude that CoTFand SFT can improve each other and achieve better per-formance.。Furthermore, ensemble fine-tuning significantlyenhances precision while marginally decreasing recall, mak-ing it more suitable for generating accurate concise goals.5.3.。Evaluating Co-trainingWe conduct an analysis of the completion ratio correspond-ing to the number of valid sub-goals during the co-trainingprocess.。Though the dimension size of goal space achieves68, the number of sub-goals for valid goals predominantlyfalls within the range of 1 to 7.。This is rational as completinga goal with an excessive number of sub-goals is exceedinglychallenging for a policy, even impossibility for human toachieve.。Furthermore, Figure 3(a) shows that the improve-Instruction Goal(Before co-training) Goal(After co-training)Stop!Whether prone position: TrueAverage velocity: StaticAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementGet down,stay hidden.Whether prone position: TrueAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementAverage velocity: StaticWhether seen by enemy: FalseEnemy!Rush and fire.Whether hold a gun: TrueWhether have bullets: TrueHorizontal direction of view: SouthwestWhether seen enemy: TrueAverage velocity: FastLength of distance moved: longAverage velocity: FastWhether hold a gun: TrueHorizontal direction of movement: SouthwestWhether seen enemy: TrueDamage to enemy: HighHorizontal direction of view: SouthwestEnemies nearby,move to defendand avoid damage.Whether prone position: TrueAverage velocity: FastWhether hold a gun: TrueHealth level: FullWhether to restore health: TrueLength of distance moved: longAverage velocity: FastWhether prone position: FalseHorizontal direction of movement: North(Position of enemy: South)Whether hold a gun: TrueTable 3: Comparison of goal-generation.。Cyan the helpful,pink the conflicting, and orange the critical sub-goals.。Itis evident that co-training enables goal-generation to avoidconflicts of sub-goals and improves reasonability by includ-ing helpful and critical sub-goals.ments mainly lies in 2≤ |g| ≤4, because |g|= 1 is tooeasy while |g| ≥5is too hard to complete.。Figure 3(b)shows a case of |g|= 3 that co-training indeed improvesthe completion ratio as the green curve.。It is noteworthythat the performance suddenly downgrades at each reset.This phenomenon is attributed to the reset of Gllmbreaksthe adaptation with πg, avoiding being trapped in local opti-mal.。Meanwhile, the performance tends to converge, whichindicates the successor loops produce a better adaptationBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps24252627282930Goal Completion Ratio (%)OursHER(a)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps15.017.520.022.525.027.530.032.5Goal Completion Ratio (%)OursHER (b)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps0.100.080.060.040.020.00Reward0.30.40.50.60.70.8Game StatisticsMean basic reward per stepMean basic reward per step (No KL)#Enemies killed#Enemies knocked down#Enemies killed (No KL)#Enemies knocked down (No KL) (c)Figure 2: (a) The goal completion rate on training dataset; (b) The goal completion rate on unseen goals, i.e., the test dataset;(c) The evaluation of policy learning in cases of w/ and w/o KL-divergence regularizer.。(a) Goal completion rate ( 1≤ |g| ≤7)Trend of  p o l i c y  a b i l i t ySmoothe d  C o m p l e t i o n  R a t eOriginal C o m p l e t i o n  R a t eReset Lo o p sGoal Com p l e t i o n  R a t i o  ( % )(b) Goal completion rate ( |g|= 3)Sub goal        g4   g8   g12   g16   g20 Time in one loop0.0250.0500.0750.1000.1250.1500.1750.200Possibility (c) Sub-goals distributionFigure 3: (a) The completion ratio of goals with dimension size ranges from 1 to 7; (b) The goal completion ratio of goalsthat|g|= 3, the trend curve reflects the improving completion ratio; (c) The sub-goals distribution changes along thetraining in one loop of co-training, where the description of each giis included in Table 14.between LLM and policy than before.。Additionally, weinvestigated the change in the generation probability of sub-goals (Table 14) during co-training.. \n",
      "the piece 174\n",
      " \n",
      "e examination reward by evaluating crucial sub-goal completion as Re=REWARD e(Gc,IS∩ Itrain,G),and formatting reward: Rm=REWARD m(gs, g), refer to Appendix K13: Update βLoRA with PPO, and merge updated βLoRA intoβllm14: end for15:end forTuning PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)SFT 0.547 0.663 0.490 0.602 0.517 0.632CoTF 0.533 0.652 0.487 0.599 0.509 0.624CoTF→SFT 0.555 0.685 0.505 0.621 0.529 0.652CoTF→SFT→EFT 0.590 0.694 0.501 0.593 0.516 0.629Table 2: Evaluation on different tuning methods.only use the target prompt without CoT data to supervisedfine-tuning, which can be regarded as a baseline for a naiveSFT; (2) CoTF: only CoT-assisted fine-tuning; (3) CoTF→SFT: further SFT target prompt after CoTF; (4) CoTF→SFT→EFT: further ensemble fine-tuning target promptafter CoTF.。With the comparison, we conclude that CoTFand SFT can improve each other and achieve better per-formance.。Furthermore, ensemble fine-tuning significantlyenhances precision while marginally decreasing recall, mak-ing it more suitable for generating accurate concise goals.5.3.。Evaluating Co-trainingWe conduct an analysis of the completion ratio correspond-ing to the number of valid sub-goals during the co-trainingprocess.。Though the dimension size of goal space achieves68, the number of sub-goals for valid goals predominantlyfalls within the range of 1 to 7.。This is rational as completinga goal with an excessive number of sub-goals is exceedinglychallenging for a policy, even impossibility for human toachieve.。Furthermore, Figure 3(a) shows that the improve-Instruction Goal(Before co-training) Goal(After co-training)Stop!Whether prone position: TrueAverage velocity: StaticAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementGet down,stay hidden.Whether prone position: TrueAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementAverage velocity: StaticWhether seen by enemy: FalseEnemy!Rush and fire.Whether hold a gun: TrueWhether have bullets: TrueHorizontal direction of view: SouthwestWhether seen enemy: TrueAverage velocity: FastLength of distance moved: longAverage velocity: FastWhether hold a gun: TrueHorizontal direction of movement: SouthwestWhether seen enemy: TrueDamage to enemy: HighHorizontal direction of view: SouthwestEnemies nearby,move to defendand avoid damage.Whether prone position: TrueAverage velocity: FastWhether hold a gun: TrueHealth level: FullWhether to restore health: TrueLength of distance moved: longAverage velocity: FastWhether prone position: FalseHorizontal direction of movement: North(Position of enemy: South)Whether hold a gun: TrueTable 3: Comparison of goal-generation.。Cyan the helpful,pink the conflicting, and orange the critical sub-goals.。Itis evident that co-training enables goal-generation to avoidconflicts of sub-goals and improves reasonability by includ-ing helpful and critical sub-goals.ments mainly lies in 2≤ |g| ≤4, because |g|= 1 is tooeasy while |g| ≥5is too hard to complete.。Figure 3(b)shows a case of |g|= 3 that co-training indeed improvesthe completion ratio as the green curve.。It is noteworthythat the performance suddenly downgrades at each reset.This phenomenon is attributed to the reset of Gllmbreaksthe adaptation with πg, avoiding being trapped in local opti-mal.。Meanwhile, the performance tends to converge, whichindicates the successor loops produce a better adaptationBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps24252627282930Goal Completion Ratio (%)OursHER(a)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps15.017.520.022.525.027.530.032.5Goal Completion Ratio (%)OursHER (b)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps0.100.080.060.040.020.00Reward0.30.40.50.60.70.8Game StatisticsMean basic reward per stepMean basic reward per step (No KL)#Enemies killed#Enemies knocked down#Enemies killed (No KL)#Enemies knocked down (No KL) (c)Figure 2: (a) The goal completion rate on training dataset; (b) The goal completion rate on unseen goals, i.e., the test dataset;(c) The evaluation of policy learning in cases of w/ and w/o KL-divergence regularizer.。(a) Goal completion rate ( 1≤ |g| ≤7)Trend of  p o l i c y  a b i l i t ySmoothe d  C o m p l e t i o n  R a t eOriginal C o m p l e t i o n  R a t eReset Lo o p sGoal Com p l e t i o n  R a t i o  ( % )(b) Goal completion rate ( |g|= 3)Sub goal        g4   g8   g12   g16   g20 Time in one loop0.0250.0500.0750.1000.1250.1500.1750.200Possibility (c) Sub-goals distributionFigure 3: (a) The completion ratio of goals with dimension size ranges from 1 to 7; (b) The goal completion ratio of goalsthat|g|= 3, the trend curve reflects the improving completion ratio; (c) The sub-goals distribution changes along thetraining in one loop of co-training, where the description of each giis included in Table 14.between LLM and policy than before.。Additionally, weinvestigated the change in the generation probability of sub-goals (Table 14) during co-training.。Specifically, Figure 3(c)illustrates changes within a training loop, while Figure 11(a)indicates changes across loops.. \n",
      "the piece 175\n",
      " \n",
      "hile marginally decreasing recall, mak-ing it more suitable for generating accurate concise goals.5.3.。Evaluating Co-trainingWe conduct an analysis of the completion ratio correspond-ing to the number of valid sub-goals during the co-trainingprocess.。Though the dimension size of goal space achieves68, the number of sub-goals for valid goals predominantlyfalls within the range of 1 to 7.。This is rational as completinga goal with an excessive number of sub-goals is exceedinglychallenging for a policy, even impossibility for human toachieve.。Furthermore, Figure 3(a) shows that the improve-Instruction Goal(Before co-training) Goal(After co-training)Stop!Whether prone position: TrueAverage velocity: StaticAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementGet down,stay hidden.Whether prone position: TrueAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementAverage velocity: StaticWhether seen by enemy: FalseEnemy!Rush and fire.Whether hold a gun: TrueWhether have bullets: TrueHorizontal direction of view: SouthwestWhether seen enemy: TrueAverage velocity: FastLength of distance moved: longAverage velocity: FastWhether hold a gun: TrueHorizontal direction of movement: SouthwestWhether seen enemy: TrueDamage to enemy: HighHorizontal direction of view: SouthwestEnemies nearby,move to defendand avoid damage.Whether prone position: TrueAverage velocity: FastWhether hold a gun: TrueHealth level: FullWhether to restore health: TrueLength of distance moved: longAverage velocity: FastWhether prone position: FalseHorizontal direction of movement: North(Position of enemy: South)Whether hold a gun: TrueTable 3: Comparison of goal-generation.。Cyan the helpful,pink the conflicting, and orange the critical sub-goals.。Itis evident that co-training enables goal-generation to avoidconflicts of sub-goals and improves reasonability by includ-ing helpful and critical sub-goals.ments mainly lies in 2≤ |g| ≤4, because |g|= 1 is tooeasy while |g| ≥5is too hard to complete.。Figure 3(b)shows a case of |g|= 3 that co-training indeed improvesthe completion ratio as the green curve.。It is noteworthythat the performance suddenly downgrades at each reset.This phenomenon is attributed to the reset of Gllmbreaksthe adaptation with πg, avoiding being trapped in local opti-mal.。Meanwhile, the performance tends to converge, whichindicates the successor loops produce a better adaptationBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps24252627282930Goal Completion Ratio (%)OursHER(a)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps15.017.520.022.525.027.530.032.5Goal Completion Ratio (%)OursHER (b)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps0.100.080.060.040.020.00Reward0.30.40.50.60.70.8Game StatisticsMean basic reward per stepMean basic reward per step (No KL)#Enemies killed#Enemies knocked down#Enemies killed (No KL)#Enemies knocked down (No KL) (c)Figure 2: (a) The goal completion rate on training dataset; (b) The goal completion rate on unseen goals, i.e., the test dataset;(c) The evaluation of policy learning in cases of w/ and w/o KL-divergence regularizer.。(a) Goal completion rate ( 1≤ |g| ≤7)Trend of  p o l i c y  a b i l i t ySmoothe d  C o m p l e t i o n  R a t eOriginal C o m p l e t i o n  R a t eReset Lo o p sGoal Com p l e t i o n  R a t i o  ( % )(b) Goal completion rate ( |g|= 3)Sub goal        g4   g8   g12   g16   g20 Time in one loop0.0250.0500.0750.1000.1250.1500.1750.200Possibility (c) Sub-goals distributionFigure 3: (a) The completion ratio of goals with dimension size ranges from 1 to 7; (b) The goal completion ratio of goalsthat|g|= 3, the trend curve reflects the improving completion ratio; (c) The sub-goals distribution changes along thetraining in one loop of co-training, where the description of each giis included in Table 14.between LLM and policy than before.。Additionally, weinvestigated the change in the generation probability of sub-goals (Table 14) during co-training.。Specifically, Figure 3(c)illustrates changes within a training loop, while Figure 11(a)indicates changes across loops.。As training progresses, theprobabilities associated with each giundergo gradual modi-fications.. \n",
      "the piece 176\n",
      " \n",
      "uating Co-trainingWe conduct an analysis of the completion ratio correspond-ing to the number of valid sub-goals during the co-trainingprocess.。Though the dimension size of goal space achieves68, the number of sub-goals for valid goals predominantlyfalls within the range of 1 to 7.。This is rational as completinga goal with an excessive number of sub-goals is exceedinglychallenging for a policy, even impossibility for human toachieve.。Furthermore, Figure 3(a) shows that the improve-Instruction Goal(Before co-training) Goal(After co-training)Stop!Whether prone position: TrueAverage velocity: StaticAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementGet down,stay hidden.Whether prone position: TrueAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementAverage velocity: StaticWhether seen by enemy: FalseEnemy!Rush and fire.Whether hold a gun: TrueWhether have bullets: TrueHorizontal direction of view: SouthwestWhether seen enemy: TrueAverage velocity: FastLength of distance moved: longAverage velocity: FastWhether hold a gun: TrueHorizontal direction of movement: SouthwestWhether seen enemy: TrueDamage to enemy: HighHorizontal direction of view: SouthwestEnemies nearby,move to defendand avoid damage.Whether prone position: TrueAverage velocity: FastWhether hold a gun: TrueHealth level: FullWhether to restore health: TrueLength of distance moved: longAverage velocity: FastWhether prone position: FalseHorizontal direction of movement: North(Position of enemy: South)Whether hold a gun: TrueTable 3: Comparison of goal-generation.。Cyan the helpful,pink the conflicting, and orange the critical sub-goals.。Itis evident that co-training enables goal-generation to avoidconflicts of sub-goals and improves reasonability by includ-ing helpful and critical sub-goals.ments mainly lies in 2≤ |g| ≤4, because |g|= 1 is tooeasy while |g| ≥5is too hard to complete.。Figure 3(b)shows a case of |g|= 3 that co-training indeed improvesthe completion ratio as the green curve.。It is noteworthythat the performance suddenly downgrades at each reset.This phenomenon is attributed to the reset of Gllmbreaksthe adaptation with πg, avoiding being trapped in local opti-mal.。Meanwhile, the performance tends to converge, whichindicates the successor loops produce a better adaptationBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps24252627282930Goal Completion Ratio (%)OursHER(a)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps15.017.520.022.525.027.530.032.5Goal Completion Ratio (%)OursHER (b)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps0.100.080.060.040.020.00Reward0.30.40.50.60.70.8Game StatisticsMean basic reward per stepMean basic reward per step (No KL)#Enemies killed#Enemies knocked down#Enemies killed (No KL)#Enemies knocked down (No KL) (c)Figure 2: (a) The goal completion rate on training dataset; (b) The goal completion rate on unseen goals, i.e., the test dataset;(c) The evaluation of policy learning in cases of w/ and w/o KL-divergence regularizer.。(a) Goal completion rate ( 1≤ |g| ≤7)Trend of  p o l i c y  a b i l i t ySmoothe d  C o m p l e t i o n  R a t eOriginal C o m p l e t i o n  R a t eReset Lo o p sGoal Com p l e t i o n  R a t i o  ( % )(b) Goal completion rate ( |g|= 3)Sub goal        g4   g8   g12   g16   g20 Time in one loop0.0250.0500.0750.1000.1250.1500.1750.200Possibility (c) Sub-goals distributionFigure 3: (a) The completion ratio of goals with dimension size ranges from 1 to 7; (b) The goal completion ratio of goalsthat|g|= 3, the trend curve reflects the improving completion ratio; (c) The sub-goals distribution changes along thetraining in one loop of co-training, where the description of each giis included in Table 14.between LLM and policy than before.。Additionally, weinvestigated the change in the generation probability of sub-goals (Table 14) during co-training.。Specifically, Figure 3(c)illustrates changes within a training loop, while Figure 11(a)indicates changes across loops.。As training progresses, theprobabilities associated with each giundergo gradual modi-fications.。For instance, sub-goals with growing probabilitiesare central to the agent private states due to their relativelyattainable nature and influence in agent interaction.. \n",
      "the piece 177\n",
      " \n",
      "f goal space achieves68, the number of sub-goals for valid goals predominantlyfalls within the range of 1 to 7.。This is rational as completinga goal with an excessive number of sub-goals is exceedinglychallenging for a policy, even impossibility for human toachieve.。Furthermore, Figure 3(a) shows that the improve-Instruction Goal(Before co-training) Goal(After co-training)Stop!Whether prone position: TrueAverage velocity: StaticAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementGet down,stay hidden.Whether prone position: TrueAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementAverage velocity: StaticWhether seen by enemy: FalseEnemy!Rush and fire.Whether hold a gun: TrueWhether have bullets: TrueHorizontal direction of view: SouthwestWhether seen enemy: TrueAverage velocity: FastLength of distance moved: longAverage velocity: FastWhether hold a gun: TrueHorizontal direction of movement: SouthwestWhether seen enemy: TrueDamage to enemy: HighHorizontal direction of view: SouthwestEnemies nearby,move to defendand avoid damage.Whether prone position: TrueAverage velocity: FastWhether hold a gun: TrueHealth level: FullWhether to restore health: TrueLength of distance moved: longAverage velocity: FastWhether prone position: FalseHorizontal direction of movement: North(Position of enemy: South)Whether hold a gun: TrueTable 3: Comparison of goal-generation.。Cyan the helpful,pink the conflicting, and orange the critical sub-goals.。Itis evident that co-training enables goal-generation to avoidconflicts of sub-goals and improves reasonability by includ-ing helpful and critical sub-goals.ments mainly lies in 2≤ |g| ≤4, because |g|= 1 is tooeasy while |g| ≥5is too hard to complete.。Figure 3(b)shows a case of |g|= 3 that co-training indeed improvesthe completion ratio as the green curve.。It is noteworthythat the performance suddenly downgrades at each reset.This phenomenon is attributed to the reset of Gllmbreaksthe adaptation with πg, avoiding being trapped in local opti-mal.。Meanwhile, the performance tends to converge, whichindicates the successor loops produce a better adaptationBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps24252627282930Goal Completion Ratio (%)OursHER(a)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps15.017.520.022.525.027.530.032.5Goal Completion Ratio (%)OursHER (b)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps0.100.080.060.040.020.00Reward0.30.40.50.60.70.8Game StatisticsMean basic reward per stepMean basic reward per step (No KL)#Enemies killed#Enemies knocked down#Enemies killed (No KL)#Enemies knocked down (No KL) (c)Figure 2: (a) The goal completion rate on training dataset; (b) The goal completion rate on unseen goals, i.e., the test dataset;(c) The evaluation of policy learning in cases of w/ and w/o KL-divergence regularizer.。(a) Goal completion rate ( 1≤ |g| ≤7)Trend of  p o l i c y  a b i l i t ySmoothe d  C o m p l e t i o n  R a t eOriginal C o m p l e t i o n  R a t eReset Lo o p sGoal Com p l e t i o n  R a t i o  ( % )(b) Goal completion rate ( |g|= 3)Sub goal        g4   g8   g12   g16   g20 Time in one loop0.0250.0500.0750.1000.1250.1500.1750.200Possibility (c) Sub-goals distributionFigure 3: (a) The completion ratio of goals with dimension size ranges from 1 to 7; (b) The goal completion ratio of goalsthat|g|= 3, the trend curve reflects the improving completion ratio; (c) The sub-goals distribution changes along thetraining in one loop of co-training, where the description of each giis included in Table 14.between LLM and policy than before.。Additionally, weinvestigated the change in the generation probability of sub-goals (Table 14) during co-training.。Specifically, Figure 3(c)illustrates changes within a training loop, while Figure 11(a)indicates changes across loops.。As training progresses, theprobabilities associated with each giundergo gradual modi-fications.。For instance, sub-goals with growing probabilitiesare central to the agent private states due to their relativelyattainable nature and influence in agent interaction.。Con-versely, sub-goals with falling probabilities are central toother players’ states, as they are not directly changed byagent actions, and Gllmtends to generate outputs for thesesub-goals only when absolutely necessary.. \n",
      "the piece 178\n",
      " \n",
      "with an excessive number of sub-goals is exceedinglychallenging for a policy, even impossibility for human toachieve.。Furthermore, Figure 3(a) shows that the improve-Instruction Goal(Before co-training) Goal(After co-training)Stop!Whether prone position: TrueAverage velocity: StaticAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementGet down,stay hidden.Whether prone position: TrueAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementAverage velocity: StaticWhether seen by enemy: FalseEnemy!Rush and fire.Whether hold a gun: TrueWhether have bullets: TrueHorizontal direction of view: SouthwestWhether seen enemy: TrueAverage velocity: FastLength of distance moved: longAverage velocity: FastWhether hold a gun: TrueHorizontal direction of movement: SouthwestWhether seen enemy: TrueDamage to enemy: HighHorizontal direction of view: SouthwestEnemies nearby,move to defendand avoid damage.Whether prone position: TrueAverage velocity: FastWhether hold a gun: TrueHealth level: FullWhether to restore health: TrueLength of distance moved: longAverage velocity: FastWhether prone position: FalseHorizontal direction of movement: North(Position of enemy: South)Whether hold a gun: TrueTable 3: Comparison of goal-generation.。Cyan the helpful,pink the conflicting, and orange the critical sub-goals.。Itis evident that co-training enables goal-generation to avoidconflicts of sub-goals and improves reasonability by includ-ing helpful and critical sub-goals.ments mainly lies in 2≤ |g| ≤4, because |g|= 1 is tooeasy while |g| ≥5is too hard to complete.。Figure 3(b)shows a case of |g|= 3 that co-training indeed improvesthe completion ratio as the green curve.。It is noteworthythat the performance suddenly downgrades at each reset.This phenomenon is attributed to the reset of Gllmbreaksthe adaptation with πg, avoiding being trapped in local opti-mal.。Meanwhile, the performance tends to converge, whichindicates the successor loops produce a better adaptationBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps24252627282930Goal Completion Ratio (%)OursHER(a)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps15.017.520.022.525.027.530.032.5Goal Completion Ratio (%)OursHER (b)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps0.100.080.060.040.020.00Reward0.30.40.50.60.70.8Game StatisticsMean basic reward per stepMean basic reward per step (No KL)#Enemies killed#Enemies knocked down#Enemies killed (No KL)#Enemies knocked down (No KL) (c)Figure 2: (a) The goal completion rate on training dataset; (b) The goal completion rate on unseen goals, i.e., the test dataset;(c) The evaluation of policy learning in cases of w/ and w/o KL-divergence regularizer.。(a) Goal completion rate ( 1≤ |g| ≤7)Trend of  p o l i c y  a b i l i t ySmoothe d  C o m p l e t i o n  R a t eOriginal C o m p l e t i o n  R a t eReset Lo o p sGoal Com p l e t i o n  R a t i o  ( % )(b) Goal completion rate ( |g|= 3)Sub goal        g4   g8   g12   g16   g20 Time in one loop0.0250.0500.0750.1000.1250.1500.1750.200Possibility (c) Sub-goals distributionFigure 3: (a) The completion ratio of goals with dimension size ranges from 1 to 7; (b) The goal completion ratio of goalsthat|g|= 3, the trend curve reflects the improving completion ratio; (c) The sub-goals distribution changes along thetraining in one loop of co-training, where the description of each giis included in Table 14.between LLM and policy than before.。Additionally, weinvestigated the change in the generation probability of sub-goals (Table 14) during co-training.。Specifically, Figure 3(c)illustrates changes within a training loop, while Figure 11(a)indicates changes across loops.。As training progresses, theprobabilities associated with each giundergo gradual modi-fications.。For instance, sub-goals with growing probabilitiesare central to the agent private states due to their relativelyattainable nature and influence in agent interaction.。Con-versely, sub-goals with falling probabilities are central toother players’ states, as they are not directly changed byagent actions, and Gllmtends to generate outputs for thesesub-goals only when absolutely necessary.。To investigatethe impact of co-training to Gllm, we have also identifiedthe changes of goal-generation for an instruction, as shownin Table 3.. \n",
      "the piece 179\n",
      " \n",
      "a) shows that the improve-Instruction Goal(Before co-training) Goal(After co-training)Stop!Whether prone position: TrueAverage velocity: StaticAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementGet down,stay hidden.Whether prone position: TrueAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementAverage velocity: StaticWhether seen by enemy: FalseEnemy!Rush and fire.Whether hold a gun: TrueWhether have bullets: TrueHorizontal direction of view: SouthwestWhether seen enemy: TrueAverage velocity: FastLength of distance moved: longAverage velocity: FastWhether hold a gun: TrueHorizontal direction of movement: SouthwestWhether seen enemy: TrueDamage to enemy: HighHorizontal direction of view: SouthwestEnemies nearby,move to defendand avoid damage.Whether prone position: TrueAverage velocity: FastWhether hold a gun: TrueHealth level: FullWhether to restore health: TrueLength of distance moved: longAverage velocity: FastWhether prone position: FalseHorizontal direction of movement: North(Position of enemy: South)Whether hold a gun: TrueTable 3: Comparison of goal-generation.。Cyan the helpful,pink the conflicting, and orange the critical sub-goals.。Itis evident that co-training enables goal-generation to avoidconflicts of sub-goals and improves reasonability by includ-ing helpful and critical sub-goals.ments mainly lies in 2≤ |g| ≤4, because |g|= 1 is tooeasy while |g| ≥5is too hard to complete.。Figure 3(b)shows a case of |g|= 3 that co-training indeed improvesthe completion ratio as the green curve.。It is noteworthythat the performance suddenly downgrades at each reset.This phenomenon is attributed to the reset of Gllmbreaksthe adaptation with πg, avoiding being trapped in local opti-mal.。Meanwhile, the performance tends to converge, whichindicates the successor loops produce a better adaptationBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps24252627282930Goal Completion Ratio (%)OursHER(a)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps15.017.520.022.525.027.530.032.5Goal Completion Ratio (%)OursHER (b)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps0.100.080.060.040.020.00Reward0.30.40.50.60.70.8Game StatisticsMean basic reward per stepMean basic reward per step (No KL)#Enemies killed#Enemies knocked down#Enemies killed (No KL)#Enemies knocked down (No KL) (c)Figure 2: (a) The goal completion rate on training dataset; (b) The goal completion rate on unseen goals, i.e., the test dataset;(c) The evaluation of policy learning in cases of w/ and w/o KL-divergence regularizer.。(a) Goal completion rate ( 1≤ |g| ≤7)Trend of  p o l i c y  a b i l i t ySmoothe d  C o m p l e t i o n  R a t eOriginal C o m p l e t i o n  R a t eReset Lo o p sGoal Com p l e t i o n  R a t i o  ( % )(b) Goal completion rate ( |g|= 3)Sub goal        g4   g8   g12   g16   g20 Time in one loop0.0250.0500.0750.1000.1250.1500.1750.200Possibility (c) Sub-goals distributionFigure 3: (a) The completion ratio of goals with dimension size ranges from 1 to 7; (b) The goal completion ratio of goalsthat|g|= 3, the trend curve reflects the improving completion ratio; (c) The sub-goals distribution changes along thetraining in one loop of co-training, where the description of each giis included in Table 14.between LLM and policy than before.。Additionally, weinvestigated the change in the generation probability of sub-goals (Table 14) during co-training.。Specifically, Figure 3(c)illustrates changes within a training loop, while Figure 11(a)indicates changes across loops.。As training progresses, theprobabilities associated with each giundergo gradual modi-fications.。For instance, sub-goals with growing probabilitiesare central to the agent private states due to their relativelyattainable nature and influence in agent interaction.。Con-versely, sub-goals with falling probabilities are central toother players’ states, as they are not directly changed byagent actions, and Gllmtends to generate outputs for thesesub-goals only when absolutely necessary.。To investigatethe impact of co-training to Gllm, we have also identifiedthe changes of goal-generation for an instruction, as shownin Table 3.。Evidently, after co-training, Gllmdemonstratesits capacity to eliminate contradictory and irrational ele-ments within the initial objectives and exhibits the ability tointroduce new sub-goals, thereby rendering the overall goalmore attainable, all while retaining its exceptional semanticcomprehension capabilities.6.. \n",
      "the piece 180\n",
      " \n",
      "ity: StaticWhether prone position: TrueLength of distance moved: No MovementGet down,stay hidden.Whether prone position: TrueAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementAverage velocity: StaticWhether seen by enemy: FalseEnemy!Rush and fire.Whether hold a gun: TrueWhether have bullets: TrueHorizontal direction of view: SouthwestWhether seen enemy: TrueAverage velocity: FastLength of distance moved: longAverage velocity: FastWhether hold a gun: TrueHorizontal direction of movement: SouthwestWhether seen enemy: TrueDamage to enemy: HighHorizontal direction of view: SouthwestEnemies nearby,move to defendand avoid damage.Whether prone position: TrueAverage velocity: FastWhether hold a gun: TrueHealth level: FullWhether to restore health: TrueLength of distance moved: longAverage velocity: FastWhether prone position: FalseHorizontal direction of movement: North(Position of enemy: South)Whether hold a gun: TrueTable 3: Comparison of goal-generation.。Cyan the helpful,pink the conflicting, and orange the critical sub-goals.。Itis evident that co-training enables goal-generation to avoidconflicts of sub-goals and improves reasonability by includ-ing helpful and critical sub-goals.ments mainly lies in 2≤ |g| ≤4, because |g|= 1 is tooeasy while |g| ≥5is too hard to complete.。Figure 3(b)shows a case of |g|= 3 that co-training indeed improvesthe completion ratio as the green curve.。It is noteworthythat the performance suddenly downgrades at each reset.This phenomenon is attributed to the reset of Gllmbreaksthe adaptation with πg, avoiding being trapped in local opti-mal.。Meanwhile, the performance tends to converge, whichindicates the successor loops produce a better adaptationBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps24252627282930Goal Completion Ratio (%)OursHER(a)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps15.017.520.022.525.027.530.032.5Goal Completion Ratio (%)OursHER (b)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps0.100.080.060.040.020.00Reward0.30.40.50.60.70.8Game StatisticsMean basic reward per stepMean basic reward per step (No KL)#Enemies killed#Enemies knocked down#Enemies killed (No KL)#Enemies knocked down (No KL) (c)Figure 2: (a) The goal completion rate on training dataset; (b) The goal completion rate on unseen goals, i.e., the test dataset;(c) The evaluation of policy learning in cases of w/ and w/o KL-divergence regularizer.。(a) Goal completion rate ( 1≤ |g| ≤7)Trend of  p o l i c y  a b i l i t ySmoothe d  C o m p l e t i o n  R a t eOriginal C o m p l e t i o n  R a t eReset Lo o p sGoal Com p l e t i o n  R a t i o  ( % )(b) Goal completion rate ( |g|= 3)Sub goal        g4   g8   g12   g16   g20 Time in one loop0.0250.0500.0750.1000.1250.1500.1750.200Possibility (c) Sub-goals distributionFigure 3: (a) The completion ratio of goals with dimension size ranges from 1 to 7; (b) The goal completion ratio of goalsthat|g|= 3, the trend curve reflects the improving completion ratio; (c) The sub-goals distribution changes along thetraining in one loop of co-training, where the description of each giis included in Table 14.between LLM and policy than before.。Additionally, weinvestigated the change in the generation probability of sub-goals (Table 14) during co-training.。Specifically, Figure 3(c)illustrates changes within a training loop, while Figure 11(a)indicates changes across loops.。As training progresses, theprobabilities associated with each giundergo gradual modi-fications.。For instance, sub-goals with growing probabilitiesare central to the agent private states due to their relativelyattainable nature and influence in agent interaction.。Con-versely, sub-goals with falling probabilities are central toother players’ states, as they are not directly changed byagent actions, and Gllmtends to generate outputs for thesesub-goals only when absolutely necessary.。To investigatethe impact of co-training to Gllm, we have also identifiedthe changes of goal-generation for an instruction, as shownin Table 3.。Evidently, after co-training, Gllmdemonstratesits capacity to eliminate contradictory and irrational ele-ments within the initial objectives and exhibits the ability tointroduce new sub-goals, thereby rendering the overall goalmore attainable, all while retaining its exceptional semanticcomprehension capabilities.6.。ConclusionIn this paper, we propose OpenPAL experts on learningopen-ended embodied agents for human-AI interaction, ex-celling in achieving instruction open-endedness through atwo-stage learning process.. \n",
      "the piece 181\n",
      " \n",
      "bility by includ-ing helpful and critical sub-goals.ments mainly lies in 2≤ |g| ≤4, because |g|= 1 is tooeasy while |g| ≥5is too hard to complete.。Figure 3(b)shows a case of |g|= 3 that co-training indeed improvesthe completion ratio as the green curve.。It is noteworthythat the performance suddenly downgrades at each reset.This phenomenon is attributed to the reset of Gllmbreaksthe adaptation with πg, avoiding being trapped in local opti-mal.。Meanwhile, the performance tends to converge, whichindicates the successor loops produce a better adaptationBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps24252627282930Goal Completion Ratio (%)OursHER(a)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps15.017.520.022.525.027.530.032.5Goal Completion Ratio (%)OursHER (b)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps0.100.080.060.040.020.00Reward0.30.40.50.60.70.8Game StatisticsMean basic reward per stepMean basic reward per step (No KL)#Enemies killed#Enemies knocked down#Enemies killed (No KL)#Enemies knocked down (No KL) (c)Figure 2: (a) The goal completion rate on training dataset; (b) The goal completion rate on unseen goals, i.e., the test dataset;(c) The evaluation of policy learning in cases of w/ and w/o KL-divergence regularizer.。(a) Goal completion rate ( 1≤ |g| ≤7)Trend of  p o l i c y  a b i l i t ySmoothe d  C o m p l e t i o n  R a t eOriginal C o m p l e t i o n  R a t eReset Lo o p sGoal Com p l e t i o n  R a t i o  ( % )(b) Goal completion rate ( |g|= 3)Sub goal        g4   g8   g12   g16   g20 Time in one loop0.0250.0500.0750.1000.1250.1500.1750.200Possibility (c) Sub-goals distributionFigure 3: (a) The completion ratio of goals with dimension size ranges from 1 to 7; (b) The goal completion ratio of goalsthat|g|= 3, the trend curve reflects the improving completion ratio; (c) The sub-goals distribution changes along thetraining in one loop of co-training, where the description of each giis included in Table 14.between LLM and policy than before.。Additionally, weinvestigated the change in the generation probability of sub-goals (Table 14) during co-training.。Specifically, Figure 3(c)illustrates changes within a training loop, while Figure 11(a)indicates changes across loops.。As training progresses, theprobabilities associated with each giundergo gradual modi-fications.。For instance, sub-goals with growing probabilitiesare central to the agent private states due to their relativelyattainable nature and influence in agent interaction.。Con-versely, sub-goals with falling probabilities are central toother players’ states, as they are not directly changed byagent actions, and Gllmtends to generate outputs for thesesub-goals only when absolutely necessary.。To investigatethe impact of co-training to Gllm, we have also identifiedthe changes of goal-generation for an instruction, as shownin Table 3.。Evidently, after co-training, Gllmdemonstratesits capacity to eliminate contradictory and irrational ele-ments within the initial objectives and exhibits the ability tointroduce new sub-goals, thereby rendering the overall goalmore attainable, all while retaining its exceptional semanticcomprehension capabilities.6.。ConclusionIn this paper, we propose OpenPAL experts on learningopen-ended embodied agents for human-AI interaction, ex-celling in achieving instruction open-endedness through atwo-stage learning process.。The empirical results on Contrarepresent that OpenPAL shows the potential as a practicalsolution for human-AI interaction in complex situations.Despite the positive results, we admit there are still somelimitations to our work that would be expected to be re-searched in the future—for instance, a truly open-endedgoal description instead of the handcrafted goal space in thecurrent version; supporting multi-modality input/output tofree from expensive feature engineering.Author Contribution StatementThe authors confirm their contribution as follows:Shaopeng Zhai : team leadership, open-ended learning,LLM/RLAF training, agent analysis, architecture designJie Wang : infrastructure/framework engineering, non-goalBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationagent training, open-ended learning, ablation studies, fea-ture engineeringTianyi Zhang : non-goal agent training, open-ended learn-ing, feature engineeringFuxian Huang : non-goal agent training, paper writing,open-ended learningQi Zhang : LLM training, RLAF training, paper writing,ablation studiesMing Zhou : co-training framework, curriculum research,paper writingJing Hou : LLM training, paper writingReferencesAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O.,and Zaremba, W. Hindsight experience replay.. \n",
      "the piece 182\n",
      " \n",
      " |g| ≤4, because |g|= 1 is tooeasy while |g| ≥5is too hard to complete.。Figure 3(b)shows a case of |g|= 3 that co-training indeed improvesthe completion ratio as the green curve.。It is noteworthythat the performance suddenly downgrades at each reset.This phenomenon is attributed to the reset of Gllmbreaksthe adaptation with πg, avoiding being trapped in local opti-mal.。Meanwhile, the performance tends to converge, whichindicates the successor loops produce a better adaptationBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps24252627282930Goal Completion Ratio (%)OursHER(a)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps15.017.520.022.525.027.530.032.5Goal Completion Ratio (%)OursHER (b)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps0.100.080.060.040.020.00Reward0.30.40.50.60.70.8Game StatisticsMean basic reward per stepMean basic reward per step (No KL)#Enemies killed#Enemies knocked down#Enemies killed (No KL)#Enemies knocked down (No KL) (c)Figure 2: (a) The goal completion rate on training dataset; (b) The goal completion rate on unseen goals, i.e., the test dataset;(c) The evaluation of policy learning in cases of w/ and w/o KL-divergence regularizer.。(a) Goal completion rate ( 1≤ |g| ≤7)Trend of  p o l i c y  a b i l i t ySmoothe d  C o m p l e t i o n  R a t eOriginal C o m p l e t i o n  R a t eReset Lo o p sGoal Com p l e t i o n  R a t i o  ( % )(b) Goal completion rate ( |g|= 3)Sub goal        g4   g8   g12   g16   g20 Time in one loop0.0250.0500.0750.1000.1250.1500.1750.200Possibility (c) Sub-goals distributionFigure 3: (a) The completion ratio of goals with dimension size ranges from 1 to 7; (b) The goal completion ratio of goalsthat|g|= 3, the trend curve reflects the improving completion ratio; (c) The sub-goals distribution changes along thetraining in one loop of co-training, where the description of each giis included in Table 14.between LLM and policy than before.。Additionally, weinvestigated the change in the generation probability of sub-goals (Table 14) during co-training.。Specifically, Figure 3(c)illustrates changes within a training loop, while Figure 11(a)indicates changes across loops.。As training progresses, theprobabilities associated with each giundergo gradual modi-fications.。For instance, sub-goals with growing probabilitiesare central to the agent private states due to their relativelyattainable nature and influence in agent interaction.。Con-versely, sub-goals with falling probabilities are central toother players’ states, as they are not directly changed byagent actions, and Gllmtends to generate outputs for thesesub-goals only when absolutely necessary.。To investigatethe impact of co-training to Gllm, we have also identifiedthe changes of goal-generation for an instruction, as shownin Table 3.。Evidently, after co-training, Gllmdemonstratesits capacity to eliminate contradictory and irrational ele-ments within the initial objectives and exhibits the ability tointroduce new sub-goals, thereby rendering the overall goalmore attainable, all while retaining its exceptional semanticcomprehension capabilities.6.。ConclusionIn this paper, we propose OpenPAL experts on learningopen-ended embodied agents for human-AI interaction, ex-celling in achieving instruction open-endedness through atwo-stage learning process.。The empirical results on Contrarepresent that OpenPAL shows the potential as a practicalsolution for human-AI interaction in complex situations.Despite the positive results, we admit there are still somelimitations to our work that would be expected to be re-searched in the future—for instance, a truly open-endedgoal description instead of the handcrafted goal space in thecurrent version; supporting multi-modality input/output tofree from expensive feature engineering.Author Contribution StatementThe authors confirm their contribution as follows:Shaopeng Zhai : team leadership, open-ended learning,LLM/RLAF training, agent analysis, architecture designJie Wang : infrastructure/framework engineering, non-goalBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationagent training, open-ended learning, ablation studies, fea-ture engineeringTianyi Zhang : non-goal agent training, open-ended learn-ing, feature engineeringFuxian Huang : non-goal agent training, paper writing,open-ended learningQi Zhang : LLM training, RLAF training, paper writing,ablation studiesMing Zhou : co-training framework, curriculum research,paper writingJing Hou : LLM training, paper writingReferencesAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O.,and Zaremba, W. Hindsight experience replay.。Advancesin neural information processing systems , 30, 2017.Bai, C., Liu, P., Zhao, W., and Tang, X.. \n",
      "the piece 183\n",
      " \n",
      "s phenomenon is attributed to the reset of Gllmbreaksthe adaptation with πg, avoiding being trapped in local opti-mal.。Meanwhile, the performance tends to converge, whichindicates the successor loops produce a better adaptationBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps24252627282930Goal Completion Ratio (%)OursHER(a)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps15.017.520.022.525.027.530.032.5Goal Completion Ratio (%)OursHER (b)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps0.100.080.060.040.020.00Reward0.30.40.50.60.70.8Game StatisticsMean basic reward per stepMean basic reward per step (No KL)#Enemies killed#Enemies knocked down#Enemies killed (No KL)#Enemies knocked down (No KL) (c)Figure 2: (a) The goal completion rate on training dataset; (b) The goal completion rate on unseen goals, i.e., the test dataset;(c) The evaluation of policy learning in cases of w/ and w/o KL-divergence regularizer.。(a) Goal completion rate ( 1≤ |g| ≤7)Trend of  p o l i c y  a b i l i t ySmoothe d  C o m p l e t i o n  R a t eOriginal C o m p l e t i o n  R a t eReset Lo o p sGoal Com p l e t i o n  R a t i o  ( % )(b) Goal completion rate ( |g|= 3)Sub goal        g4   g8   g12   g16   g20 Time in one loop0.0250.0500.0750.1000.1250.1500.1750.200Possibility (c) Sub-goals distributionFigure 3: (a) The completion ratio of goals with dimension size ranges from 1 to 7; (b) The goal completion ratio of goalsthat|g|= 3, the trend curve reflects the improving completion ratio; (c) The sub-goals distribution changes along thetraining in one loop of co-training, where the description of each giis included in Table 14.between LLM and policy than before.。Additionally, weinvestigated the change in the generation probability of sub-goals (Table 14) during co-training.。Specifically, Figure 3(c)illustrates changes within a training loop, while Figure 11(a)indicates changes across loops.。As training progresses, theprobabilities associated with each giundergo gradual modi-fications.。For instance, sub-goals with growing probabilitiesare central to the agent private states due to their relativelyattainable nature and influence in agent interaction.。Con-versely, sub-goals with falling probabilities are central toother players’ states, as they are not directly changed byagent actions, and Gllmtends to generate outputs for thesesub-goals only when absolutely necessary.。To investigatethe impact of co-training to Gllm, we have also identifiedthe changes of goal-generation for an instruction, as shownin Table 3.。Evidently, after co-training, Gllmdemonstratesits capacity to eliminate contradictory and irrational ele-ments within the initial objectives and exhibits the ability tointroduce new sub-goals, thereby rendering the overall goalmore attainable, all while retaining its exceptional semanticcomprehension capabilities.6.。ConclusionIn this paper, we propose OpenPAL experts on learningopen-ended embodied agents for human-AI interaction, ex-celling in achieving instruction open-endedness through atwo-stage learning process.。The empirical results on Contrarepresent that OpenPAL shows the potential as a practicalsolution for human-AI interaction in complex situations.Despite the positive results, we admit there are still somelimitations to our work that would be expected to be re-searched in the future—for instance, a truly open-endedgoal description instead of the handcrafted goal space in thecurrent version; supporting multi-modality input/output tofree from expensive feature engineering.Author Contribution StatementThe authors confirm their contribution as follows:Shaopeng Zhai : team leadership, open-ended learning,LLM/RLAF training, agent analysis, architecture designJie Wang : infrastructure/framework engineering, non-goalBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationagent training, open-ended learning, ablation studies, fea-ture engineeringTianyi Zhang : non-goal agent training, open-ended learn-ing, feature engineeringFuxian Huang : non-goal agent training, paper writing,open-ended learningQi Zhang : LLM training, RLAF training, paper writing,ablation studiesMing Zhou : co-training framework, curriculum research,paper writingJing Hou : LLM training, paper writingReferencesAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O.,and Zaremba, W. Hindsight experience replay.。Advancesin neural information processing systems , 30, 2017.Bai, C., Liu, P., Zhao, W., and Tang, X.。Guided goal gen-eration for hindsight multi-goal reinforcement learning.Neurocomputing , 359:353–367, 2019.Balduzzi, D., Garnelo, M., Bachrach, Y ., Czarnecki, W.,Perolat, J., Jaderberg, M., and Graepel, T. Open-endedlearning in symmetric zero-sum games.. \n",
      "the piece 184\n",
      " \n",
      " opti-mal.。Meanwhile, the performance tends to converge, whichindicates the successor loops produce a better adaptationBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps24252627282930Goal Completion Ratio (%)OursHER(a)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps15.017.520.022.525.027.530.032.5Goal Completion Ratio (%)OursHER (b)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps0.100.080.060.040.020.00Reward0.30.40.50.60.70.8Game StatisticsMean basic reward per stepMean basic reward per step (No KL)#Enemies killed#Enemies knocked down#Enemies killed (No KL)#Enemies knocked down (No KL) (c)Figure 2: (a) The goal completion rate on training dataset; (b) The goal completion rate on unseen goals, i.e., the test dataset;(c) The evaluation of policy learning in cases of w/ and w/o KL-divergence regularizer.。(a) Goal completion rate ( 1≤ |g| ≤7)Trend of  p o l i c y  a b i l i t ySmoothe d  C o m p l e t i o n  R a t eOriginal C o m p l e t i o n  R a t eReset Lo o p sGoal Com p l e t i o n  R a t i o  ( % )(b) Goal completion rate ( |g|= 3)Sub goal        g4   g8   g12   g16   g20 Time in one loop0.0250.0500.0750.1000.1250.1500.1750.200Possibility (c) Sub-goals distributionFigure 3: (a) The completion ratio of goals with dimension size ranges from 1 to 7; (b) The goal completion ratio of goalsthat|g|= 3, the trend curve reflects the improving completion ratio; (c) The sub-goals distribution changes along thetraining in one loop of co-training, where the description of each giis included in Table 14.between LLM and policy than before.。Additionally, weinvestigated the change in the generation probability of sub-goals (Table 14) during co-training.。Specifically, Figure 3(c)illustrates changes within a training loop, while Figure 11(a)indicates changes across loops.。As training progresses, theprobabilities associated with each giundergo gradual modi-fications.。For instance, sub-goals with growing probabilitiesare central to the agent private states due to their relativelyattainable nature and influence in agent interaction.。Con-versely, sub-goals with falling probabilities are central toother players’ states, as they are not directly changed byagent actions, and Gllmtends to generate outputs for thesesub-goals only when absolutely necessary.。To investigatethe impact of co-training to Gllm, we have also identifiedthe changes of goal-generation for an instruction, as shownin Table 3.。Evidently, after co-training, Gllmdemonstratesits capacity to eliminate contradictory and irrational ele-ments within the initial objectives and exhibits the ability tointroduce new sub-goals, thereby rendering the overall goalmore attainable, all while retaining its exceptional semanticcomprehension capabilities.6.。ConclusionIn this paper, we propose OpenPAL experts on learningopen-ended embodied agents for human-AI interaction, ex-celling in achieving instruction open-endedness through atwo-stage learning process.。The empirical results on Contrarepresent that OpenPAL shows the potential as a practicalsolution for human-AI interaction in complex situations.Despite the positive results, we admit there are still somelimitations to our work that would be expected to be re-searched in the future—for instance, a truly open-endedgoal description instead of the handcrafted goal space in thecurrent version; supporting multi-modality input/output tofree from expensive feature engineering.Author Contribution StatementThe authors confirm their contribution as follows:Shaopeng Zhai : team leadership, open-ended learning,LLM/RLAF training, agent analysis, architecture designJie Wang : infrastructure/framework engineering, non-goalBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationagent training, open-ended learning, ablation studies, fea-ture engineeringTianyi Zhang : non-goal agent training, open-ended learn-ing, feature engineeringFuxian Huang : non-goal agent training, paper writing,open-ended learningQi Zhang : LLM training, RLAF training, paper writing,ablation studiesMing Zhou : co-training framework, curriculum research,paper writingJing Hou : LLM training, paper writingReferencesAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O.,and Zaremba, W. Hindsight experience replay.。Advancesin neural information processing systems , 30, 2017.Bai, C., Liu, P., Zhao, W., and Tang, X.。Guided goal gen-eration for hindsight multi-goal reinforcement learning.Neurocomputing , 359:353–367, 2019.Balduzzi, D., Garnelo, M., Bachrach, Y ., Czarnecki, W.,Perolat, J., Jaderberg, M., and Graepel, T. Open-endedlearning in symmetric zero-sum games.。In InternationalConference on Machine Learning , pp.. \n",
      "the piece 185\n",
      " \n",
      "tion00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps24252627282930Goal Completion Ratio (%)OursHER(a)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps15.017.520.022.525.027.530.032.5Goal Completion Ratio (%)OursHER (b)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps0.100.080.060.040.020.00Reward0.30.40.50.60.70.8Game StatisticsMean basic reward per stepMean basic reward per step (No KL)#Enemies killed#Enemies knocked down#Enemies killed (No KL)#Enemies knocked down (No KL) (c)Figure 2: (a) The goal completion rate on training dataset; (b) The goal completion rate on unseen goals, i.e., the test dataset;(c) The evaluation of policy learning in cases of w/ and w/o KL-divergence regularizer.。(a) Goal completion rate ( 1≤ |g| ≤7)Trend of  p o l i c y  a b i l i t ySmoothe d  C o m p l e t i o n  R a t eOriginal C o m p l e t i o n  R a t eReset Lo o p sGoal Com p l e t i o n  R a t i o  ( % )(b) Goal completion rate ( |g|= 3)Sub goal        g4   g8   g12   g16   g20 Time in one loop0.0250.0500.0750.1000.1250.1500.1750.200Possibility (c) Sub-goals distributionFigure 3: (a) The completion ratio of goals with dimension size ranges from 1 to 7; (b) The goal completion ratio of goalsthat|g|= 3, the trend curve reflects the improving completion ratio; (c) The sub-goals distribution changes along thetraining in one loop of co-training, where the description of each giis included in Table 14.between LLM and policy than before.。Additionally, weinvestigated the change in the generation probability of sub-goals (Table 14) during co-training.。Specifically, Figure 3(c)illustrates changes within a training loop, while Figure 11(a)indicates changes across loops.。As training progresses, theprobabilities associated with each giundergo gradual modi-fications.。For instance, sub-goals with growing probabilitiesare central to the agent private states due to their relativelyattainable nature and influence in agent interaction.。Con-versely, sub-goals with falling probabilities are central toother players’ states, as they are not directly changed byagent actions, and Gllmtends to generate outputs for thesesub-goals only when absolutely necessary.。To investigatethe impact of co-training to Gllm, we have also identifiedthe changes of goal-generation for an instruction, as shownin Table 3.。Evidently, after co-training, Gllmdemonstratesits capacity to eliminate contradictory and irrational ele-ments within the initial objectives and exhibits the ability tointroduce new sub-goals, thereby rendering the overall goalmore attainable, all while retaining its exceptional semanticcomprehension capabilities.6.。ConclusionIn this paper, we propose OpenPAL experts on learningopen-ended embodied agents for human-AI interaction, ex-celling in achieving instruction open-endedness through atwo-stage learning process.。The empirical results on Contrarepresent that OpenPAL shows the potential as a practicalsolution for human-AI interaction in complex situations.Despite the positive results, we admit there are still somelimitations to our work that would be expected to be re-searched in the future—for instance, a truly open-endedgoal description instead of the handcrafted goal space in thecurrent version; supporting multi-modality input/output tofree from expensive feature engineering.Author Contribution StatementThe authors confirm their contribution as follows:Shaopeng Zhai : team leadership, open-ended learning,LLM/RLAF training, agent analysis, architecture designJie Wang : infrastructure/framework engineering, non-goalBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationagent training, open-ended learning, ablation studies, fea-ture engineeringTianyi Zhang : non-goal agent training, open-ended learn-ing, feature engineeringFuxian Huang : non-goal agent training, paper writing,open-ended learningQi Zhang : LLM training, RLAF training, paper writing,ablation studiesMing Zhou : co-training framework, curriculum research,paper writingJing Hou : LLM training, paper writingReferencesAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O.,and Zaremba, W. Hindsight experience replay.。Advancesin neural information processing systems , 30, 2017.Bai, C., Liu, P., Zhao, W., and Tang, X.。Guided goal gen-eration for hindsight multi-goal reinforcement learning.Neurocomputing , 359:353–367, 2019.Balduzzi, D., Garnelo, M., Bachrach, Y ., Czarnecki, W.,Perolat, J., Jaderberg, M., and Graepel, T. Open-endedlearning in symmetric zero-sum games.。In InternationalConference on Machine Learning , pp.。434–443.. \n",
      "the piece 186\n",
      " \n",
      "n  R a t i o  ( % )(b) Goal completion rate ( |g|= 3)Sub goal        g4   g8   g12   g16   g20 Time in one loop0.0250.0500.0750.1000.1250.1500.1750.200Possibility (c) Sub-goals distributionFigure 3: (a) The completion ratio of goals with dimension size ranges from 1 to 7; (b) The goal completion ratio of goalsthat|g|= 3, the trend curve reflects the improving completion ratio; (c) The sub-goals distribution changes along thetraining in one loop of co-training, where the description of each giis included in Table 14.between LLM and policy than before.。Additionally, weinvestigated the change in the generation probability of sub-goals (Table 14) during co-training.。Specifically, Figure 3(c)illustrates changes within a training loop, while Figure 11(a)indicates changes across loops.。As training progresses, theprobabilities associated with each giundergo gradual modi-fications.。For instance, sub-goals with growing probabilitiesare central to the agent private states due to their relativelyattainable nature and influence in agent interaction.。Con-versely, sub-goals with falling probabilities are central toother players’ states, as they are not directly changed byagent actions, and Gllmtends to generate outputs for thesesub-goals only when absolutely necessary.。To investigatethe impact of co-training to Gllm, we have also identifiedthe changes of goal-generation for an instruction, as shownin Table 3.。Evidently, after co-training, Gllmdemonstratesits capacity to eliminate contradictory and irrational ele-ments within the initial objectives and exhibits the ability tointroduce new sub-goals, thereby rendering the overall goalmore attainable, all while retaining its exceptional semanticcomprehension capabilities.6.。ConclusionIn this paper, we propose OpenPAL experts on learningopen-ended embodied agents for human-AI interaction, ex-celling in achieving instruction open-endedness through atwo-stage learning process.。The empirical results on Contrarepresent that OpenPAL shows the potential as a practicalsolution for human-AI interaction in complex situations.Despite the positive results, we admit there are still somelimitations to our work that would be expected to be re-searched in the future—for instance, a truly open-endedgoal description instead of the handcrafted goal space in thecurrent version; supporting multi-modality input/output tofree from expensive feature engineering.Author Contribution StatementThe authors confirm their contribution as follows:Shaopeng Zhai : team leadership, open-ended learning,LLM/RLAF training, agent analysis, architecture designJie Wang : infrastructure/framework engineering, non-goalBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationagent training, open-ended learning, ablation studies, fea-ture engineeringTianyi Zhang : non-goal agent training, open-ended learn-ing, feature engineeringFuxian Huang : non-goal agent training, paper writing,open-ended learningQi Zhang : LLM training, RLAF training, paper writing,ablation studiesMing Zhou : co-training framework, curriculum research,paper writingJing Hou : LLM training, paper writingReferencesAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O.,and Zaremba, W. Hindsight experience replay.。Advancesin neural information processing systems , 30, 2017.Bai, C., Liu, P., Zhao, W., and Tang, X.。Guided goal gen-eration for hindsight multi-goal reinforcement learning.Neurocomputing , 359:353–367, 2019.Balduzzi, D., Garnelo, M., Bachrach, Y ., Czarnecki, W.,Perolat, J., Jaderberg, M., and Graepel, T. Open-endedlearning in symmetric zero-sum games.。In InternationalConference on Machine Learning , pp.。434–443.。PMLR,2019.Berner, C., Brockman, G., Chan, B., Cheung, V ., D˛ ebiak, P.,Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse,C., et al.. \n",
      "the piece 187\n",
      " \n",
      "ile Figure 11(a)indicates changes across loops.。As training progresses, theprobabilities associated with each giundergo gradual modi-fications.。For instance, sub-goals with growing probabilitiesare central to the agent private states due to their relativelyattainable nature and influence in agent interaction.。Con-versely, sub-goals with falling probabilities are central toother players’ states, as they are not directly changed byagent actions, and Gllmtends to generate outputs for thesesub-goals only when absolutely necessary.。To investigatethe impact of co-training to Gllm, we have also identifiedthe changes of goal-generation for an instruction, as shownin Table 3.。Evidently, after co-training, Gllmdemonstratesits capacity to eliminate contradictory and irrational ele-ments within the initial objectives and exhibits the ability tointroduce new sub-goals, thereby rendering the overall goalmore attainable, all while retaining its exceptional semanticcomprehension capabilities.6.。ConclusionIn this paper, we propose OpenPAL experts on learningopen-ended embodied agents for human-AI interaction, ex-celling in achieving instruction open-endedness through atwo-stage learning process.。The empirical results on Contrarepresent that OpenPAL shows the potential as a practicalsolution for human-AI interaction in complex situations.Despite the positive results, we admit there are still somelimitations to our work that would be expected to be re-searched in the future—for instance, a truly open-endedgoal description instead of the handcrafted goal space in thecurrent version; supporting multi-modality input/output tofree from expensive feature engineering.Author Contribution StatementThe authors confirm their contribution as follows:Shaopeng Zhai : team leadership, open-ended learning,LLM/RLAF training, agent analysis, architecture designJie Wang : infrastructure/framework engineering, non-goalBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationagent training, open-ended learning, ablation studies, fea-ture engineeringTianyi Zhang : non-goal agent training, open-ended learn-ing, feature engineeringFuxian Huang : non-goal agent training, paper writing,open-ended learningQi Zhang : LLM training, RLAF training, paper writing,ablation studiesMing Zhou : co-training framework, curriculum research,paper writingJing Hou : LLM training, paper writingReferencesAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O.,and Zaremba, W. Hindsight experience replay.。Advancesin neural information processing systems , 30, 2017.Bai, C., Liu, P., Zhao, W., and Tang, X.。Guided goal gen-eration for hindsight multi-goal reinforcement learning.Neurocomputing , 359:353–367, 2019.Balduzzi, D., Garnelo, M., Bachrach, Y ., Czarnecki, W.,Perolat, J., Jaderberg, M., and Graepel, T. Open-endedlearning in symmetric zero-sum games.。In InternationalConference on Machine Learning , pp.。434–443.。PMLR,2019.Berner, C., Brockman, G., Chan, B., Cheung, V ., D˛ ebiak, P.,Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse,C., et al.。Dota 2 with large scale deep reinforcementlearning, 2019.Brohan, A., Brown, N., Carbajal, J., Chebotar, Y ., Dabis, J.,Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A.,Hsu, J., et al.. \n",
      "the piece 188\n",
      " \n",
      "ergo gradual modi-fications.。For instance, sub-goals with growing probabilitiesare central to the agent private states due to their relativelyattainable nature and influence in agent interaction.。Con-versely, sub-goals with falling probabilities are central toother players’ states, as they are not directly changed byagent actions, and Gllmtends to generate outputs for thesesub-goals only when absolutely necessary.。To investigatethe impact of co-training to Gllm, we have also identifiedthe changes of goal-generation for an instruction, as shownin Table 3.。Evidently, after co-training, Gllmdemonstratesits capacity to eliminate contradictory and irrational ele-ments within the initial objectives and exhibits the ability tointroduce new sub-goals, thereby rendering the overall goalmore attainable, all while retaining its exceptional semanticcomprehension capabilities.6.。ConclusionIn this paper, we propose OpenPAL experts on learningopen-ended embodied agents for human-AI interaction, ex-celling in achieving instruction open-endedness through atwo-stage learning process.。The empirical results on Contrarepresent that OpenPAL shows the potential as a practicalsolution for human-AI interaction in complex situations.Despite the positive results, we admit there are still somelimitations to our work that would be expected to be re-searched in the future—for instance, a truly open-endedgoal description instead of the handcrafted goal space in thecurrent version; supporting multi-modality input/output tofree from expensive feature engineering.Author Contribution StatementThe authors confirm their contribution as follows:Shaopeng Zhai : team leadership, open-ended learning,LLM/RLAF training, agent analysis, architecture designJie Wang : infrastructure/framework engineering, non-goalBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationagent training, open-ended learning, ablation studies, fea-ture engineeringTianyi Zhang : non-goal agent training, open-ended learn-ing, feature engineeringFuxian Huang : non-goal agent training, paper writing,open-ended learningQi Zhang : LLM training, RLAF training, paper writing,ablation studiesMing Zhou : co-training framework, curriculum research,paper writingJing Hou : LLM training, paper writingReferencesAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O.,and Zaremba, W. Hindsight experience replay.。Advancesin neural information processing systems , 30, 2017.Bai, C., Liu, P., Zhao, W., and Tang, X.。Guided goal gen-eration for hindsight multi-goal reinforcement learning.Neurocomputing , 359:353–367, 2019.Balduzzi, D., Garnelo, M., Bachrach, Y ., Czarnecki, W.,Perolat, J., Jaderberg, M., and Graepel, T. Open-endedlearning in symmetric zero-sum games.。In InternationalConference on Machine Learning , pp.。434–443.。PMLR,2019.Berner, C., Brockman, G., Chan, B., Cheung, V ., D˛ ebiak, P.,Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse,C., et al.。Dota 2 with large scale deep reinforcementlearning, 2019.Brohan, A., Brown, N., Carbajal, J., Chebotar, Y ., Dabis, J.,Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A.,Hsu, J., et al.。Rt-1: Robotics transformer for real-worldcontrol at scale.. \n",
      "the piece 189\n",
      " \n",
      "ue to their relativelyattainable nature and influence in agent interaction.。Con-versely, sub-goals with falling probabilities are central toother players’ states, as they are not directly changed byagent actions, and Gllmtends to generate outputs for thesesub-goals only when absolutely necessary.。To investigatethe impact of co-training to Gllm, we have also identifiedthe changes of goal-generation for an instruction, as shownin Table 3.。Evidently, after co-training, Gllmdemonstratesits capacity to eliminate contradictory and irrational ele-ments within the initial objectives and exhibits the ability tointroduce new sub-goals, thereby rendering the overall goalmore attainable, all while retaining its exceptional semanticcomprehension capabilities.6.。ConclusionIn this paper, we propose OpenPAL experts on learningopen-ended embodied agents for human-AI interaction, ex-celling in achieving instruction open-endedness through atwo-stage learning process.。The empirical results on Contrarepresent that OpenPAL shows the potential as a practicalsolution for human-AI interaction in complex situations.Despite the positive results, we admit there are still somelimitations to our work that would be expected to be re-searched in the future—for instance, a truly open-endedgoal description instead of the handcrafted goal space in thecurrent version; supporting multi-modality input/output tofree from expensive feature engineering.Author Contribution StatementThe authors confirm their contribution as follows:Shaopeng Zhai : team leadership, open-ended learning,LLM/RLAF training, agent analysis, architecture designJie Wang : infrastructure/framework engineering, non-goalBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationagent training, open-ended learning, ablation studies, fea-ture engineeringTianyi Zhang : non-goal agent training, open-ended learn-ing, feature engineeringFuxian Huang : non-goal agent training, paper writing,open-ended learningQi Zhang : LLM training, RLAF training, paper writing,ablation studiesMing Zhou : co-training framework, curriculum research,paper writingJing Hou : LLM training, paper writingReferencesAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O.,and Zaremba, W. Hindsight experience replay.。Advancesin neural information processing systems , 30, 2017.Bai, C., Liu, P., Zhao, W., and Tang, X.。Guided goal gen-eration for hindsight multi-goal reinforcement learning.Neurocomputing , 359:353–367, 2019.Balduzzi, D., Garnelo, M., Bachrach, Y ., Czarnecki, W.,Perolat, J., Jaderberg, M., and Graepel, T. Open-endedlearning in symmetric zero-sum games.。In InternationalConference on Machine Learning , pp.。434–443.。PMLR,2019.Berner, C., Brockman, G., Chan, B., Cheung, V ., D˛ ebiak, P.,Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse,C., et al.。Dota 2 with large scale deep reinforcementlearning, 2019.Brohan, A., Brown, N., Carbajal, J., Chebotar, Y ., Dabis, J.,Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A.,Hsu, J., et al.。Rt-1: Robotics transformer for real-worldcontrol at scale.。arXiv preprint arXiv:2212.06817 , 2022.Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,S., Radford, A., Sutskever, I., and Amodei, D. Languagemodels are few-shot learners, 2020.Burda, Y ., Edwards, H., Storkey, A., and Klimov, O. Explo-ration by random network distillation.. \n",
      "the piece 190\n",
      " \n",
      "s with falling probabilities are central toother players’ states, as they are not directly changed byagent actions, and Gllmtends to generate outputs for thesesub-goals only when absolutely necessary.。To investigatethe impact of co-training to Gllm, we have also identifiedthe changes of goal-generation for an instruction, as shownin Table 3.。Evidently, after co-training, Gllmdemonstratesits capacity to eliminate contradictory and irrational ele-ments within the initial objectives and exhibits the ability tointroduce new sub-goals, thereby rendering the overall goalmore attainable, all while retaining its exceptional semanticcomprehension capabilities.6.。ConclusionIn this paper, we propose OpenPAL experts on learningopen-ended embodied agents for human-AI interaction, ex-celling in achieving instruction open-endedness through atwo-stage learning process.。The empirical results on Contrarepresent that OpenPAL shows the potential as a practicalsolution for human-AI interaction in complex situations.Despite the positive results, we admit there are still somelimitations to our work that would be expected to be re-searched in the future—for instance, a truly open-endedgoal description instead of the handcrafted goal space in thecurrent version; supporting multi-modality input/output tofree from expensive feature engineering.Author Contribution StatementThe authors confirm their contribution as follows:Shaopeng Zhai : team leadership, open-ended learning,LLM/RLAF training, agent analysis, architecture designJie Wang : infrastructure/framework engineering, non-goalBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationagent training, open-ended learning, ablation studies, fea-ture engineeringTianyi Zhang : non-goal agent training, open-ended learn-ing, feature engineeringFuxian Huang : non-goal agent training, paper writing,open-ended learningQi Zhang : LLM training, RLAF training, paper writing,ablation studiesMing Zhou : co-training framework, curriculum research,paper writingJing Hou : LLM training, paper writingReferencesAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O.,and Zaremba, W. Hindsight experience replay.。Advancesin neural information processing systems , 30, 2017.Bai, C., Liu, P., Zhao, W., and Tang, X.。Guided goal gen-eration for hindsight multi-goal reinforcement learning.Neurocomputing , 359:353–367, 2019.Balduzzi, D., Garnelo, M., Bachrach, Y ., Czarnecki, W.,Perolat, J., Jaderberg, M., and Graepel, T. Open-endedlearning in symmetric zero-sum games.。In InternationalConference on Machine Learning , pp.。434–443.。PMLR,2019.Berner, C., Brockman, G., Chan, B., Cheung, V ., D˛ ebiak, P.,Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse,C., et al.。Dota 2 with large scale deep reinforcementlearning, 2019.Brohan, A., Brown, N., Carbajal, J., Chebotar, Y ., Dabis, J.,Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A.,Hsu, J., et al.。Rt-1: Robotics transformer for real-worldcontrol at scale.。arXiv preprint arXiv:2212.06817 , 2022.Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,S., Radford, A., Sutskever, I., and Amodei, D. Languagemodels are few-shot learners, 2020.Burda, Y ., Edwards, H., Storkey, A., and Klimov, O. Explo-ration by random network distillation.。In InternationalConference on Learning Representations , 2018.Ding, H., Tang, Y ., Wu, Q., Wang, B., Chen, C., andWang, Z.. \n",
      "the piece 191\n",
      " \n",
      " only when absolutely necessary.。To investigatethe impact of co-training to Gllm, we have also identifiedthe changes of goal-generation for an instruction, as shownin Table 3.。Evidently, after co-training, Gllmdemonstratesits capacity to eliminate contradictory and irrational ele-ments within the initial objectives and exhibits the ability tointroduce new sub-goals, thereby rendering the overall goalmore attainable, all while retaining its exceptional semanticcomprehension capabilities.6.。ConclusionIn this paper, we propose OpenPAL experts on learningopen-ended embodied agents for human-AI interaction, ex-celling in achieving instruction open-endedness through atwo-stage learning process.。The empirical results on Contrarepresent that OpenPAL shows the potential as a practicalsolution for human-AI interaction in complex situations.Despite the positive results, we admit there are still somelimitations to our work that would be expected to be re-searched in the future—for instance, a truly open-endedgoal description instead of the handcrafted goal space in thecurrent version; supporting multi-modality input/output tofree from expensive feature engineering.Author Contribution StatementThe authors confirm their contribution as follows:Shaopeng Zhai : team leadership, open-ended learning,LLM/RLAF training, agent analysis, architecture designJie Wang : infrastructure/framework engineering, non-goalBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationagent training, open-ended learning, ablation studies, fea-ture engineeringTianyi Zhang : non-goal agent training, open-ended learn-ing, feature engineeringFuxian Huang : non-goal agent training, paper writing,open-ended learningQi Zhang : LLM training, RLAF training, paper writing,ablation studiesMing Zhou : co-training framework, curriculum research,paper writingJing Hou : LLM training, paper writingReferencesAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O.,and Zaremba, W. Hindsight experience replay.。Advancesin neural information processing systems , 30, 2017.Bai, C., Liu, P., Zhao, W., and Tang, X.。Guided goal gen-eration for hindsight multi-goal reinforcement learning.Neurocomputing , 359:353–367, 2019.Balduzzi, D., Garnelo, M., Bachrach, Y ., Czarnecki, W.,Perolat, J., Jaderberg, M., and Graepel, T. Open-endedlearning in symmetric zero-sum games.。In InternationalConference on Machine Learning , pp.。434–443.。PMLR,2019.Berner, C., Brockman, G., Chan, B., Cheung, V ., D˛ ebiak, P.,Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse,C., et al.。Dota 2 with large scale deep reinforcementlearning, 2019.Brohan, A., Brown, N., Carbajal, J., Chebotar, Y ., Dabis, J.,Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A.,Hsu, J., et al.。Rt-1: Robotics transformer for real-worldcontrol at scale.。arXiv preprint arXiv:2212.06817 , 2022.Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,S., Radford, A., Sutskever, I., and Amodei, D. Languagemodels are few-shot learners, 2020.Burda, Y ., Edwards, H., Storkey, A., and Klimov, O. Explo-ration by random network distillation.。In InternationalConference on Learning Representations , 2018.Ding, H., Tang, Y ., Wu, Q., Wang, B., Chen, C., andWang, Z.。Magnetic field-based reward shaping for goal-conditioned reinforcement learning.. \n",
      "the piece 192\n",
      " \n",
      "ts capacity to eliminate contradictory and irrational ele-ments within the initial objectives and exhibits the ability tointroduce new sub-goals, thereby rendering the overall goalmore attainable, all while retaining its exceptional semanticcomprehension capabilities.6.。ConclusionIn this paper, we propose OpenPAL experts on learningopen-ended embodied agents for human-AI interaction, ex-celling in achieving instruction open-endedness through atwo-stage learning process.。The empirical results on Contrarepresent that OpenPAL shows the potential as a practicalsolution for human-AI interaction in complex situations.Despite the positive results, we admit there are still somelimitations to our work that would be expected to be re-searched in the future—for instance, a truly open-endedgoal description instead of the handcrafted goal space in thecurrent version; supporting multi-modality input/output tofree from expensive feature engineering.Author Contribution StatementThe authors confirm their contribution as follows:Shaopeng Zhai : team leadership, open-ended learning,LLM/RLAF training, agent analysis, architecture designJie Wang : infrastructure/framework engineering, non-goalBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationagent training, open-ended learning, ablation studies, fea-ture engineeringTianyi Zhang : non-goal agent training, open-ended learn-ing, feature engineeringFuxian Huang : non-goal agent training, paper writing,open-ended learningQi Zhang : LLM training, RLAF training, paper writing,ablation studiesMing Zhou : co-training framework, curriculum research,paper writingJing Hou : LLM training, paper writingReferencesAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O.,and Zaremba, W. Hindsight experience replay.。Advancesin neural information processing systems , 30, 2017.Bai, C., Liu, P., Zhao, W., and Tang, X.。Guided goal gen-eration for hindsight multi-goal reinforcement learning.Neurocomputing , 359:353–367, 2019.Balduzzi, D., Garnelo, M., Bachrach, Y ., Czarnecki, W.,Perolat, J., Jaderberg, M., and Graepel, T. Open-endedlearning in symmetric zero-sum games.。In InternationalConference on Machine Learning , pp.。434–443.。PMLR,2019.Berner, C., Brockman, G., Chan, B., Cheung, V ., D˛ ebiak, P.,Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse,C., et al.。Dota 2 with large scale deep reinforcementlearning, 2019.Brohan, A., Brown, N., Carbajal, J., Chebotar, Y ., Dabis, J.,Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A.,Hsu, J., et al.。Rt-1: Robotics transformer for real-worldcontrol at scale.。arXiv preprint arXiv:2212.06817 , 2022.Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,S., Radford, A., Sutskever, I., and Amodei, D. Languagemodels are few-shot learners, 2020.Burda, Y ., Edwards, H., Storkey, A., and Klimov, O. Explo-ration by random network distillation.。In InternationalConference on Learning Representations , 2018.Ding, H., Tang, Y ., Wu, Q., Wang, B., Chen, C., andWang, Z.。Magnetic field-based reward shaping for goal-conditioned reinforcement learning.。IEEE/CAA Journalof Automatica Sinica , 10(12):1–15, 2023.Du, Y ., Watkins, O., Wang, Z., Colas, C., Darrell, T., Abbeel,P., Gupta, A., and Andreas, J.. \n",
      "the piece 193\n",
      " \n",
      ", thereby rendering the overall goalmore attainable, all while retaining its exceptional semanticcomprehension capabilities.6.。ConclusionIn this paper, we propose OpenPAL experts on learningopen-ended embodied agents for human-AI interaction, ex-celling in achieving instruction open-endedness through atwo-stage learning process.。The empirical results on Contrarepresent that OpenPAL shows the potential as a practicalsolution for human-AI interaction in complex situations.Despite the positive results, we admit there are still somelimitations to our work that would be expected to be re-searched in the future—for instance, a truly open-endedgoal description instead of the handcrafted goal space in thecurrent version; supporting multi-modality input/output tofree from expensive feature engineering.Author Contribution StatementThe authors confirm their contribution as follows:Shaopeng Zhai : team leadership, open-ended learning,LLM/RLAF training, agent analysis, architecture designJie Wang : infrastructure/framework engineering, non-goalBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationagent training, open-ended learning, ablation studies, fea-ture engineeringTianyi Zhang : non-goal agent training, open-ended learn-ing, feature engineeringFuxian Huang : non-goal agent training, paper writing,open-ended learningQi Zhang : LLM training, RLAF training, paper writing,ablation studiesMing Zhou : co-training framework, curriculum research,paper writingJing Hou : LLM training, paper writingReferencesAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O.,and Zaremba, W. Hindsight experience replay.。Advancesin neural information processing systems , 30, 2017.Bai, C., Liu, P., Zhao, W., and Tang, X.。Guided goal gen-eration for hindsight multi-goal reinforcement learning.Neurocomputing , 359:353–367, 2019.Balduzzi, D., Garnelo, M., Bachrach, Y ., Czarnecki, W.,Perolat, J., Jaderberg, M., and Graepel, T. Open-endedlearning in symmetric zero-sum games.。In InternationalConference on Machine Learning , pp.。434–443.。PMLR,2019.Berner, C., Brockman, G., Chan, B., Cheung, V ., D˛ ebiak, P.,Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse,C., et al.。Dota 2 with large scale deep reinforcementlearning, 2019.Brohan, A., Brown, N., Carbajal, J., Chebotar, Y ., Dabis, J.,Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A.,Hsu, J., et al.。Rt-1: Robotics transformer for real-worldcontrol at scale.。arXiv preprint arXiv:2212.06817 , 2022.Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,S., Radford, A., Sutskever, I., and Amodei, D. Languagemodels are few-shot learners, 2020.Burda, Y ., Edwards, H., Storkey, A., and Klimov, O. Explo-ration by random network distillation.。In InternationalConference on Learning Representations , 2018.Ding, H., Tang, Y ., Wu, Q., Wang, B., Chen, C., andWang, Z.。Magnetic field-based reward shaping for goal-conditioned reinforcement learning.。IEEE/CAA Journalof Automatica Sinica , 10(12):1–15, 2023.Du, Y ., Watkins, O., Wang, Z., Colas, C., Darrell, T., Abbeel,P., Gupta, A., and Andreas, J.。Guiding pretraining inreinforcement learning with large language models.. \n",
      "the piece 194\n",
      " \n",
      "ng process.。The empirical results on Contrarepresent that OpenPAL shows the potential as a practicalsolution for human-AI interaction in complex situations.Despite the positive results, we admit there are still somelimitations to our work that would be expected to be re-searched in the future—for instance, a truly open-endedgoal description instead of the handcrafted goal space in thecurrent version; supporting multi-modality input/output tofree from expensive feature engineering.Author Contribution StatementThe authors confirm their contribution as follows:Shaopeng Zhai : team leadership, open-ended learning,LLM/RLAF training, agent analysis, architecture designJie Wang : infrastructure/framework engineering, non-goalBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationagent training, open-ended learning, ablation studies, fea-ture engineeringTianyi Zhang : non-goal agent training, open-ended learn-ing, feature engineeringFuxian Huang : non-goal agent training, paper writing,open-ended learningQi Zhang : LLM training, RLAF training, paper writing,ablation studiesMing Zhou : co-training framework, curriculum research,paper writingJing Hou : LLM training, paper writingReferencesAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O.,and Zaremba, W. Hindsight experience replay.。Advancesin neural information processing systems , 30, 2017.Bai, C., Liu, P., Zhao, W., and Tang, X.。Guided goal gen-eration for hindsight multi-goal reinforcement learning.Neurocomputing , 359:353–367, 2019.Balduzzi, D., Garnelo, M., Bachrach, Y ., Czarnecki, W.,Perolat, J., Jaderberg, M., and Graepel, T. Open-endedlearning in symmetric zero-sum games.。In InternationalConference on Machine Learning , pp.。434–443.。PMLR,2019.Berner, C., Brockman, G., Chan, B., Cheung, V ., D˛ ebiak, P.,Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse,C., et al.。Dota 2 with large scale deep reinforcementlearning, 2019.Brohan, A., Brown, N., Carbajal, J., Chebotar, Y ., Dabis, J.,Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A.,Hsu, J., et al.。Rt-1: Robotics transformer for real-worldcontrol at scale.。arXiv preprint arXiv:2212.06817 , 2022.Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,S., Radford, A., Sutskever, I., and Amodei, D. Languagemodels are few-shot learners, 2020.Burda, Y ., Edwards, H., Storkey, A., and Klimov, O. Explo-ration by random network distillation.。In InternationalConference on Learning Representations , 2018.Ding, H., Tang, Y ., Wu, Q., Wang, B., Chen, C., andWang, Z.。Magnetic field-based reward shaping for goal-conditioned reinforcement learning.。IEEE/CAA Journalof Automatica Sinica , 10(12):1–15, 2023.Du, Y ., Watkins, O., Wang, Z., Colas, C., Darrell, T., Abbeel,P., Gupta, A., and Andreas, J.。Guiding pretraining inreinforcement learning with large language models.。arXivpreprint arXiv:2302.06692 , 2023.Du, Z., Qian, Y ., Liu, X., Ding, M., Qiu, J., Yang, Z., andTang, J. Glm: General language model pretraining withautoregressive blank infilling.. \n",
      "the piece 195\n",
      " \n",
      "still somelimitations to our work that would be expected to be re-searched in the future—for instance, a truly open-endedgoal description instead of the handcrafted goal space in thecurrent version; supporting multi-modality input/output tofree from expensive feature engineering.Author Contribution StatementThe authors confirm their contribution as follows:Shaopeng Zhai : team leadership, open-ended learning,LLM/RLAF training, agent analysis, architecture designJie Wang : infrastructure/framework engineering, non-goalBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationagent training, open-ended learning, ablation studies, fea-ture engineeringTianyi Zhang : non-goal agent training, open-ended learn-ing, feature engineeringFuxian Huang : non-goal agent training, paper writing,open-ended learningQi Zhang : LLM training, RLAF training, paper writing,ablation studiesMing Zhou : co-training framework, curriculum research,paper writingJing Hou : LLM training, paper writingReferencesAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O.,and Zaremba, W. Hindsight experience replay.。Advancesin neural information processing systems , 30, 2017.Bai, C., Liu, P., Zhao, W., and Tang, X.。Guided goal gen-eration for hindsight multi-goal reinforcement learning.Neurocomputing , 359:353–367, 2019.Balduzzi, D., Garnelo, M., Bachrach, Y ., Czarnecki, W.,Perolat, J., Jaderberg, M., and Graepel, T. Open-endedlearning in symmetric zero-sum games.。In InternationalConference on Machine Learning , pp.。434–443.。PMLR,2019.Berner, C., Brockman, G., Chan, B., Cheung, V ., D˛ ebiak, P.,Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse,C., et al.。Dota 2 with large scale deep reinforcementlearning, 2019.Brohan, A., Brown, N., Carbajal, J., Chebotar, Y ., Dabis, J.,Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A.,Hsu, J., et al.。Rt-1: Robotics transformer for real-worldcontrol at scale.。arXiv preprint arXiv:2212.06817 , 2022.Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,S., Radford, A., Sutskever, I., and Amodei, D. Languagemodels are few-shot learners, 2020.Burda, Y ., Edwards, H., Storkey, A., and Klimov, O. Explo-ration by random network distillation.。In InternationalConference on Learning Representations , 2018.Ding, H., Tang, Y ., Wu, Q., Wang, B., Chen, C., andWang, Z.。Magnetic field-based reward shaping for goal-conditioned reinforcement learning.。IEEE/CAA Journalof Automatica Sinica , 10(12):1–15, 2023.Du, Y ., Watkins, O., Wang, Z., Colas, C., Darrell, T., Abbeel,P., Gupta, A., and Andreas, J.。Guiding pretraining inreinforcement learning with large language models.。arXivpreprint arXiv:2302.06692 , 2023.Du, Z., Qian, Y ., Liu, X., Ding, M., Qiu, J., Yang, Z., andTang, J. Glm: General language model pretraining withautoregressive blank infilling.。In Proceedings of the 60thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers) , pp.. \n",
      "the piece 196\n",
      " \n",
      "353–367, 2019.Balduzzi, D., Garnelo, M., Bachrach, Y ., Czarnecki, W.,Perolat, J., Jaderberg, M., and Graepel, T. Open-endedlearning in symmetric zero-sum games.。In InternationalConference on Machine Learning , pp.。434–443.。PMLR,2019.Berner, C., Brockman, G., Chan, B., Cheung, V ., D˛ ebiak, P.,Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse,C., et al.。Dota 2 with large scale deep reinforcementlearning, 2019.Brohan, A., Brown, N., Carbajal, J., Chebotar, Y ., Dabis, J.,Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A.,Hsu, J., et al.。Rt-1: Robotics transformer for real-worldcontrol at scale.。arXiv preprint arXiv:2212.06817 , 2022.Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,S., Radford, A., Sutskever, I., and Amodei, D. Languagemodels are few-shot learners, 2020.Burda, Y ., Edwards, H., Storkey, A., and Klimov, O. Explo-ration by random network distillation.。In InternationalConference on Learning Representations , 2018.Ding, H., Tang, Y ., Wu, Q., Wang, B., Chen, C., andWang, Z.。Magnetic field-based reward shaping for goal-conditioned reinforcement learning.。IEEE/CAA Journalof Automatica Sinica , 10(12):1–15, 2023.Du, Y ., Watkins, O., Wang, Z., Colas, C., Darrell, T., Abbeel,P., Gupta, A., and Andreas, J.。Guiding pretraining inreinforcement learning with large language models.。arXivpreprint arXiv:2302.06692 , 2023.Du, Z., Qian, Y ., Liu, X., Ding, M., Qiu, J., Yang, Z., andTang, J. Glm: General language model pretraining withautoregressive blank infilling.。In Proceedings of the 60thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers) , pp.。320–335, 2022.Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., andClune, J.. \n",
      "the piece 197\n",
      " \n",
      "Graepel, T. Open-endedlearning in symmetric zero-sum games.。In InternationalConference on Machine Learning , pp.。434–443.。PMLR,2019.Berner, C., Brockman, G., Chan, B., Cheung, V ., D˛ ebiak, P.,Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse,C., et al.。Dota 2 with large scale deep reinforcementlearning, 2019.Brohan, A., Brown, N., Carbajal, J., Chebotar, Y ., Dabis, J.,Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A.,Hsu, J., et al.。Rt-1: Robotics transformer for real-worldcontrol at scale.。arXiv preprint arXiv:2212.06817 , 2022.Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,S., Radford, A., Sutskever, I., and Amodei, D. Languagemodels are few-shot learners, 2020.Burda, Y ., Edwards, H., Storkey, A., and Klimov, O. Explo-ration by random network distillation.。In InternationalConference on Learning Representations , 2018.Ding, H., Tang, Y ., Wu, Q., Wang, B., Chen, C., andWang, Z.。Magnetic field-based reward shaping for goal-conditioned reinforcement learning.。IEEE/CAA Journalof Automatica Sinica , 10(12):1–15, 2023.Du, Y ., Watkins, O., Wang, Z., Colas, C., Darrell, T., Abbeel,P., Gupta, A., and Andreas, J.。Guiding pretraining inreinforcement learning with large language models.。arXivpreprint arXiv:2302.06692 , 2023.Du, Z., Qian, Y ., Liu, X., Ding, M., Qiu, J., Yang, Z., andTang, J. Glm: General language model pretraining withautoregressive blank infilling.。In Proceedings of the 60thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers) , pp.。320–335, 2022.Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., andClune, J.。First return, then explore.. \n",
      "the piece 198\n",
      " \n",
      " al.。Dota 2 with large scale deep reinforcementlearning, 2019.Brohan, A., Brown, N., Carbajal, J., Chebotar, Y ., Dabis, J.,Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A.,Hsu, J., et al.。Rt-1: Robotics transformer for real-worldcontrol at scale.。arXiv preprint arXiv:2212.06817 , 2022.Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,S., Radford, A., Sutskever, I., and Amodei, D. Languagemodels are few-shot learners, 2020.Burda, Y ., Edwards, H., Storkey, A., and Klimov, O. Explo-ration by random network distillation.。In InternationalConference on Learning Representations , 2018.Ding, H., Tang, Y ., Wu, Q., Wang, B., Chen, C., andWang, Z.。Magnetic field-based reward shaping for goal-conditioned reinforcement learning.。IEEE/CAA Journalof Automatica Sinica , 10(12):1–15, 2023.Du, Y ., Watkins, O., Wang, Z., Colas, C., Darrell, T., Abbeel,P., Gupta, A., and Andreas, J.。Guiding pretraining inreinforcement learning with large language models.。arXivpreprint arXiv:2302.06692 , 2023.Du, Z., Qian, Y ., Liu, X., Ding, M., Qiu, J., Yang, Z., andTang, J. Glm: General language model pretraining withautoregressive blank infilling.。In Proceedings of the 60thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers) , pp.。320–335, 2022.Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., andClune, J.。First return, then explore.。Nature , 590(7847):580–586, 2021.Fang, M., Zhou, T., Du, Y ., Han, L., and Zhang, Z.Curriculum-guided hindsight experience replay.. \n",
      "the piece 199\n",
      " \n",
      "g, 2019.Brohan, A., Brown, N., Carbajal, J., Chebotar, Y ., Dabis, J.,Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A.,Hsu, J., et al.。Rt-1: Robotics transformer for real-worldcontrol at scale.。arXiv preprint arXiv:2212.06817 , 2022.Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,S., Radford, A., Sutskever, I., and Amodei, D. Languagemodels are few-shot learners, 2020.Burda, Y ., Edwards, H., Storkey, A., and Klimov, O. Explo-ration by random network distillation.。In InternationalConference on Learning Representations , 2018.Ding, H., Tang, Y ., Wu, Q., Wang, B., Chen, C., andWang, Z.。Magnetic field-based reward shaping for goal-conditioned reinforcement learning.。IEEE/CAA Journalof Automatica Sinica , 10(12):1–15, 2023.Du, Y ., Watkins, O., Wang, Z., Colas, C., Darrell, T., Abbeel,P., Gupta, A., and Andreas, J.。Guiding pretraining inreinforcement learning with large language models.。arXivpreprint arXiv:2302.06692 , 2023.Du, Z., Qian, Y ., Liu, X., Ding, M., Qiu, J., Yang, Z., andTang, J. Glm: General language model pretraining withautoregressive blank infilling.。In Proceedings of the 60thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers) , pp.。320–335, 2022.Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., andClune, J.。First return, then explore.。Nature , 590(7847):580–586, 2021.Fang, M., Zhou, T., Du, Y ., Han, L., and Zhang, Z.Curriculum-guided hindsight experience replay.。Ad-vances in neural information processing systems , 32,2019.Gautam, A., Jain, H., Senger, A., and Dhand, G. Battleroyale: First-person shooter game.. \n",
      "the piece 200\n",
      " \n",
      "ohan, A., Brown, N., Carbajal, J., Chebotar, Y ., Dabis, J.,Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A.,Hsu, J., et al.。Rt-1: Robotics transformer for real-worldcontrol at scale.。arXiv preprint arXiv:2212.06817 , 2022.Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,S., Radford, A., Sutskever, I., and Amodei, D. Languagemodels are few-shot learners, 2020.Burda, Y ., Edwards, H., Storkey, A., and Klimov, O. Explo-ration by random network distillation.。In InternationalConference on Learning Representations , 2018.Ding, H., Tang, Y ., Wu, Q., Wang, B., Chen, C., andWang, Z.。Magnetic field-based reward shaping for goal-conditioned reinforcement learning.。IEEE/CAA Journalof Automatica Sinica , 10(12):1–15, 2023.Du, Y ., Watkins, O., Wang, Z., Colas, C., Darrell, T., Abbeel,P., Gupta, A., and Andreas, J.。Guiding pretraining inreinforcement learning with large language models.。arXivpreprint arXiv:2302.06692 , 2023.Du, Z., Qian, Y ., Liu, X., Ding, M., Qiu, J., Yang, Z., andTang, J. Glm: General language model pretraining withautoregressive blank infilling.。In Proceedings of the 60thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers) , pp.。320–335, 2022.Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., andClune, J.。First return, then explore.。Nature , 590(7847):580–586, 2021.Fang, M., Zhou, T., Du, Y ., Han, L., and Zhang, Z.Curriculum-guided hindsight experience replay.。Ad-vances in neural information processing systems , 32,2019.Gautam, A., Jain, H., Senger, A., and Dhand, G. Battleroyale: First-person shooter game.。In Proceedings ofthe International Conference on Innovative Computing &Communication (ICICC) , 2021.Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R.,Soyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg,M., Teplyashin, D., et al.. \n",
      "the piece 201\n",
      " \n",
      "tics transformer for real-worldcontrol at scale.。arXiv preprint arXiv:2212.06817 , 2022.Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,S., Radford, A., Sutskever, I., and Amodei, D. Languagemodels are few-shot learners, 2020.Burda, Y ., Edwards, H., Storkey, A., and Klimov, O. Explo-ration by random network distillation.。In InternationalConference on Learning Representations , 2018.Ding, H., Tang, Y ., Wu, Q., Wang, B., Chen, C., andWang, Z.。Magnetic field-based reward shaping for goal-conditioned reinforcement learning.。IEEE/CAA Journalof Automatica Sinica , 10(12):1–15, 2023.Du, Y ., Watkins, O., Wang, Z., Colas, C., Darrell, T., Abbeel,P., Gupta, A., and Andreas, J.。Guiding pretraining inreinforcement learning with large language models.。arXivpreprint arXiv:2302.06692 , 2023.Du, Z., Qian, Y ., Liu, X., Ding, M., Qiu, J., Yang, Z., andTang, J. Glm: General language model pretraining withautoregressive blank infilling.。In Proceedings of the 60thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers) , pp.。320–335, 2022.Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., andClune, J.。First return, then explore.。Nature , 590(7847):580–586, 2021.Fang, M., Zhou, T., Du, Y ., Han, L., and Zhang, Z.Curriculum-guided hindsight experience replay.。Ad-vances in neural information processing systems , 32,2019.Gautam, A., Jain, H., Senger, A., and Dhand, G. Battleroyale: First-person shooter game.。In Proceedings ofthe International Conference on Innovative Computing &Communication (ICICC) , 2021.Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R.,Soyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg,M., Teplyashin, D., et al.。Grounded language learning ina simulated 3d world.. \n",
      "the piece 202\n",
      " \n",
      "astry, G.,Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,S., Radford, A., Sutskever, I., and Amodei, D. Languagemodels are few-shot learners, 2020.Burda, Y ., Edwards, H., Storkey, A., and Klimov, O. Explo-ration by random network distillation.。In InternationalConference on Learning Representations , 2018.Ding, H., Tang, Y ., Wu, Q., Wang, B., Chen, C., andWang, Z.。Magnetic field-based reward shaping for goal-conditioned reinforcement learning.。IEEE/CAA Journalof Automatica Sinica , 10(12):1–15, 2023.Du, Y ., Watkins, O., Wang, Z., Colas, C., Darrell, T., Abbeel,P., Gupta, A., and Andreas, J.。Guiding pretraining inreinforcement learning with large language models.。arXivpreprint arXiv:2302.06692 , 2023.Du, Z., Qian, Y ., Liu, X., Ding, M., Qiu, J., Yang, Z., andTang, J. Glm: General language model pretraining withautoregressive blank infilling.。In Proceedings of the 60thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers) , pp.。320–335, 2022.Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., andClune, J.。First return, then explore.。Nature , 590(7847):580–586, 2021.Fang, M., Zhou, T., Du, Y ., Han, L., and Zhang, Z.Curriculum-guided hindsight experience replay.。Ad-vances in neural information processing systems , 32,2019.Gautam, A., Jain, H., Senger, A., and Dhand, G. Battleroyale: First-person shooter game.。In Proceedings ofthe International Conference on Innovative Computing &Communication (ICICC) , 2021.Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R.,Soyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg,M., Teplyashin, D., et al.。Grounded language learning ina simulated 3d world.。arXiv preprint arXiv:1706.06551 ,2017.Hu, B., Zhao, C., Zhang, P., Zhou, Z., Yang, Y ., Xu, Z., andLiu, B.. \n",
      "the piece 203\n",
      " \n",
      "r, G.,Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,S., Radford, A., Sutskever, I., and Amodei, D. Languagemodels are few-shot learners, 2020.Burda, Y ., Edwards, H., Storkey, A., and Klimov, O. Explo-ration by random network distillation.。In InternationalConference on Learning Representations , 2018.Ding, H., Tang, Y ., Wu, Q., Wang, B., Chen, C., andWang, Z.。Magnetic field-based reward shaping for goal-conditioned reinforcement learning.。IEEE/CAA Journalof Automatica Sinica , 10(12):1–15, 2023.Du, Y ., Watkins, O., Wang, Z., Colas, C., Darrell, T., Abbeel,P., Gupta, A., and Andreas, J.。Guiding pretraining inreinforcement learning with large language models.。arXivpreprint arXiv:2302.06692 , 2023.Du, Z., Qian, Y ., Liu, X., Ding, M., Qiu, J., Yang, Z., andTang, J. Glm: General language model pretraining withautoregressive blank infilling.。In Proceedings of the 60thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers) , pp.。320–335, 2022.Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., andClune, J.。First return, then explore.。Nature , 590(7847):580–586, 2021.Fang, M., Zhou, T., Du, Y ., Han, L., and Zhang, Z.Curriculum-guided hindsight experience replay.。Ad-vances in neural information processing systems , 32,2019.Gautam, A., Jain, H., Senger, A., and Dhand, G. Battleroyale: First-person shooter game.。In Proceedings ofthe International Conference on Innovative Computing &Communication (ICICC) , 2021.Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R.,Soyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg,M., Teplyashin, D., et al.。Grounded language learning ina simulated 3d world.。arXiv preprint arXiv:1706.06551 ,2017.Hu, B., Zhao, C., Zhang, P., Zhou, Z., Yang, Y ., Xu, Z., andLiu, B.。Enabling intelligent interactions between an agentand an llm: A reinforcement learning approach, 2023.Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,S., Wang, L., and Chen, W. Lora: Low-rank adaptationof large language models, 2021.Huang, W., Wang, C., Zhang, R., Li, Y ., Wu, J., and Fei-Fei,L.. \n",
      "the piece 204\n",
      " \n",
      ".。IEEE/CAA Journalof Automatica Sinica , 10(12):1–15, 2023.Du, Y ., Watkins, O., Wang, Z., Colas, C., Darrell, T., Abbeel,P., Gupta, A., and Andreas, J.。Guiding pretraining inreinforcement learning with large language models.。arXivpreprint arXiv:2302.06692 , 2023.Du, Z., Qian, Y ., Liu, X., Ding, M., Qiu, J., Yang, Z., andTang, J. Glm: General language model pretraining withautoregressive blank infilling.。In Proceedings of the 60thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers) , pp.。320–335, 2022.Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., andClune, J.。First return, then explore.。Nature , 590(7847):580–586, 2021.Fang, M., Zhou, T., Du, Y ., Han, L., and Zhang, Z.Curriculum-guided hindsight experience replay.。Ad-vances in neural information processing systems , 32,2019.Gautam, A., Jain, H., Senger, A., and Dhand, G. Battleroyale: First-person shooter game.。In Proceedings ofthe International Conference on Innovative Computing &Communication (ICICC) , 2021.Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R.,Soyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg,M., Teplyashin, D., et al.。Grounded language learning ina simulated 3d world.。arXiv preprint arXiv:1706.06551 ,2017.Hu, B., Zhao, C., Zhang, P., Zhou, Z., Yang, Y ., Xu, Z., andLiu, B.。Enabling intelligent interactions between an agentand an llm: A reinforcement learning approach, 2023.Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,S., Wang, L., and Chen, W. Lora: Low-rank adaptationof large language models, 2021.Huang, W., Wang, C., Zhang, R., Li, Y ., Wu, J., and Fei-Fei,L.。V oxposer: Composable 3d value maps for roboticmanipulation with language models.. \n",
      "the piece 205\n",
      " \n",
      ", Gupta, A., and Andreas, J.。Guiding pretraining inreinforcement learning with large language models.。arXivpreprint arXiv:2302.06692 , 2023.Du, Z., Qian, Y ., Liu, X., Ding, M., Qiu, J., Yang, Z., andTang, J. Glm: General language model pretraining withautoregressive blank infilling.。In Proceedings of the 60thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers) , pp.。320–335, 2022.Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., andClune, J.。First return, then explore.。Nature , 590(7847):580–586, 2021.Fang, M., Zhou, T., Du, Y ., Han, L., and Zhang, Z.Curriculum-guided hindsight experience replay.。Ad-vances in neural information processing systems , 32,2019.Gautam, A., Jain, H., Senger, A., and Dhand, G. Battleroyale: First-person shooter game.。In Proceedings ofthe International Conference on Innovative Computing &Communication (ICICC) , 2021.Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R.,Soyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg,M., Teplyashin, D., et al.。Grounded language learning ina simulated 3d world.。arXiv preprint arXiv:1706.06551 ,2017.Hu, B., Zhao, C., Zhang, P., Zhou, Z., Yang, Y ., Xu, Z., andLiu, B.。Enabling intelligent interactions between an agentand an llm: A reinforcement learning approach, 2023.Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,S., Wang, L., and Chen, W. Lora: Low-rank adaptationof large language models, 2021.Huang, W., Wang, C., Zhang, R., Li, Y ., Wu, J., and Fei-Fei,L.。V oxposer: Composable 3d value maps for roboticmanipulation with language models.。In Conference onRobot Learning , pp.. \n",
      "the piece 206\n",
      " \n",
      "ge language models.。arXivpreprint arXiv:2302.06692 , 2023.Du, Z., Qian, Y ., Liu, X., Ding, M., Qiu, J., Yang, Z., andTang, J. Glm: General language model pretraining withautoregressive blank infilling.。In Proceedings of the 60thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers) , pp.。320–335, 2022.Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., andClune, J.。First return, then explore.。Nature , 590(7847):580–586, 2021.Fang, M., Zhou, T., Du, Y ., Han, L., and Zhang, Z.Curriculum-guided hindsight experience replay.。Ad-vances in neural information processing systems , 32,2019.Gautam, A., Jain, H., Senger, A., and Dhand, G. Battleroyale: First-person shooter game.。In Proceedings ofthe International Conference on Innovative Computing &Communication (ICICC) , 2021.Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R.,Soyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg,M., Teplyashin, D., et al.。Grounded language learning ina simulated 3d world.。arXiv preprint arXiv:1706.06551 ,2017.Hu, B., Zhao, C., Zhang, P., Zhou, Z., Yang, Y ., Xu, Z., andLiu, B.。Enabling intelligent interactions between an agentand an llm: A reinforcement learning approach, 2023.Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,S., Wang, L., and Chen, W. Lora: Low-rank adaptationof large language models, 2021.Huang, W., Wang, C., Zhang, R., Li, Y ., Wu, J., and Fei-Fei,L.。V oxposer: Composable 3d value maps for roboticmanipulation with language models.。In Conference onRobot Learning , pp.。540–562.. \n",
      "the piece 207\n",
      " \n",
      "el pretraining withautoregressive blank infilling.。In Proceedings of the 60thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers) , pp.。320–335, 2022.Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., andClune, J.。First return, then explore.。Nature , 590(7847):580–586, 2021.Fang, M., Zhou, T., Du, Y ., Han, L., and Zhang, Z.Curriculum-guided hindsight experience replay.。Ad-vances in neural information processing systems , 32,2019.Gautam, A., Jain, H., Senger, A., and Dhand, G. Battleroyale: First-person shooter game.。In Proceedings ofthe International Conference on Innovative Computing &Communication (ICICC) , 2021.Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R.,Soyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg,M., Teplyashin, D., et al.。Grounded language learning ina simulated 3d world.。arXiv preprint arXiv:1706.06551 ,2017.Hu, B., Zhao, C., Zhang, P., Zhou, Z., Yang, Y ., Xu, Z., andLiu, B.。Enabling intelligent interactions between an agentand an llm: A reinforcement learning approach, 2023.Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,S., Wang, L., and Chen, W. Lora: Low-rank adaptationof large language models, 2021.Huang, W., Wang, C., Zhang, R., Li, Y ., Wu, J., and Fei-Fei,L.。V oxposer: Composable 3d value maps for roboticmanipulation with language models.。In Conference onRobot Learning , pp.。540–562.。PMLR, 2023.Jiang, Y ., Gupta, A., Zhang, Z., Wang, G., Dou, Y ., Chen, Y .,Fei-Fei, L., Anandkumar, A., Zhu, Y ., and Fan, L. Vima:Robot manipulation with multimodal prompts.. \n",
      "the piece 208\n",
      " \n",
      "0thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers) , pp.。320–335, 2022.Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., andClune, J.。First return, then explore.。Nature , 590(7847):580–586, 2021.Fang, M., Zhou, T., Du, Y ., Han, L., and Zhang, Z.Curriculum-guided hindsight experience replay.。Ad-vances in neural information processing systems , 32,2019.Gautam, A., Jain, H., Senger, A., and Dhand, G. Battleroyale: First-person shooter game.。In Proceedings ofthe International Conference on Innovative Computing &Communication (ICICC) , 2021.Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R.,Soyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg,M., Teplyashin, D., et al.。Grounded language learning ina simulated 3d world.。arXiv preprint arXiv:1706.06551 ,2017.Hu, B., Zhao, C., Zhang, P., Zhou, Z., Yang, Y ., Xu, Z., andLiu, B.。Enabling intelligent interactions between an agentand an llm: A reinforcement learning approach, 2023.Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,S., Wang, L., and Chen, W. Lora: Low-rank adaptationof large language models, 2021.Huang, W., Wang, C., Zhang, R., Li, Y ., Wu, J., and Fei-Fei,L.。V oxposer: Composable 3d value maps for roboticmanipulation with language models.。In Conference onRobot Learning , pp.。540–562.。PMLR, 2023.Jiang, Y ., Gupta, A., Zhang, Z., Wang, G., Dou, Y ., Chen, Y .,Fei-Fei, L., Anandkumar, A., Zhu, Y ., and Fan, L. Vima:Robot manipulation with multimodal prompts.。2023.Khandelwal, A., Weihs, L., Mottaghi, R., and Kembhavi,A.. \n",
      "the piece 209\n",
      " \n",
      "return, then explore.。Nature , 590(7847):580–586, 2021.Fang, M., Zhou, T., Du, Y ., Han, L., and Zhang, Z.Curriculum-guided hindsight experience replay.。Ad-vances in neural information processing systems , 32,2019.Gautam, A., Jain, H., Senger, A., and Dhand, G. Battleroyale: First-person shooter game.。In Proceedings ofthe International Conference on Innovative Computing &Communication (ICICC) , 2021.Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R.,Soyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg,M., Teplyashin, D., et al.。Grounded language learning ina simulated 3d world.。arXiv preprint arXiv:1706.06551 ,2017.Hu, B., Zhao, C., Zhang, P., Zhou, Z., Yang, Y ., Xu, Z., andLiu, B.。Enabling intelligent interactions between an agentand an llm: A reinforcement learning approach, 2023.Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,S., Wang, L., and Chen, W. Lora: Low-rank adaptationof large language models, 2021.Huang, W., Wang, C., Zhang, R., Li, Y ., Wu, J., and Fei-Fei,L.。V oxposer: Composable 3d value maps for roboticmanipulation with language models.。In Conference onRobot Learning , pp.。540–562.。PMLR, 2023.Jiang, Y ., Gupta, A., Zhang, Z., Wang, G., Dou, Y ., Chen, Y .,Fei-Fei, L., Anandkumar, A., Zhu, Y ., and Fan, L. Vima:Robot manipulation with multimodal prompts.。2023.Khandelwal, A., Weihs, L., Mottaghi, R., and Kembhavi,A.。Simple but effective: Clip embeddings for embodiedai.. \n",
      "the piece 210\n",
      " \n",
      "ded hindsight experience replay.。Ad-vances in neural information processing systems , 32,2019.Gautam, A., Jain, H., Senger, A., and Dhand, G. Battleroyale: First-person shooter game.。In Proceedings ofthe International Conference on Innovative Computing &Communication (ICICC) , 2021.Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R.,Soyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg,M., Teplyashin, D., et al.。Grounded language learning ina simulated 3d world.。arXiv preprint arXiv:1706.06551 ,2017.Hu, B., Zhao, C., Zhang, P., Zhou, Z., Yang, Y ., Xu, Z., andLiu, B.。Enabling intelligent interactions between an agentand an llm: A reinforcement learning approach, 2023.Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,S., Wang, L., and Chen, W. Lora: Low-rank adaptationof large language models, 2021.Huang, W., Wang, C., Zhang, R., Li, Y ., Wu, J., and Fei-Fei,L.。V oxposer: Composable 3d value maps for roboticmanipulation with language models.。In Conference onRobot Learning , pp.。540–562.。PMLR, 2023.Jiang, Y ., Gupta, A., Zhang, Z., Wang, G., Dou, Y ., Chen, Y .,Fei-Fei, L., Anandkumar, A., Zhu, Y ., and Fan, L. Vima:Robot manipulation with multimodal prompts.。2023.Khandelwal, A., Weihs, L., Mottaghi, R., and Kembhavi,A.。Simple but effective: Clip embeddings for embodiedai.。In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition , pp.. \n",
      "the piece 211\n",
      " \n",
      " , 32,2019.Gautam, A., Jain, H., Senger, A., and Dhand, G. Battleroyale: First-person shooter game.。In Proceedings ofthe International Conference on Innovative Computing &Communication (ICICC) , 2021.Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R.,Soyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg,M., Teplyashin, D., et al.。Grounded language learning ina simulated 3d world.。arXiv preprint arXiv:1706.06551 ,2017.Hu, B., Zhao, C., Zhang, P., Zhou, Z., Yang, Y ., Xu, Z., andLiu, B.。Enabling intelligent interactions between an agentand an llm: A reinforcement learning approach, 2023.Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,S., Wang, L., and Chen, W. Lora: Low-rank adaptationof large language models, 2021.Huang, W., Wang, C., Zhang, R., Li, Y ., Wu, J., and Fei-Fei,L.。V oxposer: Composable 3d value maps for roboticmanipulation with language models.。In Conference onRobot Learning , pp.。540–562.。PMLR, 2023.Jiang, Y ., Gupta, A., Zhang, Z., Wang, G., Dou, Y ., Chen, Y .,Fei-Fei, L., Anandkumar, A., Zhu, Y ., and Fan, L. Vima:Robot manipulation with multimodal prompts.。2023.Khandelwal, A., Weihs, L., Mottaghi, R., and Kembhavi,A.。Simple but effective: Clip embeddings for embodiedai.。In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition , pp.。14829–14838,2022.Liu, M., Zhu, M., and Zhang, W. Goal-conditioned re-inforcement learning: Problems and solutions.. \n",
      "the piece 212\n",
      " \n",
      "H., Senger, A., and Dhand, G. Battleroyale: First-person shooter game.。In Proceedings ofthe International Conference on Innovative Computing &Communication (ICICC) , 2021.Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R.,Soyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg,M., Teplyashin, D., et al.。Grounded language learning ina simulated 3d world.。arXiv preprint arXiv:1706.06551 ,2017.Hu, B., Zhao, C., Zhang, P., Zhou, Z., Yang, Y ., Xu, Z., andLiu, B.。Enabling intelligent interactions between an agentand an llm: A reinforcement learning approach, 2023.Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,S., Wang, L., and Chen, W. Lora: Low-rank adaptationof large language models, 2021.Huang, W., Wang, C., Zhang, R., Li, Y ., Wu, J., and Fei-Fei,L.。V oxposer: Composable 3d value maps for roboticmanipulation with language models.。In Conference onRobot Learning , pp.。540–562.。PMLR, 2023.Jiang, Y ., Gupta, A., Zhang, Z., Wang, G., Dou, Y ., Chen, Y .,Fei-Fei, L., Anandkumar, A., Zhu, Y ., and Fan, L. Vima:Robot manipulation with multimodal prompts.。2023.Khandelwal, A., Weihs, L., Mottaghi, R., and Kembhavi,A.。Simple but effective: Clip embeddings for embodiedai.。In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition , pp.。14829–14838,2022.Liu, M., Zhu, M., and Zhang, W. Goal-conditioned re-inforcement learning: Problems and solutions.。arXivpreprint arXiv:2201.08299 , 2022.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,C., Hesse, C., Jain, S., Kosaraju, V ., Saunders, W., et al.Webgpt: Browser-assisted question-answering with hu-man feedback.. \n",
      "the piece 213\n",
      " \n",
      "omputing &Communication (ICICC) , 2021.Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R.,Soyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg,M., Teplyashin, D., et al.。Grounded language learning ina simulated 3d world.。arXiv preprint arXiv:1706.06551 ,2017.Hu, B., Zhao, C., Zhang, P., Zhou, Z., Yang, Y ., Xu, Z., andLiu, B.。Enabling intelligent interactions between an agentand an llm: A reinforcement learning approach, 2023.Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,S., Wang, L., and Chen, W. Lora: Low-rank adaptationof large language models, 2021.Huang, W., Wang, C., Zhang, R., Li, Y ., Wu, J., and Fei-Fei,L.。V oxposer: Composable 3d value maps for roboticmanipulation with language models.。In Conference onRobot Learning , pp.。540–562.。PMLR, 2023.Jiang, Y ., Gupta, A., Zhang, Z., Wang, G., Dou, Y ., Chen, Y .,Fei-Fei, L., Anandkumar, A., Zhu, Y ., and Fan, L. Vima:Robot manipulation with multimodal prompts.。2023.Khandelwal, A., Weihs, L., Mottaghi, R., and Kembhavi,A.。Simple but effective: Clip embeddings for embodiedai.。In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition , pp.。14829–14838,2022.Liu, M., Zhu, M., and Zhang, W. Goal-conditioned re-inforcement learning: Problems and solutions.。arXivpreprint arXiv:2201.08299 , 2022.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,C., Hesse, C., Jain, S., Kosaraju, V ., Saunders, W., et al.Webgpt: Browser-assisted question-answering with hu-man feedback.。arXiv preprint arXiv:2112.09332 , 2021.Ng, A. Y ., Harada, D., and Russell, S. Policy invarianceunder reward transformations: Theory and applicationto reward shaping.. \n",
      "the piece 214\n",
      " \n",
      "rg,M., Teplyashin, D., et al.。Grounded language learning ina simulated 3d world.。arXiv preprint arXiv:1706.06551 ,2017.Hu, B., Zhao, C., Zhang, P., Zhou, Z., Yang, Y ., Xu, Z., andLiu, B.。Enabling intelligent interactions between an agentand an llm: A reinforcement learning approach, 2023.Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,S., Wang, L., and Chen, W. Lora: Low-rank adaptationof large language models, 2021.Huang, W., Wang, C., Zhang, R., Li, Y ., Wu, J., and Fei-Fei,L.。V oxposer: Composable 3d value maps for roboticmanipulation with language models.。In Conference onRobot Learning , pp.。540–562.。PMLR, 2023.Jiang, Y ., Gupta, A., Zhang, Z., Wang, G., Dou, Y ., Chen, Y .,Fei-Fei, L., Anandkumar, A., Zhu, Y ., and Fan, L. Vima:Robot manipulation with multimodal prompts.。2023.Khandelwal, A., Weihs, L., Mottaghi, R., and Kembhavi,A.。Simple but effective: Clip embeddings for embodiedai.。In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition , pp.。14829–14838,2022.Liu, M., Zhu, M., and Zhang, W. Goal-conditioned re-inforcement learning: Problems and solutions.。arXivpreprint arXiv:2201.08299 , 2022.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,C., Hesse, C., Jain, S., Kosaraju, V ., Saunders, W., et al.Webgpt: Browser-assisted question-answering with hu-man feedback.。arXiv preprint arXiv:2112.09332 , 2021.Ng, A. Y ., Harada, D., and Russell, S. Policy invarianceunder reward transformations: Theory and applicationto reward shaping.。In Icml, volume 99, pp.. \n",
      "the piece 215\n",
      " \n",
      "n llm: A reinforcement learning approach, 2023.Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,S., Wang, L., and Chen, W. Lora: Low-rank adaptationof large language models, 2021.Huang, W., Wang, C., Zhang, R., Li, Y ., Wu, J., and Fei-Fei,L.。V oxposer: Composable 3d value maps for roboticmanipulation with language models.。In Conference onRobot Learning , pp.。540–562.。PMLR, 2023.Jiang, Y ., Gupta, A., Zhang, Z., Wang, G., Dou, Y ., Chen, Y .,Fei-Fei, L., Anandkumar, A., Zhu, Y ., and Fan, L. Vima:Robot manipulation with multimodal prompts.。2023.Khandelwal, A., Weihs, L., Mottaghi, R., and Kembhavi,A.。Simple but effective: Clip embeddings for embodiedai.。In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition , pp.。14829–14838,2022.Liu, M., Zhu, M., and Zhang, W. Goal-conditioned re-inforcement learning: Problems and solutions.。arXivpreprint arXiv:2201.08299 , 2022.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,C., Hesse, C., Jain, S., Kosaraju, V ., Saunders, W., et al.Webgpt: Browser-assisted question-answering with hu-man feedback.。arXiv preprint arXiv:2112.09332 , 2021.Ng, A. Y ., Harada, D., and Russell, S. Policy invarianceunder reward transformations: Theory and applicationto reward shaping.。In Icml, volume 99, pp.。278–287.Citeseer, 1999.Nikulin, A., Kurenkov, V ., Tarasov, D., and Kolesnikov, S.Anti-exploration by random network distillation.. \n",
      "the piece 216\n",
      " \n",
      ". J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,S., Wang, L., and Chen, W. Lora: Low-rank adaptationof large language models, 2021.Huang, W., Wang, C., Zhang, R., Li, Y ., Wu, J., and Fei-Fei,L.。V oxposer: Composable 3d value maps for roboticmanipulation with language models.。In Conference onRobot Learning , pp.。540–562.。PMLR, 2023.Jiang, Y ., Gupta, A., Zhang, Z., Wang, G., Dou, Y ., Chen, Y .,Fei-Fei, L., Anandkumar, A., Zhu, Y ., and Fan, L. Vima:Robot manipulation with multimodal prompts.。2023.Khandelwal, A., Weihs, L., Mottaghi, R., and Kembhavi,A.。Simple but effective: Clip embeddings for embodiedai.。In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition , pp.。14829–14838,2022.Liu, M., Zhu, M., and Zhang, W. Goal-conditioned re-inforcement learning: Problems and solutions.。arXivpreprint arXiv:2201.08299 , 2022.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,C., Hesse, C., Jain, S., Kosaraju, V ., Saunders, W., et al.Webgpt: Browser-assisted question-answering with hu-man feedback.。arXiv preprint arXiv:2112.09332 , 2021.Ng, A. Y ., Harada, D., and Russell, S. Policy invarianceunder reward transformations: Theory and applicationto reward shaping.。In Icml, volume 99, pp.。278–287.Citeseer, 1999.Nikulin, A., Kurenkov, V ., Tarasov, D., and Kolesnikov, S.Anti-exploration by random network distillation.。arXivpreprint arXiv:2301.13616 , 2023.OpenAI.. \n",
      "the piece 217\n",
      " \n",
      "onof large language models, 2021.Huang, W., Wang, C., Zhang, R., Li, Y ., Wu, J., and Fei-Fei,L.。V oxposer: Composable 3d value maps for roboticmanipulation with language models.。In Conference onRobot Learning , pp.。540–562.。PMLR, 2023.Jiang, Y ., Gupta, A., Zhang, Z., Wang, G., Dou, Y ., Chen, Y .,Fei-Fei, L., Anandkumar, A., Zhu, Y ., and Fan, L. Vima:Robot manipulation with multimodal prompts.。2023.Khandelwal, A., Weihs, L., Mottaghi, R., and Kembhavi,A.。Simple but effective: Clip embeddings for embodiedai.。In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition , pp.。14829–14838,2022.Liu, M., Zhu, M., and Zhang, W. Goal-conditioned re-inforcement learning: Problems and solutions.。arXivpreprint arXiv:2201.08299 , 2022.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,C., Hesse, C., Jain, S., Kosaraju, V ., Saunders, W., et al.Webgpt: Browser-assisted question-answering with hu-man feedback.。arXiv preprint arXiv:2112.09332 , 2021.Ng, A. Y ., Harada, D., and Russell, S. Policy invarianceunder reward transformations: Theory and applicationto reward shaping.。In Icml, volume 99, pp.。278–287.Citeseer, 1999.Nikulin, A., Kurenkov, V ., Tarasov, D., and Kolesnikov, S.Anti-exploration by random network distillation.。arXivpreprint arXiv:2301.13616 , 2023.OpenAI.。Gpt-4 technical report, 2023.Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,et al.. \n",
      "the piece 218\n",
      " \n",
      "Anandkumar, A., Zhu, Y ., and Fan, L. Vima:Robot manipulation with multimodal prompts.。2023.Khandelwal, A., Weihs, L., Mottaghi, R., and Kembhavi,A.。Simple but effective: Clip embeddings for embodiedai.。In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition , pp.。14829–14838,2022.Liu, M., Zhu, M., and Zhang, W. Goal-conditioned re-inforcement learning: Problems and solutions.。arXivpreprint arXiv:2201.08299 , 2022.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,C., Hesse, C., Jain, S., Kosaraju, V ., Saunders, W., et al.Webgpt: Browser-assisted question-answering with hu-man feedback.。arXiv preprint arXiv:2112.09332 , 2021.Ng, A. Y ., Harada, D., and Russell, S. Policy invarianceunder reward transformations: Theory and applicationto reward shaping.。In Icml, volume 99, pp.。278–287.Citeseer, 1999.Nikulin, A., Kurenkov, V ., Tarasov, D., and Kolesnikov, S.Anti-exploration by random network distillation.。arXivpreprint arXiv:2301.13616 , 2023.OpenAI.。Gpt-4 technical report, 2023.Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,et al.。Training language models to follow instructionswith human feedback.. \n",
      "the piece 219\n",
      " \n",
      "ts.。2023.Khandelwal, A., Weihs, L., Mottaghi, R., and Kembhavi,A.。Simple but effective: Clip embeddings for embodiedai.。In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition , pp.。14829–14838,2022.Liu, M., Zhu, M., and Zhang, W. Goal-conditioned re-inforcement learning: Problems and solutions.。arXivpreprint arXiv:2201.08299 , 2022.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,C., Hesse, C., Jain, S., Kosaraju, V ., Saunders, W., et al.Webgpt: Browser-assisted question-answering with hu-man feedback.。arXiv preprint arXiv:2112.09332 , 2021.Ng, A. Y ., Harada, D., and Russell, S. Policy invarianceunder reward transformations: Theory and applicationto reward shaping.。In Icml, volume 99, pp.。278–287.Citeseer, 1999.Nikulin, A., Kurenkov, V ., Tarasov, D., and Kolesnikov, S.Anti-exploration by random network distillation.。arXivpreprint arXiv:2301.13616 , 2023.OpenAI.。Gpt-4 technical report, 2023.Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,et al.。Training language models to follow instructionswith human feedback.。Advances in Neural InformationProcessing Systems , 35:27730–27744, 2022.Ren, Z., Dong, K., Zhou, Y ., Liu, Q., and Peng, J. Explo-ration via hindsight goal generation.. \n",
      "the piece 220\n",
      " \n",
      "ttaghi, R., and Kembhavi,A.。Simple but effective: Clip embeddings for embodiedai.。In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition , pp.。14829–14838,2022.Liu, M., Zhu, M., and Zhang, W. Goal-conditioned re-inforcement learning: Problems and solutions.。arXivpreprint arXiv:2201.08299 , 2022.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,C., Hesse, C., Jain, S., Kosaraju, V ., Saunders, W., et al.Webgpt: Browser-assisted question-answering with hu-man feedback.。arXiv preprint arXiv:2112.09332 , 2021.Ng, A. Y ., Harada, D., and Russell, S. Policy invarianceunder reward transformations: Theory and applicationto reward shaping.。In Icml, volume 99, pp.。278–287.Citeseer, 1999.Nikulin, A., Kurenkov, V ., Tarasov, D., and Kolesnikov, S.Anti-exploration by random network distillation.。arXivpreprint arXiv:2301.13616 , 2023.OpenAI.。Gpt-4 technical report, 2023.Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,et al.。Training language models to follow instructionswith human feedback.。Advances in Neural InformationProcessing Systems , 35:27730–27744, 2022.Ren, Z., Dong, K., Zhou, Y ., Liu, Q., and Peng, J. Explo-ration via hindsight goal generation.。Advances in NeuralInformation Processing Systems , 32, 2019.Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-tized experience replay.. \n",
      "the piece 221\n",
      " \n",
      ", and Kembhavi,A.。Simple but effective: Clip embeddings for embodiedai.。In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition , pp.。14829–14838,2022.Liu, M., Zhu, M., and Zhang, W. Goal-conditioned re-inforcement learning: Problems and solutions.。arXivpreprint arXiv:2201.08299 , 2022.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,C., Hesse, C., Jain, S., Kosaraju, V ., Saunders, W., et al.Webgpt: Browser-assisted question-answering with hu-man feedback.。arXiv preprint arXiv:2112.09332 , 2021.Ng, A. Y ., Harada, D., and Russell, S. Policy invarianceunder reward transformations: Theory and applicationto reward shaping.。In Icml, volume 99, pp.。278–287.Citeseer, 1999.Nikulin, A., Kurenkov, V ., Tarasov, D., and Kolesnikov, S.Anti-exploration by random network distillation.。arXivpreprint arXiv:2301.13616 , 2023.OpenAI.。Gpt-4 technical report, 2023.Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,et al.。Training language models to follow instructionswith human feedback.。Advances in Neural InformationProcessing Systems , 35:27730–27744, 2022.Ren, Z., Dong, K., Zhou, Y ., Liu, Q., and Peng, J. Explo-ration via hindsight goal generation.。Advances in NeuralInformation Processing Systems , 32, 2019.Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-tized experience replay.。arXiv preprint arXiv:1511.05952 ,2015.Schulman, J., Wolski, F., Dhariwal, P., Radford, A., andKlimov, O. Proximal policy optimization algorithms,2017.Team, O. E. L., Stooke, A., Mahajan, A., Barros, C., Deck,C., Bauer, J., Sygnowski, J., Trebacz, M., Jaderberg, M.,Mathieu, M., et al.. \n",
      "the piece 222\n",
      " \n",
      "2022.Liu, M., Zhu, M., and Zhang, W. Goal-conditioned re-inforcement learning: Problems and solutions.。arXivpreprint arXiv:2201.08299 , 2022.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,C., Hesse, C., Jain, S., Kosaraju, V ., Saunders, W., et al.Webgpt: Browser-assisted question-answering with hu-man feedback.。arXiv preprint arXiv:2112.09332 , 2021.Ng, A. Y ., Harada, D., and Russell, S. Policy invarianceunder reward transformations: Theory and applicationto reward shaping.。In Icml, volume 99, pp.。278–287.Citeseer, 1999.Nikulin, A., Kurenkov, V ., Tarasov, D., and Kolesnikov, S.Anti-exploration by random network distillation.。arXivpreprint arXiv:2301.13616 , 2023.OpenAI.。Gpt-4 technical report, 2023.Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,et al.。Training language models to follow instructionswith human feedback.。Advances in Neural InformationProcessing Systems , 35:27730–27744, 2022.Ren, Z., Dong, K., Zhou, Y ., Liu, Q., and Peng, J. Explo-ration via hindsight goal generation.。Advances in NeuralInformation Processing Systems , 32, 2019.Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-tized experience replay.。arXiv preprint arXiv:1511.05952 ,2015.Schulman, J., Wolski, F., Dhariwal, P., Radford, A., andKlimov, O. Proximal policy optimization algorithms,2017.Team, O. E. L., Stooke, A., Mahajan, A., Barros, C., Deck,C., Bauer, J., Sygnowski, J., Trebacz, M., Jaderberg, M.,Mathieu, M., et al.。Open-ended learning leads to gener-ally capable agents.. \n",
      "the piece 223\n",
      " \n",
      "ement learning: Problems and solutions.。arXivpreprint arXiv:2201.08299 , 2022.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,C., Hesse, C., Jain, S., Kosaraju, V ., Saunders, W., et al.Webgpt: Browser-assisted question-answering with hu-man feedback.。arXiv preprint arXiv:2112.09332 , 2021.Ng, A. Y ., Harada, D., and Russell, S. Policy invarianceunder reward transformations: Theory and applicationto reward shaping.。In Icml, volume 99, pp.。278–287.Citeseer, 1999.Nikulin, A., Kurenkov, V ., Tarasov, D., and Kolesnikov, S.Anti-exploration by random network distillation.。arXivpreprint arXiv:2301.13616 , 2023.OpenAI.。Gpt-4 technical report, 2023.Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,et al.。Training language models to follow instructionswith human feedback.。Advances in Neural InformationProcessing Systems , 35:27730–27744, 2022.Ren, Z., Dong, K., Zhou, Y ., Liu, Q., and Peng, J. Explo-ration via hindsight goal generation.。Advances in NeuralInformation Processing Systems , 32, 2019.Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-tized experience replay.。arXiv preprint arXiv:1511.05952 ,2015.Schulman, J., Wolski, F., Dhariwal, P., Radford, A., andKlimov, O. Proximal policy optimization algorithms,2017.Team, O. E. L., Stooke, A., Mahajan, A., Barros, C., Deck,C., Bauer, J., Sygnowski, J., Trebacz, M., Jaderberg, M.,Mathieu, M., et al.。Open-ended learning leads to gener-ally capable agents.。arXiv preprint arXiv:2107.12808 ,2021.Trott, A., Zheng, S., Xiong, C., and Socher, R. Keepingyour distance: Solving sparse reward tasks using self-balancing shaped rewards, 2019.Van der Maaten, L. and Hinton, G. Visualizing data usingt-sne.. \n",
      "the piece 224\n",
      " \n",
      "rXiv:2201.08299 , 2022.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,C., Hesse, C., Jain, S., Kosaraju, V ., Saunders, W., et al.Webgpt: Browser-assisted question-answering with hu-man feedback.。arXiv preprint arXiv:2112.09332 , 2021.Ng, A. Y ., Harada, D., and Russell, S. Policy invarianceunder reward transformations: Theory and applicationto reward shaping.。In Icml, volume 99, pp.。278–287.Citeseer, 1999.Nikulin, A., Kurenkov, V ., Tarasov, D., and Kolesnikov, S.Anti-exploration by random network distillation.。arXivpreprint arXiv:2301.13616 , 2023.OpenAI.。Gpt-4 technical report, 2023.Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,et al.。Training language models to follow instructionswith human feedback.。Advances in Neural InformationProcessing Systems , 35:27730–27744, 2022.Ren, Z., Dong, K., Zhou, Y ., Liu, Q., and Peng, J. Explo-ration via hindsight goal generation.。Advances in NeuralInformation Processing Systems , 32, 2019.Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-tized experience replay.。arXiv preprint arXiv:1511.05952 ,2015.Schulman, J., Wolski, F., Dhariwal, P., Radford, A., andKlimov, O. Proximal policy optimization algorithms,2017.Team, O. E. L., Stooke, A., Mahajan, A., Barros, C., Deck,C., Bauer, J., Sygnowski, J., Trebacz, M., Jaderberg, M.,Mathieu, M., et al.。Open-ended learning leads to gener-ally capable agents.。arXiv preprint arXiv:2107.12808 ,2021.Trott, A., Zheng, S., Xiong, C., and Socher, R. Keepingyour distance: Solving sparse reward tasks using self-balancing shaped rewards, 2019.Van der Maaten, L. and Hinton, G. Visualizing data usingt-sne.。Journal of machine learning research , 9(11), 2008.Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jader-berg, M., Silver, D., and Kavukcuoglu, K. Feudal net-works for hierarchical reinforcement learning.. \n",
      "the piece 225\n",
      " \n",
      "daptationNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,C., Hesse, C., Jain, S., Kosaraju, V ., Saunders, W., et al.Webgpt: Browser-assisted question-answering with hu-man feedback.。arXiv preprint arXiv:2112.09332 , 2021.Ng, A. Y ., Harada, D., and Russell, S. Policy invarianceunder reward transformations: Theory and applicationto reward shaping.。In Icml, volume 99, pp.。278–287.Citeseer, 1999.Nikulin, A., Kurenkov, V ., Tarasov, D., and Kolesnikov, S.Anti-exploration by random network distillation.。arXivpreprint arXiv:2301.13616 , 2023.OpenAI.。Gpt-4 technical report, 2023.Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,et al.。Training language models to follow instructionswith human feedback.。Advances in Neural InformationProcessing Systems , 35:27730–27744, 2022.Ren, Z., Dong, K., Zhou, Y ., Liu, Q., and Peng, J. Explo-ration via hindsight goal generation.。Advances in NeuralInformation Processing Systems , 32, 2019.Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-tized experience replay.。arXiv preprint arXiv:1511.05952 ,2015.Schulman, J., Wolski, F., Dhariwal, P., Radford, A., andKlimov, O. Proximal policy optimization algorithms,2017.Team, O. E. L., Stooke, A., Mahajan, A., Barros, C., Deck,C., Bauer, J., Sygnowski, J., Trebacz, M., Jaderberg, M.,Mathieu, M., et al.。Open-ended learning leads to gener-ally capable agents.。arXiv preprint arXiv:2107.12808 ,2021.Trott, A., Zheng, S., Xiong, C., and Socher, R. Keepingyour distance: Solving sparse reward tasks using self-balancing shaped rewards, 2019.Van der Maaten, L. and Hinton, G. Visualizing data usingt-sne.。Journal of machine learning research , 9(11), 2008.Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jader-berg, M., Silver, D., and Kavukcuoglu, K. Feudal net-works for hierarchical reinforcement learning.。In Interna-tional Conference on Machine Learning , pp.. \n",
      "the piece 226\n",
      " \n",
      "s, W., et al.Webgpt: Browser-assisted question-answering with hu-man feedback.。arXiv preprint arXiv:2112.09332 , 2021.Ng, A. Y ., Harada, D., and Russell, S. Policy invarianceunder reward transformations: Theory and applicationto reward shaping.。In Icml, volume 99, pp.。278–287.Citeseer, 1999.Nikulin, A., Kurenkov, V ., Tarasov, D., and Kolesnikov, S.Anti-exploration by random network distillation.。arXivpreprint arXiv:2301.13616 , 2023.OpenAI.。Gpt-4 technical report, 2023.Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,et al.。Training language models to follow instructionswith human feedback.。Advances in Neural InformationProcessing Systems , 35:27730–27744, 2022.Ren, Z., Dong, K., Zhou, Y ., Liu, Q., and Peng, J. Explo-ration via hindsight goal generation.。Advances in NeuralInformation Processing Systems , 32, 2019.Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-tized experience replay.。arXiv preprint arXiv:1511.05952 ,2015.Schulman, J., Wolski, F., Dhariwal, P., Radford, A., andKlimov, O. Proximal policy optimization algorithms,2017.Team, O. E. L., Stooke, A., Mahajan, A., Barros, C., Deck,C., Bauer, J., Sygnowski, J., Trebacz, M., Jaderberg, M.,Mathieu, M., et al.。Open-ended learning leads to gener-ally capable agents.。arXiv preprint arXiv:2107.12808 ,2021.Trott, A., Zheng, S., Xiong, C., and Socher, R. Keepingyour distance: Solving sparse reward tasks using self-balancing shaped rewards, 2019.Van der Maaten, L. and Hinton, G. Visualizing data usingt-sne.。Journal of machine learning research , 9(11), 2008.Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jader-berg, M., Silver, D., and Kavukcuoglu, K. Feudal net-works for hierarchical reinforcement learning.。In Interna-tional Conference on Machine Learning , pp.。3540–3549.PMLR, 2017.Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds,T., Georgiev, P., et al.. \n",
      "the piece 227\n",
      " \n",
      ", Kurenkov, V ., Tarasov, D., and Kolesnikov, S.Anti-exploration by random network distillation.。arXivpreprint arXiv:2301.13616 , 2023.OpenAI.。Gpt-4 technical report, 2023.Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,et al.。Training language models to follow instructionswith human feedback.。Advances in Neural InformationProcessing Systems , 35:27730–27744, 2022.Ren, Z., Dong, K., Zhou, Y ., Liu, Q., and Peng, J. Explo-ration via hindsight goal generation.。Advances in NeuralInformation Processing Systems , 32, 2019.Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-tized experience replay.。arXiv preprint arXiv:1511.05952 ,2015.Schulman, J., Wolski, F., Dhariwal, P., Radford, A., andKlimov, O. Proximal policy optimization algorithms,2017.Team, O. E. L., Stooke, A., Mahajan, A., Barros, C., Deck,C., Bauer, J., Sygnowski, J., Trebacz, M., Jaderberg, M.,Mathieu, M., et al.。Open-ended learning leads to gener-ally capable agents.。arXiv preprint arXiv:2107.12808 ,2021.Trott, A., Zheng, S., Xiong, C., and Socher, R. Keepingyour distance: Solving sparse reward tasks using self-balancing shaped rewards, 2019.Van der Maaten, L. and Hinton, G. Visualizing data usingt-sne.。Journal of machine learning research , 9(11), 2008.Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jader-berg, M., Silver, D., and Kavukcuoglu, K. Feudal net-works for hierarchical reinforcement learning.。In Interna-tional Conference on Machine Learning , pp.。3540–3549.PMLR, 2017.Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds,T., Georgiev, P., et al.。Grandmaster level in starcraft iiusing multi-agent reinforcement learning.. \n",
      "the piece 228\n",
      " \n",
      "023.Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,et al.。Training language models to follow instructionswith human feedback.。Advances in Neural InformationProcessing Systems , 35:27730–27744, 2022.Ren, Z., Dong, K., Zhou, Y ., Liu, Q., and Peng, J. Explo-ration via hindsight goal generation.。Advances in NeuralInformation Processing Systems , 32, 2019.Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-tized experience replay.。arXiv preprint arXiv:1511.05952 ,2015.Schulman, J., Wolski, F., Dhariwal, P., Radford, A., andKlimov, O. Proximal policy optimization algorithms,2017.Team, O. E. L., Stooke, A., Mahajan, A., Barros, C., Deck,C., Bauer, J., Sygnowski, J., Trebacz, M., Jaderberg, M.,Mathieu, M., et al.。Open-ended learning leads to gener-ally capable agents.。arXiv preprint arXiv:2107.12808 ,2021.Trott, A., Zheng, S., Xiong, C., and Socher, R. Keepingyour distance: Solving sparse reward tasks using self-balancing shaped rewards, 2019.Van der Maaten, L. and Hinton, G. Visualizing data usingt-sne.。Journal of machine learning research , 9(11), 2008.Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jader-berg, M., Silver, D., and Kavukcuoglu, K. Feudal net-works for hierarchical reinforcement learning.。In Interna-tional Conference on Machine Learning , pp.。3540–3549.PMLR, 2017.Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds,T., Georgiev, P., et al.。Grandmaster level in starcraft iiusing multi-agent reinforcement learning.。Nature , 575(7782):350–354, 2019.Wagner, R. A. and Fischer, M. J.. \n",
      "the piece 229\n",
      " \n",
      "iang, X., Almeida, D., Wainwright, C.,Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,et al.。Training language models to follow instructionswith human feedback.。Advances in Neural InformationProcessing Systems , 35:27730–27744, 2022.Ren, Z., Dong, K., Zhou, Y ., Liu, Q., and Peng, J. Explo-ration via hindsight goal generation.。Advances in NeuralInformation Processing Systems , 32, 2019.Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-tized experience replay.。arXiv preprint arXiv:1511.05952 ,2015.Schulman, J., Wolski, F., Dhariwal, P., Radford, A., andKlimov, O. Proximal policy optimization algorithms,2017.Team, O. E. L., Stooke, A., Mahajan, A., Barros, C., Deck,C., Bauer, J., Sygnowski, J., Trebacz, M., Jaderberg, M.,Mathieu, M., et al.。Open-ended learning leads to gener-ally capable agents.。arXiv preprint arXiv:2107.12808 ,2021.Trott, A., Zheng, S., Xiong, C., and Socher, R. Keepingyour distance: Solving sparse reward tasks using self-balancing shaped rewards, 2019.Van der Maaten, L. and Hinton, G. Visualizing data usingt-sne.。Journal of machine learning research , 9(11), 2008.Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jader-berg, M., Silver, D., and Kavukcuoglu, K. Feudal net-works for hierarchical reinforcement learning.。In Interna-tional Conference on Machine Learning , pp.。3540–3549.PMLR, 2017.Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds,T., Georgiev, P., et al.。Grandmaster level in starcraft iiusing multi-agent reinforcement learning.。Nature , 575(7782):350–354, 2019.Wagner, R. A. and Fischer, M. J.。The string-to-string cor-rection problem.. \n",
      "the piece 230\n",
      " \n",
      "low instructionswith human feedback.。Advances in Neural InformationProcessing Systems , 35:27730–27744, 2022.Ren, Z., Dong, K., Zhou, Y ., Liu, Q., and Peng, J. Explo-ration via hindsight goal generation.。Advances in NeuralInformation Processing Systems , 32, 2019.Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-tized experience replay.。arXiv preprint arXiv:1511.05952 ,2015.Schulman, J., Wolski, F., Dhariwal, P., Radford, A., andKlimov, O. Proximal policy optimization algorithms,2017.Team, O. E. L., Stooke, A., Mahajan, A., Barros, C., Deck,C., Bauer, J., Sygnowski, J., Trebacz, M., Jaderberg, M.,Mathieu, M., et al.。Open-ended learning leads to gener-ally capable agents.。arXiv preprint arXiv:2107.12808 ,2021.Trott, A., Zheng, S., Xiong, C., and Socher, R. Keepingyour distance: Solving sparse reward tasks using self-balancing shaped rewards, 2019.Van der Maaten, L. and Hinton, G. Visualizing data usingt-sne.。Journal of machine learning research , 9(11), 2008.Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jader-berg, M., Silver, D., and Kavukcuoglu, K. Feudal net-works for hierarchical reinforcement learning.。In Interna-tional Conference on Machine Learning , pp.。3540–3549.PMLR, 2017.Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds,T., Georgiev, P., et al.。Grandmaster level in starcraft iiusing multi-agent reinforcement learning.。Nature , 575(7782):350–354, 2019.Wagner, R. A. and Fischer, M. J.。The string-to-string cor-rection problem.。Journal of the ACM (JACM) , 21(1):168–173, 1974.Wang, G., Xie, Y ., Jiang, Y ., Mandlekar, A., Xiao, C., Zhu,Y ., Fan, L., and Anandkumar, A. V oyager: An open-ended embodied agent with large language models.. \n",
      "the piece 231\n",
      " \n",
      "n Neural InformationProcessing Systems , 35:27730–27744, 2022.Ren, Z., Dong, K., Zhou, Y ., Liu, Q., and Peng, J. Explo-ration via hindsight goal generation.。Advances in NeuralInformation Processing Systems , 32, 2019.Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-tized experience replay.。arXiv preprint arXiv:1511.05952 ,2015.Schulman, J., Wolski, F., Dhariwal, P., Radford, A., andKlimov, O. Proximal policy optimization algorithms,2017.Team, O. E. L., Stooke, A., Mahajan, A., Barros, C., Deck,C., Bauer, J., Sygnowski, J., Trebacz, M., Jaderberg, M.,Mathieu, M., et al.。Open-ended learning leads to gener-ally capable agents.。arXiv preprint arXiv:2107.12808 ,2021.Trott, A., Zheng, S., Xiong, C., and Socher, R. Keepingyour distance: Solving sparse reward tasks using self-balancing shaped rewards, 2019.Van der Maaten, L. and Hinton, G. Visualizing data usingt-sne.。Journal of machine learning research , 9(11), 2008.Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jader-berg, M., Silver, D., and Kavukcuoglu, K. Feudal net-works for hierarchical reinforcement learning.。In Interna-tional Conference on Machine Learning , pp.。3540–3549.PMLR, 2017.Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds,T., Georgiev, P., et al.。Grandmaster level in starcraft iiusing multi-agent reinforcement learning.。Nature , 575(7782):350–354, 2019.Wagner, R. A. and Fischer, M. J.。The string-to-string cor-rection problem.。Journal of the ACM (JACM) , 21(1):168–173, 1974.Wang, G., Xie, Y ., Jiang, Y ., Mandlekar, A., Xiao, C., Zhu,Y ., Fan, L., and Anandkumar, A. V oyager: An open-ended embodied agent with large language models.。arXivpreprint arXiv:2305.16291 , 2023a.Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y .. \n",
      "the piece 232\n",
      " \n",
      "tion.。Advances in NeuralInformation Processing Systems , 32, 2019.Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-tized experience replay.。arXiv preprint arXiv:1511.05952 ,2015.Schulman, J., Wolski, F., Dhariwal, P., Radford, A., andKlimov, O. Proximal policy optimization algorithms,2017.Team, O. E. L., Stooke, A., Mahajan, A., Barros, C., Deck,C., Bauer, J., Sygnowski, J., Trebacz, M., Jaderberg, M.,Mathieu, M., et al.。Open-ended learning leads to gener-ally capable agents.。arXiv preprint arXiv:2107.12808 ,2021.Trott, A., Zheng, S., Xiong, C., and Socher, R. Keepingyour distance: Solving sparse reward tasks using self-balancing shaped rewards, 2019.Van der Maaten, L. and Hinton, G. Visualizing data usingt-sne.。Journal of machine learning research , 9(11), 2008.Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jader-berg, M., Silver, D., and Kavukcuoglu, K. Feudal net-works for hierarchical reinforcement learning.。In Interna-tional Conference on Machine Learning , pp.。3540–3549.PMLR, 2017.Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds,T., Georgiev, P., et al.。Grandmaster level in starcraft iiusing multi-agent reinforcement learning.。Nature , 575(7782):350–354, 2019.Wagner, R. A. and Fischer, M. J.。The string-to-string cor-rection problem.。Journal of the ACM (JACM) , 21(1):168–173, 1974.Wang, G., Xie, Y ., Jiang, Y ., Mandlekar, A., Xiao, C., Zhu,Y ., Fan, L., and Anandkumar, A. V oyager: An open-ended embodied agent with large language models.。arXivpreprint arXiv:2305.16291 , 2023a.Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y .。Describe,explain, plan and select: Interactive planning with largelanguage models enables open-world multi-task agents.arXiv preprint arXiv:2302.01560 , 2023b.Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thoughtprompting elicits reasoning in large language models,2023.Weng, L. Curriculum for reinforcement learn-ing.. \n",
      "the piece 233\n",
      " \n",
      "aul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-tized experience replay.。arXiv preprint arXiv:1511.05952 ,2015.Schulman, J., Wolski, F., Dhariwal, P., Radford, A., andKlimov, O. Proximal policy optimization algorithms,2017.Team, O. E. L., Stooke, A., Mahajan, A., Barros, C., Deck,C., Bauer, J., Sygnowski, J., Trebacz, M., Jaderberg, M.,Mathieu, M., et al.。Open-ended learning leads to gener-ally capable agents.。arXiv preprint arXiv:2107.12808 ,2021.Trott, A., Zheng, S., Xiong, C., and Socher, R. Keepingyour distance: Solving sparse reward tasks using self-balancing shaped rewards, 2019.Van der Maaten, L. and Hinton, G. Visualizing data usingt-sne.。Journal of machine learning research , 9(11), 2008.Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jader-berg, M., Silver, D., and Kavukcuoglu, K. Feudal net-works for hierarchical reinforcement learning.。In Interna-tional Conference on Machine Learning , pp.。3540–3549.PMLR, 2017.Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds,T., Georgiev, P., et al.。Grandmaster level in starcraft iiusing multi-agent reinforcement learning.。Nature , 575(7782):350–354, 2019.Wagner, R. A. and Fischer, M. J.。The string-to-string cor-rection problem.。Journal of the ACM (JACM) , 21(1):168–173, 1974.Wang, G., Xie, Y ., Jiang, Y ., Mandlekar, A., Xiao, C., Zhu,Y ., Fan, L., and Anandkumar, A. V oyager: An open-ended embodied agent with large language models.。arXivpreprint arXiv:2305.16291 , 2023a.Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y .。Describe,explain, plan and select: Interactive planning with largelanguage models enables open-world multi-task agents.arXiv preprint arXiv:2302.01560 , 2023b.Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thoughtprompting elicits reasoning in large language models,2023.Weng, L. Curriculum for reinforcement learn-ing.。lilianweng.github.io , Jan 2020.. \n",
      "the piece 234\n",
      " \n",
      "A., andKlimov, O. Proximal policy optimization algorithms,2017.Team, O. E. L., Stooke, A., Mahajan, A., Barros, C., Deck,C., Bauer, J., Sygnowski, J., Trebacz, M., Jaderberg, M.,Mathieu, M., et al.。Open-ended learning leads to gener-ally capable agents.。arXiv preprint arXiv:2107.12808 ,2021.Trott, A., Zheng, S., Xiong, C., and Socher, R. Keepingyour distance: Solving sparse reward tasks using self-balancing shaped rewards, 2019.Van der Maaten, L. and Hinton, G. Visualizing data usingt-sne.。Journal of machine learning research , 9(11), 2008.Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jader-berg, M., Silver, D., and Kavukcuoglu, K. Feudal net-works for hierarchical reinforcement learning.。In Interna-tional Conference on Machine Learning , pp.。3540–3549.PMLR, 2017.Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds,T., Georgiev, P., et al.。Grandmaster level in starcraft iiusing multi-agent reinforcement learning.。Nature , 575(7782):350–354, 2019.Wagner, R. A. and Fischer, M. J.。The string-to-string cor-rection problem.。Journal of the ACM (JACM) , 21(1):168–173, 1974.Wang, G., Xie, Y ., Jiang, Y ., Mandlekar, A., Xiao, C., Zhu,Y ., Fan, L., and Anandkumar, A. V oyager: An open-ended embodied agent with large language models.。arXivpreprint arXiv:2305.16291 , 2023a.Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y .。Describe,explain, plan and select: Interactive planning with largelanguage models enables open-world multi-task agents.arXiv preprint arXiv:2302.01560 , 2023b.Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thoughtprompting elicits reasoning in large language models,2023.Weng, L. Curriculum for reinforcement learn-ing.。lilianweng.github.io , Jan 2020.。URLhttps://lilianweng.github.io/posts/2020-01-29-curriculum-rl/ .Zhang, T., Xu, H., Wang, X., Wu, Y ., Keutzer, K., Gonza-lez, J. E., and Tian, Y .. \n",
      "the piece 235\n",
      " \n",
      " J., Trebacz, M., Jaderberg, M.,Mathieu, M., et al.。Open-ended learning leads to gener-ally capable agents.。arXiv preprint arXiv:2107.12808 ,2021.Trott, A., Zheng, S., Xiong, C., and Socher, R. Keepingyour distance: Solving sparse reward tasks using self-balancing shaped rewards, 2019.Van der Maaten, L. and Hinton, G. Visualizing data usingt-sne.。Journal of machine learning research , 9(11), 2008.Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jader-berg, M., Silver, D., and Kavukcuoglu, K. Feudal net-works for hierarchical reinforcement learning.。In Interna-tional Conference on Machine Learning , pp.。3540–3549.PMLR, 2017.Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds,T., Georgiev, P., et al.。Grandmaster level in starcraft iiusing multi-agent reinforcement learning.。Nature , 575(7782):350–354, 2019.Wagner, R. A. and Fischer, M. J.。The string-to-string cor-rection problem.。Journal of the ACM (JACM) , 21(1):168–173, 1974.Wang, G., Xie, Y ., Jiang, Y ., Mandlekar, A., Xiao, C., Zhu,Y ., Fan, L., and Anandkumar, A. V oyager: An open-ended embodied agent with large language models.。arXivpreprint arXiv:2305.16291 , 2023a.Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y .。Describe,explain, plan and select: Interactive planning with largelanguage models enables open-world multi-task agents.arXiv preprint arXiv:2302.01560 , 2023b.Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thoughtprompting elicits reasoning in large language models,2023.Weng, L. Curriculum for reinforcement learn-ing.。lilianweng.github.io , Jan 2020.。URLhttps://lilianweng.github.io/posts/2020-01-29-curriculum-rl/ .Zhang, T., Xu, H., Wang, X., Wu, Y ., Keutzer, K., Gonza-lez, J. E., and Tian, Y .。Noveld: A simple yet effectiveexploration criterion.. \n",
      "the piece 236\n",
      " \n",
      "Van der Maaten, L. and Hinton, G. Visualizing data usingt-sne.。Journal of machine learning research , 9(11), 2008.Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jader-berg, M., Silver, D., and Kavukcuoglu, K. Feudal net-works for hierarchical reinforcement learning.。In Interna-tional Conference on Machine Learning , pp.。3540–3549.PMLR, 2017.Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds,T., Georgiev, P., et al.。Grandmaster level in starcraft iiusing multi-agent reinforcement learning.。Nature , 575(7782):350–354, 2019.Wagner, R. A. and Fischer, M. J.。The string-to-string cor-rection problem.。Journal of the ACM (JACM) , 21(1):168–173, 1974.Wang, G., Xie, Y ., Jiang, Y ., Mandlekar, A., Xiao, C., Zhu,Y ., Fan, L., and Anandkumar, A. V oyager: An open-ended embodied agent with large language models.。arXivpreprint arXiv:2305.16291 , 2023a.Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y .。Describe,explain, plan and select: Interactive planning with largelanguage models enables open-world multi-task agents.arXiv preprint arXiv:2302.01560 , 2023b.Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thoughtprompting elicits reasoning in large language models,2023.Weng, L. Curriculum for reinforcement learn-ing.。lilianweng.github.io , Jan 2020.。URLhttps://lilianweng.github.io/posts/2020-01-29-curriculum-rl/ .Zhang, T., Xu, H., Wang, X., Wu, Y ., Keutzer, K., Gonza-lez, J. E., and Tian, Y .。Noveld: A simple yet effectiveexploration criterion.。Advances in Neural InformationProcessing Systems , 34:25217–25230, 2021.Zheng, Q., Zhang, A., and Grover, A. Online decision trans-former.. \n",
      "the piece 237\n",
      " \n",
      "-sne.。Journal of machine learning research , 9(11), 2008.Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jader-berg, M., Silver, D., and Kavukcuoglu, K. Feudal net-works for hierarchical reinforcement learning.。In Interna-tional Conference on Machine Learning , pp.。3540–3549.PMLR, 2017.Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds,T., Georgiev, P., et al.。Grandmaster level in starcraft iiusing multi-agent reinforcement learning.。Nature , 575(7782):350–354, 2019.Wagner, R. A. and Fischer, M. J.。The string-to-string cor-rection problem.。Journal of the ACM (JACM) , 21(1):168–173, 1974.Wang, G., Xie, Y ., Jiang, Y ., Mandlekar, A., Xiao, C., Zhu,Y ., Fan, L., and Anandkumar, A. V oyager: An open-ended embodied agent with large language models.。arXivpreprint arXiv:2305.16291 , 2023a.Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y .。Describe,explain, plan and select: Interactive planning with largelanguage models enables open-world multi-task agents.arXiv preprint arXiv:2302.01560 , 2023b.Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thoughtprompting elicits reasoning in large language models,2023.Weng, L. Curriculum for reinforcement learn-ing.。lilianweng.github.io , Jan 2020.。URLhttps://lilianweng.github.io/posts/2020-01-29-curriculum-rl/ .Zhang, T., Xu, H., Wang, X., Wu, Y ., Keutzer, K., Gonza-lez, J. E., and Tian, Y .。Noveld: A simple yet effectiveexploration criterion.。Advances in Neural InformationProcessing Systems , 34:25217–25230, 2021.Zheng, Q., Zhang, A., and Grover, A. Online decision trans-former.。In international conference on machine learning ,pp.. \n",
      "the piece 238\n",
      " \n",
      "rence on Machine Learning , pp.。3540–3549.PMLR, 2017.Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds,T., Georgiev, P., et al.。Grandmaster level in starcraft iiusing multi-agent reinforcement learning.。Nature , 575(7782):350–354, 2019.Wagner, R. A. and Fischer, M. J.。The string-to-string cor-rection problem.。Journal of the ACM (JACM) , 21(1):168–173, 1974.Wang, G., Xie, Y ., Jiang, Y ., Mandlekar, A., Xiao, C., Zhu,Y ., Fan, L., and Anandkumar, A. V oyager: An open-ended embodied agent with large language models.。arXivpreprint arXiv:2305.16291 , 2023a.Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y .。Describe,explain, plan and select: Interactive planning with largelanguage models enables open-world multi-task agents.arXiv preprint arXiv:2302.01560 , 2023b.Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thoughtprompting elicits reasoning in large language models,2023.Weng, L. Curriculum for reinforcement learn-ing.。lilianweng.github.io , Jan 2020.。URLhttps://lilianweng.github.io/posts/2020-01-29-curriculum-rl/ .Zhang, T., Xu, H., Wang, X., Wu, Y ., Keutzer, K., Gonza-lez, J. E., and Tian, Y .。Noveld: A simple yet effectiveexploration criterion.。Advances in Neural InformationProcessing Systems , 34:25217–25230, 2021.Zheng, Q., Zhang, A., and Grover, A. Online decision trans-former.。In international conference on machine learning ,pp.。27042–27059.. \n",
      "the piece 239\n",
      " \n",
      "tarcraft iiusing multi-agent reinforcement learning.。Nature , 575(7782):350–354, 2019.Wagner, R. A. and Fischer, M. J.。The string-to-string cor-rection problem.。Journal of the ACM (JACM) , 21(1):168–173, 1974.Wang, G., Xie, Y ., Jiang, Y ., Mandlekar, A., Xiao, C., Zhu,Y ., Fan, L., and Anandkumar, A. V oyager: An open-ended embodied agent with large language models.。arXivpreprint arXiv:2305.16291 , 2023a.Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y .。Describe,explain, plan and select: Interactive planning with largelanguage models enables open-world multi-task agents.arXiv preprint arXiv:2302.01560 , 2023b.Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thoughtprompting elicits reasoning in large language models,2023.Weng, L. Curriculum for reinforcement learn-ing.。lilianweng.github.io , Jan 2020.。URLhttps://lilianweng.github.io/posts/2020-01-29-curriculum-rl/ .Zhang, T., Xu, H., Wang, X., Wu, Y ., Keutzer, K., Gonza-lez, J. E., and Tian, Y .。Noveld: A simple yet effectiveexploration criterion.。Advances in Neural InformationProcessing Systems , 34:25217–25230, 2021.Zheng, Q., Zhang, A., and Grover, A. Online decision trans-former.。In international conference on machine learning ,pp.。27042–27059.。PMLR, 2022.Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F.,Wu, J., Wohlhart, P., Welker, S., Wahid, A., et al.. \n",
      "the piece 240\n",
      " \n",
      "ure , 575(7782):350–354, 2019.Wagner, R. A. and Fischer, M. J.。The string-to-string cor-rection problem.。Journal of the ACM (JACM) , 21(1):168–173, 1974.Wang, G., Xie, Y ., Jiang, Y ., Mandlekar, A., Xiao, C., Zhu,Y ., Fan, L., and Anandkumar, A. V oyager: An open-ended embodied agent with large language models.。arXivpreprint arXiv:2305.16291 , 2023a.Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y .。Describe,explain, plan and select: Interactive planning with largelanguage models enables open-world multi-task agents.arXiv preprint arXiv:2302.01560 , 2023b.Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thoughtprompting elicits reasoning in large language models,2023.Weng, L. Curriculum for reinforcement learn-ing.。lilianweng.github.io , Jan 2020.。URLhttps://lilianweng.github.io/posts/2020-01-29-curriculum-rl/ .Zhang, T., Xu, H., Wang, X., Wu, Y ., Keutzer, K., Gonza-lez, J. E., and Tian, Y .。Noveld: A simple yet effectiveexploration criterion.。Advances in Neural InformationProcessing Systems , 34:25217–25230, 2021.Zheng, Q., Zhang, A., and Grover, A. Online decision trans-former.。In international conference on machine learning ,pp.。27042–27059.。PMLR, 2022.Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F.,Wu, J., Wohlhart, P., Welker, S., Wahid, A., et al.。Rt-2:Vision-language-action models transfer web knowledgeto robotic control.. \n",
      "the piece 241\n",
      " \n",
      ", Xie, Y ., Jiang, Y ., Mandlekar, A., Xiao, C., Zhu,Y ., Fan, L., and Anandkumar, A. V oyager: An open-ended embodied agent with large language models.。arXivpreprint arXiv:2305.16291 , 2023a.Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y .。Describe,explain, plan and select: Interactive planning with largelanguage models enables open-world multi-task agents.arXiv preprint arXiv:2302.01560 , 2023b.Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thoughtprompting elicits reasoning in large language models,2023.Weng, L. Curriculum for reinforcement learn-ing.。lilianweng.github.io , Jan 2020.。URLhttps://lilianweng.github.io/posts/2020-01-29-curriculum-rl/ .Zhang, T., Xu, H., Wang, X., Wu, Y ., Keutzer, K., Gonza-lez, J. E., and Tian, Y .。Noveld: A simple yet effectiveexploration criterion.。Advances in Neural InformationProcessing Systems , 34:25217–25230, 2021.Zheng, Q., Zhang, A., and Grover, A. Online decision trans-former.。In international conference on machine learning ,pp.。27042–27059.。PMLR, 2022.Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F.,Wu, J., Wohlhart, P., Welker, S., Wahid, A., et al.。Rt-2:Vision-language-action models transfer web knowledgeto robotic control.。In 7th Annual Conference on RobotLearning , 2023.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationA.. \n",
      "the piece 242\n",
      " \n",
      "kumar, A. V oyager: An open-ended embodied agent with large language models.。arXivpreprint arXiv:2305.16291 , 2023a.Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y .。Describe,explain, plan and select: Interactive planning with largelanguage models enables open-world multi-task agents.arXiv preprint arXiv:2302.01560 , 2023b.Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thoughtprompting elicits reasoning in large language models,2023.Weng, L. Curriculum for reinforcement learn-ing.。lilianweng.github.io , Jan 2020.。URLhttps://lilianweng.github.io/posts/2020-01-29-curriculum-rl/ .Zhang, T., Xu, H., Wang, X., Wu, Y ., Keutzer, K., Gonza-lez, J. E., and Tian, Y .。Noveld: A simple yet effectiveexploration criterion.。Advances in Neural InformationProcessing Systems , 34:25217–25230, 2021.Zheng, Q., Zhang, A., and Grover, A. Online decision trans-former.。In international conference on machine learning ,pp.。27042–27059.。PMLR, 2022.Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F.,Wu, J., Wohlhart, P., Welker, S., Wahid, A., et al.。Rt-2:Vision-language-action models transfer web knowledgeto robotic control.。In 7th Annual Conference on RobotLearning , 2023.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationA.。Contra: The EnvironmentContra seamlessly merges the last-man-standing gameplay dynamics with the survival, exploration, and scavenging elementsinherent in first-person shooting games (Gautam et al., 2021).. \n",
      "the piece 243\n",
      " \n",
      "e models.。arXivpreprint arXiv:2305.16291 , 2023a.Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y .。Describe,explain, plan and select: Interactive planning with largelanguage models enables open-world multi-task agents.arXiv preprint arXiv:2302.01560 , 2023b.Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thoughtprompting elicits reasoning in large language models,2023.Weng, L. Curriculum for reinforcement learn-ing.。lilianweng.github.io , Jan 2020.。URLhttps://lilianweng.github.io/posts/2020-01-29-curriculum-rl/ .Zhang, T., Xu, H., Wang, X., Wu, Y ., Keutzer, K., Gonza-lez, J. E., and Tian, Y .。Noveld: A simple yet effectiveexploration criterion.。Advances in Neural InformationProcessing Systems , 34:25217–25230, 2021.Zheng, Q., Zhang, A., and Grover, A. Online decision trans-former.。In international conference on machine learning ,pp.。27042–27059.。PMLR, 2022.Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F.,Wu, J., Wohlhart, P., Welker, S., Wahid, A., et al.。Rt-2:Vision-language-action models transfer web knowledgeto robotic control.。In 7th Annual Conference on RobotLearning , 2023.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationA.。Contra: The EnvironmentContra seamlessly merges the last-man-standing gameplay dynamics with the survival, exploration, and scavenging elementsinherent in first-person shooting games (Gautam et al., 2021).。The game unfolds with multiple hostile teams, necessitatingplayers to collaborate with teammates, withstand adversaries, and strive to outlast others in the ever-changing arena.. \n",
      "the piece 244\n",
      " \n",
      "2023a.Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y .。Describe,explain, plan and select: Interactive planning with largelanguage models enables open-world multi-task agents.arXiv preprint arXiv:2302.01560 , 2023b.Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thoughtprompting elicits reasoning in large language models,2023.Weng, L. Curriculum for reinforcement learn-ing.。lilianweng.github.io , Jan 2020.。URLhttps://lilianweng.github.io/posts/2020-01-29-curriculum-rl/ .Zhang, T., Xu, H., Wang, X., Wu, Y ., Keutzer, K., Gonza-lez, J. E., and Tian, Y .。Noveld: A simple yet effectiveexploration criterion.。Advances in Neural InformationProcessing Systems , 34:25217–25230, 2021.Zheng, Q., Zhang, A., and Grover, A. Online decision trans-former.。In international conference on machine learning ,pp.。27042–27059.。PMLR, 2022.Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F.,Wu, J., Wohlhart, P., Welker, S., Wahid, A., et al.。Rt-2:Vision-language-action models transfer web knowledgeto robotic control.。In 7th Annual Conference on RobotLearning , 2023.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationA.。Contra: The EnvironmentContra seamlessly merges the last-man-standing gameplay dynamics with the survival, exploration, and scavenging elementsinherent in first-person shooting games (Gautam et al., 2021).。The game unfolds with multiple hostile teams, necessitatingplayers to collaborate with teammates, withstand adversaries, and strive to outlast others in the ever-changing arena.。Theagent’s objectives encompass individual survival and the elimination of encountered enemies.. \n",
      "the piece 245\n",
      " \n",
      "2023b.Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thoughtprompting elicits reasoning in large language models,2023.Weng, L. Curriculum for reinforcement learn-ing.。lilianweng.github.io , Jan 2020.。URLhttps://lilianweng.github.io/posts/2020-01-29-curriculum-rl/ .Zhang, T., Xu, H., Wang, X., Wu, Y ., Keutzer, K., Gonza-lez, J. E., and Tian, Y .。Noveld: A simple yet effectiveexploration criterion.。Advances in Neural InformationProcessing Systems , 34:25217–25230, 2021.Zheng, Q., Zhang, A., and Grover, A. Online decision trans-former.。In international conference on machine learning ,pp.。27042–27059.。PMLR, 2022.Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F.,Wu, J., Wohlhart, P., Welker, S., Wahid, A., et al.。Rt-2:Vision-language-action models transfer web knowledgeto robotic control.。In 7th Annual Conference on RobotLearning , 2023.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationA.。Contra: The EnvironmentContra seamlessly merges the last-man-standing gameplay dynamics with the survival, exploration, and scavenging elementsinherent in first-person shooting games (Gautam et al., 2021).。The game unfolds with multiple hostile teams, necessitatingplayers to collaborate with teammates, withstand adversaries, and strive to outlast others in the ever-changing arena.。Theagent’s objectives encompass individual survival and the elimination of encountered enemies.。In Contra, the agent’s actioninterface is designed to mirror human capabilities, encompassing basic movements and actions like moving and shooting,with action intervals around 200ms, similar to the frequency of human operations.. \n",
      "the piece 246\n",
      " \n",
      "d Zhou, D. Chain-of-thoughtprompting elicits reasoning in large language models,2023.Weng, L. Curriculum for reinforcement learn-ing.。lilianweng.github.io , Jan 2020.。URLhttps://lilianweng.github.io/posts/2020-01-29-curriculum-rl/ .Zhang, T., Xu, H., Wang, X., Wu, Y ., Keutzer, K., Gonza-lez, J. E., and Tian, Y .。Noveld: A simple yet effectiveexploration criterion.。Advances in Neural InformationProcessing Systems , 34:25217–25230, 2021.Zheng, Q., Zhang, A., and Grover, A. Online decision trans-former.。In international conference on machine learning ,pp.。27042–27059.。PMLR, 2022.Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F.,Wu, J., Wohlhart, P., Welker, S., Wahid, A., et al.。Rt-2:Vision-language-action models transfer web knowledgeto robotic control.。In 7th Annual Conference on RobotLearning , 2023.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationA.。Contra: The EnvironmentContra seamlessly merges the last-man-standing gameplay dynamics with the survival, exploration, and scavenging elementsinherent in first-person shooting games (Gautam et al., 2021).。The game unfolds with multiple hostile teams, necessitatingplayers to collaborate with teammates, withstand adversaries, and strive to outlast others in the ever-changing arena.。Theagent’s objectives encompass individual survival and the elimination of encountered enemies.。In Contra, the agent’s actioninterface is designed to mirror human capabilities, encompassing basic movements and actions like moving and shooting,with action intervals around 200ms, similar to the frequency of human operations.。Through these actions, an agent in Contramandates a sequential acquisition of skills, starting from fundamental abilities like walking, jumping, running, and itemcollection.. \n",
      "the piece 247\n",
      " \n",
      " Neural InformationProcessing Systems , 34:25217–25230, 2021.Zheng, Q., Zhang, A., and Grover, A. Online decision trans-former.。In international conference on machine learning ,pp.。27042–27059.。PMLR, 2022.Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F.,Wu, J., Wohlhart, P., Welker, S., Wahid, A., et al.。Rt-2:Vision-language-action models transfer web knowledgeto robotic control.。In 7th Annual Conference on RobotLearning , 2023.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationA.。Contra: The EnvironmentContra seamlessly merges the last-man-standing gameplay dynamics with the survival, exploration, and scavenging elementsinherent in first-person shooting games (Gautam et al., 2021).。The game unfolds with multiple hostile teams, necessitatingplayers to collaborate with teammates, withstand adversaries, and strive to outlast others in the ever-changing arena.。Theagent’s objectives encompass individual survival and the elimination of encountered enemies.。In Contra, the agent’s actioninterface is designed to mirror human capabilities, encompassing basic movements and actions like moving and shooting,with action intervals around 200ms, similar to the frequency of human operations.。Through these actions, an agent in Contramandates a sequential acquisition of skills, starting from fundamental abilities like walking, jumping, running, and itemcollection.。As the learning proceeds, an agent must master more intricate skills such as evading enemy projectiles andcoordinating tactics with teammates.. \n",
      "the piece 248\n",
      " \n",
      "ems , 34:25217–25230, 2021.Zheng, Q., Zhang, A., and Grover, A. Online decision trans-former.。In international conference on machine learning ,pp.。27042–27059.。PMLR, 2022.Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F.,Wu, J., Wohlhart, P., Welker, S., Wahid, A., et al.。Rt-2:Vision-language-action models transfer web knowledgeto robotic control.。In 7th Annual Conference on RobotLearning , 2023.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationA.。Contra: The EnvironmentContra seamlessly merges the last-man-standing gameplay dynamics with the survival, exploration, and scavenging elementsinherent in first-person shooting games (Gautam et al., 2021).。The game unfolds with multiple hostile teams, necessitatingplayers to collaborate with teammates, withstand adversaries, and strive to outlast others in the ever-changing arena.。Theagent’s objectives encompass individual survival and the elimination of encountered enemies.。In Contra, the agent’s actioninterface is designed to mirror human capabilities, encompassing basic movements and actions like moving and shooting,with action intervals around 200ms, similar to the frequency of human operations.。Through these actions, an agent in Contramandates a sequential acquisition of skills, starting from fundamental abilities like walking, jumping, running, and itemcollection.。As the learning proceeds, an agent must master more intricate skills such as evading enemy projectiles andcoordinating tactics with teammates.。This characteristic defines an open-ended learning process where the agent continuallyexplores the game environment to refine mastered skills and acquire new ones.Observation Space.. \n",
      "the piece 249\n",
      " \n",
      "042–27059.。PMLR, 2022.Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F.,Wu, J., Wohlhart, P., Welker, S., Wahid, A., et al.。Rt-2:Vision-language-action models transfer web knowledgeto robotic control.。In 7th Annual Conference on RobotLearning , 2023.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationA.。Contra: The EnvironmentContra seamlessly merges the last-man-standing gameplay dynamics with the survival, exploration, and scavenging elementsinherent in first-person shooting games (Gautam et al., 2021).。The game unfolds with multiple hostile teams, necessitatingplayers to collaborate with teammates, withstand adversaries, and strive to outlast others in the ever-changing arena.。Theagent’s objectives encompass individual survival and the elimination of encountered enemies.。In Contra, the agent’s actioninterface is designed to mirror human capabilities, encompassing basic movements and actions like moving and shooting,with action intervals around 200ms, similar to the frequency of human operations.。Through these actions, an agent in Contramandates a sequential acquisition of skills, starting from fundamental abilities like walking, jumping, running, and itemcollection.。As the learning proceeds, an agent must master more intricate skills such as evading enemy projectiles andcoordinating tactics with teammates.。This characteristic defines an open-ended learning process where the agent continuallyexplores the game environment to refine mastered skills and acquire new ones.Observation Space.。The observation space encompasses various factors, comprising unit features delineating the agent’sstatus, as well as that of other players.. \n",
      "the piece 250\n",
      " \n",
      "u, P., Xiao, T., Xia, F.,Wu, J., Wohlhart, P., Welker, S., Wahid, A., et al.。Rt-2:Vision-language-action models transfer web knowledgeto robotic control.。In 7th Annual Conference on RobotLearning , 2023.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationA.。Contra: The EnvironmentContra seamlessly merges the last-man-standing gameplay dynamics with the survival, exploration, and scavenging elementsinherent in first-person shooting games (Gautam et al., 2021).。The game unfolds with multiple hostile teams, necessitatingplayers to collaborate with teammates, withstand adversaries, and strive to outlast others in the ever-changing arena.。Theagent’s objectives encompass individual survival and the elimination of encountered enemies.。In Contra, the agent’s actioninterface is designed to mirror human capabilities, encompassing basic movements and actions like moving and shooting,with action intervals around 200ms, similar to the frequency of human operations.。Through these actions, an agent in Contramandates a sequential acquisition of skills, starting from fundamental abilities like walking, jumping, running, and itemcollection.。As the learning proceeds, an agent must master more intricate skills such as evading enemy projectiles andcoordinating tactics with teammates.。This characteristic defines an open-ended learning process where the agent continuallyexplores the game environment to refine mastered skills and acquire new ones.Observation Space.。The observation space encompasses various factors, comprising unit features delineating the agent’sstatus, as well as that of other players.。Additionally, it includes environmental features characterizing interaction eventsand an agent-centric RGB bird’s-eye-view of the local observation.. \n",
      "the piece 251\n",
      " \n",
      "otic control.。In 7th Annual Conference on RobotLearning , 2023.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationA.。Contra: The EnvironmentContra seamlessly merges the last-man-standing gameplay dynamics with the survival, exploration, and scavenging elementsinherent in first-person shooting games (Gautam et al., 2021).。The game unfolds with multiple hostile teams, necessitatingplayers to collaborate with teammates, withstand adversaries, and strive to outlast others in the ever-changing arena.。Theagent’s objectives encompass individual survival and the elimination of encountered enemies.。In Contra, the agent’s actioninterface is designed to mirror human capabilities, encompassing basic movements and actions like moving and shooting,with action intervals around 200ms, similar to the frequency of human operations.。Through these actions, an agent in Contramandates a sequential acquisition of skills, starting from fundamental abilities like walking, jumping, running, and itemcollection.。As the learning proceeds, an agent must master more intricate skills such as evading enemy projectiles andcoordinating tactics with teammates.。This characteristic defines an open-ended learning process where the agent continuallyexplores the game environment to refine mastered skills and acquire new ones.Observation Space.。The observation space encompasses various factors, comprising unit features delineating the agent’sstatus, as well as that of other players.。Additionally, it includes environmental features characterizing interaction eventsand an agent-centric RGB bird’s-eye-view of the local observation.。For the details, we include them in Table 4.. \n",
      "the piece 252\n",
      " \n",
      "g , 2023.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationA.。Contra: The EnvironmentContra seamlessly merges the last-man-standing gameplay dynamics with the survival, exploration, and scavenging elementsinherent in first-person shooting games (Gautam et al., 2021).。The game unfolds with multiple hostile teams, necessitatingplayers to collaborate with teammates, withstand adversaries, and strive to outlast others in the ever-changing arena.。Theagent’s objectives encompass individual survival and the elimination of encountered enemies.。In Contra, the agent’s actioninterface is designed to mirror human capabilities, encompassing basic movements and actions like moving and shooting,with action intervals around 200ms, similar to the frequency of human operations.。Through these actions, an agent in Contramandates a sequential acquisition of skills, starting from fundamental abilities like walking, jumping, running, and itemcollection.。As the learning proceeds, an agent must master more intricate skills such as evading enemy projectiles andcoordinating tactics with teammates.。This characteristic defines an open-ended learning process where the agent continuallyexplores the game environment to refine mastered skills and acquire new ones.Observation Space.。The observation space encompasses various factors, comprising unit features delineating the agent’sstatus, as well as that of other players.。Additionally, it includes environmental features characterizing interaction eventsand an agent-centric RGB bird’s-eye-view of the local observation.。For the details, we include them in Table 4.。Given theheterogeneity in the shapes and data types of these features, we adopt independent feature processing for each of them,subsequently concatenating them to serve as input for the policy and value networks.. \n",
      "the piece 253\n",
      " \n",
      "ing Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationA.。Contra: The EnvironmentContra seamlessly merges the last-man-standing gameplay dynamics with the survival, exploration, and scavenging elementsinherent in first-person shooting games (Gautam et al., 2021).。The game unfolds with multiple hostile teams, necessitatingplayers to collaborate with teammates, withstand adversaries, and strive to outlast others in the ever-changing arena.。Theagent’s objectives encompass individual survival and the elimination of encountered enemies.。In Contra, the agent’s actioninterface is designed to mirror human capabilities, encompassing basic movements and actions like moving and shooting,with action intervals around 200ms, similar to the frequency of human operations.。Through these actions, an agent in Contramandates a sequential acquisition of skills, starting from fundamental abilities like walking, jumping, running, and itemcollection.。As the learning proceeds, an agent must master more intricate skills such as evading enemy projectiles andcoordinating tactics with teammates.。This characteristic defines an open-ended learning process where the agent continuallyexplores the game environment to refine mastered skills and acquire new ones.Observation Space.。The observation space encompasses various factors, comprising unit features delineating the agent’sstatus, as well as that of other players.。Additionally, it includes environmental features characterizing interaction eventsand an agent-centric RGB bird’s-eye-view of the local observation.。For the details, we include them in Table 4.。Given theheterogeneity in the shapes and data types of these features, we adopt independent feature processing for each of them,subsequently concatenating them to serve as input for the policy and value networks.。In Figure 4, we present the foundationalUnit Features([j�iiiu-,-----------------'The Agent RBG Bird's-eye-viewOther players -----------------'Environmental featuresFigure 4: Preprocessing for an observation with four types of features.process for handling an observation.. \n",
      "the piece 254\n",
      " \n",
      "ges the last-man-standing gameplay dynamics with the survival, exploration, and scavenging elementsinherent in first-person shooting games (Gautam et al., 2021).。The game unfolds with multiple hostile teams, necessitatingplayers to collaborate with teammates, withstand adversaries, and strive to outlast others in the ever-changing arena.。Theagent’s objectives encompass individual survival and the elimination of encountered enemies.。In Contra, the agent’s actioninterface is designed to mirror human capabilities, encompassing basic movements and actions like moving and shooting,with action intervals around 200ms, similar to the frequency of human operations.。Through these actions, an agent in Contramandates a sequential acquisition of skills, starting from fundamental abilities like walking, jumping, running, and itemcollection.。As the learning proceeds, an agent must master more intricate skills such as evading enemy projectiles andcoordinating tactics with teammates.。This characteristic defines an open-ended learning process where the agent continuallyexplores the game environment to refine mastered skills and acquire new ones.Observation Space.。The observation space encompasses various factors, comprising unit features delineating the agent’sstatus, as well as that of other players.。Additionally, it includes environmental features characterizing interaction eventsand an agent-centric RGB bird’s-eye-view of the local observation.。For the details, we include them in Table 4.。Given theheterogeneity in the shapes and data types of these features, we adopt independent feature processing for each of them,subsequently concatenating them to serve as input for the policy and value networks.。In Figure 4, we present the foundationalUnit Features([j�iiiu-,-----------------'The Agent RBG Bird's-eye-viewOther players -----------------'Environmental featuresFigure 4: Preprocessing for an observation with four types of features.process for handling an observation.。Each observation consists of four types of feature, and each type is associated with anindependent network dedicated to processing and outputting the corresponding embedding.Action Space.. \n",
      "the piece 255\n",
      " \n",
      "d scavenging elementsinherent in first-person shooting games (Gautam et al., 2021).。The game unfolds with multiple hostile teams, necessitatingplayers to collaborate with teammates, withstand adversaries, and strive to outlast others in the ever-changing arena.。Theagent’s objectives encompass individual survival and the elimination of encountered enemies.。In Contra, the agent’s actioninterface is designed to mirror human capabilities, encompassing basic movements and actions like moving and shooting,with action intervals around 200ms, similar to the frequency of human operations.。Through these actions, an agent in Contramandates a sequential acquisition of skills, starting from fundamental abilities like walking, jumping, running, and itemcollection.。As the learning proceeds, an agent must master more intricate skills such as evading enemy projectiles andcoordinating tactics with teammates.。This characteristic defines an open-ended learning process where the agent continuallyexplores the game environment to refine mastered skills and acquire new ones.Observation Space.。The observation space encompasses various factors, comprising unit features delineating the agent’sstatus, as well as that of other players.。Additionally, it includes environmental features characterizing interaction eventsand an agent-centric RGB bird’s-eye-view of the local observation.。For the details, we include them in Table 4.。Given theheterogeneity in the shapes and data types of these features, we adopt independent feature processing for each of them,subsequently concatenating them to serve as input for the policy and value networks.。In Figure 4, we present the foundationalUnit Features([j�iiiu-,-----------------'The Agent RBG Bird's-eye-viewOther players -----------------'Environmental featuresFigure 4: Preprocessing for an observation with four types of features.process for handling an observation.。Each observation consists of four types of feature, and each type is associated with anindependent network dedicated to processing and outputting the corresponding embedding.Action Space.。As introduced in Table 6, the instantiation of action space is achieved through the utilization of the micro-operation API within Contra.. \n",
      "the piece 256\n",
      " \n",
      "cessitatingplayers to collaborate with teammates, withstand adversaries, and strive to outlast others in the ever-changing arena.。Theagent’s objectives encompass individual survival and the elimination of encountered enemies.。In Contra, the agent’s actioninterface is designed to mirror human capabilities, encompassing basic movements and actions like moving and shooting,with action intervals around 200ms, similar to the frequency of human operations.。Through these actions, an agent in Contramandates a sequential acquisition of skills, starting from fundamental abilities like walking, jumping, running, and itemcollection.。As the learning proceeds, an agent must master more intricate skills such as evading enemy projectiles andcoordinating tactics with teammates.。This characteristic defines an open-ended learning process where the agent continuallyexplores the game environment to refine mastered skills and acquire new ones.Observation Space.。The observation space encompasses various factors, comprising unit features delineating the agent’sstatus, as well as that of other players.。Additionally, it includes environmental features characterizing interaction eventsand an agent-centric RGB bird’s-eye-view of the local observation.。For the details, we include them in Table 4.。Given theheterogeneity in the shapes and data types of these features, we adopt independent feature processing for each of them,subsequently concatenating them to serve as input for the policy and value networks.。In Figure 4, we present the foundationalUnit Features([j�iiiu-,-----------------'The Agent RBG Bird's-eye-viewOther players -----------------'Environmental featuresFigure 4: Preprocessing for an observation with four types of features.process for handling an observation.。Each observation consists of four types of feature, and each type is associated with anindependent network dedicated to processing and outputting the corresponding embedding.Action Space.。As introduced in Table 6, the instantiation of action space is achieved through the utilization of the micro-operation API within Contra.。This process gives rise to a compilation of actions characterized by diverse levels of granularity.In a detailed breakdown, the action space comprises several distinct types, namely (1) movement direction action space , itprovides 16 discrete choices, each evenly distributed across a 360◦spectrum, (2) camera yaw direction action space , itoffers 16 choices with an equitable division of 360◦, (3) camera pitch direction action space , it encompasses three distinctvalues: −45◦,0◦,45◦, (4) body action space , it incorporates nine values: slide, stop, crouch, run, jump, ground, openor close door, rescue, and no-op and (5) attacking action space , it comprises: fire, reloading, treat, pick up supply, dropsupply, stop and fire, stop adjust and fire, and (6) weapon switch , it manifests three values: use weapon 0, use weapon 1,and no-weapon.. \n",
      "the piece 257\n",
      " \n",
      "countered enemies.。In Contra, the agent’s actioninterface is designed to mirror human capabilities, encompassing basic movements and actions like moving and shooting,with action intervals around 200ms, similar to the frequency of human operations.。Through these actions, an agent in Contramandates a sequential acquisition of skills, starting from fundamental abilities like walking, jumping, running, and itemcollection.。As the learning proceeds, an agent must master more intricate skills such as evading enemy projectiles andcoordinating tactics with teammates.。This characteristic defines an open-ended learning process where the agent continuallyexplores the game environment to refine mastered skills and acquire new ones.Observation Space.。The observation space encompasses various factors, comprising unit features delineating the agent’sstatus, as well as that of other players.。Additionally, it includes environmental features characterizing interaction eventsand an agent-centric RGB bird’s-eye-view of the local observation.。For the details, we include them in Table 4.。Given theheterogeneity in the shapes and data types of these features, we adopt independent feature processing for each of them,subsequently concatenating them to serve as input for the policy and value networks.。In Figure 4, we present the foundationalUnit Features([j�iiiu-,-----------------'The Agent RBG Bird's-eye-viewOther players -----------------'Environmental featuresFigure 4: Preprocessing for an observation with four types of features.process for handling an observation.。Each observation consists of four types of feature, and each type is associated with anindependent network dedicated to processing and outputting the corresponding embedding.Action Space.。As introduced in Table 6, the instantiation of action space is achieved through the utilization of the micro-operation API within Contra.。This process gives rise to a compilation of actions characterized by diverse levels of granularity.In a detailed breakdown, the action space comprises several distinct types, namely (1) movement direction action space , itprovides 16 discrete choices, each evenly distributed across a 360◦spectrum, (2) camera yaw direction action space , itoffers 16 choices with an equitable division of 360◦, (3) camera pitch direction action space , it encompasses three distinctvalues: −45◦,0◦,45◦, (4) body action space , it incorporates nine values: slide, stop, crouch, run, jump, ground, openor close door, rescue, and no-op and (5) attacking action space , it comprises: fire, reloading, treat, pick up supply, dropsupply, stop and fire, stop adjust and fire, and (6) weapon switch , it manifests three values: use weapon 0, use weapon 1,and no-weapon.。The aggregated dimensionality of the action space is quantified at 54 in total.Reward Engineering.. \n",
      "the piece 258\n",
      " \n",
      "ntervals around 200ms, similar to the frequency of human operations.。Through these actions, an agent in Contramandates a sequential acquisition of skills, starting from fundamental abilities like walking, jumping, running, and itemcollection.。As the learning proceeds, an agent must master more intricate skills such as evading enemy projectiles andcoordinating tactics with teammates.。This characteristic defines an open-ended learning process where the agent continuallyexplores the game environment to refine mastered skills and acquire new ones.Observation Space.。The observation space encompasses various factors, comprising unit features delineating the agent’sstatus, as well as that of other players.。Additionally, it includes environmental features characterizing interaction eventsand an agent-centric RGB bird’s-eye-view of the local observation.。For the details, we include them in Table 4.。Given theheterogeneity in the shapes and data types of these features, we adopt independent feature processing for each of them,subsequently concatenating them to serve as input for the policy and value networks.。In Figure 4, we present the foundationalUnit Features([j�iiiu-,-----------------'The Agent RBG Bird's-eye-viewOther players -----------------'Environmental featuresFigure 4: Preprocessing for an observation with four types of features.process for handling an observation.。Each observation consists of four types of feature, and each type is associated with anindependent network dedicated to processing and outputting the corresponding embedding.Action Space.。As introduced in Table 6, the instantiation of action space is achieved through the utilization of the micro-operation API within Contra.。This process gives rise to a compilation of actions characterized by diverse levels of granularity.In a detailed breakdown, the action space comprises several distinct types, namely (1) movement direction action space , itprovides 16 discrete choices, each evenly distributed across a 360◦spectrum, (2) camera yaw direction action space , itoffers 16 choices with an equitable division of 360◦, (3) camera pitch direction action space , it encompasses three distinctvalues: −45◦,0◦,45◦, (4) body action space , it incorporates nine values: slide, stop, crouch, run, jump, ground, openor close door, rescue, and no-op and (5) attacking action space , it comprises: fire, reloading, treat, pick up supply, dropsupply, stop and fire, stop adjust and fire, and (6) weapon switch , it manifests three values: use weapon 0, use weapon 1,and no-weapon.。The aggregated dimensionality of the action space is quantified at 54 in total.Reward Engineering.。The primary objective of our training regimen is to equip the agent with the ability to playwith other players in Contra while concurrently optimizing its strength in eliminating opponents.. \n",
      "the piece 259\n",
      " \n",
      "ent in Contramandates a sequential acquisition of skills, starting from fundamental abilities like walking, jumping, running, and itemcollection.。As the learning proceeds, an agent must master more intricate skills such as evading enemy projectiles andcoordinating tactics with teammates.。This characteristic defines an open-ended learning process where the agent continuallyexplores the game environment to refine mastered skills and acquire new ones.Observation Space.。The observation space encompasses various factors, comprising unit features delineating the agent’sstatus, as well as that of other players.。Additionally, it includes environmental features characterizing interaction eventsand an agent-centric RGB bird’s-eye-view of the local observation.。For the details, we include them in Table 4.。Given theheterogeneity in the shapes and data types of these features, we adopt independent feature processing for each of them,subsequently concatenating them to serve as input for the policy and value networks.。In Figure 4, we present the foundationalUnit Features([j�iiiu-,-----------------'The Agent RBG Bird's-eye-viewOther players -----------------'Environmental featuresFigure 4: Preprocessing for an observation with four types of features.process for handling an observation.。Each observation consists of four types of feature, and each type is associated with anindependent network dedicated to processing and outputting the corresponding embedding.Action Space.。As introduced in Table 6, the instantiation of action space is achieved through the utilization of the micro-operation API within Contra.。This process gives rise to a compilation of actions characterized by diverse levels of granularity.In a detailed breakdown, the action space comprises several distinct types, namely (1) movement direction action space , itprovides 16 discrete choices, each evenly distributed across a 360◦spectrum, (2) camera yaw direction action space , itoffers 16 choices with an equitable division of 360◦, (3) camera pitch direction action space , it encompasses three distinctvalues: −45◦,0◦,45◦, (4) body action space , it incorporates nine values: slide, stop, crouch, run, jump, ground, openor close door, rescue, and no-op and (5) attacking action space , it comprises: fire, reloading, treat, pick up supply, dropsupply, stop and fire, stop adjust and fire, and (6) weapon switch , it manifests three values: use weapon 0, use weapon 1,and no-weapon.。The aggregated dimensionality of the action space is quantified at 54 in total.Reward Engineering.。The primary objective of our training regimen is to equip the agent with the ability to playwith other players in Contra while concurrently optimizing its strength in eliminating opponents.。To achieve this ob-jective, we have formulated a diverse array of rewards designed to guide policy learning.. \n",
      "the piece 260\n",
      " \n",
      " enemy projectiles andcoordinating tactics with teammates.。This characteristic defines an open-ended learning process where the agent continuallyexplores the game environment to refine mastered skills and acquire new ones.Observation Space.。The observation space encompasses various factors, comprising unit features delineating the agent’sstatus, as well as that of other players.。Additionally, it includes environmental features characterizing interaction eventsand an agent-centric RGB bird’s-eye-view of the local observation.。For the details, we include them in Table 4.。Given theheterogeneity in the shapes and data types of these features, we adopt independent feature processing for each of them,subsequently concatenating them to serve as input for the policy and value networks.。In Figure 4, we present the foundationalUnit Features([j�iiiu-,-----------------'The Agent RBG Bird's-eye-viewOther players -----------------'Environmental featuresFigure 4: Preprocessing for an observation with four types of features.process for handling an observation.。Each observation consists of four types of feature, and each type is associated with anindependent network dedicated to processing and outputting the corresponding embedding.Action Space.。As introduced in Table 6, the instantiation of action space is achieved through the utilization of the micro-operation API within Contra.。This process gives rise to a compilation of actions characterized by diverse levels of granularity.In a detailed breakdown, the action space comprises several distinct types, namely (1) movement direction action space , itprovides 16 discrete choices, each evenly distributed across a 360◦spectrum, (2) camera yaw direction action space , itoffers 16 choices with an equitable division of 360◦, (3) camera pitch direction action space , it encompasses three distinctvalues: −45◦,0◦,45◦, (4) body action space , it incorporates nine values: slide, stop, crouch, run, jump, ground, openor close door, rescue, and no-op and (5) attacking action space , it comprises: fire, reloading, treat, pick up supply, dropsupply, stop and fire, stop adjust and fire, and (6) weapon switch , it manifests three values: use weapon 0, use weapon 1,and no-weapon.。The aggregated dimensionality of the action space is quantified at 54 in total.Reward Engineering.。The primary objective of our training regimen is to equip the agent with the ability to playwith other players in Contra while concurrently optimizing its strength in eliminating opponents.。To achieve this ob-jective, we have formulated a diverse array of rewards designed to guide policy learning.。However, the complexityinvolved in designing and fine-tuning these rewards is evident.. \n",
      "the piece 261\n",
      " \n",
      "to refine mastered skills and acquire new ones.Observation Space.。The observation space encompasses various factors, comprising unit features delineating the agent’sstatus, as well as that of other players.。Additionally, it includes environmental features characterizing interaction eventsand an agent-centric RGB bird’s-eye-view of the local observation.。For the details, we include them in Table 4.。Given theheterogeneity in the shapes and data types of these features, we adopt independent feature processing for each of them,subsequently concatenating them to serve as input for the policy and value networks.。In Figure 4, we present the foundationalUnit Features([j�iiiu-,-----------------'The Agent RBG Bird's-eye-viewOther players -----------------'Environmental featuresFigure 4: Preprocessing for an observation with four types of features.process for handling an observation.。Each observation consists of four types of feature, and each type is associated with anindependent network dedicated to processing and outputting the corresponding embedding.Action Space.。As introduced in Table 6, the instantiation of action space is achieved through the utilization of the micro-operation API within Contra.。This process gives rise to a compilation of actions characterized by diverse levels of granularity.In a detailed breakdown, the action space comprises several distinct types, namely (1) movement direction action space , itprovides 16 discrete choices, each evenly distributed across a 360◦spectrum, (2) camera yaw direction action space , itoffers 16 choices with an equitable division of 360◦, (3) camera pitch direction action space , it encompasses three distinctvalues: −45◦,0◦,45◦, (4) body action space , it incorporates nine values: slide, stop, crouch, run, jump, ground, openor close door, rescue, and no-op and (5) attacking action space , it comprises: fire, reloading, treat, pick up supply, dropsupply, stop and fire, stop adjust and fire, and (6) weapon switch , it manifests three values: use weapon 0, use weapon 1,and no-weapon.。The aggregated dimensionality of the action space is quantified at 54 in total.Reward Engineering.。The primary objective of our training regimen is to equip the agent with the ability to playwith other players in Contra while concurrently optimizing its strength in eliminating opponents.。To achieve this ob-jective, we have formulated a diverse array of rewards designed to guide policy learning.。However, the complexityinvolved in designing and fine-tuning these rewards is evident.。To simplify the engineering, our design is character-ized allocating weights based on the expected value of each reward, ensuring a proportionate influence on the learningprocess.. \n",
      "the piece 262\n",
      " \n",
      "lineating the agent’sstatus, as well as that of other players.。Additionally, it includes environmental features characterizing interaction eventsand an agent-centric RGB bird’s-eye-view of the local observation.。For the details, we include them in Table 4.。Given theheterogeneity in the shapes and data types of these features, we adopt independent feature processing for each of them,subsequently concatenating them to serve as input for the policy and value networks.。In Figure 4, we present the foundationalUnit Features([j�iiiu-,-----------------'The Agent RBG Bird's-eye-viewOther players -----------------'Environmental featuresFigure 4: Preprocessing for an observation with four types of features.process for handling an observation.。Each observation consists of four types of feature, and each type is associated with anindependent network dedicated to processing and outputting the corresponding embedding.Action Space.。As introduced in Table 6, the instantiation of action space is achieved through the utilization of the micro-operation API within Contra.。This process gives rise to a compilation of actions characterized by diverse levels of granularity.In a detailed breakdown, the action space comprises several distinct types, namely (1) movement direction action space , itprovides 16 discrete choices, each evenly distributed across a 360◦spectrum, (2) camera yaw direction action space , itoffers 16 choices with an equitable division of 360◦, (3) camera pitch direction action space , it encompasses three distinctvalues: −45◦,0◦,45◦, (4) body action space , it incorporates nine values: slide, stop, crouch, run, jump, ground, openor close door, rescue, and no-op and (5) attacking action space , it comprises: fire, reloading, treat, pick up supply, dropsupply, stop and fire, stop adjust and fire, and (6) weapon switch , it manifests three values: use weapon 0, use weapon 1,and no-weapon.。The aggregated dimensionality of the action space is quantified at 54 in total.Reward Engineering.。The primary objective of our training regimen is to equip the agent with the ability to playwith other players in Contra while concurrently optimizing its strength in eliminating opponents.。To achieve this ob-jective, we have formulated a diverse array of rewards designed to guide policy learning.。However, the complexityinvolved in designing and fine-tuning these rewards is evident.。To simplify the engineering, our design is character-ized allocating weights based on the expected value of each reward, ensuring a proportionate influence on the learningprocess.。In accordance with the principle, we assume a referenced maximum return of 20, with different rewards as-signed proportions based on their relative importance.. \n",
      "the piece 263\n",
      " \n",
      "ew of the local observation.。For the details, we include them in Table 4.。Given theheterogeneity in the shapes and data types of these features, we adopt independent feature processing for each of them,subsequently concatenating them to serve as input for the policy and value networks.。In Figure 4, we present the foundationalUnit Features([j�iiiu-,-----------------'The Agent RBG Bird's-eye-viewOther players -----------------'Environmental featuresFigure 4: Preprocessing for an observation with four types of features.process for handling an observation.。Each observation consists of four types of feature, and each type is associated with anindependent network dedicated to processing and outputting the corresponding embedding.Action Space.。As introduced in Table 6, the instantiation of action space is achieved through the utilization of the micro-operation API within Contra.。This process gives rise to a compilation of actions characterized by diverse levels of granularity.In a detailed breakdown, the action space comprises several distinct types, namely (1) movement direction action space , itprovides 16 discrete choices, each evenly distributed across a 360◦spectrum, (2) camera yaw direction action space , itoffers 16 choices with an equitable division of 360◦, (3) camera pitch direction action space , it encompasses three distinctvalues: −45◦,0◦,45◦, (4) body action space , it incorporates nine values: slide, stop, crouch, run, jump, ground, openor close door, rescue, and no-op and (5) attacking action space , it comprises: fire, reloading, treat, pick up supply, dropsupply, stop and fire, stop adjust and fire, and (6) weapon switch , it manifests three values: use weapon 0, use weapon 1,and no-weapon.。The aggregated dimensionality of the action space is quantified at 54 in total.Reward Engineering.。The primary objective of our training regimen is to equip the agent with the ability to playwith other players in Contra while concurrently optimizing its strength in eliminating opponents.。To achieve this ob-jective, we have formulated a diverse array of rewards designed to guide policy learning.。However, the complexityinvolved in designing and fine-tuning these rewards is evident.。To simplify the engineering, our design is character-ized allocating weights based on the expected value of each reward, ensuring a proportionate influence on the learningprocess.。In accordance with the principle, we assume a referenced maximum return of 20, with different rewards as-signed proportions based on their relative importance.。Specifically, for critical actions such as knocking down or killingan enemy, their values are set to approximately 4 (20% out of 20).. \n",
      "the piece 264\n",
      " \n",
      "s, we adopt independent feature processing for each of them,subsequently concatenating them to serve as input for the policy and value networks.。In Figure 4, we present the foundationalUnit Features([j�iiiu-,-----------------'The Agent RBG Bird's-eye-viewOther players -----------------'Environmental featuresFigure 4: Preprocessing for an observation with four types of features.process for handling an observation.。Each observation consists of four types of feature, and each type is associated with anindependent network dedicated to processing and outputting the corresponding embedding.Action Space.。As introduced in Table 6, the instantiation of action space is achieved through the utilization of the micro-operation API within Contra.。This process gives rise to a compilation of actions characterized by diverse levels of granularity.In a detailed breakdown, the action space comprises several distinct types, namely (1) movement direction action space , itprovides 16 discrete choices, each evenly distributed across a 360◦spectrum, (2) camera yaw direction action space , itoffers 16 choices with an equitable division of 360◦, (3) camera pitch direction action space , it encompasses three distinctvalues: −45◦,0◦,45◦, (4) body action space , it incorporates nine values: slide, stop, crouch, run, jump, ground, openor close door, rescue, and no-op and (5) attacking action space , it comprises: fire, reloading, treat, pick up supply, dropsupply, stop and fire, stop adjust and fire, and (6) weapon switch , it manifests three values: use weapon 0, use weapon 1,and no-weapon.。The aggregated dimensionality of the action space is quantified at 54 in total.Reward Engineering.。The primary objective of our training regimen is to equip the agent with the ability to playwith other players in Contra while concurrently optimizing its strength in eliminating opponents.。To achieve this ob-jective, we have formulated a diverse array of rewards designed to guide policy learning.。However, the complexityinvolved in designing and fine-tuning these rewards is evident.。To simplify the engineering, our design is character-ized allocating weights based on the expected value of each reward, ensuring a proportionate influence on the learningprocess.。In accordance with the principle, we assume a referenced maximum return of 20, with different rewards as-signed proportions based on their relative importance.。Specifically, for critical actions such as knocking down or killingan enemy, their values are set to approximately 4 (20% out of 20).。Conversely, for less critical actions like scoutingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationor incurring time penalties, their values are set to less than 1 (5% out of 20).. \n",
      "the piece 265\n",
      " \n",
      "gure 4, we present the foundationalUnit Features([j�iiiu-,-----------------'The Agent RBG Bird's-eye-viewOther players -----------------'Environmental featuresFigure 4: Preprocessing for an observation with four types of features.process for handling an observation.。Each observation consists of four types of feature, and each type is associated with anindependent network dedicated to processing and outputting the corresponding embedding.Action Space.。As introduced in Table 6, the instantiation of action space is achieved through the utilization of the micro-operation API within Contra.。This process gives rise to a compilation of actions characterized by diverse levels of granularity.In a detailed breakdown, the action space comprises several distinct types, namely (1) movement direction action space , itprovides 16 discrete choices, each evenly distributed across a 360◦spectrum, (2) camera yaw direction action space , itoffers 16 choices with an equitable division of 360◦, (3) camera pitch direction action space , it encompasses three distinctvalues: −45◦,0◦,45◦, (4) body action space , it incorporates nine values: slide, stop, crouch, run, jump, ground, openor close door, rescue, and no-op and (5) attacking action space , it comprises: fire, reloading, treat, pick up supply, dropsupply, stop and fire, stop adjust and fire, and (6) weapon switch , it manifests three values: use weapon 0, use weapon 1,and no-weapon.。The aggregated dimensionality of the action space is quantified at 54 in total.Reward Engineering.。The primary objective of our training regimen is to equip the agent with the ability to playwith other players in Contra while concurrently optimizing its strength in eliminating opponents.。To achieve this ob-jective, we have formulated a diverse array of rewards designed to guide policy learning.。However, the complexityinvolved in designing and fine-tuning these rewards is evident.。To simplify the engineering, our design is character-ized allocating weights based on the expected value of each reward, ensuring a proportionate influence on the learningprocess.。In accordance with the principle, we assume a referenced maximum return of 20, with different rewards as-signed proportions based on their relative importance.。Specifically, for critical actions such as knocking down or killingan enemy, their values are set to approximately 4 (20% out of 20).。Conversely, for less critical actions like scoutingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationor incurring time penalties, their values are set to less than 1 (5% out of 20).。Detailed specifications are outlined inTable 5.. \n",
      "the piece 266\n",
      " \n",
      "es([j�iiiu-,-----------------'The Agent RBG Bird's-eye-viewOther players -----------------'Environmental featuresFigure 4: Preprocessing for an observation with four types of features.process for handling an observation.。Each observation consists of four types of feature, and each type is associated with anindependent network dedicated to processing and outputting the corresponding embedding.Action Space.。As introduced in Table 6, the instantiation of action space is achieved through the utilization of the micro-operation API within Contra.。This process gives rise to a compilation of actions characterized by diverse levels of granularity.In a detailed breakdown, the action space comprises several distinct types, namely (1) movement direction action space , itprovides 16 discrete choices, each evenly distributed across a 360◦spectrum, (2) camera yaw direction action space , itoffers 16 choices with an equitable division of 360◦, (3) camera pitch direction action space , it encompasses three distinctvalues: −45◦,0◦,45◦, (4) body action space , it incorporates nine values: slide, stop, crouch, run, jump, ground, openor close door, rescue, and no-op and (5) attacking action space , it comprises: fire, reloading, treat, pick up supply, dropsupply, stop and fire, stop adjust and fire, and (6) weapon switch , it manifests three values: use weapon 0, use weapon 1,and no-weapon.。The aggregated dimensionality of the action space is quantified at 54 in total.Reward Engineering.。The primary objective of our training regimen is to equip the agent with the ability to playwith other players in Contra while concurrently optimizing its strength in eliminating opponents.。To achieve this ob-jective, we have formulated a diverse array of rewards designed to guide policy learning.。However, the complexityinvolved in designing and fine-tuning these rewards is evident.。To simplify the engineering, our design is character-ized allocating weights based on the expected value of each reward, ensuring a proportionate influence on the learningprocess.。In accordance with the principle, we assume a referenced maximum return of 20, with different rewards as-signed proportions based on their relative importance.。Specifically, for critical actions such as knocking down or killingan enemy, their values are set to approximately 4 (20% out of 20).。Conversely, for less critical actions like scoutingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationor incurring time penalties, their values are set to less than 1 (5% out of 20).。Detailed specifications are outlined inTable 5.。In accordance with the aforementioned principles, we can now construct the reward function r(s, a)by lin-early composing these factors, facilitating their collaborative influence on guiding policy learning.. \n",
      "the piece 267\n",
      " \n",
      "ation.。Each observation consists of four types of feature, and each type is associated with anindependent network dedicated to processing and outputting the corresponding embedding.Action Space.。As introduced in Table 6, the instantiation of action space is achieved through the utilization of the micro-operation API within Contra.。This process gives rise to a compilation of actions characterized by diverse levels of granularity.In a detailed breakdown, the action space comprises several distinct types, namely (1) movement direction action space , itprovides 16 discrete choices, each evenly distributed across a 360◦spectrum, (2) camera yaw direction action space , itoffers 16 choices with an equitable division of 360◦, (3) camera pitch direction action space , it encompasses three distinctvalues: −45◦,0◦,45◦, (4) body action space , it incorporates nine values: slide, stop, crouch, run, jump, ground, openor close door, rescue, and no-op and (5) attacking action space , it comprises: fire, reloading, treat, pick up supply, dropsupply, stop and fire, stop adjust and fire, and (6) weapon switch , it manifests three values: use weapon 0, use weapon 1,and no-weapon.。The aggregated dimensionality of the action space is quantified at 54 in total.Reward Engineering.。The primary objective of our training regimen is to equip the agent with the ability to playwith other players in Contra while concurrently optimizing its strength in eliminating opponents.。To achieve this ob-jective, we have formulated a diverse array of rewards designed to guide policy learning.。However, the complexityinvolved in designing and fine-tuning these rewards is evident.。To simplify the engineering, our design is character-ized allocating weights based on the expected value of each reward, ensuring a proportionate influence on the learningprocess.。In accordance with the principle, we assume a referenced maximum return of 20, with different rewards as-signed proportions based on their relative importance.。Specifically, for critical actions such as knocking down or killingan enemy, their values are set to approximately 4 (20% out of 20).。Conversely, for less critical actions like scoutingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationor incurring time penalties, their values are set to less than 1 (5% out of 20).。Detailed specifications are outlined inTable 5.。In accordance with the aforementioned principles, we can now construct the reward function r(s, a)by lin-early composing these factors, facilitating their collaborative influence on guiding policy learning.。As delineated inAlgorithm 3, these factors are broadly classified into three categories: basic rewards rb, obstacle avoidance rewardsroa, and goal achievement reward rg.. \n",
      "the piece 268\n",
      " \n",
      "h the utilization of the micro-operation API within Contra.。This process gives rise to a compilation of actions characterized by diverse levels of granularity.In a detailed breakdown, the action space comprises several distinct types, namely (1) movement direction action space , itprovides 16 discrete choices, each evenly distributed across a 360◦spectrum, (2) camera yaw direction action space , itoffers 16 choices with an equitable division of 360◦, (3) camera pitch direction action space , it encompasses three distinctvalues: −45◦,0◦,45◦, (4) body action space , it incorporates nine values: slide, stop, crouch, run, jump, ground, openor close door, rescue, and no-op and (5) attacking action space , it comprises: fire, reloading, treat, pick up supply, dropsupply, stop and fire, stop adjust and fire, and (6) weapon switch , it manifests three values: use weapon 0, use weapon 1,and no-weapon.。The aggregated dimensionality of the action space is quantified at 54 in total.Reward Engineering.。The primary objective of our training regimen is to equip the agent with the ability to playwith other players in Contra while concurrently optimizing its strength in eliminating opponents.。To achieve this ob-jective, we have formulated a diverse array of rewards designed to guide policy learning.。However, the complexityinvolved in designing and fine-tuning these rewards is evident.。To simplify the engineering, our design is character-ized allocating weights based on the expected value of each reward, ensuring a proportionate influence on the learningprocess.。In accordance with the principle, we assume a referenced maximum return of 20, with different rewards as-signed proportions based on their relative importance.。Specifically, for critical actions such as knocking down or killingan enemy, their values are set to approximately 4 (20% out of 20).。Conversely, for less critical actions like scoutingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationor incurring time penalties, their values are set to less than 1 (5% out of 20).。Detailed specifications are outlined inTable 5.。In accordance with the aforementioned principles, we can now construct the reward function r(s, a)by lin-early composing these factors, facilitating their collaborative influence on guiding policy learning.。As delineated inAlgorithm 3, these factors are broadly classified into three categories: basic rewards rb, obstacle avoidance rewardsroa, and goal achievement reward rg.。The basic rewards are primarily directed at steering the agent towards enhancedgame-playing performance, encompassing collaborative engagement with teammates and eliminating adversaries, etc.Feature Class Field Description Dimension1.Unit feature Scalar Includes heroes, teammates, enemies, monster 527Attribute Character ID,team ID,size,skills 28Status HP, oxygen,speed, peek type, alive state, body state,etc.. \n",
      "the piece 269\n",
      " \n",
      "ction space comprises several distinct types, namely (1) movement direction action space , itprovides 16 discrete choices, each evenly distributed across a 360◦spectrum, (2) camera yaw direction action space , itoffers 16 choices with an equitable division of 360◦, (3) camera pitch direction action space , it encompasses three distinctvalues: −45◦,0◦,45◦, (4) body action space , it incorporates nine values: slide, stop, crouch, run, jump, ground, openor close door, rescue, and no-op and (5) attacking action space , it comprises: fire, reloading, treat, pick up supply, dropsupply, stop and fire, stop adjust and fire, and (6) weapon switch , it manifests three values: use weapon 0, use weapon 1,and no-weapon.。The aggregated dimensionality of the action space is quantified at 54 in total.Reward Engineering.。The primary objective of our training regimen is to equip the agent with the ability to playwith other players in Contra while concurrently optimizing its strength in eliminating opponents.。To achieve this ob-jective, we have formulated a diverse array of rewards designed to guide policy learning.。However, the complexityinvolved in designing and fine-tuning these rewards is evident.。To simplify the engineering, our design is character-ized allocating weights based on the expected value of each reward, ensuring a proportionate influence on the learningprocess.。In accordance with the principle, we assume a referenced maximum return of 20, with different rewards as-signed proportions based on their relative importance.。Specifically, for critical actions such as knocking down or killingan enemy, their values are set to approximately 4 (20% out of 20).。Conversely, for less critical actions like scoutingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationor incurring time penalties, their values are set to less than 1 (5% out of 20).。Detailed specifications are outlined inTable 5.。In accordance with the aforementioned principles, we can now construct the reward function r(s, a)by lin-early composing these factors, facilitating their collaborative influence on guiding policy learning.。As delineated inAlgorithm 3, these factors are broadly classified into three categories: basic rewards rb, obstacle avoidance rewardsroa, and goal achievement reward rg.。The basic rewards are primarily directed at steering the agent towards enhancedgame-playing performance, encompassing collaborative engagement with teammates and eliminating adversaries, etc.Feature Class Field Description Dimension1.Unit feature Scalar Includes heroes, teammates, enemies, monster 527Attribute Character ID,team ID,size,skills 28Status HP, oxygen,speed, peek type, alive state, body state,etc.。44Heroes Pose Position, rotation, camera position, camera rotation, etc.. \n",
      "the piece 270\n",
      " \n",
      "ributed across a 360◦spectrum, (2) camera yaw direction action space , itoffers 16 choices with an equitable division of 360◦, (3) camera pitch direction action space , it encompasses three distinctvalues: −45◦,0◦,45◦, (4) body action space , it incorporates nine values: slide, stop, crouch, run, jump, ground, openor close door, rescue, and no-op and (5) attacking action space , it comprises: fire, reloading, treat, pick up supply, dropsupply, stop and fire, stop adjust and fire, and (6) weapon switch , it manifests three values: use weapon 0, use weapon 1,and no-weapon.。The aggregated dimensionality of the action space is quantified at 54 in total.Reward Engineering.。The primary objective of our training regimen is to equip the agent with the ability to playwith other players in Contra while concurrently optimizing its strength in eliminating opponents.。To achieve this ob-jective, we have formulated a diverse array of rewards designed to guide policy learning.。However, the complexityinvolved in designing and fine-tuning these rewards is evident.。To simplify the engineering, our design is character-ized allocating weights based on the expected value of each reward, ensuring a proportionate influence on the learningprocess.。In accordance with the principle, we assume a referenced maximum return of 20, with different rewards as-signed proportions based on their relative importance.。Specifically, for critical actions such as knocking down or killingan enemy, their values are set to approximately 4 (20% out of 20).。Conversely, for less critical actions like scoutingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationor incurring time penalties, their values are set to less than 1 (5% out of 20).。Detailed specifications are outlined inTable 5.。In accordance with the aforementioned principles, we can now construct the reward function r(s, a)by lin-early composing these factors, facilitating their collaborative influence on guiding policy learning.。As delineated inAlgorithm 3, these factors are broadly classified into three categories: basic rewards rb, obstacle avoidance rewardsroa, and goal achievement reward rg.。The basic rewards are primarily directed at steering the agent towards enhancedgame-playing performance, encompassing collaborative engagement with teammates and eliminating adversaries, etc.Feature Class Field Description Dimension1.Unit feature Scalar Includes heroes, teammates, enemies, monster 527Attribute Character ID,team ID,size,skills 28Status HP, oxygen,speed, peek type, alive state, body state,etc.。44Heroes Pose Position, rotation, camera position, camera rotation, etc.。76Item Backpack,weapon 144Attribute Character ID,team ID,size,skills 28Teammates Status HP, oxygen,speed, peek type, alive state, body state,etc.. \n",
      "the piece 271\n",
      " \n",
      "minating opponents.。To achieve this ob-jective, we have formulated a diverse array of rewards designed to guide policy learning.。However, the complexityinvolved in designing and fine-tuning these rewards is evident.。To simplify the engineering, our design is character-ized allocating weights based on the expected value of each reward, ensuring a proportionate influence on the learningprocess.。In accordance with the principle, we assume a referenced maximum return of 20, with different rewards as-signed proportions based on their relative importance.。Specifically, for critical actions such as knocking down or killingan enemy, their values are set to approximately 4 (20% out of 20).。Conversely, for less critical actions like scoutingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationor incurring time penalties, their values are set to less than 1 (5% out of 20).。Detailed specifications are outlined inTable 5.。In accordance with the aforementioned principles, we can now construct the reward function r(s, a)by lin-early composing these factors, facilitating their collaborative influence on guiding policy learning.。As delineated inAlgorithm 3, these factors are broadly classified into three categories: basic rewards rb, obstacle avoidance rewardsroa, and goal achievement reward rg.。The basic rewards are primarily directed at steering the agent towards enhancedgame-playing performance, encompassing collaborative engagement with teammates and eliminating adversaries, etc.Feature Class Field Description Dimension1.Unit feature Scalar Includes heroes, teammates, enemies, monster 527Attribute Character ID,team ID,size,skills 28Status HP, oxygen,speed, peek type, alive state, body state,etc.。44Heroes Pose Position, rotation, camera position, camera rotation, etc.。76Item Backpack,weapon 144Attribute Character ID,team ID,size,skills 28Teammates Status HP, oxygen,speed, peek type, alive state, body state,etc.。30Pose Position, rotation, camera position, camera rotation, etc.. \n",
      "the piece 272\n",
      " \n",
      "ed to guide policy learning.。However, the complexityinvolved in designing and fine-tuning these rewards is evident.。To simplify the engineering, our design is character-ized allocating weights based on the expected value of each reward, ensuring a proportionate influence on the learningprocess.。In accordance with the principle, we assume a referenced maximum return of 20, with different rewards as-signed proportions based on their relative importance.。Specifically, for critical actions such as knocking down or killingan enemy, their values are set to approximately 4 (20% out of 20).。Conversely, for less critical actions like scoutingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationor incurring time penalties, their values are set to less than 1 (5% out of 20).。Detailed specifications are outlined inTable 5.。In accordance with the aforementioned principles, we can now construct the reward function r(s, a)by lin-early composing these factors, facilitating their collaborative influence on guiding policy learning.。As delineated inAlgorithm 3, these factors are broadly classified into three categories: basic rewards rb, obstacle avoidance rewardsroa, and goal achievement reward rg.。The basic rewards are primarily directed at steering the agent towards enhancedgame-playing performance, encompassing collaborative engagement with teammates and eliminating adversaries, etc.Feature Class Field Description Dimension1.Unit feature Scalar Includes heroes, teammates, enemies, monster 527Attribute Character ID,team ID,size,skills 28Status HP, oxygen,speed, peek type, alive state, body state,etc.。44Heroes Pose Position, rotation, camera position, camera rotation, etc.。76Item Backpack,weapon 144Attribute Character ID,team ID,size,skills 28Teammates Status HP, oxygen,speed, peek type, alive state, body state,etc.。30Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Character ID,team ID,size,skills 28Enemies Status HP, oxygen,speed, peek type, alive state, body state,etc.. \n",
      "the piece 273\n",
      " \n",
      "s based on the expected value of each reward, ensuring a proportionate influence on the learningprocess.。In accordance with the principle, we assume a referenced maximum return of 20, with different rewards as-signed proportions based on their relative importance.。Specifically, for critical actions such as knocking down or killingan enemy, their values are set to approximately 4 (20% out of 20).。Conversely, for less critical actions like scoutingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationor incurring time penalties, their values are set to less than 1 (5% out of 20).。Detailed specifications are outlined inTable 5.。In accordance with the aforementioned principles, we can now construct the reward function r(s, a)by lin-early composing these factors, facilitating their collaborative influence on guiding policy learning.。As delineated inAlgorithm 3, these factors are broadly classified into three categories: basic rewards rb, obstacle avoidance rewardsroa, and goal achievement reward rg.。The basic rewards are primarily directed at steering the agent towards enhancedgame-playing performance, encompassing collaborative engagement with teammates and eliminating adversaries, etc.Feature Class Field Description Dimension1.Unit feature Scalar Includes heroes, teammates, enemies, monster 527Attribute Character ID,team ID,size,skills 28Status HP, oxygen,speed, peek type, alive state, body state,etc.。44Heroes Pose Position, rotation, camera position, camera rotation, etc.。76Item Backpack,weapon 144Attribute Character ID,team ID,size,skills 28Teammates Status HP, oxygen,speed, peek type, alive state, body state,etc.。30Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Character ID,team ID,size,skills 28Enemies Status HP, oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.. \n",
      "the piece 274\n",
      " \n",
      "cordance with the principle, we assume a referenced maximum return of 20, with different rewards as-signed proportions based on their relative importance.。Specifically, for critical actions such as knocking down or killingan enemy, their values are set to approximately 4 (20% out of 20).。Conversely, for less critical actions like scoutingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationor incurring time penalties, their values are set to less than 1 (5% out of 20).。Detailed specifications are outlined inTable 5.。In accordance with the aforementioned principles, we can now construct the reward function r(s, a)by lin-early composing these factors, facilitating their collaborative influence on guiding policy learning.。As delineated inAlgorithm 3, these factors are broadly classified into three categories: basic rewards rb, obstacle avoidance rewardsroa, and goal achievement reward rg.。The basic rewards are primarily directed at steering the agent towards enhancedgame-playing performance, encompassing collaborative engagement with teammates and eliminating adversaries, etc.Feature Class Field Description Dimension1.Unit feature Scalar Includes heroes, teammates, enemies, monster 527Attribute Character ID,team ID,size,skills 28Status HP, oxygen,speed, peek type, alive state, body state,etc.。44Heroes Pose Position, rotation, camera position, camera rotation, etc.。76Item Backpack,weapon 144Attribute Character ID,team ID,size,skills 28Teammates Status HP, oxygen,speed, peek type, alive state, body state,etc.。30Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Character ID,team ID,size,skills 28Enemies Status HP, oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Monster type,size 8Monsters Status HP, max HP, HP percent, target type 6Pose Position, rotation, relative position, distance 162.Global feature Scalar Includes circle, event, door and supply 65Status State,pain,radius 4Circle Position Blue circle, white circle 6Time existence time, rest time, total time, delay time, appear time 5Attribute Type,damage,elapsed time 8Event Position Occurred position 3Attribute Type,state 5Door Pose Position,relative position,rotation 8Status Type,size,quantity,etc.. \n",
      "the piece 275\n",
      " \n",
      " rewards as-signed proportions based on their relative importance.。Specifically, for critical actions such as knocking down or killingan enemy, their values are set to approximately 4 (20% out of 20).。Conversely, for less critical actions like scoutingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationor incurring time penalties, their values are set to less than 1 (5% out of 20).。Detailed specifications are outlined inTable 5.。In accordance with the aforementioned principles, we can now construct the reward function r(s, a)by lin-early composing these factors, facilitating their collaborative influence on guiding policy learning.。As delineated inAlgorithm 3, these factors are broadly classified into three categories: basic rewards rb, obstacle avoidance rewardsroa, and goal achievement reward rg.。The basic rewards are primarily directed at steering the agent towards enhancedgame-playing performance, encompassing collaborative engagement with teammates and eliminating adversaries, etc.Feature Class Field Description Dimension1.Unit feature Scalar Includes heroes, teammates, enemies, monster 527Attribute Character ID,team ID,size,skills 28Status HP, oxygen,speed, peek type, alive state, body state,etc.。44Heroes Pose Position, rotation, camera position, camera rotation, etc.。76Item Backpack,weapon 144Attribute Character ID,team ID,size,skills 28Teammates Status HP, oxygen,speed, peek type, alive state, body state,etc.。30Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Character ID,team ID,size,skills 28Enemies Status HP, oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Monster type,size 8Monsters Status HP, max HP, HP percent, target type 6Pose Position, rotation, relative position, distance 162.Global feature Scalar Includes circle, event, door and supply 65Status State,pain,radius 4Circle Position Blue circle, white circle 6Time existence time, rest time, total time, delay time, appear time 5Attribute Type,damage,elapsed time 8Event Position Occurred position 3Attribute Type,state 5Door Pose Position,relative position,rotation 8Status Type,size,quantity,etc.。19Supply Position Position,relative position,distance 73.Invisible enemy feature Scalar Invisible (nearby) enemy feature only for value estimation 104Attribute Character ID,team ID,size,skills 28Invisible enemies Status HP,oxygen,speed, peek type, alive state, body state,etc.. \n",
      "the piece 276\n",
      " \n",
      " 4 (20% out of 20).。Conversely, for less critical actions like scoutingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationor incurring time penalties, their values are set to less than 1 (5% out of 20).。Detailed specifications are outlined inTable 5.。In accordance with the aforementioned principles, we can now construct the reward function r(s, a)by lin-early composing these factors, facilitating their collaborative influence on guiding policy learning.。As delineated inAlgorithm 3, these factors are broadly classified into three categories: basic rewards rb, obstacle avoidance rewardsroa, and goal achievement reward rg.。The basic rewards are primarily directed at steering the agent towards enhancedgame-playing performance, encompassing collaborative engagement with teammates and eliminating adversaries, etc.Feature Class Field Description Dimension1.Unit feature Scalar Includes heroes, teammates, enemies, monster 527Attribute Character ID,team ID,size,skills 28Status HP, oxygen,speed, peek type, alive state, body state,etc.。44Heroes Pose Position, rotation, camera position, camera rotation, etc.。76Item Backpack,weapon 144Attribute Character ID,team ID,size,skills 28Teammates Status HP, oxygen,speed, peek type, alive state, body state,etc.。30Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Character ID,team ID,size,skills 28Enemies Status HP, oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Monster type,size 8Monsters Status HP, max HP, HP percent, target type 6Pose Position, rotation, relative position, distance 162.Global feature Scalar Includes circle, event, door and supply 65Status State,pain,radius 4Circle Position Blue circle, white circle 6Time existence time, rest time, total time, delay time, appear time 5Attribute Type,damage,elapsed time 8Event Position Occurred position 3Attribute Type,state 5Door Pose Position,relative position,rotation 8Status Type,size,quantity,etc.。19Supply Position Position,relative position,distance 73.Invisible enemy feature Scalar Invisible (nearby) enemy feature only for value estimation 104Attribute Character ID,team ID,size,skills 28Invisible enemies Status HP,oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.. \n",
      "the piece 277\n",
      " \n",
      "g time penalties, their values are set to less than 1 (5% out of 20).。Detailed specifications are outlined inTable 5.。In accordance with the aforementioned principles, we can now construct the reward function r(s, a)by lin-early composing these factors, facilitating their collaborative influence on guiding policy learning.。As delineated inAlgorithm 3, these factors are broadly classified into three categories: basic rewards rb, obstacle avoidance rewardsroa, and goal achievement reward rg.。The basic rewards are primarily directed at steering the agent towards enhancedgame-playing performance, encompassing collaborative engagement with teammates and eliminating adversaries, etc.Feature Class Field Description Dimension1.Unit feature Scalar Includes heroes, teammates, enemies, monster 527Attribute Character ID,team ID,size,skills 28Status HP, oxygen,speed, peek type, alive state, body state,etc.。44Heroes Pose Position, rotation, camera position, camera rotation, etc.。76Item Backpack,weapon 144Attribute Character ID,team ID,size,skills 28Teammates Status HP, oxygen,speed, peek type, alive state, body state,etc.。30Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Character ID,team ID,size,skills 28Enemies Status HP, oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Monster type,size 8Monsters Status HP, max HP, HP percent, target type 6Pose Position, rotation, relative position, distance 162.Global feature Scalar Includes circle, event, door and supply 65Status State,pain,radius 4Circle Position Blue circle, white circle 6Time existence time, rest time, total time, delay time, appear time 5Attribute Type,damage,elapsed time 8Event Position Occurred position 3Attribute Type,state 5Door Pose Position,relative position,rotation 8Status Type,size,quantity,etc.。19Supply Position Position,relative position,distance 73.Invisible enemy feature Scalar Invisible (nearby) enemy feature only for value estimation 104Attribute Character ID,team ID,size,skills 28Invisible enemies Status HP,oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。434.Spatial feature Scalar BEV 12288BEV Region Altitude map and aerial view map 3x64x64Table 4: The details of features in the observation space.In the case of roa, the objec-tive is to promote natural naviga-tion and forestall the agent fromencountering obstacles, such asstones and trees.. \n",
      "the piece 278\n",
      " \n",
      "h the aforementioned principles, we can now construct the reward function r(s, a)by lin-early composing these factors, facilitating their collaborative influence on guiding policy learning.。As delineated inAlgorithm 3, these factors are broadly classified into three categories: basic rewards rb, obstacle avoidance rewardsroa, and goal achievement reward rg.。The basic rewards are primarily directed at steering the agent towards enhancedgame-playing performance, encompassing collaborative engagement with teammates and eliminating adversaries, etc.Feature Class Field Description Dimension1.Unit feature Scalar Includes heroes, teammates, enemies, monster 527Attribute Character ID,team ID,size,skills 28Status HP, oxygen,speed, peek type, alive state, body state,etc.。44Heroes Pose Position, rotation, camera position, camera rotation, etc.。76Item Backpack,weapon 144Attribute Character ID,team ID,size,skills 28Teammates Status HP, oxygen,speed, peek type, alive state, body state,etc.。30Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Character ID,team ID,size,skills 28Enemies Status HP, oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Monster type,size 8Monsters Status HP, max HP, HP percent, target type 6Pose Position, rotation, relative position, distance 162.Global feature Scalar Includes circle, event, door and supply 65Status State,pain,radius 4Circle Position Blue circle, white circle 6Time existence time, rest time, total time, delay time, appear time 5Attribute Type,damage,elapsed time 8Event Position Occurred position 3Attribute Type,state 5Door Pose Position,relative position,rotation 8Status Type,size,quantity,etc.。19Supply Position Position,relative position,distance 73.Invisible enemy feature Scalar Invisible (nearby) enemy feature only for value estimation 104Attribute Character ID,team ID,size,skills 28Invisible enemies Status HP,oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。434.Spatial feature Scalar BEV 12288BEV Region Altitude map and aerial view map 3x64x64Table 4: The details of features in the observation space.In the case of roa, the objec-tive is to promote natural naviga-tion and forestall the agent fromencountering obstacles, such asstones and trees.。Regarding theimplementation, penalties areimposed on the agent for devia-tions from the optimal path.. \n",
      "the piece 279\n",
      " \n",
      "thm 3, these factors are broadly classified into three categories: basic rewards rb, obstacle avoidance rewardsroa, and goal achievement reward rg.。The basic rewards are primarily directed at steering the agent towards enhancedgame-playing performance, encompassing collaborative engagement with teammates and eliminating adversaries, etc.Feature Class Field Description Dimension1.Unit feature Scalar Includes heroes, teammates, enemies, monster 527Attribute Character ID,team ID,size,skills 28Status HP, oxygen,speed, peek type, alive state, body state,etc.。44Heroes Pose Position, rotation, camera position, camera rotation, etc.。76Item Backpack,weapon 144Attribute Character ID,team ID,size,skills 28Teammates Status HP, oxygen,speed, peek type, alive state, body state,etc.。30Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Character ID,team ID,size,skills 28Enemies Status HP, oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Monster type,size 8Monsters Status HP, max HP, HP percent, target type 6Pose Position, rotation, relative position, distance 162.Global feature Scalar Includes circle, event, door and supply 65Status State,pain,radius 4Circle Position Blue circle, white circle 6Time existence time, rest time, total time, delay time, appear time 5Attribute Type,damage,elapsed time 8Event Position Occurred position 3Attribute Type,state 5Door Pose Position,relative position,rotation 8Status Type,size,quantity,etc.。19Supply Position Position,relative position,distance 73.Invisible enemy feature Scalar Invisible (nearby) enemy feature only for value estimation 104Attribute Character ID,team ID,size,skills 28Invisible enemies Status HP,oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。434.Spatial feature Scalar BEV 12288BEV Region Altitude map and aerial view map 3x64x64Table 4: The details of features in the observation space.In the case of roa, the objec-tive is to promote natural naviga-tion and forestall the agent fromencountering obstacles, such asstones and trees.。Regarding theimplementation, penalties areimposed on the agent for devia-tions from the optimal path.。Thisoptimal trajectory is determinedby assessing the cosine similar-ity between the agent’s currentmovement direction, a 2D unitvector, provided as an environ-mental signal, and the expectedobstacle-free trajectory derivedfrom the action sequence in thetrajectory:roat=denvt·d⋆t∥denvt∥2∗ ∥d⋆t∥2−1,(8)where denvt the actual move-ment direction of the agent thatcan be directly accessed fromthe environment feedback, d⋆tthe expected movement direc-tion, which is derived by com-bining the expected movementdirection from the previous mo-ment with the movement actiontaken at the current moment.. \n",
      "the piece 280\n",
      " \n",
      "three categories: basic rewards rb, obstacle avoidance rewardsroa, and goal achievement reward rg.。The basic rewards are primarily directed at steering the agent towards enhancedgame-playing performance, encompassing collaborative engagement with teammates and eliminating adversaries, etc.Feature Class Field Description Dimension1.Unit feature Scalar Includes heroes, teammates, enemies, monster 527Attribute Character ID,team ID,size,skills 28Status HP, oxygen,speed, peek type, alive state, body state,etc.。44Heroes Pose Position, rotation, camera position, camera rotation, etc.。76Item Backpack,weapon 144Attribute Character ID,team ID,size,skills 28Teammates Status HP, oxygen,speed, peek type, alive state, body state,etc.。30Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Character ID,team ID,size,skills 28Enemies Status HP, oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Monster type,size 8Monsters Status HP, max HP, HP percent, target type 6Pose Position, rotation, relative position, distance 162.Global feature Scalar Includes circle, event, door and supply 65Status State,pain,radius 4Circle Position Blue circle, white circle 6Time existence time, rest time, total time, delay time, appear time 5Attribute Type,damage,elapsed time 8Event Position Occurred position 3Attribute Type,state 5Door Pose Position,relative position,rotation 8Status Type,size,quantity,etc.。19Supply Position Position,relative position,distance 73.Invisible enemy feature Scalar Invisible (nearby) enemy feature only for value estimation 104Attribute Character ID,team ID,size,skills 28Invisible enemies Status HP,oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。434.Spatial feature Scalar BEV 12288BEV Region Altitude map and aerial view map 3x64x64Table 4: The details of features in the observation space.In the case of roa, the objec-tive is to promote natural naviga-tion and forestall the agent fromencountering obstacles, such asstones and trees.。Regarding theimplementation, penalties areimposed on the agent for devia-tions from the optimal path.。Thisoptimal trajectory is determinedby assessing the cosine similar-ity between the agent’s currentmovement direction, a 2D unitvector, provided as an environ-mental signal, and the expectedobstacle-free trajectory derivedfrom the action sequence in thetrajectory:roat=denvt·d⋆t∥denvt∥2∗ ∥d⋆t∥2−1,(8)where denvt the actual move-ment direction of the agent thatcan be directly accessed fromthe environment feedback, d⋆tthe expected movement direc-tion, which is derived by com-bining the expected movementdirection from the previous mo-ment with the movement actiontaken at the current moment.。Forthe convenience, we summarizethe corresponding pseudo-code in Algorithm 2.. \n",
      "the piece 281\n",
      " \n",
      "mpassing collaborative engagement with teammates and eliminating adversaries, etc.Feature Class Field Description Dimension1.Unit feature Scalar Includes heroes, teammates, enemies, monster 527Attribute Character ID,team ID,size,skills 28Status HP, oxygen,speed, peek type, alive state, body state,etc.。44Heroes Pose Position, rotation, camera position, camera rotation, etc.。76Item Backpack,weapon 144Attribute Character ID,team ID,size,skills 28Teammates Status HP, oxygen,speed, peek type, alive state, body state,etc.。30Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Character ID,team ID,size,skills 28Enemies Status HP, oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Monster type,size 8Monsters Status HP, max HP, HP percent, target type 6Pose Position, rotation, relative position, distance 162.Global feature Scalar Includes circle, event, door and supply 65Status State,pain,radius 4Circle Position Blue circle, white circle 6Time existence time, rest time, total time, delay time, appear time 5Attribute Type,damage,elapsed time 8Event Position Occurred position 3Attribute Type,state 5Door Pose Position,relative position,rotation 8Status Type,size,quantity,etc.。19Supply Position Position,relative position,distance 73.Invisible enemy feature Scalar Invisible (nearby) enemy feature only for value estimation 104Attribute Character ID,team ID,size,skills 28Invisible enemies Status HP,oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。434.Spatial feature Scalar BEV 12288BEV Region Altitude map and aerial view map 3x64x64Table 4: The details of features in the observation space.In the case of roa, the objec-tive is to promote natural naviga-tion and forestall the agent fromencountering obstacles, such asstones and trees.。Regarding theimplementation, penalties areimposed on the agent for devia-tions from the optimal path.。Thisoptimal trajectory is determinedby assessing the cosine similar-ity between the agent’s currentmovement direction, a 2D unitvector, provided as an environ-mental signal, and the expectedobstacle-free trajectory derivedfrom the action sequence in thetrajectory:roat=denvt·d⋆t∥denvt∥2∗ ∥d⋆t∥2−1,(8)where denvt the actual move-ment direction of the agent thatcan be directly accessed fromthe environment feedback, d⋆tthe expected movement direc-tion, which is derived by com-bining the expected movementdirection from the previous mo-ment with the movement actiontaken at the current moment.。Forthe convenience, we summarizethe corresponding pseudo-code in Algorithm 2.。For the line 5 in Algorithm 2, we use ∆tto denote the shifting from agentmovement direction action atto the camera vision degree ω.. \n",
      "the piece 282\n",
      " \n",
      ", enemies, monster 527Attribute Character ID,team ID,size,skills 28Status HP, oxygen,speed, peek type, alive state, body state,etc.。44Heroes Pose Position, rotation, camera position, camera rotation, etc.。76Item Backpack,weapon 144Attribute Character ID,team ID,size,skills 28Teammates Status HP, oxygen,speed, peek type, alive state, body state,etc.。30Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Character ID,team ID,size,skills 28Enemies Status HP, oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Monster type,size 8Monsters Status HP, max HP, HP percent, target type 6Pose Position, rotation, relative position, distance 162.Global feature Scalar Includes circle, event, door and supply 65Status State,pain,radius 4Circle Position Blue circle, white circle 6Time existence time, rest time, total time, delay time, appear time 5Attribute Type,damage,elapsed time 8Event Position Occurred position 3Attribute Type,state 5Door Pose Position,relative position,rotation 8Status Type,size,quantity,etc.。19Supply Position Position,relative position,distance 73.Invisible enemy feature Scalar Invisible (nearby) enemy feature only for value estimation 104Attribute Character ID,team ID,size,skills 28Invisible enemies Status HP,oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。434.Spatial feature Scalar BEV 12288BEV Region Altitude map and aerial view map 3x64x64Table 4: The details of features in the observation space.In the case of roa, the objec-tive is to promote natural naviga-tion and forestall the agent fromencountering obstacles, such asstones and trees.。Regarding theimplementation, penalties areimposed on the agent for devia-tions from the optimal path.。Thisoptimal trajectory is determinedby assessing the cosine similar-ity between the agent’s currentmovement direction, a 2D unitvector, provided as an environ-mental signal, and the expectedobstacle-free trajectory derivedfrom the action sequence in thetrajectory:roat=denvt·d⋆t∥denvt∥2∗ ∥d⋆t∥2−1,(8)where denvt the actual move-ment direction of the agent thatcan be directly accessed fromthe environment feedback, d⋆tthe expected movement direc-tion, which is derived by com-bining the expected movementdirection from the previous mo-ment with the movement actiontaken at the current moment.。Forthe convenience, we summarizethe corresponding pseudo-code in Algorithm 2.。For the line 5 in Algorithm 2, we use ∆tto denote the shifting from agentmovement direction action atto the camera vision degree ω.。To address the issue of the agent getting stuck on obstacles dueto short-term action sequences, we employ a smaller γfor the corresponding value head.. \n",
      "the piece 283\n",
      " \n",
      "tc.。43Attribute Character ID,team ID,size,skills 28Enemies Status HP, oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Monster type,size 8Monsters Status HP, max HP, HP percent, target type 6Pose Position, rotation, relative position, distance 162.Global feature Scalar Includes circle, event, door and supply 65Status State,pain,radius 4Circle Position Blue circle, white circle 6Time existence time, rest time, total time, delay time, appear time 5Attribute Type,damage,elapsed time 8Event Position Occurred position 3Attribute Type,state 5Door Pose Position,relative position,rotation 8Status Type,size,quantity,etc.。19Supply Position Position,relative position,distance 73.Invisible enemy feature Scalar Invisible (nearby) enemy feature only for value estimation 104Attribute Character ID,team ID,size,skills 28Invisible enemies Status HP,oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。434.Spatial feature Scalar BEV 12288BEV Region Altitude map and aerial view map 3x64x64Table 4: The details of features in the observation space.In the case of roa, the objec-tive is to promote natural naviga-tion and forestall the agent fromencountering obstacles, such asstones and trees.。Regarding theimplementation, penalties areimposed on the agent for devia-tions from the optimal path.。Thisoptimal trajectory is determinedby assessing the cosine similar-ity between the agent’s currentmovement direction, a 2D unitvector, provided as an environ-mental signal, and the expectedobstacle-free trajectory derivedfrom the action sequence in thetrajectory:roat=denvt·d⋆t∥denvt∥2∗ ∥d⋆t∥2−1,(8)where denvt the actual move-ment direction of the agent thatcan be directly accessed fromthe environment feedback, d⋆tthe expected movement direc-tion, which is derived by com-bining the expected movementdirection from the previous mo-ment with the movement actiontaken at the current moment.。Forthe convenience, we summarizethe corresponding pseudo-code in Algorithm 2.。For the line 5 in Algorithm 2, we use ∆tto denote the shifting from agentmovement direction action atto the camera vision degree ω.。To address the issue of the agent getting stuck on obstacles dueto short-term action sequences, we employ a smaller γfor the corresponding value head.。Specifically, we set γto 0.92.. \n",
      "the piece 284\n",
      " \n",
      "en,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Monster type,size 8Monsters Status HP, max HP, HP percent, target type 6Pose Position, rotation, relative position, distance 162.Global feature Scalar Includes circle, event, door and supply 65Status State,pain,radius 4Circle Position Blue circle, white circle 6Time existence time, rest time, total time, delay time, appear time 5Attribute Type,damage,elapsed time 8Event Position Occurred position 3Attribute Type,state 5Door Pose Position,relative position,rotation 8Status Type,size,quantity,etc.。19Supply Position Position,relative position,distance 73.Invisible enemy feature Scalar Invisible (nearby) enemy feature only for value estimation 104Attribute Character ID,team ID,size,skills 28Invisible enemies Status HP,oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。434.Spatial feature Scalar BEV 12288BEV Region Altitude map and aerial view map 3x64x64Table 4: The details of features in the observation space.In the case of roa, the objec-tive is to promote natural naviga-tion and forestall the agent fromencountering obstacles, such asstones and trees.。Regarding theimplementation, penalties areimposed on the agent for devia-tions from the optimal path.。Thisoptimal trajectory is determinedby assessing the cosine similar-ity between the agent’s currentmovement direction, a 2D unitvector, provided as an environ-mental signal, and the expectedobstacle-free trajectory derivedfrom the action sequence in thetrajectory:roat=denvt·d⋆t∥denvt∥2∗ ∥d⋆t∥2−1,(8)where denvt the actual move-ment direction of the agent thatcan be directly accessed fromthe environment feedback, d⋆tthe expected movement direc-tion, which is derived by com-bining the expected movementdirection from the previous mo-ment with the movement actiontaken at the current moment.。Forthe convenience, we summarizethe corresponding pseudo-code in Algorithm 2.。For the line 5 in Algorithm 2, we use ∆tto denote the shifting from agentmovement direction action atto the camera vision degree ω.。To address the issue of the agent getting stuck on obstacles dueto short-term action sequences, we employ a smaller γfor the corresponding value head.。Specifically, we set γto 0.92.。Thisadjustment helps mitigate the impact of the obstacle avoidance reward on long-term credit assignment, allowing for a morebalanced consideration of immediate and future rewards in the agent’s decision-making process.. \n",
      "the piece 285\n",
      " \n",
      "Monsters Status HP, max HP, HP percent, target type 6Pose Position, rotation, relative position, distance 162.Global feature Scalar Includes circle, event, door and supply 65Status State,pain,radius 4Circle Position Blue circle, white circle 6Time existence time, rest time, total time, delay time, appear time 5Attribute Type,damage,elapsed time 8Event Position Occurred position 3Attribute Type,state 5Door Pose Position,relative position,rotation 8Status Type,size,quantity,etc.。19Supply Position Position,relative position,distance 73.Invisible enemy feature Scalar Invisible (nearby) enemy feature only for value estimation 104Attribute Character ID,team ID,size,skills 28Invisible enemies Status HP,oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。434.Spatial feature Scalar BEV 12288BEV Region Altitude map and aerial view map 3x64x64Table 4: The details of features in the observation space.In the case of roa, the objec-tive is to promote natural naviga-tion and forestall the agent fromencountering obstacles, such asstones and trees.。Regarding theimplementation, penalties areimposed on the agent for devia-tions from the optimal path.。Thisoptimal trajectory is determinedby assessing the cosine similar-ity between the agent’s currentmovement direction, a 2D unitvector, provided as an environ-mental signal, and the expectedobstacle-free trajectory derivedfrom the action sequence in thetrajectory:roat=denvt·d⋆t∥denvt∥2∗ ∥d⋆t∥2−1,(8)where denvt the actual move-ment direction of the agent thatcan be directly accessed fromthe environment feedback, d⋆tthe expected movement direc-tion, which is derived by com-bining the expected movementdirection from the previous mo-ment with the movement actiontaken at the current moment.。Forthe convenience, we summarizethe corresponding pseudo-code in Algorithm 2.。For the line 5 in Algorithm 2, we use ∆tto denote the shifting from agentmovement direction action atto the camera vision degree ω.。To address the issue of the agent getting stuck on obstacles dueto short-term action sequences, we employ a smaller γfor the corresponding value head.。Specifically, we set γto 0.92.。Thisadjustment helps mitigate the impact of the obstacle avoidance reward on long-term credit assignment, allowing for a morebalanced consideration of immediate and future rewards in the agent’s decision-making process.。As for the goal-achievingreward, we’ve introduced in the main text, please refer to Section 4.2.B.. \n",
      "the piece 286\n",
      " \n",
      " rotation, relative position, distance 162.Global feature Scalar Includes circle, event, door and supply 65Status State,pain,radius 4Circle Position Blue circle, white circle 6Time existence time, rest time, total time, delay time, appear time 5Attribute Type,damage,elapsed time 8Event Position Occurred position 3Attribute Type,state 5Door Pose Position,relative position,rotation 8Status Type,size,quantity,etc.。19Supply Position Position,relative position,distance 73.Invisible enemy feature Scalar Invisible (nearby) enemy feature only for value estimation 104Attribute Character ID,team ID,size,skills 28Invisible enemies Status HP,oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。434.Spatial feature Scalar BEV 12288BEV Region Altitude map and aerial view map 3x64x64Table 4: The details of features in the observation space.In the case of roa, the objec-tive is to promote natural naviga-tion and forestall the agent fromencountering obstacles, such asstones and trees.。Regarding theimplementation, penalties areimposed on the agent for devia-tions from the optimal path.。Thisoptimal trajectory is determinedby assessing the cosine similar-ity between the agent’s currentmovement direction, a 2D unitvector, provided as an environ-mental signal, and the expectedobstacle-free trajectory derivedfrom the action sequence in thetrajectory:roat=denvt·d⋆t∥denvt∥2∗ ∥d⋆t∥2−1,(8)where denvt the actual move-ment direction of the agent thatcan be directly accessed fromthe environment feedback, d⋆tthe expected movement direc-tion, which is derived by com-bining the expected movementdirection from the previous mo-ment with the movement actiontaken at the current moment.。Forthe convenience, we summarizethe corresponding pseudo-code in Algorithm 2.。For the line 5 in Algorithm 2, we use ∆tto denote the shifting from agentmovement direction action atto the camera vision degree ω.。To address the issue of the agent getting stuck on obstacles dueto short-term action sequences, we employ a smaller γfor the corresponding value head.。Specifically, we set γto 0.92.。Thisadjustment helps mitigate the impact of the obstacle avoidance reward on long-term credit assignment, allowing for a morebalanced consideration of immediate and future rewards in the agent’s decision-making process.。As for the goal-achievingreward, we’ve introduced in the main text, please refer to Section 4.2.B.。Environmental State Abstraction and Goal SpaceFor a comprehensive understanding of the game environment, a language model undergoes a fine-tuning process dueto the scarcity of textual information within the simulation environment.. \n",
      "the piece 287\n",
      " \n",
      "ain,radius 4Circle Position Blue circle, white circle 6Time existence time, rest time, total time, delay time, appear time 5Attribute Type,damage,elapsed time 8Event Position Occurred position 3Attribute Type,state 5Door Pose Position,relative position,rotation 8Status Type,size,quantity,etc.。19Supply Position Position,relative position,distance 73.Invisible enemy feature Scalar Invisible (nearby) enemy feature only for value estimation 104Attribute Character ID,team ID,size,skills 28Invisible enemies Status HP,oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。434.Spatial feature Scalar BEV 12288BEV Region Altitude map and aerial view map 3x64x64Table 4: The details of features in the observation space.In the case of roa, the objec-tive is to promote natural naviga-tion and forestall the agent fromencountering obstacles, such asstones and trees.。Regarding theimplementation, penalties areimposed on the agent for devia-tions from the optimal path.。Thisoptimal trajectory is determinedby assessing the cosine similar-ity between the agent’s currentmovement direction, a 2D unitvector, provided as an environ-mental signal, and the expectedobstacle-free trajectory derivedfrom the action sequence in thetrajectory:roat=denvt·d⋆t∥denvt∥2∗ ∥d⋆t∥2−1,(8)where denvt the actual move-ment direction of the agent thatcan be directly accessed fromthe environment feedback, d⋆tthe expected movement direc-tion, which is derived by com-bining the expected movementdirection from the previous mo-ment with the movement actiontaken at the current moment.。Forthe convenience, we summarizethe corresponding pseudo-code in Algorithm 2.。For the line 5 in Algorithm 2, we use ∆tto denote the shifting from agentmovement direction action atto the camera vision degree ω.。To address the issue of the agent getting stuck on obstacles dueto short-term action sequences, we employ a smaller γfor the corresponding value head.。Specifically, we set γto 0.92.。Thisadjustment helps mitigate the impact of the obstacle avoidance reward on long-term credit assignment, allowing for a morebalanced consideration of immediate and future rewards in the agent’s decision-making process.。As for the goal-achievingreward, we’ve introduced in the main text, please refer to Section 4.2.B.。Environmental State Abstraction and Goal SpaceFor a comprehensive understanding of the game environment, a language model undergoes a fine-tuning process dueto the scarcity of textual information within the simulation environment.。The need arises to articulate non-linguisticelements, and the interaction between an agent and the simulation environment is centered on continuously observ-ing the environment’s state and generating corresponding actions.. \n",
      "the piece 288\n",
      " \n",
      "ce time, rest time, total time, delay time, appear time 5Attribute Type,damage,elapsed time 8Event Position Occurred position 3Attribute Type,state 5Door Pose Position,relative position,rotation 8Status Type,size,quantity,etc.。19Supply Position Position,relative position,distance 73.Invisible enemy feature Scalar Invisible (nearby) enemy feature only for value estimation 104Attribute Character ID,team ID,size,skills 28Invisible enemies Status HP,oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。434.Spatial feature Scalar BEV 12288BEV Region Altitude map and aerial view map 3x64x64Table 4: The details of features in the observation space.In the case of roa, the objec-tive is to promote natural naviga-tion and forestall the agent fromencountering obstacles, such asstones and trees.。Regarding theimplementation, penalties areimposed on the agent for devia-tions from the optimal path.。Thisoptimal trajectory is determinedby assessing the cosine similar-ity between the agent’s currentmovement direction, a 2D unitvector, provided as an environ-mental signal, and the expectedobstacle-free trajectory derivedfrom the action sequence in thetrajectory:roat=denvt·d⋆t∥denvt∥2∗ ∥d⋆t∥2−1,(8)where denvt the actual move-ment direction of the agent thatcan be directly accessed fromthe environment feedback, d⋆tthe expected movement direc-tion, which is derived by com-bining the expected movementdirection from the previous mo-ment with the movement actiontaken at the current moment.。Forthe convenience, we summarizethe corresponding pseudo-code in Algorithm 2.。For the line 5 in Algorithm 2, we use ∆tto denote the shifting from agentmovement direction action atto the camera vision degree ω.。To address the issue of the agent getting stuck on obstacles dueto short-term action sequences, we employ a smaller γfor the corresponding value head.。Specifically, we set γto 0.92.。Thisadjustment helps mitigate the impact of the obstacle avoidance reward on long-term credit assignment, allowing for a morebalanced consideration of immediate and future rewards in the agent’s decision-making process.。As for the goal-achievingreward, we’ve introduced in the main text, please refer to Section 4.2.B.。Environmental State Abstraction and Goal SpaceFor a comprehensive understanding of the game environment, a language model undergoes a fine-tuning process dueto the scarcity of textual information within the simulation environment.。The need arises to articulate non-linguisticelements, and the interaction between an agent and the simulation environment is centered on continuously observ-ing the environment’s state and generating corresponding actions.。Therefore, the key aspects requiring verbalizationprimarily involve the state and actions of the agent.. \n",
      "the piece 289\n",
      " \n",
      "ition, rotation, camera position, camera rotation, etc.。434.Spatial feature Scalar BEV 12288BEV Region Altitude map and aerial view map 3x64x64Table 4: The details of features in the observation space.In the case of roa, the objec-tive is to promote natural naviga-tion and forestall the agent fromencountering obstacles, such asstones and trees.。Regarding theimplementation, penalties areimposed on the agent for devia-tions from the optimal path.。Thisoptimal trajectory is determinedby assessing the cosine similar-ity between the agent’s currentmovement direction, a 2D unitvector, provided as an environ-mental signal, and the expectedobstacle-free trajectory derivedfrom the action sequence in thetrajectory:roat=denvt·d⋆t∥denvt∥2∗ ∥d⋆t∥2−1,(8)where denvt the actual move-ment direction of the agent thatcan be directly accessed fromthe environment feedback, d⋆tthe expected movement direc-tion, which is derived by com-bining the expected movementdirection from the previous mo-ment with the movement actiontaken at the current moment.。Forthe convenience, we summarizethe corresponding pseudo-code in Algorithm 2.。For the line 5 in Algorithm 2, we use ∆tto denote the shifting from agentmovement direction action atto the camera vision degree ω.。To address the issue of the agent getting stuck on obstacles dueto short-term action sequences, we employ a smaller γfor the corresponding value head.。Specifically, we set γto 0.92.。Thisadjustment helps mitigate the impact of the obstacle avoidance reward on long-term credit assignment, allowing for a morebalanced consideration of immediate and future rewards in the agent’s decision-making process.。As for the goal-achievingreward, we’ve introduced in the main text, please refer to Section 4.2.B.。Environmental State Abstraction and Goal SpaceFor a comprehensive understanding of the game environment, a language model undergoes a fine-tuning process dueto the scarcity of textual information within the simulation environment.。The need arises to articulate non-linguisticelements, and the interaction between an agent and the simulation environment is centered on continuously observ-ing the environment’s state and generating corresponding actions.。Therefore, the key aspects requiring verbalizationprimarily involve the state and actions of the agent.。However, given the abundance of possible observation states inthe simulation environment, it is impractical to use all of these states directly as prompts for the language model, es-pecially considering token limitations.. \n",
      "the piece 290\n",
      " \n",
      "stall the agent fromencountering obstacles, such asstones and trees.。Regarding theimplementation, penalties areimposed on the agent for devia-tions from the optimal path.。Thisoptimal trajectory is determinedby assessing the cosine similar-ity between the agent’s currentmovement direction, a 2D unitvector, provided as an environ-mental signal, and the expectedobstacle-free trajectory derivedfrom the action sequence in thetrajectory:roat=denvt·d⋆t∥denvt∥2∗ ∥d⋆t∥2−1,(8)where denvt the actual move-ment direction of the agent thatcan be directly accessed fromthe environment feedback, d⋆tthe expected movement direc-tion, which is derived by com-bining the expected movementdirection from the previous mo-ment with the movement actiontaken at the current moment.。Forthe convenience, we summarizethe corresponding pseudo-code in Algorithm 2.。For the line 5 in Algorithm 2, we use ∆tto denote the shifting from agentmovement direction action atto the camera vision degree ω.。To address the issue of the agent getting stuck on obstacles dueto short-term action sequences, we employ a smaller γfor the corresponding value head.。Specifically, we set γto 0.92.。Thisadjustment helps mitigate the impact of the obstacle avoidance reward on long-term credit assignment, allowing for a morebalanced consideration of immediate and future rewards in the agent’s decision-making process.。As for the goal-achievingreward, we’ve introduced in the main text, please refer to Section 4.2.B.。Environmental State Abstraction and Goal SpaceFor a comprehensive understanding of the game environment, a language model undergoes a fine-tuning process dueto the scarcity of textual information within the simulation environment.。The need arises to articulate non-linguisticelements, and the interaction between an agent and the simulation environment is centered on continuously observ-ing the environment’s state and generating corresponding actions.。Therefore, the key aspects requiring verbalizationprimarily involve the state and actions of the agent.。However, given the abundance of possible observation states inthe simulation environment, it is impractical to use all of these states directly as prompts for the language model, es-pecially considering token limitations.。Consequently, there is a crucial need to extract and linguistically transformthe most significant meta-states to facilitate successful model interaction.. \n",
      "the piece 291\n",
      " \n",
      ".。Regarding theimplementation, penalties areimposed on the agent for devia-tions from the optimal path.。Thisoptimal trajectory is determinedby assessing the cosine similar-ity between the agent’s currentmovement direction, a 2D unitvector, provided as an environ-mental signal, and the expectedobstacle-free trajectory derivedfrom the action sequence in thetrajectory:roat=denvt·d⋆t∥denvt∥2∗ ∥d⋆t∥2−1,(8)where denvt the actual move-ment direction of the agent thatcan be directly accessed fromthe environment feedback, d⋆tthe expected movement direc-tion, which is derived by com-bining the expected movementdirection from the previous mo-ment with the movement actiontaken at the current moment.。Forthe convenience, we summarizethe corresponding pseudo-code in Algorithm 2.。For the line 5 in Algorithm 2, we use ∆tto denote the shifting from agentmovement direction action atto the camera vision degree ω.。To address the issue of the agent getting stuck on obstacles dueto short-term action sequences, we employ a smaller γfor the corresponding value head.。Specifically, we set γto 0.92.。Thisadjustment helps mitigate the impact of the obstacle avoidance reward on long-term credit assignment, allowing for a morebalanced consideration of immediate and future rewards in the agent’s decision-making process.。As for the goal-achievingreward, we’ve introduced in the main text, please refer to Section 4.2.B.。Environmental State Abstraction and Goal SpaceFor a comprehensive understanding of the game environment, a language model undergoes a fine-tuning process dueto the scarcity of textual information within the simulation environment.。The need arises to articulate non-linguisticelements, and the interaction between an agent and the simulation environment is centered on continuously observ-ing the environment’s state and generating corresponding actions.。Therefore, the key aspects requiring verbalizationprimarily involve the state and actions of the agent.。However, given the abundance of possible observation states inthe simulation environment, it is impractical to use all of these states directly as prompts for the language model, es-pecially considering token limitations.。Consequently, there is a crucial need to extract and linguistically transformthe most significant meta-states to facilitate successful model interaction.。It is noteworthy that smaller languageBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 2 CALCULATION OF d⋆tInput: d⋆t−1,atOutput: d⋆t1:ifat∈movement direction action space then2:d⋆t←\u0014cos(at)sin(at)\u00153:else if at∈yaw direction action space then4:5: ∆t=at−ω6:d⋆t←\u0014cos(∆ t)−sin(∆ t)sin(∆ t) cos(∆ t)\u0015d⋆t−17:else if at∈{stop; rescue; stop and fire; stop adjust and fire } then8:d⋆t←\u00140\u00159:else10: d⋆t←d⋆t−111:end ifmodels have limitations in comprehending and manipulating numerical values effectively.. \n",
      "the piece 292\n",
      " \n",
      "edobstacle-free trajectory derivedfrom the action sequence in thetrajectory:roat=denvt·d⋆t∥denvt∥2∗ ∥d⋆t∥2−1,(8)where denvt the actual move-ment direction of the agent thatcan be directly accessed fromthe environment feedback, d⋆tthe expected movement direc-tion, which is derived by com-bining the expected movementdirection from the previous mo-ment with the movement actiontaken at the current moment.。Forthe convenience, we summarizethe corresponding pseudo-code in Algorithm 2.。For the line 5 in Algorithm 2, we use ∆tto denote the shifting from agentmovement direction action atto the camera vision degree ω.。To address the issue of the agent getting stuck on obstacles dueto short-term action sequences, we employ a smaller γfor the corresponding value head.。Specifically, we set γto 0.92.。Thisadjustment helps mitigate the impact of the obstacle avoidance reward on long-term credit assignment, allowing for a morebalanced consideration of immediate and future rewards in the agent’s decision-making process.。As for the goal-achievingreward, we’ve introduced in the main text, please refer to Section 4.2.B.。Environmental State Abstraction and Goal SpaceFor a comprehensive understanding of the game environment, a language model undergoes a fine-tuning process dueto the scarcity of textual information within the simulation environment.。The need arises to articulate non-linguisticelements, and the interaction between an agent and the simulation environment is centered on continuously observ-ing the environment’s state and generating corresponding actions.。Therefore, the key aspects requiring verbalizationprimarily involve the state and actions of the agent.。However, given the abundance of possible observation states inthe simulation environment, it is impractical to use all of these states directly as prompts for the language model, es-pecially considering token limitations.。Consequently, there is a crucial need to extract and linguistically transformthe most significant meta-states to facilitate successful model interaction.。It is noteworthy that smaller languageBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 2 CALCULATION OF d⋆tInput: d⋆t−1,atOutput: d⋆t1:ifat∈movement direction action space then2:d⋆t←\u0014cos(at)sin(at)\u00153:else if at∈yaw direction action space then4:5: ∆t=at−ω6:d⋆t←\u0014cos(∆ t)−sin(∆ t)sin(∆ t) cos(∆ t)\u0015d⋆t−17:else if at∈{stop; rescue; stop and fire; stop adjust and fire } then8:d⋆t←\u00140\u00159:else10: d⋆t←d⋆t−111:end ifmodels have limitations in comprehending and manipulating numerical values effectively.。To address this challenge,a deliberate effort is made to minimize the use of numerical values during the environmental linguistics process.Sub Action Space Dim Sizemovement direction 16yaw direction 16pitch direction 3body action 9basic action 7switch weapon action 3Table 6: Action space.For example, instead of specifying an agent’s speed with specific numeric metricslike “speed: 1m/s →3m/s” a qualitative representation such as “speed: slower →faster” is adopted.. \n",
      "the piece 293\n",
      " \n",
      "t∥2−1,(8)where denvt the actual move-ment direction of the agent thatcan be directly accessed fromthe environment feedback, d⋆tthe expected movement direc-tion, which is derived by com-bining the expected movementdirection from the previous mo-ment with the movement actiontaken at the current moment.。Forthe convenience, we summarizethe corresponding pseudo-code in Algorithm 2.。For the line 5 in Algorithm 2, we use ∆tto denote the shifting from agentmovement direction action atto the camera vision degree ω.。To address the issue of the agent getting stuck on obstacles dueto short-term action sequences, we employ a smaller γfor the corresponding value head.。Specifically, we set γto 0.92.。Thisadjustment helps mitigate the impact of the obstacle avoidance reward on long-term credit assignment, allowing for a morebalanced consideration of immediate and future rewards in the agent’s decision-making process.。As for the goal-achievingreward, we’ve introduced in the main text, please refer to Section 4.2.B.。Environmental State Abstraction and Goal SpaceFor a comprehensive understanding of the game environment, a language model undergoes a fine-tuning process dueto the scarcity of textual information within the simulation environment.。The need arises to articulate non-linguisticelements, and the interaction between an agent and the simulation environment is centered on continuously observ-ing the environment’s state and generating corresponding actions.。Therefore, the key aspects requiring verbalizationprimarily involve the state and actions of the agent.。However, given the abundance of possible observation states inthe simulation environment, it is impractical to use all of these states directly as prompts for the language model, es-pecially considering token limitations.。Consequently, there is a crucial need to extract and linguistically transformthe most significant meta-states to facilitate successful model interaction.。It is noteworthy that smaller languageBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 2 CALCULATION OF d⋆tInput: d⋆t−1,atOutput: d⋆t1:ifat∈movement direction action space then2:d⋆t←\u0014cos(at)sin(at)\u00153:else if at∈yaw direction action space then4:5: ∆t=at−ω6:d⋆t←\u0014cos(∆ t)−sin(∆ t)sin(∆ t) cos(∆ t)\u0015d⋆t−17:else if at∈{stop; rescue; stop and fire; stop adjust and fire } then8:d⋆t←\u00140\u00159:else10: d⋆t←d⋆t−111:end ifmodels have limitations in comprehending and manipulating numerical values effectively.。To address this challenge,a deliberate effort is made to minimize the use of numerical values during the environmental linguistics process.Sub Action Space Dim Sizemovement direction 16yaw direction 16pitch direction 3body action 9basic action 7switch weapon action 3Table 6: Action space.For example, instead of specifying an agent’s speed with specific numeric metricslike “speed: 1m/s →3m/s” a qualitative representation such as “speed: slower →faster” is adopted.。This technique transforms the original continuous state into alimited, discrete meta-state, thereby enhancing the language model’s understanding.Similarly, for expediting language model understanding, a discrete action space isadopted, with each action accompanied by a succinct artificial language description.This discreet articulation of actions contributes to the overall interpretability of thelanguage model within the simulation environment.. \n",
      "the piece 294\n",
      " \n",
      "on sequences, we employ a smaller γfor the corresponding value head.。Specifically, we set γto 0.92.。Thisadjustment helps mitigate the impact of the obstacle avoidance reward on long-term credit assignment, allowing for a morebalanced consideration of immediate and future rewards in the agent’s decision-making process.。As for the goal-achievingreward, we’ve introduced in the main text, please refer to Section 4.2.B.。Environmental State Abstraction and Goal SpaceFor a comprehensive understanding of the game environment, a language model undergoes a fine-tuning process dueto the scarcity of textual information within the simulation environment.。The need arises to articulate non-linguisticelements, and the interaction between an agent and the simulation environment is centered on continuously observ-ing the environment’s state and generating corresponding actions.。Therefore, the key aspects requiring verbalizationprimarily involve the state and actions of the agent.。However, given the abundance of possible observation states inthe simulation environment, it is impractical to use all of these states directly as prompts for the language model, es-pecially considering token limitations.。Consequently, there is a crucial need to extract and linguistically transformthe most significant meta-states to facilitate successful model interaction.。It is noteworthy that smaller languageBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 2 CALCULATION OF d⋆tInput: d⋆t−1,atOutput: d⋆t1:ifat∈movement direction action space then2:d⋆t←\u0014cos(at)sin(at)\u00153:else if at∈yaw direction action space then4:5: ∆t=at−ω6:d⋆t←\u0014cos(∆ t)−sin(∆ t)sin(∆ t) cos(∆ t)\u0015d⋆t−17:else if at∈{stop; rescue; stop and fire; stop adjust and fire } then8:d⋆t←\u00140\u00159:else10: d⋆t←d⋆t−111:end ifmodels have limitations in comprehending and manipulating numerical values effectively.。To address this challenge,a deliberate effort is made to minimize the use of numerical values during the environmental linguistics process.Sub Action Space Dim Sizemovement direction 16yaw direction 16pitch direction 3body action 9basic action 7switch weapon action 3Table 6: Action space.For example, instead of specifying an agent’s speed with specific numeric metricslike “speed: 1m/s →3m/s” a qualitative representation such as “speed: slower →faster” is adopted.。This technique transforms the original continuous state into alimited, discrete meta-state, thereby enhancing the language model’s understanding.Similarly, for expediting language model understanding, a discrete action space isadopted, with each action accompanied by a succinct artificial language description.This discreet articulation of actions contributes to the overall interpretability of thelanguage model within the simulation environment.。We list the details in Table 7.C.. \n",
      "the piece 295\n",
      " \n",
      "ly, we set γto 0.92.。Thisadjustment helps mitigate the impact of the obstacle avoidance reward on long-term credit assignment, allowing for a morebalanced consideration of immediate and future rewards in the agent’s decision-making process.。As for the goal-achievingreward, we’ve introduced in the main text, please refer to Section 4.2.B.。Environmental State Abstraction and Goal SpaceFor a comprehensive understanding of the game environment, a language model undergoes a fine-tuning process dueto the scarcity of textual information within the simulation environment.。The need arises to articulate non-linguisticelements, and the interaction between an agent and the simulation environment is centered on continuously observ-ing the environment’s state and generating corresponding actions.。Therefore, the key aspects requiring verbalizationprimarily involve the state and actions of the agent.。However, given the abundance of possible observation states inthe simulation environment, it is impractical to use all of these states directly as prompts for the language model, es-pecially considering token limitations.。Consequently, there is a crucial need to extract and linguistically transformthe most significant meta-states to facilitate successful model interaction.。It is noteworthy that smaller languageBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 2 CALCULATION OF d⋆tInput: d⋆t−1,atOutput: d⋆t1:ifat∈movement direction action space then2:d⋆t←\u0014cos(at)sin(at)\u00153:else if at∈yaw direction action space then4:5: ∆t=at−ω6:d⋆t←\u0014cos(∆ t)−sin(∆ t)sin(∆ t) cos(∆ t)\u0015d⋆t−17:else if at∈{stop; rescue; stop and fire; stop adjust and fire } then8:d⋆t←\u00140\u00159:else10: d⋆t←d⋆t−111:end ifmodels have limitations in comprehending and manipulating numerical values effectively.。To address this challenge,a deliberate effort is made to minimize the use of numerical values during the environmental linguistics process.Sub Action Space Dim Sizemovement direction 16yaw direction 16pitch direction 3body action 9basic action 7switch weapon action 3Table 6: Action space.For example, instead of specifying an agent’s speed with specific numeric metricslike “speed: 1m/s →3m/s” a qualitative representation such as “speed: slower →faster” is adopted.。This technique transforms the original continuous state into alimited, discrete meta-state, thereby enhancing the language model’s understanding.Similarly, for expediting language model understanding, a discrete action space isadopted, with each action accompanied by a succinct artificial language description.This discreet articulation of actions contributes to the overall interpretability of thelanguage model within the simulation environment.。We list the details in Table 7.C.。Policy NetworkFigure 5 is the network architecture used for non-goal RL, corresponding to thepolicy π.. \n",
      "the piece 296\n",
      " \n",
      "ng for a morebalanced consideration of immediate and future rewards in the agent’s decision-making process.。As for the goal-achievingreward, we’ve introduced in the main text, please refer to Section 4.2.B.。Environmental State Abstraction and Goal SpaceFor a comprehensive understanding of the game environment, a language model undergoes a fine-tuning process dueto the scarcity of textual information within the simulation environment.。The need arises to articulate non-linguisticelements, and the interaction between an agent and the simulation environment is centered on continuously observ-ing the environment’s state and generating corresponding actions.。Therefore, the key aspects requiring verbalizationprimarily involve the state and actions of the agent.。However, given the abundance of possible observation states inthe simulation environment, it is impractical to use all of these states directly as prompts for the language model, es-pecially considering token limitations.。Consequently, there is a crucial need to extract and linguistically transformthe most significant meta-states to facilitate successful model interaction.。It is noteworthy that smaller languageBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 2 CALCULATION OF d⋆tInput: d⋆t−1,atOutput: d⋆t1:ifat∈movement direction action space then2:d⋆t←\u0014cos(at)sin(at)\u00153:else if at∈yaw direction action space then4:5: ∆t=at−ω6:d⋆t←\u0014cos(∆ t)−sin(∆ t)sin(∆ t) cos(∆ t)\u0015d⋆t−17:else if at∈{stop; rescue; stop and fire; stop adjust and fire } then8:d⋆t←\u00140\u00159:else10: d⋆t←d⋆t−111:end ifmodels have limitations in comprehending and manipulating numerical values effectively.。To address this challenge,a deliberate effort is made to minimize the use of numerical values during the environmental linguistics process.Sub Action Space Dim Sizemovement direction 16yaw direction 16pitch direction 3body action 9basic action 7switch weapon action 3Table 6: Action space.For example, instead of specifying an agent’s speed with specific numeric metricslike “speed: 1m/s →3m/s” a qualitative representation such as “speed: slower →faster” is adopted.。This technique transforms the original continuous state into alimited, discrete meta-state, thereby enhancing the language model’s understanding.Similarly, for expediting language model understanding, a discrete action space isadopted, with each action accompanied by a succinct artificial language description.This discreet articulation of actions contributes to the overall interpretability of thelanguage model within the simulation environment.。We list the details in Table 7.C.。Policy NetworkFigure 5 is the network architecture used for non-goal RL, corresponding to thepolicy π.。On top of observation preprocessing in Appendix A, we introduce abackbone implemented with a fully-connected layer followed by three Residual Blocks.. \n",
      "the piece 297\n",
      " \n",
      "duced in the main text, please refer to Section 4.2.B.。Environmental State Abstraction and Goal SpaceFor a comprehensive understanding of the game environment, a language model undergoes a fine-tuning process dueto the scarcity of textual information within the simulation environment.。The need arises to articulate non-linguisticelements, and the interaction between an agent and the simulation environment is centered on continuously observ-ing the environment’s state and generating corresponding actions.。Therefore, the key aspects requiring verbalizationprimarily involve the state and actions of the agent.。However, given the abundance of possible observation states inthe simulation environment, it is impractical to use all of these states directly as prompts for the language model, es-pecially considering token limitations.。Consequently, there is a crucial need to extract and linguistically transformthe most significant meta-states to facilitate successful model interaction.。It is noteworthy that smaller languageBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 2 CALCULATION OF d⋆tInput: d⋆t−1,atOutput: d⋆t1:ifat∈movement direction action space then2:d⋆t←\u0014cos(at)sin(at)\u00153:else if at∈yaw direction action space then4:5: ∆t=at−ω6:d⋆t←\u0014cos(∆ t)−sin(∆ t)sin(∆ t) cos(∆ t)\u0015d⋆t−17:else if at∈{stop; rescue; stop and fire; stop adjust and fire } then8:d⋆t←\u00140\u00159:else10: d⋆t←d⋆t−111:end ifmodels have limitations in comprehending and manipulating numerical values effectively.。To address this challenge,a deliberate effort is made to minimize the use of numerical values during the environmental linguistics process.Sub Action Space Dim Sizemovement direction 16yaw direction 16pitch direction 3body action 9basic action 7switch weapon action 3Table 6: Action space.For example, instead of specifying an agent’s speed with specific numeric metricslike “speed: 1m/s →3m/s” a qualitative representation such as “speed: slower →faster” is adopted.。This technique transforms the original continuous state into alimited, discrete meta-state, thereby enhancing the language model’s understanding.Similarly, for expediting language model understanding, a discrete action space isadopted, with each action accompanied by a succinct artificial language description.This discreet articulation of actions contributes to the overall interpretability of thelanguage model within the simulation environment.。We list the details in Table 7.C.。Policy NetworkFigure 5 is the network architecture used for non-goal RL, corresponding to thepolicy π.。On top of observation preprocessing in Appendix A, we introduce abackbone implemented with a fully-connected layer followed by three Residual Blocks.。As for the policy head and threevalue heads, we implemented each of them as two connected Residual Blocks.. \n",
      "the piece 298\n",
      " \n",
      "efer to Section 4.2.B.。Environmental State Abstraction and Goal SpaceFor a comprehensive understanding of the game environment, a language model undergoes a fine-tuning process dueto the scarcity of textual information within the simulation environment.。The need arises to articulate non-linguisticelements, and the interaction between an agent and the simulation environment is centered on continuously observ-ing the environment’s state and generating corresponding actions.。Therefore, the key aspects requiring verbalizationprimarily involve the state and actions of the agent.。However, given the abundance of possible observation states inthe simulation environment, it is impractical to use all of these states directly as prompts for the language model, es-pecially considering token limitations.。Consequently, there is a crucial need to extract and linguistically transformthe most significant meta-states to facilitate successful model interaction.。It is noteworthy that smaller languageBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 2 CALCULATION OF d⋆tInput: d⋆t−1,atOutput: d⋆t1:ifat∈movement direction action space then2:d⋆t←\u0014cos(at)sin(at)\u00153:else if at∈yaw direction action space then4:5: ∆t=at−ω6:d⋆t←\u0014cos(∆ t)−sin(∆ t)sin(∆ t) cos(∆ t)\u0015d⋆t−17:else if at∈{stop; rescue; stop and fire; stop adjust and fire } then8:d⋆t←\u00140\u00159:else10: d⋆t←d⋆t−111:end ifmodels have limitations in comprehending and manipulating numerical values effectively.。To address this challenge,a deliberate effort is made to minimize the use of numerical values during the environmental linguistics process.Sub Action Space Dim Sizemovement direction 16yaw direction 16pitch direction 3body action 9basic action 7switch weapon action 3Table 6: Action space.For example, instead of specifying an agent’s speed with specific numeric metricslike “speed: 1m/s →3m/s” a qualitative representation such as “speed: slower →faster” is adopted.。This technique transforms the original continuous state into alimited, discrete meta-state, thereby enhancing the language model’s understanding.Similarly, for expediting language model understanding, a discrete action space isadopted, with each action accompanied by a succinct artificial language description.This discreet articulation of actions contributes to the overall interpretability of thelanguage model within the simulation environment.。We list the details in Table 7.C.。Policy NetworkFigure 5 is the network architecture used for non-goal RL, corresponding to thepolicy π.。On top of observation preprocessing in Appendix A, we introduce abackbone implemented with a fully-connected layer followed by three Residual Blocks.。As for the policy head and threevalue heads, we implemented each of them as two connected Residual Blocks.。It is noteworthy that the invisible enemyinformation, such as the nearest enemy’s location, has also been introduced as an input to the value estimation, for theconsideration of stabilizing the policy learning (Vinyals et al., 2019).Figure 5: Network structure of our proposed policy.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationFeature Weight Descriptionenemy discovery 0.02 reward for see an enemydetected by enemy -0.002 punishment for being seen by an enemyscout 0.0001 reward for search for an enemyno-op -0.0002 punishment for stopping and doing nothingbullet 0.015 reward for using and refilling bulletshealth point 0.03 reward for health point changesbe knocked down -2.5 punishment for being knocked downdead -3.5 punishment for being killeddamage enemy 0.1 reward for damaging an enemyknock down enemy 4.5 reward for knocking down an enemykill enemy 3.5 reward for killing an enemyapproach a downed teammate 0.001 reward for approaching a downed teammatehelp a downed teammate up 0.8 reward for helping up a downed teammatenot save a downed teammate -0.5 punishment for not saving a downed teammatego to blue circle 0.00015 reward for going to blue circlebe in white circle -0.00005 small punishment for being outside of white circleoutside blue circle -0.012 punishment for being outside of blue circleteammate damage enemy 0.03 reward from teammate damaging enemiesteammate get up 0.6 reward from teammate getting upI help teammate up 4 reward for helping teammate upinterrupt helping teammate up -0.05 punishment for the interruption to help teammate upobstacle avoidance 0.012 punishment for being stuckgoal 1 reward of goal completionTable 5: The introduction of different rewards.Date Iteration #params Change4/14/2023 1 1802702 Experiment started4/27/2023 1808552 1802702 Env-init: Random weapons5/8/2023 2829170 1803087 Action: Add a fire action for long distance5/10/2023 3034011 1803087 Env-init:Random safe area in the whole map5/11/2023 3130353 1803855 Observation: Add number of remaining players in the game5/12/2023 3198564 2412975 Observation: Add BEV feature5/16/2023 3673506 2418111 Observation: Add history rotation feature5/22/2023 4519567 2418368 Observation: Add rotation change feature5/29/2023 5442025 2418368 Reward: Add rewards for teamwork6/2/2023 5899503 2418368 Update new game version6/13/2023 7306607 3013409 Network: Add obstacle avoidance reward and corresponding value head6/14/2023 7404118 3015457 Observation: Add distance feature to nearby obstacles6/16/2023 7628098 3015457 Env-init: Player numbers per team increased to 46/19/2023 7974450 3109267 Action: Use attention to select target to attackTable 8: The major changes in the training procedure.Goal Processing.. \n",
      "the piece 299\n",
      " \n",
      "thin the simulation environment.。The need arises to articulate non-linguisticelements, and the interaction between an agent and the simulation environment is centered on continuously observ-ing the environment’s state and generating corresponding actions.。Therefore, the key aspects requiring verbalizationprimarily involve the state and actions of the agent.。However, given the abundance of possible observation states inthe simulation environment, it is impractical to use all of these states directly as prompts for the language model, es-pecially considering token limitations.。Consequently, there is a crucial need to extract and linguistically transformthe most significant meta-states to facilitate successful model interaction.。It is noteworthy that smaller languageBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 2 CALCULATION OF d⋆tInput: d⋆t−1,atOutput: d⋆t1:ifat∈movement direction action space then2:d⋆t←\u0014cos(at)sin(at)\u00153:else if at∈yaw direction action space then4:5: ∆t=at−ω6:d⋆t←\u0014cos(∆ t)−sin(∆ t)sin(∆ t) cos(∆ t)\u0015d⋆t−17:else if at∈{stop; rescue; stop and fire; stop adjust and fire } then8:d⋆t←\u00140\u00159:else10: d⋆t←d⋆t−111:end ifmodels have limitations in comprehending and manipulating numerical values effectively.。To address this challenge,a deliberate effort is made to minimize the use of numerical values during the environmental linguistics process.Sub Action Space Dim Sizemovement direction 16yaw direction 16pitch direction 3body action 9basic action 7switch weapon action 3Table 6: Action space.For example, instead of specifying an agent’s speed with specific numeric metricslike “speed: 1m/s →3m/s” a qualitative representation such as “speed: slower →faster” is adopted.。This technique transforms the original continuous state into alimited, discrete meta-state, thereby enhancing the language model’s understanding.Similarly, for expediting language model understanding, a discrete action space isadopted, with each action accompanied by a succinct artificial language description.This discreet articulation of actions contributes to the overall interpretability of thelanguage model within the simulation environment.。We list the details in Table 7.C.。Policy NetworkFigure 5 is the network architecture used for non-goal RL, corresponding to thepolicy π.。On top of observation preprocessing in Appendix A, we introduce abackbone implemented with a fully-connected layer followed by three Residual Blocks.。As for the policy head and threevalue heads, we implemented each of them as two connected Residual Blocks.。It is noteworthy that the invisible enemyinformation, such as the nearest enemy’s location, has also been introduced as an input to the value estimation, for theconsideration of stabilizing the policy learning (Vinyals et al., 2019).Figure 5: Network structure of our proposed policy.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationFeature Weight Descriptionenemy discovery 0.02 reward for see an enemydetected by enemy -0.002 punishment for being seen by an enemyscout 0.0001 reward for search for an enemyno-op -0.0002 punishment for stopping and doing nothingbullet 0.015 reward for using and refilling bulletshealth point 0.03 reward for health point changesbe knocked down -2.5 punishment for being knocked downdead -3.5 punishment for being killeddamage enemy 0.1 reward for damaging an enemyknock down enemy 4.5 reward for knocking down an enemykill enemy 3.5 reward for killing an enemyapproach a downed teammate 0.001 reward for approaching a downed teammatehelp a downed teammate up 0.8 reward for helping up a downed teammatenot save a downed teammate -0.5 punishment for not saving a downed teammatego to blue circle 0.00015 reward for going to blue circlebe in white circle -0.00005 small punishment for being outside of white circleoutside blue circle -0.012 punishment for being outside of blue circleteammate damage enemy 0.03 reward from teammate damaging enemiesteammate get up 0.6 reward from teammate getting upI help teammate up 4 reward for helping teammate upinterrupt helping teammate up -0.05 punishment for the interruption to help teammate upobstacle avoidance 0.012 punishment for being stuckgoal 1 reward of goal completionTable 5: The introduction of different rewards.Date Iteration #params Change4/14/2023 1 1802702 Experiment started4/27/2023 1808552 1802702 Env-init: Random weapons5/8/2023 2829170 1803087 Action: Add a fire action for long distance5/10/2023 3034011 1803087 Env-init:Random safe area in the whole map5/11/2023 3130353 1803855 Observation: Add number of remaining players in the game5/12/2023 3198564 2412975 Observation: Add BEV feature5/16/2023 3673506 2418111 Observation: Add history rotation feature5/22/2023 4519567 2418368 Observation: Add rotation change feature5/29/2023 5442025 2418368 Reward: Add rewards for teamwork6/2/2023 5899503 2418368 Update new game version6/13/2023 7306607 3013409 Network: Add obstacle avoidance reward and corresponding value head6/14/2023 7404118 3015457 Observation: Add distance feature to nearby obstacles6/16/2023 7628098 3015457 Env-init: Player numbers per team increased to 46/19/2023 7974450 3109267 Action: Use attention to select target to attackTable 8: The major changes in the training procedure.Goal Processing.。To construct the goal-conditioned policy πgon top of π, we need toencode a goal gthat is generated from Grnd,GopandGllm.. \n",
      "the piece 300\n",
      " \n",
      "action between an agent and the simulation environment is centered on continuously observ-ing the environment’s state and generating corresponding actions.。Therefore, the key aspects requiring verbalizationprimarily involve the state and actions of the agent.。However, given the abundance of possible observation states inthe simulation environment, it is impractical to use all of these states directly as prompts for the language model, es-pecially considering token limitations.。Consequently, there is a crucial need to extract and linguistically transformthe most significant meta-states to facilitate successful model interaction.。It is noteworthy that smaller languageBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 2 CALCULATION OF d⋆tInput: d⋆t−1,atOutput: d⋆t1:ifat∈movement direction action space then2:d⋆t←\u0014cos(at)sin(at)\u00153:else if at∈yaw direction action space then4:5: ∆t=at−ω6:d⋆t←\u0014cos(∆ t)−sin(∆ t)sin(∆ t) cos(∆ t)\u0015d⋆t−17:else if at∈{stop; rescue; stop and fire; stop adjust and fire } then8:d⋆t←\u00140\u00159:else10: d⋆t←d⋆t−111:end ifmodels have limitations in comprehending and manipulating numerical values effectively.。To address this challenge,a deliberate effort is made to minimize the use of numerical values during the environmental linguistics process.Sub Action Space Dim Sizemovement direction 16yaw direction 16pitch direction 3body action 9basic action 7switch weapon action 3Table 6: Action space.For example, instead of specifying an agent’s speed with specific numeric metricslike “speed: 1m/s →3m/s” a qualitative representation such as “speed: slower →faster” is adopted.。This technique transforms the original continuous state into alimited, discrete meta-state, thereby enhancing the language model’s understanding.Similarly, for expediting language model understanding, a discrete action space isadopted, with each action accompanied by a succinct artificial language description.This discreet articulation of actions contributes to the overall interpretability of thelanguage model within the simulation environment.。We list the details in Table 7.C.。Policy NetworkFigure 5 is the network architecture used for non-goal RL, corresponding to thepolicy π.。On top of observation preprocessing in Appendix A, we introduce abackbone implemented with a fully-connected layer followed by three Residual Blocks.。As for the policy head and threevalue heads, we implemented each of them as two connected Residual Blocks.。It is noteworthy that the invisible enemyinformation, such as the nearest enemy’s location, has also been introduced as an input to the value estimation, for theconsideration of stabilizing the policy learning (Vinyals et al., 2019).Figure 5: Network structure of our proposed policy.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationFeature Weight Descriptionenemy discovery 0.02 reward for see an enemydetected by enemy -0.002 punishment for being seen by an enemyscout 0.0001 reward for search for an enemyno-op -0.0002 punishment for stopping and doing nothingbullet 0.015 reward for using and refilling bulletshealth point 0.03 reward for health point changesbe knocked down -2.5 punishment for being knocked downdead -3.5 punishment for being killeddamage enemy 0.1 reward for damaging an enemyknock down enemy 4.5 reward for knocking down an enemykill enemy 3.5 reward for killing an enemyapproach a downed teammate 0.001 reward for approaching a downed teammatehelp a downed teammate up 0.8 reward for helping up a downed teammatenot save a downed teammate -0.5 punishment for not saving a downed teammatego to blue circle 0.00015 reward for going to blue circlebe in white circle -0.00005 small punishment for being outside of white circleoutside blue circle -0.012 punishment for being outside of blue circleteammate damage enemy 0.03 reward from teammate damaging enemiesteammate get up 0.6 reward from teammate getting upI help teammate up 4 reward for helping teammate upinterrupt helping teammate up -0.05 punishment for the interruption to help teammate upobstacle avoidance 0.012 punishment for being stuckgoal 1 reward of goal completionTable 5: The introduction of different rewards.Date Iteration #params Change4/14/2023 1 1802702 Experiment started4/27/2023 1808552 1802702 Env-init: Random weapons5/8/2023 2829170 1803087 Action: Add a fire action for long distance5/10/2023 3034011 1803087 Env-init:Random safe area in the whole map5/11/2023 3130353 1803855 Observation: Add number of remaining players in the game5/12/2023 3198564 2412975 Observation: Add BEV feature5/16/2023 3673506 2418111 Observation: Add history rotation feature5/22/2023 4519567 2418368 Observation: Add rotation change feature5/29/2023 5442025 2418368 Reward: Add rewards for teamwork6/2/2023 5899503 2418368 Update new game version6/13/2023 7306607 3013409 Network: Add obstacle avoidance reward and corresponding value head6/14/2023 7404118 3015457 Observation: Add distance feature to nearby obstacles6/16/2023 7628098 3015457 Env-init: Player numbers per team increased to 46/19/2023 7974450 3109267 Action: Use attention to select target to attackTable 8: The major changes in the training procedure.Goal Processing.。To construct the goal-conditioned policy πgon top of π, we need toencode a goal gthat is generated from Grnd,GopandGllm.。Thus, we propose a network Fas anencoder to achieve that.. \n",
      "the piece 301\n",
      " \n",
      "e and actions of the agent.。However, given the abundance of possible observation states inthe simulation environment, it is impractical to use all of these states directly as prompts for the language model, es-pecially considering token limitations.。Consequently, there is a crucial need to extract and linguistically transformthe most significant meta-states to facilitate successful model interaction.。It is noteworthy that smaller languageBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 2 CALCULATION OF d⋆tInput: d⋆t−1,atOutput: d⋆t1:ifat∈movement direction action space then2:d⋆t←\u0014cos(at)sin(at)\u00153:else if at∈yaw direction action space then4:5: ∆t=at−ω6:d⋆t←\u0014cos(∆ t)−sin(∆ t)sin(∆ t) cos(∆ t)\u0015d⋆t−17:else if at∈{stop; rescue; stop and fire; stop adjust and fire } then8:d⋆t←\u00140\u00159:else10: d⋆t←d⋆t−111:end ifmodels have limitations in comprehending and manipulating numerical values effectively.。To address this challenge,a deliberate effort is made to minimize the use of numerical values during the environmental linguistics process.Sub Action Space Dim Sizemovement direction 16yaw direction 16pitch direction 3body action 9basic action 7switch weapon action 3Table 6: Action space.For example, instead of specifying an agent’s speed with specific numeric metricslike “speed: 1m/s →3m/s” a qualitative representation such as “speed: slower →faster” is adopted.。This technique transforms the original continuous state into alimited, discrete meta-state, thereby enhancing the language model’s understanding.Similarly, for expediting language model understanding, a discrete action space isadopted, with each action accompanied by a succinct artificial language description.This discreet articulation of actions contributes to the overall interpretability of thelanguage model within the simulation environment.。We list the details in Table 7.C.。Policy NetworkFigure 5 is the network architecture used for non-goal RL, corresponding to thepolicy π.。On top of observation preprocessing in Appendix A, we introduce abackbone implemented with a fully-connected layer followed by three Residual Blocks.。As for the policy head and threevalue heads, we implemented each of them as two connected Residual Blocks.。It is noteworthy that the invisible enemyinformation, such as the nearest enemy’s location, has also been introduced as an input to the value estimation, for theconsideration of stabilizing the policy learning (Vinyals et al., 2019).Figure 5: Network structure of our proposed policy.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationFeature Weight Descriptionenemy discovery 0.02 reward for see an enemydetected by enemy -0.002 punishment for being seen by an enemyscout 0.0001 reward for search for an enemyno-op -0.0002 punishment for stopping and doing nothingbullet 0.015 reward for using and refilling bulletshealth point 0.03 reward for health point changesbe knocked down -2.5 punishment for being knocked downdead -3.5 punishment for being killeddamage enemy 0.1 reward for damaging an enemyknock down enemy 4.5 reward for knocking down an enemykill enemy 3.5 reward for killing an enemyapproach a downed teammate 0.001 reward for approaching a downed teammatehelp a downed teammate up 0.8 reward for helping up a downed teammatenot save a downed teammate -0.5 punishment for not saving a downed teammatego to blue circle 0.00015 reward for going to blue circlebe in white circle -0.00005 small punishment for being outside of white circleoutside blue circle -0.012 punishment for being outside of blue circleteammate damage enemy 0.03 reward from teammate damaging enemiesteammate get up 0.6 reward from teammate getting upI help teammate up 4 reward for helping teammate upinterrupt helping teammate up -0.05 punishment for the interruption to help teammate upobstacle avoidance 0.012 punishment for being stuckgoal 1 reward of goal completionTable 5: The introduction of different rewards.Date Iteration #params Change4/14/2023 1 1802702 Experiment started4/27/2023 1808552 1802702 Env-init: Random weapons5/8/2023 2829170 1803087 Action: Add a fire action for long distance5/10/2023 3034011 1803087 Env-init:Random safe area in the whole map5/11/2023 3130353 1803855 Observation: Add number of remaining players in the game5/12/2023 3198564 2412975 Observation: Add BEV feature5/16/2023 3673506 2418111 Observation: Add history rotation feature5/22/2023 4519567 2418368 Observation: Add rotation change feature5/29/2023 5442025 2418368 Reward: Add rewards for teamwork6/2/2023 5899503 2418368 Update new game version6/13/2023 7306607 3013409 Network: Add obstacle avoidance reward and corresponding value head6/14/2023 7404118 3015457 Observation: Add distance feature to nearby obstacles6/16/2023 7628098 3015457 Env-init: Player numbers per team increased to 46/19/2023 7974450 3109267 Action: Use attention to select target to attackTable 8: The major changes in the training procedure.Goal Processing.。To construct the goal-conditioned policy πgon top of π, we need toencode a goal gthat is generated from Grnd,GopandGllm.。Thus, we propose a network Fas anencoder to achieve that.。In practice, to improvethe representation, we further include other inputsbesides gas (1) goal_mode_info : a 10-dimensionalvector that indicates whether current learning isgoal-conditioned, the achieved times of g, andthe rest time to complete this goal (as we presetthe maximum timestep for goal completion is30s), short in vinfo; (2) sub_goal_masking : a68-dimensional 0-1 vector that indicates whichsub-goals are masked for their corresponding values are 0, short in vmask ; (3) expected_goal_of_current_state : namely, forstatt, we generate a goal g′=PROJ(st)which is a 68-dimensional vector as the same as g, short in g′.. \n",
      "the piece 302\n",
      " \n",
      "dering token limitations.。Consequently, there is a crucial need to extract and linguistically transformthe most significant meta-states to facilitate successful model interaction.。It is noteworthy that smaller languageBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 2 CALCULATION OF d⋆tInput: d⋆t−1,atOutput: d⋆t1:ifat∈movement direction action space then2:d⋆t←\u0014cos(at)sin(at)\u00153:else if at∈yaw direction action space then4:5: ∆t=at−ω6:d⋆t←\u0014cos(∆ t)−sin(∆ t)sin(∆ t) cos(∆ t)\u0015d⋆t−17:else if at∈{stop; rescue; stop and fire; stop adjust and fire } then8:d⋆t←\u00140\u00159:else10: d⋆t←d⋆t−111:end ifmodels have limitations in comprehending and manipulating numerical values effectively.。To address this challenge,a deliberate effort is made to minimize the use of numerical values during the environmental linguistics process.Sub Action Space Dim Sizemovement direction 16yaw direction 16pitch direction 3body action 9basic action 7switch weapon action 3Table 6: Action space.For example, instead of specifying an agent’s speed with specific numeric metricslike “speed: 1m/s →3m/s” a qualitative representation such as “speed: slower →faster” is adopted.。This technique transforms the original continuous state into alimited, discrete meta-state, thereby enhancing the language model’s understanding.Similarly, for expediting language model understanding, a discrete action space isadopted, with each action accompanied by a succinct artificial language description.This discreet articulation of actions contributes to the overall interpretability of thelanguage model within the simulation environment.。We list the details in Table 7.C.。Policy NetworkFigure 5 is the network architecture used for non-goal RL, corresponding to thepolicy π.。On top of observation preprocessing in Appendix A, we introduce abackbone implemented with a fully-connected layer followed by three Residual Blocks.。As for the policy head and threevalue heads, we implemented each of them as two connected Residual Blocks.。It is noteworthy that the invisible enemyinformation, such as the nearest enemy’s location, has also been introduced as an input to the value estimation, for theconsideration of stabilizing the policy learning (Vinyals et al., 2019).Figure 5: Network structure of our proposed policy.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationFeature Weight Descriptionenemy discovery 0.02 reward for see an enemydetected by enemy -0.002 punishment for being seen by an enemyscout 0.0001 reward for search for an enemyno-op -0.0002 punishment for stopping and doing nothingbullet 0.015 reward for using and refilling bulletshealth point 0.03 reward for health point changesbe knocked down -2.5 punishment for being knocked downdead -3.5 punishment for being killeddamage enemy 0.1 reward for damaging an enemyknock down enemy 4.5 reward for knocking down an enemykill enemy 3.5 reward for killing an enemyapproach a downed teammate 0.001 reward for approaching a downed teammatehelp a downed teammate up 0.8 reward for helping up a downed teammatenot save a downed teammate -0.5 punishment for not saving a downed teammatego to blue circle 0.00015 reward for going to blue circlebe in white circle -0.00005 small punishment for being outside of white circleoutside blue circle -0.012 punishment for being outside of blue circleteammate damage enemy 0.03 reward from teammate damaging enemiesteammate get up 0.6 reward from teammate getting upI help teammate up 4 reward for helping teammate upinterrupt helping teammate up -0.05 punishment for the interruption to help teammate upobstacle avoidance 0.012 punishment for being stuckgoal 1 reward of goal completionTable 5: The introduction of different rewards.Date Iteration #params Change4/14/2023 1 1802702 Experiment started4/27/2023 1808552 1802702 Env-init: Random weapons5/8/2023 2829170 1803087 Action: Add a fire action for long distance5/10/2023 3034011 1803087 Env-init:Random safe area in the whole map5/11/2023 3130353 1803855 Observation: Add number of remaining players in the game5/12/2023 3198564 2412975 Observation: Add BEV feature5/16/2023 3673506 2418111 Observation: Add history rotation feature5/22/2023 4519567 2418368 Observation: Add rotation change feature5/29/2023 5442025 2418368 Reward: Add rewards for teamwork6/2/2023 5899503 2418368 Update new game version6/13/2023 7306607 3013409 Network: Add obstacle avoidance reward and corresponding value head6/14/2023 7404118 3015457 Observation: Add distance feature to nearby obstacles6/16/2023 7628098 3015457 Env-init: Player numbers per team increased to 46/19/2023 7974450 3109267 Action: Use attention to select target to attackTable 8: The major changes in the training procedure.Goal Processing.。To construct the goal-conditioned policy πgon top of π, we need toencode a goal gthat is generated from Grnd,GopandGllm.。Thus, we propose a network Fas anencoder to achieve that.。In practice, to improvethe representation, we further include other inputsbesides gas (1) goal_mode_info : a 10-dimensionalvector that indicates whether current learning isgoal-conditioned, the achieved times of g, andthe rest time to complete this goal (as we presetthe maximum timestep for goal completion is30s), short in vinfo; (2) sub_goal_masking : a68-dimensional 0-1 vector that indicates whichsub-goals are masked for their corresponding values are 0, short in vmask ; (3) expected_goal_of_current_state : namely, forstatt, we generate a goal g′=PROJ(st)which is a 68-dimensional vector as the same as g, short in g′.。Then, we form theinput for Fasx= (g, g′, vinfo, vmask).. \n",
      "the piece 303\n",
      " \n",
      "e most significant meta-states to facilitate successful model interaction.。It is noteworthy that smaller languageBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 2 CALCULATION OF d⋆tInput: d⋆t−1,atOutput: d⋆t1:ifat∈movement direction action space then2:d⋆t←\u0014cos(at)sin(at)\u00153:else if at∈yaw direction action space then4:5: ∆t=at−ω6:d⋆t←\u0014cos(∆ t)−sin(∆ t)sin(∆ t) cos(∆ t)\u0015d⋆t−17:else if at∈{stop; rescue; stop and fire; stop adjust and fire } then8:d⋆t←\u00140\u00159:else10: d⋆t←d⋆t−111:end ifmodels have limitations in comprehending and manipulating numerical values effectively.。To address this challenge,a deliberate effort is made to minimize the use of numerical values during the environmental linguistics process.Sub Action Space Dim Sizemovement direction 16yaw direction 16pitch direction 3body action 9basic action 7switch weapon action 3Table 6: Action space.For example, instead of specifying an agent’s speed with specific numeric metricslike “speed: 1m/s →3m/s” a qualitative representation such as “speed: slower →faster” is adopted.。This technique transforms the original continuous state into alimited, discrete meta-state, thereby enhancing the language model’s understanding.Similarly, for expediting language model understanding, a discrete action space isadopted, with each action accompanied by a succinct artificial language description.This discreet articulation of actions contributes to the overall interpretability of thelanguage model within the simulation environment.。We list the details in Table 7.C.。Policy NetworkFigure 5 is the network architecture used for non-goal RL, corresponding to thepolicy π.。On top of observation preprocessing in Appendix A, we introduce abackbone implemented with a fully-connected layer followed by three Residual Blocks.。As for the policy head and threevalue heads, we implemented each of them as two connected Residual Blocks.。It is noteworthy that the invisible enemyinformation, such as the nearest enemy’s location, has also been introduced as an input to the value estimation, for theconsideration of stabilizing the policy learning (Vinyals et al., 2019).Figure 5: Network structure of our proposed policy.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationFeature Weight Descriptionenemy discovery 0.02 reward for see an enemydetected by enemy -0.002 punishment for being seen by an enemyscout 0.0001 reward for search for an enemyno-op -0.0002 punishment for stopping and doing nothingbullet 0.015 reward for using and refilling bulletshealth point 0.03 reward for health point changesbe knocked down -2.5 punishment for being knocked downdead -3.5 punishment for being killeddamage enemy 0.1 reward for damaging an enemyknock down enemy 4.5 reward for knocking down an enemykill enemy 3.5 reward for killing an enemyapproach a downed teammate 0.001 reward for approaching a downed teammatehelp a downed teammate up 0.8 reward for helping up a downed teammatenot save a downed teammate -0.5 punishment for not saving a downed teammatego to blue circle 0.00015 reward for going to blue circlebe in white circle -0.00005 small punishment for being outside of white circleoutside blue circle -0.012 punishment for being outside of blue circleteammate damage enemy 0.03 reward from teammate damaging enemiesteammate get up 0.6 reward from teammate getting upI help teammate up 4 reward for helping teammate upinterrupt helping teammate up -0.05 punishment for the interruption to help teammate upobstacle avoidance 0.012 punishment for being stuckgoal 1 reward of goal completionTable 5: The introduction of different rewards.Date Iteration #params Change4/14/2023 1 1802702 Experiment started4/27/2023 1808552 1802702 Env-init: Random weapons5/8/2023 2829170 1803087 Action: Add a fire action for long distance5/10/2023 3034011 1803087 Env-init:Random safe area in the whole map5/11/2023 3130353 1803855 Observation: Add number of remaining players in the game5/12/2023 3198564 2412975 Observation: Add BEV feature5/16/2023 3673506 2418111 Observation: Add history rotation feature5/22/2023 4519567 2418368 Observation: Add rotation change feature5/29/2023 5442025 2418368 Reward: Add rewards for teamwork6/2/2023 5899503 2418368 Update new game version6/13/2023 7306607 3013409 Network: Add obstacle avoidance reward and corresponding value head6/14/2023 7404118 3015457 Observation: Add distance feature to nearby obstacles6/16/2023 7628098 3015457 Env-init: Player numbers per team increased to 46/19/2023 7974450 3109267 Action: Use attention to select target to attackTable 8: The major changes in the training procedure.Goal Processing.。To construct the goal-conditioned policy πgon top of π, we need toencode a goal gthat is generated from Grnd,GopandGllm.。Thus, we propose a network Fas anencoder to achieve that.。In practice, to improvethe representation, we further include other inputsbesides gas (1) goal_mode_info : a 10-dimensionalvector that indicates whether current learning isgoal-conditioned, the achieved times of g, andthe rest time to complete this goal (as we presetthe maximum timestep for goal completion is30s), short in vinfo; (2) sub_goal_masking : a68-dimensional 0-1 vector that indicates whichsub-goals are masked for their corresponding values are 0, short in vmask ; (3) expected_goal_of_current_state : namely, forstatt, we generate a goal g′=PROJ(st)which is a 68-dimensional vector as the same as g, short in g′.。Then, we form theinput for Fasx= (g, g′, vinfo, vmask).。Considering that each item in xis heterogeneous on the data type, so we transformthem with independent FC layers and then follows a ResNet block for each of them.. \n",
      "the piece 304\n",
      " \n",
      "nput: d⋆t−1,atOutput: d⋆t1:ifat∈movement direction action space then2:d⋆t←\u0014cos(at)sin(at)\u00153:else if at∈yaw direction action space then4:5: ∆t=at−ω6:d⋆t←\u0014cos(∆ t)−sin(∆ t)sin(∆ t) cos(∆ t)\u0015d⋆t−17:else if at∈{stop; rescue; stop and fire; stop adjust and fire } then8:d⋆t←\u00140\u00159:else10: d⋆t←d⋆t−111:end ifmodels have limitations in comprehending and manipulating numerical values effectively.。To address this challenge,a deliberate effort is made to minimize the use of numerical values during the environmental linguistics process.Sub Action Space Dim Sizemovement direction 16yaw direction 16pitch direction 3body action 9basic action 7switch weapon action 3Table 6: Action space.For example, instead of specifying an agent’s speed with specific numeric metricslike “speed: 1m/s →3m/s” a qualitative representation such as “speed: slower →faster” is adopted.。This technique transforms the original continuous state into alimited, discrete meta-state, thereby enhancing the language model’s understanding.Similarly, for expediting language model understanding, a discrete action space isadopted, with each action accompanied by a succinct artificial language description.This discreet articulation of actions contributes to the overall interpretability of thelanguage model within the simulation environment.。We list the details in Table 7.C.。Policy NetworkFigure 5 is the network architecture used for non-goal RL, corresponding to thepolicy π.。On top of observation preprocessing in Appendix A, we introduce abackbone implemented with a fully-connected layer followed by three Residual Blocks.。As for the policy head and threevalue heads, we implemented each of them as two connected Residual Blocks.。It is noteworthy that the invisible enemyinformation, such as the nearest enemy’s location, has also been introduced as an input to the value estimation, for theconsideration of stabilizing the policy learning (Vinyals et al., 2019).Figure 5: Network structure of our proposed policy.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationFeature Weight Descriptionenemy discovery 0.02 reward for see an enemydetected by enemy -0.002 punishment for being seen by an enemyscout 0.0001 reward for search for an enemyno-op -0.0002 punishment for stopping and doing nothingbullet 0.015 reward for using and refilling bulletshealth point 0.03 reward for health point changesbe knocked down -2.5 punishment for being knocked downdead -3.5 punishment for being killeddamage enemy 0.1 reward for damaging an enemyknock down enemy 4.5 reward for knocking down an enemykill enemy 3.5 reward for killing an enemyapproach a downed teammate 0.001 reward for approaching a downed teammatehelp a downed teammate up 0.8 reward for helping up a downed teammatenot save a downed teammate -0.5 punishment for not saving a downed teammatego to blue circle 0.00015 reward for going to blue circlebe in white circle -0.00005 small punishment for being outside of white circleoutside blue circle -0.012 punishment for being outside of blue circleteammate damage enemy 0.03 reward from teammate damaging enemiesteammate get up 0.6 reward from teammate getting upI help teammate up 4 reward for helping teammate upinterrupt helping teammate up -0.05 punishment for the interruption to help teammate upobstacle avoidance 0.012 punishment for being stuckgoal 1 reward of goal completionTable 5: The introduction of different rewards.Date Iteration #params Change4/14/2023 1 1802702 Experiment started4/27/2023 1808552 1802702 Env-init: Random weapons5/8/2023 2829170 1803087 Action: Add a fire action for long distance5/10/2023 3034011 1803087 Env-init:Random safe area in the whole map5/11/2023 3130353 1803855 Observation: Add number of remaining players in the game5/12/2023 3198564 2412975 Observation: Add BEV feature5/16/2023 3673506 2418111 Observation: Add history rotation feature5/22/2023 4519567 2418368 Observation: Add rotation change feature5/29/2023 5442025 2418368 Reward: Add rewards for teamwork6/2/2023 5899503 2418368 Update new game version6/13/2023 7306607 3013409 Network: Add obstacle avoidance reward and corresponding value head6/14/2023 7404118 3015457 Observation: Add distance feature to nearby obstacles6/16/2023 7628098 3015457 Env-init: Player numbers per team increased to 46/19/2023 7974450 3109267 Action: Use attention to select target to attackTable 8: The major changes in the training procedure.Goal Processing.。To construct the goal-conditioned policy πgon top of π, we need toencode a goal gthat is generated from Grnd,GopandGllm.。Thus, we propose a network Fas anencoder to achieve that.。In practice, to improvethe representation, we further include other inputsbesides gas (1) goal_mode_info : a 10-dimensionalvector that indicates whether current learning isgoal-conditioned, the achieved times of g, andthe rest time to complete this goal (as we presetthe maximum timestep for goal completion is30s), short in vinfo; (2) sub_goal_masking : a68-dimensional 0-1 vector that indicates whichsub-goals are masked for their corresponding values are 0, short in vmask ; (3) expected_goal_of_current_state : namely, forstatt, we generate a goal g′=PROJ(st)which is a 68-dimensional vector as the same as g, short in g′.。Then, we form theinput for Fasx= (g, g′, vinfo, vmask).。Considering that each item in xis heterogeneous on the data type, so we transformthem with independent FC layers and then follows a ResNet block for each of them.。We can express this process aseg=RESNET(FC(g)), e g′=RESNET(FC(g′)), e info=RESNET(FC(vinfo)), e mask =RESNET(FC(vmask)),where eg,eg′,einfo,emask are the embedding representation corresponding to each input item, and all of them are the samein dimension size.. \n",
      "the piece 305\n",
      " \n",
      "s(∆ t)−sin(∆ t)sin(∆ t) cos(∆ t)\u0015d⋆t−17:else if at∈{stop; rescue; stop and fire; stop adjust and fire } then8:d⋆t←\u00140\u00159:else10: d⋆t←d⋆t−111:end ifmodels have limitations in comprehending and manipulating numerical values effectively.。To address this challenge,a deliberate effort is made to minimize the use of numerical values during the environmental linguistics process.Sub Action Space Dim Sizemovement direction 16yaw direction 16pitch direction 3body action 9basic action 7switch weapon action 3Table 6: Action space.For example, instead of specifying an agent’s speed with specific numeric metricslike “speed: 1m/s →3m/s” a qualitative representation such as “speed: slower →faster” is adopted.。This technique transforms the original continuous state into alimited, discrete meta-state, thereby enhancing the language model’s understanding.Similarly, for expediting language model understanding, a discrete action space isadopted, with each action accompanied by a succinct artificial language description.This discreet articulation of actions contributes to the overall interpretability of thelanguage model within the simulation environment.。We list the details in Table 7.C.。Policy NetworkFigure 5 is the network architecture used for non-goal RL, corresponding to thepolicy π.。On top of observation preprocessing in Appendix A, we introduce abackbone implemented with a fully-connected layer followed by three Residual Blocks.。As for the policy head and threevalue heads, we implemented each of them as two connected Residual Blocks.。It is noteworthy that the invisible enemyinformation, such as the nearest enemy’s location, has also been introduced as an input to the value estimation, for theconsideration of stabilizing the policy learning (Vinyals et al., 2019).Figure 5: Network structure of our proposed policy.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationFeature Weight Descriptionenemy discovery 0.02 reward for see an enemydetected by enemy -0.002 punishment for being seen by an enemyscout 0.0001 reward for search for an enemyno-op -0.0002 punishment for stopping and doing nothingbullet 0.015 reward for using and refilling bulletshealth point 0.03 reward for health point changesbe knocked down -2.5 punishment for being knocked downdead -3.5 punishment for being killeddamage enemy 0.1 reward for damaging an enemyknock down enemy 4.5 reward for knocking down an enemykill enemy 3.5 reward for killing an enemyapproach a downed teammate 0.001 reward for approaching a downed teammatehelp a downed teammate up 0.8 reward for helping up a downed teammatenot save a downed teammate -0.5 punishment for not saving a downed teammatego to blue circle 0.00015 reward for going to blue circlebe in white circle -0.00005 small punishment for being outside of white circleoutside blue circle -0.012 punishment for being outside of blue circleteammate damage enemy 0.03 reward from teammate damaging enemiesteammate get up 0.6 reward from teammate getting upI help teammate up 4 reward for helping teammate upinterrupt helping teammate up -0.05 punishment for the interruption to help teammate upobstacle avoidance 0.012 punishment for being stuckgoal 1 reward of goal completionTable 5: The introduction of different rewards.Date Iteration #params Change4/14/2023 1 1802702 Experiment started4/27/2023 1808552 1802702 Env-init: Random weapons5/8/2023 2829170 1803087 Action: Add a fire action for long distance5/10/2023 3034011 1803087 Env-init:Random safe area in the whole map5/11/2023 3130353 1803855 Observation: Add number of remaining players in the game5/12/2023 3198564 2412975 Observation: Add BEV feature5/16/2023 3673506 2418111 Observation: Add history rotation feature5/22/2023 4519567 2418368 Observation: Add rotation change feature5/29/2023 5442025 2418368 Reward: Add rewards for teamwork6/2/2023 5899503 2418368 Update new game version6/13/2023 7306607 3013409 Network: Add obstacle avoidance reward and corresponding value head6/14/2023 7404118 3015457 Observation: Add distance feature to nearby obstacles6/16/2023 7628098 3015457 Env-init: Player numbers per team increased to 46/19/2023 7974450 3109267 Action: Use attention to select target to attackTable 8: The major changes in the training procedure.Goal Processing.。To construct the goal-conditioned policy πgon top of π, we need toencode a goal gthat is generated from Grnd,GopandGllm.。Thus, we propose a network Fas anencoder to achieve that.。In practice, to improvethe representation, we further include other inputsbesides gas (1) goal_mode_info : a 10-dimensionalvector that indicates whether current learning isgoal-conditioned, the achieved times of g, andthe rest time to complete this goal (as we presetthe maximum timestep for goal completion is30s), short in vinfo; (2) sub_goal_masking : a68-dimensional 0-1 vector that indicates whichsub-goals are masked for their corresponding values are 0, short in vmask ; (3) expected_goal_of_current_state : namely, forstatt, we generate a goal g′=PROJ(st)which is a 68-dimensional vector as the same as g, short in g′.。Then, we form theinput for Fasx= (g, g′, vinfo, vmask).。Considering that each item in xis heterogeneous on the data type, so we transformthem with independent FC layers and then follows a ResNet block for each of them.。We can express this process aseg=RESNET(FC(g)), e g′=RESNET(FC(g′)), e info=RESNET(FC(vinfo)), e mask =RESNET(FC(vmask)),where eg,eg′,einfo,emask are the embedding representation corresponding to each input item, and all of them are the samein dimension size.。Then, we aggregate them with computing the average of them as to get a fusion embedding asefusion = (eg+eg′+einfo+emask)/4.With surgery, we now concatenate the embedding of backbone of πwithefusion and fusing them via a FC layer, to form abackbone for πg.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub-goal Class CandidatesDamage to enemy [Zero,Low,Little low,Medium,Little high,High]Whether knock down enemy [True,False]Whether kill enemy [True,False]Whether seen enemy [True,False]Whether seen by enemy [True,False]Number of enemies have ever seen [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]Length of distance moved [No movement,Short,Medium,Long,Very long]Average velocity [Static,Slow,Medium,Fast,Falling]Horizontal direction of movement [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Horizontal direction of view [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Pitch direction of view [Low,Little low,Medium,Little high,High]Health level [Empty,Low,Medium,High,Full]Whether to restore health [True,False]Whether the health is damaged [True,False]Whether rescued teammate [True,False]Whether be knocked down [True,False]Whether prone position [True,False]Whether have a gun [True,False]Whether have bullets [True,False]Whether have medical kits [True,False]Distance with nearest enemy [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether closer with nearest enemy [True,False]Whether crouch position [True,False]Whether hold a gun [True,False]Length of distance from agent to teammate [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether seen by teammate [True,False]Teammate’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Whether follow with the views of teammate [True,False]Whether target the same enemy as teammate [True,False]Whether follow with the movement direction of teammate [True,False]Horizontal direction of movement of enemy [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Velocity of enemy [Static,Slow,Medium,Fast,Falling,None]Enemy’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Table 7: Overview of sub-goal classes, we show a part of them here.D.. \n",
      "the piece 306\n",
      " \n",
      "stead of specifying an agent’s speed with specific numeric metricslike “speed: 1m/s →3m/s” a qualitative representation such as “speed: slower →faster” is adopted.。This technique transforms the original continuous state into alimited, discrete meta-state, thereby enhancing the language model’s understanding.Similarly, for expediting language model understanding, a discrete action space isadopted, with each action accompanied by a succinct artificial language description.This discreet articulation of actions contributes to the overall interpretability of thelanguage model within the simulation environment.。We list the details in Table 7.C.。Policy NetworkFigure 5 is the network architecture used for non-goal RL, corresponding to thepolicy π.。On top of observation preprocessing in Appendix A, we introduce abackbone implemented with a fully-connected layer followed by three Residual Blocks.。As for the policy head and threevalue heads, we implemented each of them as two connected Residual Blocks.。It is noteworthy that the invisible enemyinformation, such as the nearest enemy’s location, has also been introduced as an input to the value estimation, for theconsideration of stabilizing the policy learning (Vinyals et al., 2019).Figure 5: Network structure of our proposed policy.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationFeature Weight Descriptionenemy discovery 0.02 reward for see an enemydetected by enemy -0.002 punishment for being seen by an enemyscout 0.0001 reward for search for an enemyno-op -0.0002 punishment for stopping and doing nothingbullet 0.015 reward for using and refilling bulletshealth point 0.03 reward for health point changesbe knocked down -2.5 punishment for being knocked downdead -3.5 punishment for being killeddamage enemy 0.1 reward for damaging an enemyknock down enemy 4.5 reward for knocking down an enemykill enemy 3.5 reward for killing an enemyapproach a downed teammate 0.001 reward for approaching a downed teammatehelp a downed teammate up 0.8 reward for helping up a downed teammatenot save a downed teammate -0.5 punishment for not saving a downed teammatego to blue circle 0.00015 reward for going to blue circlebe in white circle -0.00005 small punishment for being outside of white circleoutside blue circle -0.012 punishment for being outside of blue circleteammate damage enemy 0.03 reward from teammate damaging enemiesteammate get up 0.6 reward from teammate getting upI help teammate up 4 reward for helping teammate upinterrupt helping teammate up -0.05 punishment for the interruption to help teammate upobstacle avoidance 0.012 punishment for being stuckgoal 1 reward of goal completionTable 5: The introduction of different rewards.Date Iteration #params Change4/14/2023 1 1802702 Experiment started4/27/2023 1808552 1802702 Env-init: Random weapons5/8/2023 2829170 1803087 Action: Add a fire action for long distance5/10/2023 3034011 1803087 Env-init:Random safe area in the whole map5/11/2023 3130353 1803855 Observation: Add number of remaining players in the game5/12/2023 3198564 2412975 Observation: Add BEV feature5/16/2023 3673506 2418111 Observation: Add history rotation feature5/22/2023 4519567 2418368 Observation: Add rotation change feature5/29/2023 5442025 2418368 Reward: Add rewards for teamwork6/2/2023 5899503 2418368 Update new game version6/13/2023 7306607 3013409 Network: Add obstacle avoidance reward and corresponding value head6/14/2023 7404118 3015457 Observation: Add distance feature to nearby obstacles6/16/2023 7628098 3015457 Env-init: Player numbers per team increased to 46/19/2023 7974450 3109267 Action: Use attention to select target to attackTable 8: The major changes in the training procedure.Goal Processing.。To construct the goal-conditioned policy πgon top of π, we need toencode a goal gthat is generated from Grnd,GopandGllm.。Thus, we propose a network Fas anencoder to achieve that.。In practice, to improvethe representation, we further include other inputsbesides gas (1) goal_mode_info : a 10-dimensionalvector that indicates whether current learning isgoal-conditioned, the achieved times of g, andthe rest time to complete this goal (as we presetthe maximum timestep for goal completion is30s), short in vinfo; (2) sub_goal_masking : a68-dimensional 0-1 vector that indicates whichsub-goals are masked for their corresponding values are 0, short in vmask ; (3) expected_goal_of_current_state : namely, forstatt, we generate a goal g′=PROJ(st)which is a 68-dimensional vector as the same as g, short in g′.。Then, we form theinput for Fasx= (g, g′, vinfo, vmask).。Considering that each item in xis heterogeneous on the data type, so we transformthem with independent FC layers and then follows a ResNet block for each of them.。We can express this process aseg=RESNET(FC(g)), e g′=RESNET(FC(g′)), e info=RESNET(FC(vinfo)), e mask =RESNET(FC(vmask)),where eg,eg′,einfo,emask are the embedding representation corresponding to each input item, and all of them are the samein dimension size.。Then, we aggregate them with computing the average of them as to get a fusion embedding asefusion = (eg+eg′+einfo+emask)/4.With surgery, we now concatenate the embedding of backbone of πwithefusion and fusing them via a FC layer, to form abackbone for πg.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub-goal Class CandidatesDamage to enemy [Zero,Low,Little low,Medium,Little high,High]Whether knock down enemy [True,False]Whether kill enemy [True,False]Whether seen enemy [True,False]Whether seen by enemy [True,False]Number of enemies have ever seen [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]Length of distance moved [No movement,Short,Medium,Long,Very long]Average velocity [Static,Slow,Medium,Fast,Falling]Horizontal direction of movement [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Horizontal direction of view [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Pitch direction of view [Low,Little low,Medium,Little high,High]Health level [Empty,Low,Medium,High,Full]Whether to restore health [True,False]Whether the health is damaged [True,False]Whether rescued teammate [True,False]Whether be knocked down [True,False]Whether prone position [True,False]Whether have a gun [True,False]Whether have bullets [True,False]Whether have medical kits [True,False]Distance with nearest enemy [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether closer with nearest enemy [True,False]Whether crouch position [True,False]Whether hold a gun [True,False]Length of distance from agent to teammate [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether seen by teammate [True,False]Teammate’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Whether follow with the views of teammate [True,False]Whether target the same enemy as teammate [True,False]Whether follow with the movement direction of teammate [True,False]Horizontal direction of movement of enemy [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Velocity of enemy [Static,Slow,Medium,Fast,Falling,None]Enemy’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Table 7: Overview of sub-goal classes, we show a part of them here.D.。SurgeryAs the project proceeded, Contra was continuously improved to satisfy richer featured environment dynamics.. \n",
      "the piece 307\n",
      " \n",
      "ption.This discreet articulation of actions contributes to the overall interpretability of thelanguage model within the simulation environment.。We list the details in Table 7.C.。Policy NetworkFigure 5 is the network architecture used for non-goal RL, corresponding to thepolicy π.。On top of observation preprocessing in Appendix A, we introduce abackbone implemented with a fully-connected layer followed by three Residual Blocks.。As for the policy head and threevalue heads, we implemented each of them as two connected Residual Blocks.。It is noteworthy that the invisible enemyinformation, such as the nearest enemy’s location, has also been introduced as an input to the value estimation, for theconsideration of stabilizing the policy learning (Vinyals et al., 2019).Figure 5: Network structure of our proposed policy.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationFeature Weight Descriptionenemy discovery 0.02 reward for see an enemydetected by enemy -0.002 punishment for being seen by an enemyscout 0.0001 reward for search for an enemyno-op -0.0002 punishment for stopping and doing nothingbullet 0.015 reward for using and refilling bulletshealth point 0.03 reward for health point changesbe knocked down -2.5 punishment for being knocked downdead -3.5 punishment for being killeddamage enemy 0.1 reward for damaging an enemyknock down enemy 4.5 reward for knocking down an enemykill enemy 3.5 reward for killing an enemyapproach a downed teammate 0.001 reward for approaching a downed teammatehelp a downed teammate up 0.8 reward for helping up a downed teammatenot save a downed teammate -0.5 punishment for not saving a downed teammatego to blue circle 0.00015 reward for going to blue circlebe in white circle -0.00005 small punishment for being outside of white circleoutside blue circle -0.012 punishment for being outside of blue circleteammate damage enemy 0.03 reward from teammate damaging enemiesteammate get up 0.6 reward from teammate getting upI help teammate up 4 reward for helping teammate upinterrupt helping teammate up -0.05 punishment for the interruption to help teammate upobstacle avoidance 0.012 punishment for being stuckgoal 1 reward of goal completionTable 5: The introduction of different rewards.Date Iteration #params Change4/14/2023 1 1802702 Experiment started4/27/2023 1808552 1802702 Env-init: Random weapons5/8/2023 2829170 1803087 Action: Add a fire action for long distance5/10/2023 3034011 1803087 Env-init:Random safe area in the whole map5/11/2023 3130353 1803855 Observation: Add number of remaining players in the game5/12/2023 3198564 2412975 Observation: Add BEV feature5/16/2023 3673506 2418111 Observation: Add history rotation feature5/22/2023 4519567 2418368 Observation: Add rotation change feature5/29/2023 5442025 2418368 Reward: Add rewards for teamwork6/2/2023 5899503 2418368 Update new game version6/13/2023 7306607 3013409 Network: Add obstacle avoidance reward and corresponding value head6/14/2023 7404118 3015457 Observation: Add distance feature to nearby obstacles6/16/2023 7628098 3015457 Env-init: Player numbers per team increased to 46/19/2023 7974450 3109267 Action: Use attention to select target to attackTable 8: The major changes in the training procedure.Goal Processing.。To construct the goal-conditioned policy πgon top of π, we need toencode a goal gthat is generated from Grnd,GopandGllm.。Thus, we propose a network Fas anencoder to achieve that.。In practice, to improvethe representation, we further include other inputsbesides gas (1) goal_mode_info : a 10-dimensionalvector that indicates whether current learning isgoal-conditioned, the achieved times of g, andthe rest time to complete this goal (as we presetthe maximum timestep for goal completion is30s), short in vinfo; (2) sub_goal_masking : a68-dimensional 0-1 vector that indicates whichsub-goals are masked for their corresponding values are 0, short in vmask ; (3) expected_goal_of_current_state : namely, forstatt, we generate a goal g′=PROJ(st)which is a 68-dimensional vector as the same as g, short in g′.。Then, we form theinput for Fasx= (g, g′, vinfo, vmask).。Considering that each item in xis heterogeneous on the data type, so we transformthem with independent FC layers and then follows a ResNet block for each of them.。We can express this process aseg=RESNET(FC(g)), e g′=RESNET(FC(g′)), e info=RESNET(FC(vinfo)), e mask =RESNET(FC(vmask)),where eg,eg′,einfo,emask are the embedding representation corresponding to each input item, and all of them are the samein dimension size.。Then, we aggregate them with computing the average of them as to get a fusion embedding asefusion = (eg+eg′+einfo+emask)/4.With surgery, we now concatenate the embedding of backbone of πwithefusion and fusing them via a FC layer, to form abackbone for πg.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub-goal Class CandidatesDamage to enemy [Zero,Low,Little low,Medium,Little high,High]Whether knock down enemy [True,False]Whether kill enemy [True,False]Whether seen enemy [True,False]Whether seen by enemy [True,False]Number of enemies have ever seen [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]Length of distance moved [No movement,Short,Medium,Long,Very long]Average velocity [Static,Slow,Medium,Fast,Falling]Horizontal direction of movement [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Horizontal direction of view [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Pitch direction of view [Low,Little low,Medium,Little high,High]Health level [Empty,Low,Medium,High,Full]Whether to restore health [True,False]Whether the health is damaged [True,False]Whether rescued teammate [True,False]Whether be knocked down [True,False]Whether prone position [True,False]Whether have a gun [True,False]Whether have bullets [True,False]Whether have medical kits [True,False]Distance with nearest enemy [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether closer with nearest enemy [True,False]Whether crouch position [True,False]Whether hold a gun [True,False]Length of distance from agent to teammate [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether seen by teammate [True,False]Teammate’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Whether follow with the views of teammate [True,False]Whether target the same enemy as teammate [True,False]Whether follow with the movement direction of teammate [True,False]Horizontal direction of movement of enemy [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Velocity of enemy [Static,Slow,Medium,Fast,Falling,None]Enemy’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Table 7: Overview of sub-goal classes, we show a part of them here.D.。SurgeryAs the project proceeded, Contra was continuously improved to satisfy richer featured environment dynamics.。However,such an iterative development poses some challenges to the research of open-ended learning in an embodied situation, asthe changes in API and environment attributes will make the training be non-stationary.. \n",
      "the piece 308\n",
      " \n",
      "ead and threevalue heads, we implemented each of them as two connected Residual Blocks.。It is noteworthy that the invisible enemyinformation, such as the nearest enemy’s location, has also been introduced as an input to the value estimation, for theconsideration of stabilizing the policy learning (Vinyals et al., 2019).Figure 5: Network structure of our proposed policy.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationFeature Weight Descriptionenemy discovery 0.02 reward for see an enemydetected by enemy -0.002 punishment for being seen by an enemyscout 0.0001 reward for search for an enemyno-op -0.0002 punishment for stopping and doing nothingbullet 0.015 reward for using and refilling bulletshealth point 0.03 reward for health point changesbe knocked down -2.5 punishment for being knocked downdead -3.5 punishment for being killeddamage enemy 0.1 reward for damaging an enemyknock down enemy 4.5 reward for knocking down an enemykill enemy 3.5 reward for killing an enemyapproach a downed teammate 0.001 reward for approaching a downed teammatehelp a downed teammate up 0.8 reward for helping up a downed teammatenot save a downed teammate -0.5 punishment for not saving a downed teammatego to blue circle 0.00015 reward for going to blue circlebe in white circle -0.00005 small punishment for being outside of white circleoutside blue circle -0.012 punishment for being outside of blue circleteammate damage enemy 0.03 reward from teammate damaging enemiesteammate get up 0.6 reward from teammate getting upI help teammate up 4 reward for helping teammate upinterrupt helping teammate up -0.05 punishment for the interruption to help teammate upobstacle avoidance 0.012 punishment for being stuckgoal 1 reward of goal completionTable 5: The introduction of different rewards.Date Iteration #params Change4/14/2023 1 1802702 Experiment started4/27/2023 1808552 1802702 Env-init: Random weapons5/8/2023 2829170 1803087 Action: Add a fire action for long distance5/10/2023 3034011 1803087 Env-init:Random safe area in the whole map5/11/2023 3130353 1803855 Observation: Add number of remaining players in the game5/12/2023 3198564 2412975 Observation: Add BEV feature5/16/2023 3673506 2418111 Observation: Add history rotation feature5/22/2023 4519567 2418368 Observation: Add rotation change feature5/29/2023 5442025 2418368 Reward: Add rewards for teamwork6/2/2023 5899503 2418368 Update new game version6/13/2023 7306607 3013409 Network: Add obstacle avoidance reward and corresponding value head6/14/2023 7404118 3015457 Observation: Add distance feature to nearby obstacles6/16/2023 7628098 3015457 Env-init: Player numbers per team increased to 46/19/2023 7974450 3109267 Action: Use attention to select target to attackTable 8: The major changes in the training procedure.Goal Processing.。To construct the goal-conditioned policy πgon top of π, we need toencode a goal gthat is generated from Grnd,GopandGllm.。Thus, we propose a network Fas anencoder to achieve that.。In practice, to improvethe representation, we further include other inputsbesides gas (1) goal_mode_info : a 10-dimensionalvector that indicates whether current learning isgoal-conditioned, the achieved times of g, andthe rest time to complete this goal (as we presetthe maximum timestep for goal completion is30s), short in vinfo; (2) sub_goal_masking : a68-dimensional 0-1 vector that indicates whichsub-goals are masked for their corresponding values are 0, short in vmask ; (3) expected_goal_of_current_state : namely, forstatt, we generate a goal g′=PROJ(st)which is a 68-dimensional vector as the same as g, short in g′.。Then, we form theinput for Fasx= (g, g′, vinfo, vmask).。Considering that each item in xis heterogeneous on the data type, so we transformthem with independent FC layers and then follows a ResNet block for each of them.。We can express this process aseg=RESNET(FC(g)), e g′=RESNET(FC(g′)), e info=RESNET(FC(vinfo)), e mask =RESNET(FC(vmask)),where eg,eg′,einfo,emask are the embedding representation corresponding to each input item, and all of them are the samein dimension size.。Then, we aggregate them with computing the average of them as to get a fusion embedding asefusion = (eg+eg′+einfo+emask)/4.With surgery, we now concatenate the embedding of backbone of πwithefusion and fusing them via a FC layer, to form abackbone for πg.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub-goal Class CandidatesDamage to enemy [Zero,Low,Little low,Medium,Little high,High]Whether knock down enemy [True,False]Whether kill enemy [True,False]Whether seen enemy [True,False]Whether seen by enemy [True,False]Number of enemies have ever seen [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]Length of distance moved [No movement,Short,Medium,Long,Very long]Average velocity [Static,Slow,Medium,Fast,Falling]Horizontal direction of movement [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Horizontal direction of view [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Pitch direction of view [Low,Little low,Medium,Little high,High]Health level [Empty,Low,Medium,High,Full]Whether to restore health [True,False]Whether the health is damaged [True,False]Whether rescued teammate [True,False]Whether be knocked down [True,False]Whether prone position [True,False]Whether have a gun [True,False]Whether have bullets [True,False]Whether have medical kits [True,False]Distance with nearest enemy [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether closer with nearest enemy [True,False]Whether crouch position [True,False]Whether hold a gun [True,False]Length of distance from agent to teammate [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether seen by teammate [True,False]Teammate’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Whether follow with the views of teammate [True,False]Whether target the same enemy as teammate [True,False]Whether follow with the movement direction of teammate [True,False]Horizontal direction of movement of enemy [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Velocity of enemy [Static,Slow,Medium,Fast,Falling,None]Enemy’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Table 7: Overview of sub-goal classes, we show a part of them here.D.。SurgeryAs the project proceeded, Contra was continuously improved to satisfy richer featured environment dynamics.。However,such an iterative development poses some challenges to the research of open-ended learning in an embodied situation, asthe changes in API and environment attributes will make the training be non-stationary.。A popular solution to resolve thisissue is the surgery introduced by Berner et al.. \n",
      "the piece 309\n",
      " \n",
      "ented each of them as two connected Residual Blocks.。It is noteworthy that the invisible enemyinformation, such as the nearest enemy’s location, has also been introduced as an input to the value estimation, for theconsideration of stabilizing the policy learning (Vinyals et al., 2019).Figure 5: Network structure of our proposed policy.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationFeature Weight Descriptionenemy discovery 0.02 reward for see an enemydetected by enemy -0.002 punishment for being seen by an enemyscout 0.0001 reward for search for an enemyno-op -0.0002 punishment for stopping and doing nothingbullet 0.015 reward for using and refilling bulletshealth point 0.03 reward for health point changesbe knocked down -2.5 punishment for being knocked downdead -3.5 punishment for being killeddamage enemy 0.1 reward for damaging an enemyknock down enemy 4.5 reward for knocking down an enemykill enemy 3.5 reward for killing an enemyapproach a downed teammate 0.001 reward for approaching a downed teammatehelp a downed teammate up 0.8 reward for helping up a downed teammatenot save a downed teammate -0.5 punishment for not saving a downed teammatego to blue circle 0.00015 reward for going to blue circlebe in white circle -0.00005 small punishment for being outside of white circleoutside blue circle -0.012 punishment for being outside of blue circleteammate damage enemy 0.03 reward from teammate damaging enemiesteammate get up 0.6 reward from teammate getting upI help teammate up 4 reward for helping teammate upinterrupt helping teammate up -0.05 punishment for the interruption to help teammate upobstacle avoidance 0.012 punishment for being stuckgoal 1 reward of goal completionTable 5: The introduction of different rewards.Date Iteration #params Change4/14/2023 1 1802702 Experiment started4/27/2023 1808552 1802702 Env-init: Random weapons5/8/2023 2829170 1803087 Action: Add a fire action for long distance5/10/2023 3034011 1803087 Env-init:Random safe area in the whole map5/11/2023 3130353 1803855 Observation: Add number of remaining players in the game5/12/2023 3198564 2412975 Observation: Add BEV feature5/16/2023 3673506 2418111 Observation: Add history rotation feature5/22/2023 4519567 2418368 Observation: Add rotation change feature5/29/2023 5442025 2418368 Reward: Add rewards for teamwork6/2/2023 5899503 2418368 Update new game version6/13/2023 7306607 3013409 Network: Add obstacle avoidance reward and corresponding value head6/14/2023 7404118 3015457 Observation: Add distance feature to nearby obstacles6/16/2023 7628098 3015457 Env-init: Player numbers per team increased to 46/19/2023 7974450 3109267 Action: Use attention to select target to attackTable 8: The major changes in the training procedure.Goal Processing.。To construct the goal-conditioned policy πgon top of π, we need toencode a goal gthat is generated from Grnd,GopandGllm.。Thus, we propose a network Fas anencoder to achieve that.。In practice, to improvethe representation, we further include other inputsbesides gas (1) goal_mode_info : a 10-dimensionalvector that indicates whether current learning isgoal-conditioned, the achieved times of g, andthe rest time to complete this goal (as we presetthe maximum timestep for goal completion is30s), short in vinfo; (2) sub_goal_masking : a68-dimensional 0-1 vector that indicates whichsub-goals are masked for their corresponding values are 0, short in vmask ; (3) expected_goal_of_current_state : namely, forstatt, we generate a goal g′=PROJ(st)which is a 68-dimensional vector as the same as g, short in g′.。Then, we form theinput for Fasx= (g, g′, vinfo, vmask).。Considering that each item in xis heterogeneous on the data type, so we transformthem with independent FC layers and then follows a ResNet block for each of them.。We can express this process aseg=RESNET(FC(g)), e g′=RESNET(FC(g′)), e info=RESNET(FC(vinfo)), e mask =RESNET(FC(vmask)),where eg,eg′,einfo,emask are the embedding representation corresponding to each input item, and all of them are the samein dimension size.。Then, we aggregate them with computing the average of them as to get a fusion embedding asefusion = (eg+eg′+einfo+emask)/4.With surgery, we now concatenate the embedding of backbone of πwithefusion and fusing them via a FC layer, to form abackbone for πg.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub-goal Class CandidatesDamage to enemy [Zero,Low,Little low,Medium,Little high,High]Whether knock down enemy [True,False]Whether kill enemy [True,False]Whether seen enemy [True,False]Whether seen by enemy [True,False]Number of enemies have ever seen [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]Length of distance moved [No movement,Short,Medium,Long,Very long]Average velocity [Static,Slow,Medium,Fast,Falling]Horizontal direction of movement [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Horizontal direction of view [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Pitch direction of view [Low,Little low,Medium,Little high,High]Health level [Empty,Low,Medium,High,Full]Whether to restore health [True,False]Whether the health is damaged [True,False]Whether rescued teammate [True,False]Whether be knocked down [True,False]Whether prone position [True,False]Whether have a gun [True,False]Whether have bullets [True,False]Whether have medical kits [True,False]Distance with nearest enemy [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether closer with nearest enemy [True,False]Whether crouch position [True,False]Whether hold a gun [True,False]Length of distance from agent to teammate [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether seen by teammate [True,False]Teammate’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Whether follow with the views of teammate [True,False]Whether target the same enemy as teammate [True,False]Whether follow with the movement direction of teammate [True,False]Horizontal direction of movement of enemy [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Velocity of enemy [Static,Slow,Medium,Fast,Falling,None]Enemy’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Table 7: Overview of sub-goal classes, we show a part of them here.D.。SurgeryAs the project proceeded, Contra was continuously improved to satisfy richer featured environment dynamics.。However,such an iterative development poses some challenges to the research of open-ended learning in an embodied situation, asthe changes in API and environment attributes will make the training be non-stationary.。A popular solution to resolve thisissue is the surgery introduced by Berner et al.。(2019), which significantly reduces training time by maximizing retention ofpreviously learned abilities.. \n",
      "the piece 310\n",
      " \n",
      "n, such as the nearest enemy’s location, has also been introduced as an input to the value estimation, for theconsideration of stabilizing the policy learning (Vinyals et al., 2019).Figure 5: Network structure of our proposed policy.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationFeature Weight Descriptionenemy discovery 0.02 reward for see an enemydetected by enemy -0.002 punishment for being seen by an enemyscout 0.0001 reward for search for an enemyno-op -0.0002 punishment for stopping and doing nothingbullet 0.015 reward for using and refilling bulletshealth point 0.03 reward for health point changesbe knocked down -2.5 punishment for being knocked downdead -3.5 punishment for being killeddamage enemy 0.1 reward for damaging an enemyknock down enemy 4.5 reward for knocking down an enemykill enemy 3.5 reward for killing an enemyapproach a downed teammate 0.001 reward for approaching a downed teammatehelp a downed teammate up 0.8 reward for helping up a downed teammatenot save a downed teammate -0.5 punishment for not saving a downed teammatego to blue circle 0.00015 reward for going to blue circlebe in white circle -0.00005 small punishment for being outside of white circleoutside blue circle -0.012 punishment for being outside of blue circleteammate damage enemy 0.03 reward from teammate damaging enemiesteammate get up 0.6 reward from teammate getting upI help teammate up 4 reward for helping teammate upinterrupt helping teammate up -0.05 punishment for the interruption to help teammate upobstacle avoidance 0.012 punishment for being stuckgoal 1 reward of goal completionTable 5: The introduction of different rewards.Date Iteration #params Change4/14/2023 1 1802702 Experiment started4/27/2023 1808552 1802702 Env-init: Random weapons5/8/2023 2829170 1803087 Action: Add a fire action for long distance5/10/2023 3034011 1803087 Env-init:Random safe area in the whole map5/11/2023 3130353 1803855 Observation: Add number of remaining players in the game5/12/2023 3198564 2412975 Observation: Add BEV feature5/16/2023 3673506 2418111 Observation: Add history rotation feature5/22/2023 4519567 2418368 Observation: Add rotation change feature5/29/2023 5442025 2418368 Reward: Add rewards for teamwork6/2/2023 5899503 2418368 Update new game version6/13/2023 7306607 3013409 Network: Add obstacle avoidance reward and corresponding value head6/14/2023 7404118 3015457 Observation: Add distance feature to nearby obstacles6/16/2023 7628098 3015457 Env-init: Player numbers per team increased to 46/19/2023 7974450 3109267 Action: Use attention to select target to attackTable 8: The major changes in the training procedure.Goal Processing.。To construct the goal-conditioned policy πgon top of π, we need toencode a goal gthat is generated from Grnd,GopandGllm.。Thus, we propose a network Fas anencoder to achieve that.。In practice, to improvethe representation, we further include other inputsbesides gas (1) goal_mode_info : a 10-dimensionalvector that indicates whether current learning isgoal-conditioned, the achieved times of g, andthe rest time to complete this goal (as we presetthe maximum timestep for goal completion is30s), short in vinfo; (2) sub_goal_masking : a68-dimensional 0-1 vector that indicates whichsub-goals are masked for their corresponding values are 0, short in vmask ; (3) expected_goal_of_current_state : namely, forstatt, we generate a goal g′=PROJ(st)which is a 68-dimensional vector as the same as g, short in g′.。Then, we form theinput for Fasx= (g, g′, vinfo, vmask).。Considering that each item in xis heterogeneous on the data type, so we transformthem with independent FC layers and then follows a ResNet block for each of them.。We can express this process aseg=RESNET(FC(g)), e g′=RESNET(FC(g′)), e info=RESNET(FC(vinfo)), e mask =RESNET(FC(vmask)),where eg,eg′,einfo,emask are the embedding representation corresponding to each input item, and all of them are the samein dimension size.。Then, we aggregate them with computing the average of them as to get a fusion embedding asefusion = (eg+eg′+einfo+emask)/4.With surgery, we now concatenate the embedding of backbone of πwithefusion and fusing them via a FC layer, to form abackbone for πg.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub-goal Class CandidatesDamage to enemy [Zero,Low,Little low,Medium,Little high,High]Whether knock down enemy [True,False]Whether kill enemy [True,False]Whether seen enemy [True,False]Whether seen by enemy [True,False]Number of enemies have ever seen [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]Length of distance moved [No movement,Short,Medium,Long,Very long]Average velocity [Static,Slow,Medium,Fast,Falling]Horizontal direction of movement [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Horizontal direction of view [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Pitch direction of view [Low,Little low,Medium,Little high,High]Health level [Empty,Low,Medium,High,Full]Whether to restore health [True,False]Whether the health is damaged [True,False]Whether rescued teammate [True,False]Whether be knocked down [True,False]Whether prone position [True,False]Whether have a gun [True,False]Whether have bullets [True,False]Whether have medical kits [True,False]Distance with nearest enemy [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether closer with nearest enemy [True,False]Whether crouch position [True,False]Whether hold a gun [True,False]Length of distance from agent to teammate [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether seen by teammate [True,False]Teammate’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Whether follow with the views of teammate [True,False]Whether target the same enemy as teammate [True,False]Whether follow with the movement direction of teammate [True,False]Horizontal direction of movement of enemy [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Velocity of enemy [Static,Slow,Medium,Fast,Falling,None]Enemy’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Table 7: Overview of sub-goal classes, we show a part of them here.D.。SurgeryAs the project proceeded, Contra was continuously improved to satisfy richer featured environment dynamics.。However,such an iterative development poses some challenges to the research of open-ended learning in an embodied situation, asthe changes in API and environment attributes will make the training be non-stationary.。A popular solution to resolve thisissue is the surgery introduced by Berner et al.。(2019), which significantly reduces training time by maximizing retention ofpreviously learned abilities.。Similarly, we leverage surgery in four aspects to ensure the training adapts to the new changes,including model architecture, observation space, action space, and reward functions.. \n",
      "the piece 311\n",
      " \n",
      "earning (Vinyals et al., 2019).Figure 5: Network structure of our proposed policy.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationFeature Weight Descriptionenemy discovery 0.02 reward for see an enemydetected by enemy -0.002 punishment for being seen by an enemyscout 0.0001 reward for search for an enemyno-op -0.0002 punishment for stopping and doing nothingbullet 0.015 reward for using and refilling bulletshealth point 0.03 reward for health point changesbe knocked down -2.5 punishment for being knocked downdead -3.5 punishment for being killeddamage enemy 0.1 reward for damaging an enemyknock down enemy 4.5 reward for knocking down an enemykill enemy 3.5 reward for killing an enemyapproach a downed teammate 0.001 reward for approaching a downed teammatehelp a downed teammate up 0.8 reward for helping up a downed teammatenot save a downed teammate -0.5 punishment for not saving a downed teammatego to blue circle 0.00015 reward for going to blue circlebe in white circle -0.00005 small punishment for being outside of white circleoutside blue circle -0.012 punishment for being outside of blue circleteammate damage enemy 0.03 reward from teammate damaging enemiesteammate get up 0.6 reward from teammate getting upI help teammate up 4 reward for helping teammate upinterrupt helping teammate up -0.05 punishment for the interruption to help teammate upobstacle avoidance 0.012 punishment for being stuckgoal 1 reward of goal completionTable 5: The introduction of different rewards.Date Iteration #params Change4/14/2023 1 1802702 Experiment started4/27/2023 1808552 1802702 Env-init: Random weapons5/8/2023 2829170 1803087 Action: Add a fire action for long distance5/10/2023 3034011 1803087 Env-init:Random safe area in the whole map5/11/2023 3130353 1803855 Observation: Add number of remaining players in the game5/12/2023 3198564 2412975 Observation: Add BEV feature5/16/2023 3673506 2418111 Observation: Add history rotation feature5/22/2023 4519567 2418368 Observation: Add rotation change feature5/29/2023 5442025 2418368 Reward: Add rewards for teamwork6/2/2023 5899503 2418368 Update new game version6/13/2023 7306607 3013409 Network: Add obstacle avoidance reward and corresponding value head6/14/2023 7404118 3015457 Observation: Add distance feature to nearby obstacles6/16/2023 7628098 3015457 Env-init: Player numbers per team increased to 46/19/2023 7974450 3109267 Action: Use attention to select target to attackTable 8: The major changes in the training procedure.Goal Processing.。To construct the goal-conditioned policy πgon top of π, we need toencode a goal gthat is generated from Grnd,GopandGllm.。Thus, we propose a network Fas anencoder to achieve that.。In practice, to improvethe representation, we further include other inputsbesides gas (1) goal_mode_info : a 10-dimensionalvector that indicates whether current learning isgoal-conditioned, the achieved times of g, andthe rest time to complete this goal (as we presetthe maximum timestep for goal completion is30s), short in vinfo; (2) sub_goal_masking : a68-dimensional 0-1 vector that indicates whichsub-goals are masked for their corresponding values are 0, short in vmask ; (3) expected_goal_of_current_state : namely, forstatt, we generate a goal g′=PROJ(st)which is a 68-dimensional vector as the same as g, short in g′.。Then, we form theinput for Fasx= (g, g′, vinfo, vmask).。Considering that each item in xis heterogeneous on the data type, so we transformthem with independent FC layers and then follows a ResNet block for each of them.。We can express this process aseg=RESNET(FC(g)), e g′=RESNET(FC(g′)), e info=RESNET(FC(vinfo)), e mask =RESNET(FC(vmask)),where eg,eg′,einfo,emask are the embedding representation corresponding to each input item, and all of them are the samein dimension size.。Then, we aggregate them with computing the average of them as to get a fusion embedding asefusion = (eg+eg′+einfo+emask)/4.With surgery, we now concatenate the embedding of backbone of πwithefusion and fusing them via a FC layer, to form abackbone for πg.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub-goal Class CandidatesDamage to enemy [Zero,Low,Little low,Medium,Little high,High]Whether knock down enemy [True,False]Whether kill enemy [True,False]Whether seen enemy [True,False]Whether seen by enemy [True,False]Number of enemies have ever seen [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]Length of distance moved [No movement,Short,Medium,Long,Very long]Average velocity [Static,Slow,Medium,Fast,Falling]Horizontal direction of movement [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Horizontal direction of view [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Pitch direction of view [Low,Little low,Medium,Little high,High]Health level [Empty,Low,Medium,High,Full]Whether to restore health [True,False]Whether the health is damaged [True,False]Whether rescued teammate [True,False]Whether be knocked down [True,False]Whether prone position [True,False]Whether have a gun [True,False]Whether have bullets [True,False]Whether have medical kits [True,False]Distance with nearest enemy [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether closer with nearest enemy [True,False]Whether crouch position [True,False]Whether hold a gun [True,False]Length of distance from agent to teammate [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether seen by teammate [True,False]Teammate’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Whether follow with the views of teammate [True,False]Whether target the same enemy as teammate [True,False]Whether follow with the movement direction of teammate [True,False]Horizontal direction of movement of enemy [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Velocity of enemy [Static,Slow,Medium,Fast,Falling,None]Enemy’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Table 7: Overview of sub-goal classes, we show a part of them here.D.。SurgeryAs the project proceeded, Contra was continuously improved to satisfy richer featured environment dynamics.。However,such an iterative development poses some challenges to the research of open-ended learning in an embodied situation, asthe changes in API and environment attributes will make the training be non-stationary.。A popular solution to resolve thisissue is the surgery introduced by Berner et al.。(2019), which significantly reduces training time by maximizing retention ofpreviously learned abilities.。Similarly, we leverage surgery in four aspects to ensure the training adapts to the new changes,including model architecture, observation space, action space, and reward functions.。Table 8 illustrates the main changeswe conducted and the corresponding parameters.. \n",
      "the piece 312\n",
      " \n",
      "ed Agent via Language-Policy Bidirectional AdaptationFeature Weight Descriptionenemy discovery 0.02 reward for see an enemydetected by enemy -0.002 punishment for being seen by an enemyscout 0.0001 reward for search for an enemyno-op -0.0002 punishment for stopping and doing nothingbullet 0.015 reward for using and refilling bulletshealth point 0.03 reward for health point changesbe knocked down -2.5 punishment for being knocked downdead -3.5 punishment for being killeddamage enemy 0.1 reward for damaging an enemyknock down enemy 4.5 reward for knocking down an enemykill enemy 3.5 reward for killing an enemyapproach a downed teammate 0.001 reward for approaching a downed teammatehelp a downed teammate up 0.8 reward for helping up a downed teammatenot save a downed teammate -0.5 punishment for not saving a downed teammatego to blue circle 0.00015 reward for going to blue circlebe in white circle -0.00005 small punishment for being outside of white circleoutside blue circle -0.012 punishment for being outside of blue circleteammate damage enemy 0.03 reward from teammate damaging enemiesteammate get up 0.6 reward from teammate getting upI help teammate up 4 reward for helping teammate upinterrupt helping teammate up -0.05 punishment for the interruption to help teammate upobstacle avoidance 0.012 punishment for being stuckgoal 1 reward of goal completionTable 5: The introduction of different rewards.Date Iteration #params Change4/14/2023 1 1802702 Experiment started4/27/2023 1808552 1802702 Env-init: Random weapons5/8/2023 2829170 1803087 Action: Add a fire action for long distance5/10/2023 3034011 1803087 Env-init:Random safe area in the whole map5/11/2023 3130353 1803855 Observation: Add number of remaining players in the game5/12/2023 3198564 2412975 Observation: Add BEV feature5/16/2023 3673506 2418111 Observation: Add history rotation feature5/22/2023 4519567 2418368 Observation: Add rotation change feature5/29/2023 5442025 2418368 Reward: Add rewards for teamwork6/2/2023 5899503 2418368 Update new game version6/13/2023 7306607 3013409 Network: Add obstacle avoidance reward and corresponding value head6/14/2023 7404118 3015457 Observation: Add distance feature to nearby obstacles6/16/2023 7628098 3015457 Env-init: Player numbers per team increased to 46/19/2023 7974450 3109267 Action: Use attention to select target to attackTable 8: The major changes in the training procedure.Goal Processing.。To construct the goal-conditioned policy πgon top of π, we need toencode a goal gthat is generated from Grnd,GopandGllm.。Thus, we propose a network Fas anencoder to achieve that.。In practice, to improvethe representation, we further include other inputsbesides gas (1) goal_mode_info : a 10-dimensionalvector that indicates whether current learning isgoal-conditioned, the achieved times of g, andthe rest time to complete this goal (as we presetthe maximum timestep for goal completion is30s), short in vinfo; (2) sub_goal_masking : a68-dimensional 0-1 vector that indicates whichsub-goals are masked for their corresponding values are 0, short in vmask ; (3) expected_goal_of_current_state : namely, forstatt, we generate a goal g′=PROJ(st)which is a 68-dimensional vector as the same as g, short in g′.。Then, we form theinput for Fasx= (g, g′, vinfo, vmask).。Considering that each item in xis heterogeneous on the data type, so we transformthem with independent FC layers and then follows a ResNet block for each of them.。We can express this process aseg=RESNET(FC(g)), e g′=RESNET(FC(g′)), e info=RESNET(FC(vinfo)), e mask =RESNET(FC(vmask)),where eg,eg′,einfo,emask are the embedding representation corresponding to each input item, and all of them are the samein dimension size.。Then, we aggregate them with computing the average of them as to get a fusion embedding asefusion = (eg+eg′+einfo+emask)/4.With surgery, we now concatenate the embedding of backbone of πwithefusion and fusing them via a FC layer, to form abackbone for πg.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub-goal Class CandidatesDamage to enemy [Zero,Low,Little low,Medium,Little high,High]Whether knock down enemy [True,False]Whether kill enemy [True,False]Whether seen enemy [True,False]Whether seen by enemy [True,False]Number of enemies have ever seen [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]Length of distance moved [No movement,Short,Medium,Long,Very long]Average velocity [Static,Slow,Medium,Fast,Falling]Horizontal direction of movement [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Horizontal direction of view [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Pitch direction of view [Low,Little low,Medium,Little high,High]Health level [Empty,Low,Medium,High,Full]Whether to restore health [True,False]Whether the health is damaged [True,False]Whether rescued teammate [True,False]Whether be knocked down [True,False]Whether prone position [True,False]Whether have a gun [True,False]Whether have bullets [True,False]Whether have medical kits [True,False]Distance with nearest enemy [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether closer with nearest enemy [True,False]Whether crouch position [True,False]Whether hold a gun [True,False]Length of distance from agent to teammate [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether seen by teammate [True,False]Teammate’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Whether follow with the views of teammate [True,False]Whether target the same enemy as teammate [True,False]Whether follow with the movement direction of teammate [True,False]Horizontal direction of movement of enemy [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Velocity of enemy [Static,Slow,Medium,Fast,Falling,None]Enemy’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Table 7: Overview of sub-goal classes, we show a part of them here.D.。SurgeryAs the project proceeded, Contra was continuously improved to satisfy richer featured environment dynamics.。However,such an iterative development poses some challenges to the research of open-ended learning in an embodied situation, asthe changes in API and environment attributes will make the training be non-stationary.。A popular solution to resolve thisissue is the surgery introduced by Berner et al.。(2019), which significantly reduces training time by maximizing retention ofpreviously learned abilities.。Similarly, we leverage surgery in four aspects to ensure the training adapts to the new changes,including model architecture, observation space, action space, and reward functions.。Table 8 illustrates the main changeswe conducted and the corresponding parameters.。For the surgery of observation space and model architecture, we haveintroduced a decoupled encoding in Appendix A; for the surgery of action space, we directly extend the policy head in widthto satisfy the new action space; for the reward functions, the essentials are to include the newly introduced features whichcan contribute to the learning, as we introduced in Appendix A, a linear combination has been considered to satisfy thisrequirement.. \n",
      "the piece 313\n",
      " \n",
      "at indicates whether current learning isgoal-conditioned, the achieved times of g, andthe rest time to complete this goal (as we presetthe maximum timestep for goal completion is30s), short in vinfo; (2) sub_goal_masking : a68-dimensional 0-1 vector that indicates whichsub-goals are masked for their corresponding values are 0, short in vmask ; (3) expected_goal_of_current_state : namely, forstatt, we generate a goal g′=PROJ(st)which is a 68-dimensional vector as the same as g, short in g′.。Then, we form theinput for Fasx= (g, g′, vinfo, vmask).。Considering that each item in xis heterogeneous on the data type, so we transformthem with independent FC layers and then follows a ResNet block for each of them.。We can express this process aseg=RESNET(FC(g)), e g′=RESNET(FC(g′)), e info=RESNET(FC(vinfo)), e mask =RESNET(FC(vmask)),where eg,eg′,einfo,emask are the embedding representation corresponding to each input item, and all of them are the samein dimension size.。Then, we aggregate them with computing the average of them as to get a fusion embedding asefusion = (eg+eg′+einfo+emask)/4.With surgery, we now concatenate the embedding of backbone of πwithefusion and fusing them via a FC layer, to form abackbone for πg.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub-goal Class CandidatesDamage to enemy [Zero,Low,Little low,Medium,Little high,High]Whether knock down enemy [True,False]Whether kill enemy [True,False]Whether seen enemy [True,False]Whether seen by enemy [True,False]Number of enemies have ever seen [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]Length of distance moved [No movement,Short,Medium,Long,Very long]Average velocity [Static,Slow,Medium,Fast,Falling]Horizontal direction of movement [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Horizontal direction of view [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Pitch direction of view [Low,Little low,Medium,Little high,High]Health level [Empty,Low,Medium,High,Full]Whether to restore health [True,False]Whether the health is damaged [True,False]Whether rescued teammate [True,False]Whether be knocked down [True,False]Whether prone position [True,False]Whether have a gun [True,False]Whether have bullets [True,False]Whether have medical kits [True,False]Distance with nearest enemy [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether closer with nearest enemy [True,False]Whether crouch position [True,False]Whether hold a gun [True,False]Length of distance from agent to teammate [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether seen by teammate [True,False]Teammate’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Whether follow with the views of teammate [True,False]Whether target the same enemy as teammate [True,False]Whether follow with the movement direction of teammate [True,False]Horizontal direction of movement of enemy [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Velocity of enemy [Static,Slow,Medium,Fast,Falling,None]Enemy’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Table 7: Overview of sub-goal classes, we show a part of them here.D.。SurgeryAs the project proceeded, Contra was continuously improved to satisfy richer featured environment dynamics.。However,such an iterative development poses some challenges to the research of open-ended learning in an embodied situation, asthe changes in API and environment attributes will make the training be non-stationary.。A popular solution to resolve thisissue is the surgery introduced by Berner et al.。(2019), which significantly reduces training time by maximizing retention ofpreviously learned abilities.。Similarly, we leverage surgery in four aspects to ensure the training adapts to the new changes,including model architecture, observation space, action space, and reward functions.。Table 8 illustrates the main changeswe conducted and the corresponding parameters.。For the surgery of observation space and model architecture, we haveintroduced a decoupled encoding in Appendix A; for the surgery of action space, we directly extend the policy head in widthto satisfy the new action space; for the reward functions, the essentials are to include the newly introduced features whichcan contribute to the learning, as we introduced in Appendix A, a linear combination has been considered to satisfy thisrequirement.。In our experiment, we propose three novel surgery methods, where two for model architecture and one forobservation space.. \n",
      "the piece 314\n",
      " \n",
      "(as we presetthe maximum timestep for goal completion is30s), short in vinfo; (2) sub_goal_masking : a68-dimensional 0-1 vector that indicates whichsub-goals are masked for their corresponding values are 0, short in vmask ; (3) expected_goal_of_current_state : namely, forstatt, we generate a goal g′=PROJ(st)which is a 68-dimensional vector as the same as g, short in g′.。Then, we form theinput for Fasx= (g, g′, vinfo, vmask).。Considering that each item in xis heterogeneous on the data type, so we transformthem with independent FC layers and then follows a ResNet block for each of them.。We can express this process aseg=RESNET(FC(g)), e g′=RESNET(FC(g′)), e info=RESNET(FC(vinfo)), e mask =RESNET(FC(vmask)),where eg,eg′,einfo,emask are the embedding representation corresponding to each input item, and all of them are the samein dimension size.。Then, we aggregate them with computing the average of them as to get a fusion embedding asefusion = (eg+eg′+einfo+emask)/4.With surgery, we now concatenate the embedding of backbone of πwithefusion and fusing them via a FC layer, to form abackbone for πg.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub-goal Class CandidatesDamage to enemy [Zero,Low,Little low,Medium,Little high,High]Whether knock down enemy [True,False]Whether kill enemy [True,False]Whether seen enemy [True,False]Whether seen by enemy [True,False]Number of enemies have ever seen [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]Length of distance moved [No movement,Short,Medium,Long,Very long]Average velocity [Static,Slow,Medium,Fast,Falling]Horizontal direction of movement [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Horizontal direction of view [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Pitch direction of view [Low,Little low,Medium,Little high,High]Health level [Empty,Low,Medium,High,Full]Whether to restore health [True,False]Whether the health is damaged [True,False]Whether rescued teammate [True,False]Whether be knocked down [True,False]Whether prone position [True,False]Whether have a gun [True,False]Whether have bullets [True,False]Whether have medical kits [True,False]Distance with nearest enemy [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether closer with nearest enemy [True,False]Whether crouch position [True,False]Whether hold a gun [True,False]Length of distance from agent to teammate [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether seen by teammate [True,False]Teammate’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Whether follow with the views of teammate [True,False]Whether target the same enemy as teammate [True,False]Whether follow with the movement direction of teammate [True,False]Horizontal direction of movement of enemy [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Velocity of enemy [Static,Slow,Medium,Fast,Falling,None]Enemy’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Table 7: Overview of sub-goal classes, we show a part of them here.D.。SurgeryAs the project proceeded, Contra was continuously improved to satisfy richer featured environment dynamics.。However,such an iterative development poses some challenges to the research of open-ended learning in an embodied situation, asthe changes in API and environment attributes will make the training be non-stationary.。A popular solution to resolve thisissue is the surgery introduced by Berner et al.。(2019), which significantly reduces training time by maximizing retention ofpreviously learned abilities.。Similarly, we leverage surgery in four aspects to ensure the training adapts to the new changes,including model architecture, observation space, action space, and reward functions.。Table 8 illustrates the main changeswe conducted and the corresponding parameters.。For the surgery of observation space and model architecture, we haveintroduced a decoupled encoding in Appendix A; for the surgery of action space, we directly extend the policy head in widthto satisfy the new action space; for the reward functions, the essentials are to include the newly introduced features whichcan contribute to the learning, as we introduced in Appendix A, a linear combination has been considered to satisfy thisrequirement.。In our experiment, we propose three novel surgery methods, where two for model architecture and one forobservation space.。The game environment has changed several times since the training started.. \n",
      "the piece 315\n",
      " \n",
      "), short in vinfo; (2) sub_goal_masking : a68-dimensional 0-1 vector that indicates whichsub-goals are masked for their corresponding values are 0, short in vmask ; (3) expected_goal_of_current_state : namely, forstatt, we generate a goal g′=PROJ(st)which is a 68-dimensional vector as the same as g, short in g′.。Then, we form theinput for Fasx= (g, g′, vinfo, vmask).。Considering that each item in xis heterogeneous on the data type, so we transformthem with independent FC layers and then follows a ResNet block for each of them.。We can express this process aseg=RESNET(FC(g)), e g′=RESNET(FC(g′)), e info=RESNET(FC(vinfo)), e mask =RESNET(FC(vmask)),where eg,eg′,einfo,emask are the embedding representation corresponding to each input item, and all of them are the samein dimension size.。Then, we aggregate them with computing the average of them as to get a fusion embedding asefusion = (eg+eg′+einfo+emask)/4.With surgery, we now concatenate the embedding of backbone of πwithefusion and fusing them via a FC layer, to form abackbone for πg.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub-goal Class CandidatesDamage to enemy [Zero,Low,Little low,Medium,Little high,High]Whether knock down enemy [True,False]Whether kill enemy [True,False]Whether seen enemy [True,False]Whether seen by enemy [True,False]Number of enemies have ever seen [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]Length of distance moved [No movement,Short,Medium,Long,Very long]Average velocity [Static,Slow,Medium,Fast,Falling]Horizontal direction of movement [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Horizontal direction of view [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Pitch direction of view [Low,Little low,Medium,Little high,High]Health level [Empty,Low,Medium,High,Full]Whether to restore health [True,False]Whether the health is damaged [True,False]Whether rescued teammate [True,False]Whether be knocked down [True,False]Whether prone position [True,False]Whether have a gun [True,False]Whether have bullets [True,False]Whether have medical kits [True,False]Distance with nearest enemy [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether closer with nearest enemy [True,False]Whether crouch position [True,False]Whether hold a gun [True,False]Length of distance from agent to teammate [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether seen by teammate [True,False]Teammate’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Whether follow with the views of teammate [True,False]Whether target the same enemy as teammate [True,False]Whether follow with the movement direction of teammate [True,False]Horizontal direction of movement of enemy [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Velocity of enemy [Static,Slow,Medium,Fast,Falling,None]Enemy’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Table 7: Overview of sub-goal classes, we show a part of them here.D.。SurgeryAs the project proceeded, Contra was continuously improved to satisfy richer featured environment dynamics.。However,such an iterative development poses some challenges to the research of open-ended learning in an embodied situation, asthe changes in API and environment attributes will make the training be non-stationary.。A popular solution to resolve thisissue is the surgery introduced by Berner et al.。(2019), which significantly reduces training time by maximizing retention ofpreviously learned abilities.。Similarly, we leverage surgery in four aspects to ensure the training adapts to the new changes,including model architecture, observation space, action space, and reward functions.。Table 8 illustrates the main changeswe conducted and the corresponding parameters.。For the surgery of observation space and model architecture, we haveintroduced a decoupled encoding in Appendix A; for the surgery of action space, we directly extend the policy head in widthto satisfy the new action space; for the reward functions, the essentials are to include the newly introduced features whichcan contribute to the learning, as we introduced in Appendix A, a linear combination has been considered to satisfy thisrequirement.。In our experiment, we propose three novel surgery methods, where two for model architecture and one forobservation space.。The game environment has changed several times since the training started.。The changes are mainlyabout adding player characters, adding player skills, adding weapon, modifying the map, etc.. \n",
      "the piece 316\n",
      " \n",
      "e mask =RESNET(FC(vmask)),where eg,eg′,einfo,emask are the embedding representation corresponding to each input item, and all of them are the samein dimension size.。Then, we aggregate them with computing the average of them as to get a fusion embedding asefusion = (eg+eg′+einfo+emask)/4.With surgery, we now concatenate the embedding of backbone of πwithefusion and fusing them via a FC layer, to form abackbone for πg.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub-goal Class CandidatesDamage to enemy [Zero,Low,Little low,Medium,Little high,High]Whether knock down enemy [True,False]Whether kill enemy [True,False]Whether seen enemy [True,False]Whether seen by enemy [True,False]Number of enemies have ever seen [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]Length of distance moved [No movement,Short,Medium,Long,Very long]Average velocity [Static,Slow,Medium,Fast,Falling]Horizontal direction of movement [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Horizontal direction of view [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Pitch direction of view [Low,Little low,Medium,Little high,High]Health level [Empty,Low,Medium,High,Full]Whether to restore health [True,False]Whether the health is damaged [True,False]Whether rescued teammate [True,False]Whether be knocked down [True,False]Whether prone position [True,False]Whether have a gun [True,False]Whether have bullets [True,False]Whether have medical kits [True,False]Distance with nearest enemy [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether closer with nearest enemy [True,False]Whether crouch position [True,False]Whether hold a gun [True,False]Length of distance from agent to teammate [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether seen by teammate [True,False]Teammate’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Whether follow with the views of teammate [True,False]Whether target the same enemy as teammate [True,False]Whether follow with the movement direction of teammate [True,False]Horizontal direction of movement of enemy [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Velocity of enemy [Static,Slow,Medium,Fast,Falling,None]Enemy’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Table 7: Overview of sub-goal classes, we show a part of them here.D.。SurgeryAs the project proceeded, Contra was continuously improved to satisfy richer featured environment dynamics.。However,such an iterative development poses some challenges to the research of open-ended learning in an embodied situation, asthe changes in API and environment attributes will make the training be non-stationary.。A popular solution to resolve thisissue is the surgery introduced by Berner et al.。(2019), which significantly reduces training time by maximizing retention ofpreviously learned abilities.。Similarly, we leverage surgery in four aspects to ensure the training adapts to the new changes,including model architecture, observation space, action space, and reward functions.。Table 8 illustrates the main changeswe conducted and the corresponding parameters.。For the surgery of observation space and model architecture, we haveintroduced a decoupled encoding in Appendix A; for the surgery of action space, we directly extend the policy head in widthto satisfy the new action space; for the reward functions, the essentials are to include the newly introduced features whichcan contribute to the learning, as we introduced in Appendix A, a linear combination has been considered to satisfy thisrequirement.。In our experiment, we propose three novel surgery methods, where two for model architecture and one forobservation space.。The game environment has changed several times since the training started.。The changes are mainlyabout adding player characters, adding player skills, adding weapon, modifying the map, etc.。For all these changes, theproportion of new environments in which we train our policy grows slowly from 0%to100% .. \n",
      "the piece 317\n",
      " \n",
      "e embedding representation corresponding to each input item, and all of them are the samein dimension size.。Then, we aggregate them with computing the average of them as to get a fusion embedding asefusion = (eg+eg′+einfo+emask)/4.With surgery, we now concatenate the embedding of backbone of πwithefusion and fusing them via a FC layer, to form abackbone for πg.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub-goal Class CandidatesDamage to enemy [Zero,Low,Little low,Medium,Little high,High]Whether knock down enemy [True,False]Whether kill enemy [True,False]Whether seen enemy [True,False]Whether seen by enemy [True,False]Number of enemies have ever seen [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]Length of distance moved [No movement,Short,Medium,Long,Very long]Average velocity [Static,Slow,Medium,Fast,Falling]Horizontal direction of movement [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Horizontal direction of view [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Pitch direction of view [Low,Little low,Medium,Little high,High]Health level [Empty,Low,Medium,High,Full]Whether to restore health [True,False]Whether the health is damaged [True,False]Whether rescued teammate [True,False]Whether be knocked down [True,False]Whether prone position [True,False]Whether have a gun [True,False]Whether have bullets [True,False]Whether have medical kits [True,False]Distance with nearest enemy [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether closer with nearest enemy [True,False]Whether crouch position [True,False]Whether hold a gun [True,False]Length of distance from agent to teammate [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether seen by teammate [True,False]Teammate’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Whether follow with the views of teammate [True,False]Whether target the same enemy as teammate [True,False]Whether follow with the movement direction of teammate [True,False]Horizontal direction of movement of enemy [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Velocity of enemy [Static,Slow,Medium,Fast,Falling,None]Enemy’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Table 7: Overview of sub-goal classes, we show a part of them here.D.。SurgeryAs the project proceeded, Contra was continuously improved to satisfy richer featured environment dynamics.。However,such an iterative development poses some challenges to the research of open-ended learning in an embodied situation, asthe changes in API and environment attributes will make the training be non-stationary.。A popular solution to resolve thisissue is the surgery introduced by Berner et al.。(2019), which significantly reduces training time by maximizing retention ofpreviously learned abilities.。Similarly, we leverage surgery in four aspects to ensure the training adapts to the new changes,including model architecture, observation space, action space, and reward functions.。Table 8 illustrates the main changeswe conducted and the corresponding parameters.。For the surgery of observation space and model architecture, we haveintroduced a decoupled encoding in Appendix A; for the surgery of action space, we directly extend the policy head in widthto satisfy the new action space; for the reward functions, the essentials are to include the newly introduced features whichcan contribute to the learning, as we introduced in Appendix A, a linear combination has been considered to satisfy thisrequirement.。In our experiment, we propose three novel surgery methods, where two for model architecture and one forobservation space.。The game environment has changed several times since the training started.。The changes are mainlyabout adding player characters, adding player skills, adding weapon, modifying the map, etc.。For all these changes, theproportion of new environments in which we train our policy grows slowly from 0%to100% .。In this case, the variance isrelatively small and the performance would quickly recover in the new environment.. \n",
      "the piece 318\n",
      " \n",
      "em as to get a fusion embedding asefusion = (eg+eg′+einfo+emask)/4.With surgery, we now concatenate the embedding of backbone of πwithefusion and fusing them via a FC layer, to form abackbone for πg.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub-goal Class CandidatesDamage to enemy [Zero,Low,Little low,Medium,Little high,High]Whether knock down enemy [True,False]Whether kill enemy [True,False]Whether seen enemy [True,False]Whether seen by enemy [True,False]Number of enemies have ever seen [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]Length of distance moved [No movement,Short,Medium,Long,Very long]Average velocity [Static,Slow,Medium,Fast,Falling]Horizontal direction of movement [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Horizontal direction of view [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Pitch direction of view [Low,Little low,Medium,Little high,High]Health level [Empty,Low,Medium,High,Full]Whether to restore health [True,False]Whether the health is damaged [True,False]Whether rescued teammate [True,False]Whether be knocked down [True,False]Whether prone position [True,False]Whether have a gun [True,False]Whether have bullets [True,False]Whether have medical kits [True,False]Distance with nearest enemy [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether closer with nearest enemy [True,False]Whether crouch position [True,False]Whether hold a gun [True,False]Length of distance from agent to teammate [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether seen by teammate [True,False]Teammate’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Whether follow with the views of teammate [True,False]Whether target the same enemy as teammate [True,False]Whether follow with the movement direction of teammate [True,False]Horizontal direction of movement of enemy [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Velocity of enemy [Static,Slow,Medium,Fast,Falling,None]Enemy’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Table 7: Overview of sub-goal classes, we show a part of them here.D.。SurgeryAs the project proceeded, Contra was continuously improved to satisfy richer featured environment dynamics.。However,such an iterative development poses some challenges to the research of open-ended learning in an embodied situation, asthe changes in API and environment attributes will make the training be non-stationary.。A popular solution to resolve thisissue is the surgery introduced by Berner et al.。(2019), which significantly reduces training time by maximizing retention ofpreviously learned abilities.。Similarly, we leverage surgery in four aspects to ensure the training adapts to the new changes,including model architecture, observation space, action space, and reward functions.。Table 8 illustrates the main changeswe conducted and the corresponding parameters.。For the surgery of observation space and model architecture, we haveintroduced a decoupled encoding in Appendix A; for the surgery of action space, we directly extend the policy head in widthto satisfy the new action space; for the reward functions, the essentials are to include the newly introduced features whichcan contribute to the learning, as we introduced in Appendix A, a linear combination has been considered to satisfy thisrequirement.。In our experiment, we propose three novel surgery methods, where two for model architecture and one forobservation space.。The game environment has changed several times since the training started.。The changes are mainlyabout adding player characters, adding player skills, adding weapon, modifying the map, etc.。For all these changes, theproportion of new environments in which we train our policy grows slowly from 0%to100% .。In this case, the variance isrelatively small and the performance would quickly recover in the new environment.。Figure 6 evaluates the utility of surgery,illustrating the changes in basic value during the training.. \n",
      "the piece 319\n",
      " \n",
      "tional AdaptationSub-goal Class CandidatesDamage to enemy [Zero,Low,Little low,Medium,Little high,High]Whether knock down enemy [True,False]Whether kill enemy [True,False]Whether seen enemy [True,False]Whether seen by enemy [True,False]Number of enemies have ever seen [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]Length of distance moved [No movement,Short,Medium,Long,Very long]Average velocity [Static,Slow,Medium,Fast,Falling]Horizontal direction of movement [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Horizontal direction of view [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Pitch direction of view [Low,Little low,Medium,Little high,High]Health level [Empty,Low,Medium,High,Full]Whether to restore health [True,False]Whether the health is damaged [True,False]Whether rescued teammate [True,False]Whether be knocked down [True,False]Whether prone position [True,False]Whether have a gun [True,False]Whether have bullets [True,False]Whether have medical kits [True,False]Distance with nearest enemy [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether closer with nearest enemy [True,False]Whether crouch position [True,False]Whether hold a gun [True,False]Length of distance from agent to teammate [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether seen by teammate [True,False]Teammate’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Whether follow with the views of teammate [True,False]Whether target the same enemy as teammate [True,False]Whether follow with the movement direction of teammate [True,False]Horizontal direction of movement of enemy [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Velocity of enemy [Static,Slow,Medium,Fast,Falling,None]Enemy’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Table 7: Overview of sub-goal classes, we show a part of them here.D.。SurgeryAs the project proceeded, Contra was continuously improved to satisfy richer featured environment dynamics.。However,such an iterative development poses some challenges to the research of open-ended learning in an embodied situation, asthe changes in API and environment attributes will make the training be non-stationary.。A popular solution to resolve thisissue is the surgery introduced by Berner et al.。(2019), which significantly reduces training time by maximizing retention ofpreviously learned abilities.。Similarly, we leverage surgery in four aspects to ensure the training adapts to the new changes,including model architecture, observation space, action space, and reward functions.。Table 8 illustrates the main changeswe conducted and the corresponding parameters.。For the surgery of observation space and model architecture, we haveintroduced a decoupled encoding in Appendix A; for the surgery of action space, we directly extend the policy head in widthto satisfy the new action space; for the reward functions, the essentials are to include the newly introduced features whichcan contribute to the learning, as we introduced in Appendix A, a linear combination has been considered to satisfy thisrequirement.。In our experiment, we propose three novel surgery methods, where two for model architecture and one forobservation space.。The game environment has changed several times since the training started.。The changes are mainlyabout adding player characters, adding player skills, adding weapon, modifying the map, etc.。For all these changes, theproportion of new environments in which we train our policy grows slowly from 0%to100% .。In this case, the variance isrelatively small and the performance would quickly recover in the new environment.。Figure 6 evaluates the utility of surgery,illustrating the changes in basic value during the training.。It can be seen that the values change smoothly for most surgeries.Meanwhile, the values remain stable after the surgeries.. \n",
      "the piece 320\n",
      " \n",
      "stationary.。A popular solution to resolve thisissue is the surgery introduced by Berner et al.。(2019), which significantly reduces training time by maximizing retention ofpreviously learned abilities.。Similarly, we leverage surgery in four aspects to ensure the training adapts to the new changes,including model architecture, observation space, action space, and reward functions.。Table 8 illustrates the main changeswe conducted and the corresponding parameters.。For the surgery of observation space and model architecture, we haveintroduced a decoupled encoding in Appendix A; for the surgery of action space, we directly extend the policy head in widthto satisfy the new action space; for the reward functions, the essentials are to include the newly introduced features whichcan contribute to the learning, as we introduced in Appendix A, a linear combination has been considered to satisfy thisrequirement.。In our experiment, we propose three novel surgery methods, where two for model architecture and one forobservation space.。The game environment has changed several times since the training started.。The changes are mainlyabout adding player characters, adding player skills, adding weapon, modifying the map, etc.。For all these changes, theproportion of new environments in which we train our policy grows slowly from 0%to100% .。In this case, the variance isrelatively small and the performance would quickly recover in the new environment.。Figure 6 evaluates the utility of surgery,illustrating the changes in basic value during the training.。It can be seen that the values change smoothly for most surgeries.Meanwhile, the values remain stable after the surgeries.。These results prove the effectiveness of our surgery.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation04/14 04/28 05/12 05/26 06/09 06/23 07/07Dates-1012345Basic ValueFigure 6: The value changes during the training process.E.. \n",
      "the piece 321\n",
      " \n",
      "cantly reduces training time by maximizing retention ofpreviously learned abilities.。Similarly, we leverage surgery in four aspects to ensure the training adapts to the new changes,including model architecture, observation space, action space, and reward functions.。Table 8 illustrates the main changeswe conducted and the corresponding parameters.。For the surgery of observation space and model architecture, we haveintroduced a decoupled encoding in Appendix A; for the surgery of action space, we directly extend the policy head in widthto satisfy the new action space; for the reward functions, the essentials are to include the newly introduced features whichcan contribute to the learning, as we introduced in Appendix A, a linear combination has been considered to satisfy thisrequirement.。In our experiment, we propose three novel surgery methods, where two for model architecture and one forobservation space.。The game environment has changed several times since the training started.。The changes are mainlyabout adding player characters, adding player skills, adding weapon, modifying the map, etc.。For all these changes, theproportion of new environments in which we train our policy grows slowly from 0%to100% .。In this case, the variance isrelatively small and the performance would quickly recover in the new environment.。Figure 6 evaluates the utility of surgery,illustrating the changes in basic value during the training.。It can be seen that the values change smoothly for most surgeries.Meanwhile, the values remain stable after the surgeries.。These results prove the effectiveness of our surgery.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation04/14 04/28 05/12 05/26 06/09 06/23 07/07Dates-1012345Basic ValueFigure 6: The value changes during the training process.E.。Datasets ConstructionThe process of fine-tuning the language model is operationalized through a question and answer paradigm.. \n",
      "the piece 322\n",
      " \n",
      "vation space, action space, and reward functions.。Table 8 illustrates the main changeswe conducted and the corresponding parameters.。For the surgery of observation space and model architecture, we haveintroduced a decoupled encoding in Appendix A; for the surgery of action space, we directly extend the policy head in widthto satisfy the new action space; for the reward functions, the essentials are to include the newly introduced features whichcan contribute to the learning, as we introduced in Appendix A, a linear combination has been considered to satisfy thisrequirement.。In our experiment, we propose three novel surgery methods, where two for model architecture and one forobservation space.。The game environment has changed several times since the training started.。The changes are mainlyabout adding player characters, adding player skills, adding weapon, modifying the map, etc.。For all these changes, theproportion of new environments in which we train our policy grows slowly from 0%to100% .。In this case, the variance isrelatively small and the performance would quickly recover in the new environment.。Figure 6 evaluates the utility of surgery,illustrating the changes in basic value during the training.。It can be seen that the values change smoothly for most surgeries.Meanwhile, the values remain stable after the surgeries.。These results prove the effectiveness of our surgery.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation04/14 04/28 05/12 05/26 06/09 06/23 07/07Dates-1012345Basic ValueFigure 6: The value changes during the training process.E.。Datasets ConstructionThe process of fine-tuning the language model is operationalized through a question and answer paradigm.。In thisframework, we provide the language model with a comprehensive depiction of the present conditions pertaining to theagent, its companions, and adversaries.. \n",
      "the piece 323\n",
      " \n",
      "eswe conducted and the corresponding parameters.。For the surgery of observation space and model architecture, we haveintroduced a decoupled encoding in Appendix A; for the surgery of action space, we directly extend the policy head in widthto satisfy the new action space; for the reward functions, the essentials are to include the newly introduced features whichcan contribute to the learning, as we introduced in Appendix A, a linear combination has been considered to satisfy thisrequirement.。In our experiment, we propose three novel surgery methods, where two for model architecture and one forobservation space.。The game environment has changed several times since the training started.。The changes are mainlyabout adding player characters, adding player skills, adding weapon, modifying the map, etc.。For all these changes, theproportion of new environments in which we train our policy grows slowly from 0%to100% .。In this case, the variance isrelatively small and the performance would quickly recover in the new environment.。Figure 6 evaluates the utility of surgery,illustrating the changes in basic value during the training.。It can be seen that the values change smoothly for most surgeries.Meanwhile, the values remain stable after the surgeries.。These results prove the effectiveness of our surgery.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation04/14 04/28 05/12 05/26 06/09 06/23 07/07Dates-1012345Basic ValueFigure 6: The value changes during the training process.E.。Datasets ConstructionThe process of fine-tuning the language model is operationalized through a question and answer paradigm.。In thisframework, we provide the language model with a comprehensive depiction of the present conditions pertaining to theagent, its companions, and adversaries.。Additionally, we furnish the model with the linguistic instructions furnished byits teammates.. \n",
      "the piece 324\n",
      " \n",
      "e, we haveintroduced a decoupled encoding in Appendix A; for the surgery of action space, we directly extend the policy head in widthto satisfy the new action space; for the reward functions, the essentials are to include the newly introduced features whichcan contribute to the learning, as we introduced in Appendix A, a linear combination has been considered to satisfy thisrequirement.。In our experiment, we propose three novel surgery methods, where two for model architecture and one forobservation space.。The game environment has changed several times since the training started.。The changes are mainlyabout adding player characters, adding player skills, adding weapon, modifying the map, etc.。For all these changes, theproportion of new environments in which we train our policy grows slowly from 0%to100% .。In this case, the variance isrelatively small and the performance would quickly recover in the new environment.。Figure 6 evaluates the utility of surgery,illustrating the changes in basic value during the training.。It can be seen that the values change smoothly for most surgeries.Meanwhile, the values remain stable after the surgeries.。These results prove the effectiveness of our surgery.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation04/14 04/28 05/12 05/26 06/09 06/23 07/07Dates-1012345Basic ValueFigure 6: The value changes during the training process.E.。Datasets ConstructionThe process of fine-tuning the language model is operationalized through a question and answer paradigm.。In thisframework, we provide the language model with a comprehensive depiction of the present conditions pertaining to theagent, its companions, and adversaries.。Additionally, we furnish the model with the linguistic instructions furnished byits teammates.。Subsequently, the language model is tasked with generating the agent’s subsequent target meta state inaccordance with the specifics outlined in the question and answer dataset, as elucidated in Table 17.. \n",
      "the piece 325\n",
      " \n",
      "unctions, the essentials are to include the newly introduced features whichcan contribute to the learning, as we introduced in Appendix A, a linear combination has been considered to satisfy thisrequirement.。In our experiment, we propose three novel surgery methods, where two for model architecture and one forobservation space.。The game environment has changed several times since the training started.。The changes are mainlyabout adding player characters, adding player skills, adding weapon, modifying the map, etc.。For all these changes, theproportion of new environments in which we train our policy grows slowly from 0%to100% .。In this case, the variance isrelatively small and the performance would quickly recover in the new environment.。Figure 6 evaluates the utility of surgery,illustrating the changes in basic value during the training.。It can be seen that the values change smoothly for most surgeries.Meanwhile, the values remain stable after the surgeries.。These results prove the effectiveness of our surgery.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation04/14 04/28 05/12 05/26 06/09 06/23 07/07Dates-1012345Basic ValueFigure 6: The value changes during the training process.E.。Datasets ConstructionThe process of fine-tuning the language model is operationalized through a question and answer paradigm.。In thisframework, we provide the language model with a comprehensive depiction of the present conditions pertaining to theagent, its companions, and adversaries.。Additionally, we furnish the model with the linguistic instructions furnished byits teammates.。Subsequently, the language model is tasked with generating the agent’s subsequent target meta state inaccordance with the specifics outlined in the question and answer dataset, as elucidated in Table 17.。The response is generatedby GPT-4 and subsequently subjected to parsing and formatting processes facilitated by the rule-based coding mechanism.To ensure alignment between the responses generated by GPT-4 and the targeted meta-state format, a comprehensive promptengineering endeavor was conducted to establish a set of predetermined rules for GPT-4.. \n",
      "the piece 326\n",
      " \n",
      "ibute to the learning, as we introduced in Appendix A, a linear combination has been considered to satisfy thisrequirement.。In our experiment, we propose three novel surgery methods, where two for model architecture and one forobservation space.。The game environment has changed several times since the training started.。The changes are mainlyabout adding player characters, adding player skills, adding weapon, modifying the map, etc.。For all these changes, theproportion of new environments in which we train our policy grows slowly from 0%to100% .。In this case, the variance isrelatively small and the performance would quickly recover in the new environment.。Figure 6 evaluates the utility of surgery,illustrating the changes in basic value during the training.。It can be seen that the values change smoothly for most surgeries.Meanwhile, the values remain stable after the surgeries.。These results prove the effectiveness of our surgery.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation04/14 04/28 05/12 05/26 06/09 06/23 07/07Dates-1012345Basic ValueFigure 6: The value changes during the training process.E.。Datasets ConstructionThe process of fine-tuning the language model is operationalized through a question and answer paradigm.。In thisframework, we provide the language model with a comprehensive depiction of the present conditions pertaining to theagent, its companions, and adversaries.。Additionally, we furnish the model with the linguistic instructions furnished byits teammates.。Subsequently, the language model is tasked with generating the agent’s subsequent target meta state inaccordance with the specifics outlined in the question and answer dataset, as elucidated in Table 17.。The response is generatedby GPT-4 and subsequently subjected to parsing and formatting processes facilitated by the rule-based coding mechanism.To ensure alignment between the responses generated by GPT-4 and the targeted meta-state format, a comprehensive promptengineering endeavor was conducted to establish a set of predetermined rules for GPT-4.。The rule-based prompts, whichwere employed to guide GPT-4’s responses, are meticulously documented in Table 10.In an effort to mitigate the risk of overfitting our model to concise formatted outputs, thereby preserving its capacity forenvironmental reasoning, we augmented our dataset with a substantial volume of Chain of Thought data.. \n",
      "the piece 327\n",
      " \n",
      " changes, theproportion of new environments in which we train our policy grows slowly from 0%to100% .。In this case, the variance isrelatively small and the performance would quickly recover in the new environment.。Figure 6 evaluates the utility of surgery,illustrating the changes in basic value during the training.。It can be seen that the values change smoothly for most surgeries.Meanwhile, the values remain stable after the surgeries.。These results prove the effectiveness of our surgery.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation04/14 04/28 05/12 05/26 06/09 06/23 07/07Dates-1012345Basic ValueFigure 6: The value changes during the training process.E.。Datasets ConstructionThe process of fine-tuning the language model is operationalized through a question and answer paradigm.。In thisframework, we provide the language model with a comprehensive depiction of the present conditions pertaining to theagent, its companions, and adversaries.。Additionally, we furnish the model with the linguistic instructions furnished byits teammates.。Subsequently, the language model is tasked with generating the agent’s subsequent target meta state inaccordance with the specifics outlined in the question and answer dataset, as elucidated in Table 17.。The response is generatedby GPT-4 and subsequently subjected to parsing and formatting processes facilitated by the rule-based coding mechanism.To ensure alignment between the responses generated by GPT-4 and the targeted meta-state format, a comprehensive promptengineering endeavor was conducted to establish a set of predetermined rules for GPT-4.。The rule-based prompts, whichwere employed to guide GPT-4’s responses, are meticulously documented in Table 10.In an effort to mitigate the risk of overfitting our model to concise formatted outputs, thereby preserving its capacity forenvironmental reasoning, we augmented our dataset with a substantial volume of Chain of Thought data.。This augmentationapproach entails a systematic procedure whereby the large language model is guided through a step-by-step ideationprocess, ultimately culminating in the attainment of the intended target state.. \n",
      "the piece 328\n",
      " \n",
      "iance isrelatively small and the performance would quickly recover in the new environment.。Figure 6 evaluates the utility of surgery,illustrating the changes in basic value during the training.。It can be seen that the values change smoothly for most surgeries.Meanwhile, the values remain stable after the surgeries.。These results prove the effectiveness of our surgery.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation04/14 04/28 05/12 05/26 06/09 06/23 07/07Dates-1012345Basic ValueFigure 6: The value changes during the training process.E.。Datasets ConstructionThe process of fine-tuning the language model is operationalized through a question and answer paradigm.。In thisframework, we provide the language model with a comprehensive depiction of the present conditions pertaining to theagent, its companions, and adversaries.。Additionally, we furnish the model with the linguistic instructions furnished byits teammates.。Subsequently, the language model is tasked with generating the agent’s subsequent target meta state inaccordance with the specifics outlined in the question and answer dataset, as elucidated in Table 17.。The response is generatedby GPT-4 and subsequently subjected to parsing and formatting processes facilitated by the rule-based coding mechanism.To ensure alignment between the responses generated by GPT-4 and the targeted meta-state format, a comprehensive promptengineering endeavor was conducted to establish a set of predetermined rules for GPT-4.。The rule-based prompts, whichwere employed to guide GPT-4’s responses, are meticulously documented in Table 10.In an effort to mitigate the risk of overfitting our model to concise formatted outputs, thereby preserving its capacity forenvironmental reasoning, we augmented our dataset with a substantial volume of Chain of Thought data.。This augmentationapproach entails a systematic procedure whereby the large language model is guided through a step-by-step ideationprocess, ultimately culminating in the attainment of the intended target state.。Concretely, our methodology commenceswith an initial inquiry into the semantic interpretation of the given instruction, followed by the identification of pertinentstates, contemplation of state adjustments, analysis of action propensities, and an estimation of the requisite temporalconsiderations.. \n",
      "the piece 329\n",
      " \n",
      "w environment.。Figure 6 evaluates the utility of surgery,illustrating the changes in basic value during the training.。It can be seen that the values change smoothly for most surgeries.Meanwhile, the values remain stable after the surgeries.。These results prove the effectiveness of our surgery.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation04/14 04/28 05/12 05/26 06/09 06/23 07/07Dates-1012345Basic ValueFigure 6: The value changes during the training process.E.。Datasets ConstructionThe process of fine-tuning the language model is operationalized through a question and answer paradigm.。In thisframework, we provide the language model with a comprehensive depiction of the present conditions pertaining to theagent, its companions, and adversaries.。Additionally, we furnish the model with the linguistic instructions furnished byits teammates.。Subsequently, the language model is tasked with generating the agent’s subsequent target meta state inaccordance with the specifics outlined in the question and answer dataset, as elucidated in Table 17.。The response is generatedby GPT-4 and subsequently subjected to parsing and formatting processes facilitated by the rule-based coding mechanism.To ensure alignment between the responses generated by GPT-4 and the targeted meta-state format, a comprehensive promptengineering endeavor was conducted to establish a set of predetermined rules for GPT-4.。The rule-based prompts, whichwere employed to guide GPT-4’s responses, are meticulously documented in Table 10.In an effort to mitigate the risk of overfitting our model to concise formatted outputs, thereby preserving its capacity forenvironmental reasoning, we augmented our dataset with a substantial volume of Chain of Thought data.。This augmentationapproach entails a systematic procedure whereby the large language model is guided through a step-by-step ideationprocess, ultimately culminating in the attainment of the intended target state.。Concretely, our methodology commenceswith an initial inquiry into the semantic interpretation of the given instruction, followed by the identification of pertinentstates, contemplation of state adjustments, analysis of action propensities, and an estimation of the requisite temporalconsiderations.。Comprehensive documentation of the detailed prompts and ensuing responses derived from the Chainof Thought procedure can be found in Tables 16 and 9.. \n",
      "the piece 330\n",
      " \n",
      ".。It can be seen that the values change smoothly for most surgeries.Meanwhile, the values remain stable after the surgeries.。These results prove the effectiveness of our surgery.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation04/14 04/28 05/12 05/26 06/09 06/23 07/07Dates-1012345Basic ValueFigure 6: The value changes during the training process.E.。Datasets ConstructionThe process of fine-tuning the language model is operationalized through a question and answer paradigm.。In thisframework, we provide the language model with a comprehensive depiction of the present conditions pertaining to theagent, its companions, and adversaries.。Additionally, we furnish the model with the linguistic instructions furnished byits teammates.。Subsequently, the language model is tasked with generating the agent’s subsequent target meta state inaccordance with the specifics outlined in the question and answer dataset, as elucidated in Table 17.。The response is generatedby GPT-4 and subsequently subjected to parsing and formatting processes facilitated by the rule-based coding mechanism.To ensure alignment between the responses generated by GPT-4 and the targeted meta-state format, a comprehensive promptengineering endeavor was conducted to establish a set of predetermined rules for GPT-4.。The rule-based prompts, whichwere employed to guide GPT-4’s responses, are meticulously documented in Table 10.In an effort to mitigate the risk of overfitting our model to concise formatted outputs, thereby preserving its capacity forenvironmental reasoning, we augmented our dataset with a substantial volume of Chain of Thought data.。This augmentationapproach entails a systematic procedure whereby the large language model is guided through a step-by-step ideationprocess, ultimately culminating in the attainment of the intended target state.。Concretely, our methodology commenceswith an initial inquiry into the semantic interpretation of the given instruction, followed by the identification of pertinentstates, contemplation of state adjustments, analysis of action propensities, and an estimation of the requisite temporalconsiderations.。Comprehensive documentation of the detailed prompts and ensuing responses derived from the Chainof Thought procedure can be found in Tables 16 and 9.。It is noteworthy that traditional Chain of Thought processes inexisting large language models often generate sequential thoughts, a method characterized by a relatively protracted temporaltrajectory.. \n",
      "the piece 331\n",
      " \n",
      "rgeries.。These results prove the effectiveness of our surgery.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation04/14 04/28 05/12 05/26 06/09 06/23 07/07Dates-1012345Basic ValueFigure 6: The value changes during the training process.E.。Datasets ConstructionThe process of fine-tuning the language model is operationalized through a question and answer paradigm.。In thisframework, we provide the language model with a comprehensive depiction of the present conditions pertaining to theagent, its companions, and adversaries.。Additionally, we furnish the model with the linguistic instructions furnished byits teammates.。Subsequently, the language model is tasked with generating the agent’s subsequent target meta state inaccordance with the specifics outlined in the question and answer dataset, as elucidated in Table 17.。The response is generatedby GPT-4 and subsequently subjected to parsing and formatting processes facilitated by the rule-based coding mechanism.To ensure alignment between the responses generated by GPT-4 and the targeted meta-state format, a comprehensive promptengineering endeavor was conducted to establish a set of predetermined rules for GPT-4.。The rule-based prompts, whichwere employed to guide GPT-4’s responses, are meticulously documented in Table 10.In an effort to mitigate the risk of overfitting our model to concise formatted outputs, thereby preserving its capacity forenvironmental reasoning, we augmented our dataset with a substantial volume of Chain of Thought data.。This augmentationapproach entails a systematic procedure whereby the large language model is guided through a step-by-step ideationprocess, ultimately culminating in the attainment of the intended target state.。Concretely, our methodology commenceswith an initial inquiry into the semantic interpretation of the given instruction, followed by the identification of pertinentstates, contemplation of state adjustments, analysis of action propensities, and an estimation of the requisite temporalconsiderations.。Comprehensive documentation of the detailed prompts and ensuing responses derived from the Chainof Thought procedure can be found in Tables 16 and 9.。It is noteworthy that traditional Chain of Thought processes inexisting large language models often generate sequential thoughts, a method characterized by a relatively protracted temporaltrajectory.。This sequential reasoning approach may not be well-suited to the high real-time demands typically encounteredin first-person shooter (FPS) games.. \n",
      "the piece 332\n",
      " \n",
      "icy Bidirectional Adaptation04/14 04/28 05/12 05/26 06/09 06/23 07/07Dates-1012345Basic ValueFigure 6: The value changes during the training process.E.。Datasets ConstructionThe process of fine-tuning the language model is operationalized through a question and answer paradigm.。In thisframework, we provide the language model with a comprehensive depiction of the present conditions pertaining to theagent, its companions, and adversaries.。Additionally, we furnish the model with the linguistic instructions furnished byits teammates.。Subsequently, the language model is tasked with generating the agent’s subsequent target meta state inaccordance with the specifics outlined in the question and answer dataset, as elucidated in Table 17.。The response is generatedby GPT-4 and subsequently subjected to parsing and formatting processes facilitated by the rule-based coding mechanism.To ensure alignment between the responses generated by GPT-4 and the targeted meta-state format, a comprehensive promptengineering endeavor was conducted to establish a set of predetermined rules for GPT-4.。The rule-based prompts, whichwere employed to guide GPT-4’s responses, are meticulously documented in Table 10.In an effort to mitigate the risk of overfitting our model to concise formatted outputs, thereby preserving its capacity forenvironmental reasoning, we augmented our dataset with a substantial volume of Chain of Thought data.。This augmentationapproach entails a systematic procedure whereby the large language model is guided through a step-by-step ideationprocess, ultimately culminating in the attainment of the intended target state.。Concretely, our methodology commenceswith an initial inquiry into the semantic interpretation of the given instruction, followed by the identification of pertinentstates, contemplation of state adjustments, analysis of action propensities, and an estimation of the requisite temporalconsiderations.。Comprehensive documentation of the detailed prompts and ensuing responses derived from the Chainof Thought procedure can be found in Tables 16 and 9.。It is noteworthy that traditional Chain of Thought processes inexisting large language models often generate sequential thoughts, a method characterized by a relatively protracted temporaltrajectory.。This sequential reasoning approach may not be well-suited to the high real-time demands typically encounteredin first-person shooter (FPS) games.。Furthermore, the singular-step reasoning capabilities inherent in smaller languagemodels are intrinsically modest and prone to errors.. \n",
      "the piece 333\n",
      " \n",
      "he value changes during the training process.E.。Datasets ConstructionThe process of fine-tuning the language model is operationalized through a question and answer paradigm.。In thisframework, we provide the language model with a comprehensive depiction of the present conditions pertaining to theagent, its companions, and adversaries.。Additionally, we furnish the model with the linguistic instructions furnished byits teammates.。Subsequently, the language model is tasked with generating the agent’s subsequent target meta state inaccordance with the specifics outlined in the question and answer dataset, as elucidated in Table 17.。The response is generatedby GPT-4 and subsequently subjected to parsing and formatting processes facilitated by the rule-based coding mechanism.To ensure alignment between the responses generated by GPT-4 and the targeted meta-state format, a comprehensive promptengineering endeavor was conducted to establish a set of predetermined rules for GPT-4.。The rule-based prompts, whichwere employed to guide GPT-4’s responses, are meticulously documented in Table 10.In an effort to mitigate the risk of overfitting our model to concise formatted outputs, thereby preserving its capacity forenvironmental reasoning, we augmented our dataset with a substantial volume of Chain of Thought data.。This augmentationapproach entails a systematic procedure whereby the large language model is guided through a step-by-step ideationprocess, ultimately culminating in the attainment of the intended target state.。Concretely, our methodology commenceswith an initial inquiry into the semantic interpretation of the given instruction, followed by the identification of pertinentstates, contemplation of state adjustments, analysis of action propensities, and an estimation of the requisite temporalconsiderations.。Comprehensive documentation of the detailed prompts and ensuing responses derived from the Chainof Thought procedure can be found in Tables 16 and 9.。It is noteworthy that traditional Chain of Thought processes inexisting large language models often generate sequential thoughts, a method characterized by a relatively protracted temporaltrajectory.。This sequential reasoning approach may not be well-suited to the high real-time demands typically encounteredin first-person shooter (FPS) games.。Furthermore, the singular-step reasoning capabilities inherent in smaller languagemodels are intrinsically modest and prone to errors.。Consequently, the amplification of error probabilities within the Chainof Thought reasoning process may not yield superior outcomes.. \n",
      "the piece 334\n",
      " \n",
      "ionalized through a question and answer paradigm.。In thisframework, we provide the language model with a comprehensive depiction of the present conditions pertaining to theagent, its companions, and adversaries.。Additionally, we furnish the model with the linguistic instructions furnished byits teammates.。Subsequently, the language model is tasked with generating the agent’s subsequent target meta state inaccordance with the specifics outlined in the question and answer dataset, as elucidated in Table 17.。The response is generatedby GPT-4 and subsequently subjected to parsing and formatting processes facilitated by the rule-based coding mechanism.To ensure alignment between the responses generated by GPT-4 and the targeted meta-state format, a comprehensive promptengineering endeavor was conducted to establish a set of predetermined rules for GPT-4.。The rule-based prompts, whichwere employed to guide GPT-4’s responses, are meticulously documented in Table 10.In an effort to mitigate the risk of overfitting our model to concise formatted outputs, thereby preserving its capacity forenvironmental reasoning, we augmented our dataset with a substantial volume of Chain of Thought data.。This augmentationapproach entails a systematic procedure whereby the large language model is guided through a step-by-step ideationprocess, ultimately culminating in the attainment of the intended target state.。Concretely, our methodology commenceswith an initial inquiry into the semantic interpretation of the given instruction, followed by the identification of pertinentstates, contemplation of state adjustments, analysis of action propensities, and an estimation of the requisite temporalconsiderations.。Comprehensive documentation of the detailed prompts and ensuing responses derived from the Chainof Thought procedure can be found in Tables 16 and 9.。It is noteworthy that traditional Chain of Thought processes inexisting large language models often generate sequential thoughts, a method characterized by a relatively protracted temporaltrajectory.。This sequential reasoning approach may not be well-suited to the high real-time demands typically encounteredin first-person shooter (FPS) games.。Furthermore, the singular-step reasoning capabilities inherent in smaller languagemodels are intrinsically modest and prone to errors.。Consequently, the amplification of error probabilities within the Chainof Thought reasoning process may not yield superior outcomes.。In light of these considerations, we have undertaken astrategy that amalgamates Chain of Thought data with the final target state data, thereby enhancing the fine-tuning of ourlanguage model.. \n",
      "the piece 335\n",
      " \n",
      "inguistic instructions furnished byits teammates.。Subsequently, the language model is tasked with generating the agent’s subsequent target meta state inaccordance with the specifics outlined in the question and answer dataset, as elucidated in Table 17.。The response is generatedby GPT-4 and subsequently subjected to parsing and formatting processes facilitated by the rule-based coding mechanism.To ensure alignment between the responses generated by GPT-4 and the targeted meta-state format, a comprehensive promptengineering endeavor was conducted to establish a set of predetermined rules for GPT-4.。The rule-based prompts, whichwere employed to guide GPT-4’s responses, are meticulously documented in Table 10.In an effort to mitigate the risk of overfitting our model to concise formatted outputs, thereby preserving its capacity forenvironmental reasoning, we augmented our dataset with a substantial volume of Chain of Thought data.。This augmentationapproach entails a systematic procedure whereby the large language model is guided through a step-by-step ideationprocess, ultimately culminating in the attainment of the intended target state.。Concretely, our methodology commenceswith an initial inquiry into the semantic interpretation of the given instruction, followed by the identification of pertinentstates, contemplation of state adjustments, analysis of action propensities, and an estimation of the requisite temporalconsiderations.。Comprehensive documentation of the detailed prompts and ensuing responses derived from the Chainof Thought procedure can be found in Tables 16 and 9.。It is noteworthy that traditional Chain of Thought processes inexisting large language models often generate sequential thoughts, a method characterized by a relatively protracted temporaltrajectory.。This sequential reasoning approach may not be well-suited to the high real-time demands typically encounteredin first-person shooter (FPS) games.。Furthermore, the singular-step reasoning capabilities inherent in smaller languagemodels are intrinsically modest and prone to errors.。Consequently, the amplification of error probabilities within the Chainof Thought reasoning process may not yield superior outcomes.。In light of these considerations, we have undertaken astrategy that amalgamates Chain of Thought data with the final target state data, thereby enhancing the fine-tuning of ourlanguage model.。In the course of test reasoning exercises, the language model promptly generates the ultimate target state,with the Chain of Thought information being implicitly encoded within the neural network parameters.Instruction Datasets.. \n",
      "the piece 336\n",
      " \n",
      "uent target meta state inaccordance with the specifics outlined in the question and answer dataset, as elucidated in Table 17.。The response is generatedby GPT-4 and subsequently subjected to parsing and formatting processes facilitated by the rule-based coding mechanism.To ensure alignment between the responses generated by GPT-4 and the targeted meta-state format, a comprehensive promptengineering endeavor was conducted to establish a set of predetermined rules for GPT-4.。The rule-based prompts, whichwere employed to guide GPT-4’s responses, are meticulously documented in Table 10.In an effort to mitigate the risk of overfitting our model to concise formatted outputs, thereby preserving its capacity forenvironmental reasoning, we augmented our dataset with a substantial volume of Chain of Thought data.。This augmentationapproach entails a systematic procedure whereby the large language model is guided through a step-by-step ideationprocess, ultimately culminating in the attainment of the intended target state.。Concretely, our methodology commenceswith an initial inquiry into the semantic interpretation of the given instruction, followed by the identification of pertinentstates, contemplation of state adjustments, analysis of action propensities, and an estimation of the requisite temporalconsiderations.。Comprehensive documentation of the detailed prompts and ensuing responses derived from the Chainof Thought procedure can be found in Tables 16 and 9.。It is noteworthy that traditional Chain of Thought processes inexisting large language models often generate sequential thoughts, a method characterized by a relatively protracted temporaltrajectory.。This sequential reasoning approach may not be well-suited to the high real-time demands typically encounteredin first-person shooter (FPS) games.。Furthermore, the singular-step reasoning capabilities inherent in smaller languagemodels are intrinsically modest and prone to errors.。Consequently, the amplification of error probabilities within the Chainof Thought reasoning process may not yield superior outcomes.。In light of these considerations, we have undertaken astrategy that amalgamates Chain of Thought data with the final target state data, thereby enhancing the fine-tuning of ourlanguage model.。In the course of test reasoning exercises, the language model promptly generates the ultimate target state,with the Chain of Thought information being implicitly encoded within the neural network parameters.Instruction Datasets.。To cover a comprehensive range of instruction types and state distributions, we generated fourtypes of instruction sets, which, when combined with states sampled from the environment, result in four different datasets.These are the HI (Human Instruction) dataset, constructed based on human-annotated commands; the SI (State Instruction)dataset, built by reverse-generating commands based on state transitions specified by the intelligent agent; the AI (AgentInstruction) dataset, constructed by main kinds of instruction which can be complete by pre-trained Agent; and the RI(Random Instruction) dataset, generated through random sampling of agent state transitions and random commands.•IH(Human Instructions).. \n",
      "the piece 337\n",
      " \n",
      "d subsequently subjected to parsing and formatting processes facilitated by the rule-based coding mechanism.To ensure alignment between the responses generated by GPT-4 and the targeted meta-state format, a comprehensive promptengineering endeavor was conducted to establish a set of predetermined rules for GPT-4.。The rule-based prompts, whichwere employed to guide GPT-4’s responses, are meticulously documented in Table 10.In an effort to mitigate the risk of overfitting our model to concise formatted outputs, thereby preserving its capacity forenvironmental reasoning, we augmented our dataset with a substantial volume of Chain of Thought data.。This augmentationapproach entails a systematic procedure whereby the large language model is guided through a step-by-step ideationprocess, ultimately culminating in the attainment of the intended target state.。Concretely, our methodology commenceswith an initial inquiry into the semantic interpretation of the given instruction, followed by the identification of pertinentstates, contemplation of state adjustments, analysis of action propensities, and an estimation of the requisite temporalconsiderations.。Comprehensive documentation of the detailed prompts and ensuing responses derived from the Chainof Thought procedure can be found in Tables 16 and 9.。It is noteworthy that traditional Chain of Thought processes inexisting large language models often generate sequential thoughts, a method characterized by a relatively protracted temporaltrajectory.。This sequential reasoning approach may not be well-suited to the high real-time demands typically encounteredin first-person shooter (FPS) games.。Furthermore, the singular-step reasoning capabilities inherent in smaller languagemodels are intrinsically modest and prone to errors.。Consequently, the amplification of error probabilities within the Chainof Thought reasoning process may not yield superior outcomes.。In light of these considerations, we have undertaken astrategy that amalgamates Chain of Thought data with the final target state data, thereby enhancing the fine-tuning of ourlanguage model.。In the course of test reasoning exercises, the language model promptly generates the ultimate target state,with the Chain of Thought information being implicitly encoded within the neural network parameters.Instruction Datasets.。To cover a comprehensive range of instruction types and state distributions, we generated fourtypes of instruction sets, which, when combined with states sampled from the environment, result in four different datasets.These are the HI (Human Instruction) dataset, constructed based on human-annotated commands; the SI (State Instruction)dataset, built by reverse-generating commands based on state transitions specified by the intelligent agent; the AI (AgentInstruction) dataset, constructed by main kinds of instruction which can be complete by pre-trained Agent; and the RI(Random Instruction) dataset, generated through random sampling of agent state transitions and random commands.•IH(Human Instructions).。We generate open-ended instructions manually, while the corresponding states are sampledfrom the intelligent agent’s interaction logs.. \n",
      "the piece 338\n",
      " \n",
      "g mechanism.To ensure alignment between the responses generated by GPT-4 and the targeted meta-state format, a comprehensive promptengineering endeavor was conducted to establish a set of predetermined rules for GPT-4.。The rule-based prompts, whichwere employed to guide GPT-4’s responses, are meticulously documented in Table 10.In an effort to mitigate the risk of overfitting our model to concise formatted outputs, thereby preserving its capacity forenvironmental reasoning, we augmented our dataset with a substantial volume of Chain of Thought data.。This augmentationapproach entails a systematic procedure whereby the large language model is guided through a step-by-step ideationprocess, ultimately culminating in the attainment of the intended target state.。Concretely, our methodology commenceswith an initial inquiry into the semantic interpretation of the given instruction, followed by the identification of pertinentstates, contemplation of state adjustments, analysis of action propensities, and an estimation of the requisite temporalconsiderations.。Comprehensive documentation of the detailed prompts and ensuing responses derived from the Chainof Thought procedure can be found in Tables 16 and 9.。It is noteworthy that traditional Chain of Thought processes inexisting large language models often generate sequential thoughts, a method characterized by a relatively protracted temporaltrajectory.。This sequential reasoning approach may not be well-suited to the high real-time demands typically encounteredin first-person shooter (FPS) games.。Furthermore, the singular-step reasoning capabilities inherent in smaller languagemodels are intrinsically modest and prone to errors.。Consequently, the amplification of error probabilities within the Chainof Thought reasoning process may not yield superior outcomes.。In light of these considerations, we have undertaken astrategy that amalgamates Chain of Thought data with the final target state data, thereby enhancing the fine-tuning of ourlanguage model.。In the course of test reasoning exercises, the language model promptly generates the ultimate target state,with the Chain of Thought information being implicitly encoded within the neural network parameters.Instruction Datasets.。To cover a comprehensive range of instruction types and state distributions, we generated fourtypes of instruction sets, which, when combined with states sampled from the environment, result in four different datasets.These are the HI (Human Instruction) dataset, constructed based on human-annotated commands; the SI (State Instruction)dataset, built by reverse-generating commands based on state transitions specified by the intelligent agent; the AI (AgentInstruction) dataset, constructed by main kinds of instruction which can be complete by pre-trained Agent; and the RI(Random Instruction) dataset, generated through random sampling of agent state transitions and random commands.•IH(Human Instructions).。We generate open-ended instructions manually, while the corresponding states are sampledfrom the intelligent agent’s interaction logs.。These are combined and annotated using GPT-4 based on the promptingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationmethod previously described.. \n",
      "the piece 339\n",
      " \n",
      "es for GPT-4.。The rule-based prompts, whichwere employed to guide GPT-4’s responses, are meticulously documented in Table 10.In an effort to mitigate the risk of overfitting our model to concise formatted outputs, thereby preserving its capacity forenvironmental reasoning, we augmented our dataset with a substantial volume of Chain of Thought data.。This augmentationapproach entails a systematic procedure whereby the large language model is guided through a step-by-step ideationprocess, ultimately culminating in the attainment of the intended target state.。Concretely, our methodology commenceswith an initial inquiry into the semantic interpretation of the given instruction, followed by the identification of pertinentstates, contemplation of state adjustments, analysis of action propensities, and an estimation of the requisite temporalconsiderations.。Comprehensive documentation of the detailed prompts and ensuing responses derived from the Chainof Thought procedure can be found in Tables 16 and 9.。It is noteworthy that traditional Chain of Thought processes inexisting large language models often generate sequential thoughts, a method characterized by a relatively protracted temporaltrajectory.。This sequential reasoning approach may not be well-suited to the high real-time demands typically encounteredin first-person shooter (FPS) games.。Furthermore, the singular-step reasoning capabilities inherent in smaller languagemodels are intrinsically modest and prone to errors.。Consequently, the amplification of error probabilities within the Chainof Thought reasoning process may not yield superior outcomes.。In light of these considerations, we have undertaken astrategy that amalgamates Chain of Thought data with the final target state data, thereby enhancing the fine-tuning of ourlanguage model.。In the course of test reasoning exercises, the language model promptly generates the ultimate target state,with the Chain of Thought information being implicitly encoded within the neural network parameters.Instruction Datasets.。To cover a comprehensive range of instruction types and state distributions, we generated fourtypes of instruction sets, which, when combined with states sampled from the environment, result in four different datasets.These are the HI (Human Instruction) dataset, constructed based on human-annotated commands; the SI (State Instruction)dataset, built by reverse-generating commands based on state transitions specified by the intelligent agent; the AI (AgentInstruction) dataset, constructed by main kinds of instruction which can be complete by pre-trained Agent; and the RI(Random Instruction) dataset, generated through random sampling of agent state transitions and random commands.•IH(Human Instructions).。We generate open-ended instructions manually, while the corresponding states are sampledfrom the intelligent agent’s interaction logs.。These are combined and annotated using GPT-4 based on the promptingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationmethod previously described.。We found that due to varying frequencies of state changes during the agent’s interactions,some states are difficult to capture comprehensively only using random sampling.. \n",
      "the piece 340\n",
      " \n",
      "his augmentationapproach entails a systematic procedure whereby the large language model is guided through a step-by-step ideationprocess, ultimately culminating in the attainment of the intended target state.。Concretely, our methodology commenceswith an initial inquiry into the semantic interpretation of the given instruction, followed by the identification of pertinentstates, contemplation of state adjustments, analysis of action propensities, and an estimation of the requisite temporalconsiderations.。Comprehensive documentation of the detailed prompts and ensuing responses derived from the Chainof Thought procedure can be found in Tables 16 and 9.。It is noteworthy that traditional Chain of Thought processes inexisting large language models often generate sequential thoughts, a method characterized by a relatively protracted temporaltrajectory.。This sequential reasoning approach may not be well-suited to the high real-time demands typically encounteredin first-person shooter (FPS) games.。Furthermore, the singular-step reasoning capabilities inherent in smaller languagemodels are intrinsically modest and prone to errors.。Consequently, the amplification of error probabilities within the Chainof Thought reasoning process may not yield superior outcomes.。In light of these considerations, we have undertaken astrategy that amalgamates Chain of Thought data with the final target state data, thereby enhancing the fine-tuning of ourlanguage model.。In the course of test reasoning exercises, the language model promptly generates the ultimate target state,with the Chain of Thought information being implicitly encoded within the neural network parameters.Instruction Datasets.。To cover a comprehensive range of instruction types and state distributions, we generated fourtypes of instruction sets, which, when combined with states sampled from the environment, result in four different datasets.These are the HI (Human Instruction) dataset, constructed based on human-annotated commands; the SI (State Instruction)dataset, built by reverse-generating commands based on state transitions specified by the intelligent agent; the AI (AgentInstruction) dataset, constructed by main kinds of instruction which can be complete by pre-trained Agent; and the RI(Random Instruction) dataset, generated through random sampling of agent state transitions and random commands.•IH(Human Instructions).。We generate open-ended instructions manually, while the corresponding states are sampledfrom the intelligent agent’s interaction logs.。These are combined and annotated using GPT-4 based on the promptingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationmethod previously described.。We found that due to varying frequencies of state changes during the agent’s interactions,some states are difficult to capture comprehensively only using random sampling.。To ensure a more comprehensivedistribution of states in the data and to facilitate better understanding by the language model, we employ a multi-roundrejection sampling approach to construct state set.. \n",
      "the piece 341\n",
      " \n",
      " by the identification of pertinentstates, contemplation of state adjustments, analysis of action propensities, and an estimation of the requisite temporalconsiderations.。Comprehensive documentation of the detailed prompts and ensuing responses derived from the Chainof Thought procedure can be found in Tables 16 and 9.。It is noteworthy that traditional Chain of Thought processes inexisting large language models often generate sequential thoughts, a method characterized by a relatively protracted temporaltrajectory.。This sequential reasoning approach may not be well-suited to the high real-time demands typically encounteredin first-person shooter (FPS) games.。Furthermore, the singular-step reasoning capabilities inherent in smaller languagemodels are intrinsically modest and prone to errors.。Consequently, the amplification of error probabilities within the Chainof Thought reasoning process may not yield superior outcomes.。In light of these considerations, we have undertaken astrategy that amalgamates Chain of Thought data with the final target state data, thereby enhancing the fine-tuning of ourlanguage model.。In the course of test reasoning exercises, the language model promptly generates the ultimate target state,with the Chain of Thought information being implicitly encoded within the neural network parameters.Instruction Datasets.。To cover a comprehensive range of instruction types and state distributions, we generated fourtypes of instruction sets, which, when combined with states sampled from the environment, result in four different datasets.These are the HI (Human Instruction) dataset, constructed based on human-annotated commands; the SI (State Instruction)dataset, built by reverse-generating commands based on state transitions specified by the intelligent agent; the AI (AgentInstruction) dataset, constructed by main kinds of instruction which can be complete by pre-trained Agent; and the RI(Random Instruction) dataset, generated through random sampling of agent state transitions and random commands.•IH(Human Instructions).。We generate open-ended instructions manually, while the corresponding states are sampledfrom the intelligent agent’s interaction logs.。These are combined and annotated using GPT-4 based on the promptingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationmethod previously described.。We found that due to varying frequencies of state changes during the agent’s interactions,some states are difficult to capture comprehensively only using random sampling.。To ensure a more comprehensivedistribution of states in the data and to facilitate better understanding by the language model, we employ a multi-roundrejection sampling approach to construct state set.。Let Sbe the set of states waiting to be sampled.. \n",
      "the piece 342\n",
      " \n",
      "ed prompts and ensuing responses derived from the Chainof Thought procedure can be found in Tables 16 and 9.。It is noteworthy that traditional Chain of Thought processes inexisting large language models often generate sequential thoughts, a method characterized by a relatively protracted temporaltrajectory.。This sequential reasoning approach may not be well-suited to the high real-time demands typically encounteredin first-person shooter (FPS) games.。Furthermore, the singular-step reasoning capabilities inherent in smaller languagemodels are intrinsically modest and prone to errors.。Consequently, the amplification of error probabilities within the Chainof Thought reasoning process may not yield superior outcomes.。In light of these considerations, we have undertaken astrategy that amalgamates Chain of Thought data with the final target state data, thereby enhancing the fine-tuning of ourlanguage model.。In the course of test reasoning exercises, the language model promptly generates the ultimate target state,with the Chain of Thought information being implicitly encoded within the neural network parameters.Instruction Datasets.。To cover a comprehensive range of instruction types and state distributions, we generated fourtypes of instruction sets, which, when combined with states sampled from the environment, result in four different datasets.These are the HI (Human Instruction) dataset, constructed based on human-annotated commands; the SI (State Instruction)dataset, built by reverse-generating commands based on state transitions specified by the intelligent agent; the AI (AgentInstruction) dataset, constructed by main kinds of instruction which can be complete by pre-trained Agent; and the RI(Random Instruction) dataset, generated through random sampling of agent state transitions and random commands.•IH(Human Instructions).。We generate open-ended instructions manually, while the corresponding states are sampledfrom the intelligent agent’s interaction logs.。These are combined and annotated using GPT-4 based on the promptingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationmethod previously described.。We found that due to varying frequencies of state changes during the agent’s interactions,some states are difficult to capture comprehensively only using random sampling.。To ensure a more comprehensivedistribution of states in the data and to facilitate better understanding by the language model, we employ a multi-roundrejection sampling approach to construct state set.。Let Sbe the set of states waiting to be sampled.。We perform multiplerounds of sampling on S, with Sgetirepresenting the set of states sampled in the i-th round, initially empty.. \n",
      "the piece 343\n",
      " \n",
      "jectory.。This sequential reasoning approach may not be well-suited to the high real-time demands typically encounteredin first-person shooter (FPS) games.。Furthermore, the singular-step reasoning capabilities inherent in smaller languagemodels are intrinsically modest and prone to errors.。Consequently, the amplification of error probabilities within the Chainof Thought reasoning process may not yield superior outcomes.。In light of these considerations, we have undertaken astrategy that amalgamates Chain of Thought data with the final target state data, thereby enhancing the fine-tuning of ourlanguage model.。In the course of test reasoning exercises, the language model promptly generates the ultimate target state,with the Chain of Thought information being implicitly encoded within the neural network parameters.Instruction Datasets.。To cover a comprehensive range of instruction types and state distributions, we generated fourtypes of instruction sets, which, when combined with states sampled from the environment, result in four different datasets.These are the HI (Human Instruction) dataset, constructed based on human-annotated commands; the SI (State Instruction)dataset, built by reverse-generating commands based on state transitions specified by the intelligent agent; the AI (AgentInstruction) dataset, constructed by main kinds of instruction which can be complete by pre-trained Agent; and the RI(Random Instruction) dataset, generated through random sampling of agent state transitions and random commands.•IH(Human Instructions).。We generate open-ended instructions manually, while the corresponding states are sampledfrom the intelligent agent’s interaction logs.。These are combined and annotated using GPT-4 based on the promptingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationmethod previously described.。We found that due to varying frequencies of state changes during the agent’s interactions,some states are difficult to capture comprehensively only using random sampling.。To ensure a more comprehensivedistribution of states in the data and to facilitate better understanding by the language model, we employ a multi-roundrejection sampling approach to construct state set.。Let Sbe the set of states waiting to be sampled.。We perform multiplerounds of sampling on S, with Sgetirepresenting the set of states sampled in the i-th round, initially empty.。Next, wesample a state sfrom Swithout replacement and check whether shas any state values not present in Sgeti.. \n",
      "the piece 344\n",
      " \n",
      "es.。Furthermore, the singular-step reasoning capabilities inherent in smaller languagemodels are intrinsically modest and prone to errors.。Consequently, the amplification of error probabilities within the Chainof Thought reasoning process may not yield superior outcomes.。In light of these considerations, we have undertaken astrategy that amalgamates Chain of Thought data with the final target state data, thereby enhancing the fine-tuning of ourlanguage model.。In the course of test reasoning exercises, the language model promptly generates the ultimate target state,with the Chain of Thought information being implicitly encoded within the neural network parameters.Instruction Datasets.。To cover a comprehensive range of instruction types and state distributions, we generated fourtypes of instruction sets, which, when combined with states sampled from the environment, result in four different datasets.These are the HI (Human Instruction) dataset, constructed based on human-annotated commands; the SI (State Instruction)dataset, built by reverse-generating commands based on state transitions specified by the intelligent agent; the AI (AgentInstruction) dataset, constructed by main kinds of instruction which can be complete by pre-trained Agent; and the RI(Random Instruction) dataset, generated through random sampling of agent state transitions and random commands.•IH(Human Instructions).。We generate open-ended instructions manually, while the corresponding states are sampledfrom the intelligent agent’s interaction logs.。These are combined and annotated using GPT-4 based on the promptingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationmethod previously described.。We found that due to varying frequencies of state changes during the agent’s interactions,some states are difficult to capture comprehensively only using random sampling.。To ensure a more comprehensivedistribution of states in the data and to facilitate better understanding by the language model, we employ a multi-roundrejection sampling approach to construct state set.。Let Sbe the set of states waiting to be sampled.。We perform multiplerounds of sampling on S, with Sgetirepresenting the set of states sampled in the i-th round, initially empty.。Next, wesample a state sfrom Swithout replacement and check whether shas any state values not present in Sgeti.。If it does,we accept it and add it to Sgeti, otherwise we reject it.. \n",
      "the piece 345\n",
      " \n",
      "the Chainof Thought reasoning process may not yield superior outcomes.。In light of these considerations, we have undertaken astrategy that amalgamates Chain of Thought data with the final target state data, thereby enhancing the fine-tuning of ourlanguage model.。In the course of test reasoning exercises, the language model promptly generates the ultimate target state,with the Chain of Thought information being implicitly encoded within the neural network parameters.Instruction Datasets.。To cover a comprehensive range of instruction types and state distributions, we generated fourtypes of instruction sets, which, when combined with states sampled from the environment, result in four different datasets.These are the HI (Human Instruction) dataset, constructed based on human-annotated commands; the SI (State Instruction)dataset, built by reverse-generating commands based on state transitions specified by the intelligent agent; the AI (AgentInstruction) dataset, constructed by main kinds of instruction which can be complete by pre-trained Agent; and the RI(Random Instruction) dataset, generated through random sampling of agent state transitions and random commands.•IH(Human Instructions).。We generate open-ended instructions manually, while the corresponding states are sampledfrom the intelligent agent’s interaction logs.。These are combined and annotated using GPT-4 based on the promptingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationmethod previously described.。We found that due to varying frequencies of state changes during the agent’s interactions,some states are difficult to capture comprehensively only using random sampling.。To ensure a more comprehensivedistribution of states in the data and to facilitate better understanding by the language model, we employ a multi-roundrejection sampling approach to construct state set.。Let Sbe the set of states waiting to be sampled.。We perform multiplerounds of sampling on S, with Sgetirepresenting the set of states sampled in the i-th round, initially empty.。Next, wesample a state sfrom Swithout replacement and check whether shas any state values not present in Sgeti.。If it does,we accept it and add it to Sgeti, otherwise we reject it.。Once all states in Shave been sampled, one round is completed.Sgetiis the result of i-th round’s sampling, and Swill be reset for the next round.. \n",
      "the piece 346\n",
      " \n",
      "tes Chain of Thought data with the final target state data, thereby enhancing the fine-tuning of ourlanguage model.。In the course of test reasoning exercises, the language model promptly generates the ultimate target state,with the Chain of Thought information being implicitly encoded within the neural network parameters.Instruction Datasets.。To cover a comprehensive range of instruction types and state distributions, we generated fourtypes of instruction sets, which, when combined with states sampled from the environment, result in four different datasets.These are the HI (Human Instruction) dataset, constructed based on human-annotated commands; the SI (State Instruction)dataset, built by reverse-generating commands based on state transitions specified by the intelligent agent; the AI (AgentInstruction) dataset, constructed by main kinds of instruction which can be complete by pre-trained Agent; and the RI(Random Instruction) dataset, generated through random sampling of agent state transitions and random commands.•IH(Human Instructions).。We generate open-ended instructions manually, while the corresponding states are sampledfrom the intelligent agent’s interaction logs.。These are combined and annotated using GPT-4 based on the promptingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationmethod previously described.。We found that due to varying frequencies of state changes during the agent’s interactions,some states are difficult to capture comprehensively only using random sampling.。To ensure a more comprehensivedistribution of states in the data and to facilitate better understanding by the language model, we employ a multi-roundrejection sampling approach to construct state set.。Let Sbe the set of states waiting to be sampled.。We perform multiplerounds of sampling on S, with Sgetirepresenting the set of states sampled in the i-th round, initially empty.。Next, wesample a state sfrom Swithout replacement and check whether shas any state values not present in Sgeti.。If it does,we accept it and add it to Sgeti, otherwise we reject it.。Once all states in Shave been sampled, one round is completed.Sgetiis the result of i-th round’s sampling, and Swill be reset for the next round.。This sampling method is employed toenhance the comprehensiveness of state coverage in all datasets except the Random Instruction dataset.•IS(State Instructions).. \n",
      "the piece 347\n",
      " \n",
      "t reasoning exercises, the language model promptly generates the ultimate target state,with the Chain of Thought information being implicitly encoded within the neural network parameters.Instruction Datasets.。To cover a comprehensive range of instruction types and state distributions, we generated fourtypes of instruction sets, which, when combined with states sampled from the environment, result in four different datasets.These are the HI (Human Instruction) dataset, constructed based on human-annotated commands; the SI (State Instruction)dataset, built by reverse-generating commands based on state transitions specified by the intelligent agent; the AI (AgentInstruction) dataset, constructed by main kinds of instruction which can be complete by pre-trained Agent; and the RI(Random Instruction) dataset, generated through random sampling of agent state transitions and random commands.•IH(Human Instructions).。We generate open-ended instructions manually, while the corresponding states are sampledfrom the intelligent agent’s interaction logs.。These are combined and annotated using GPT-4 based on the promptingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationmethod previously described.。We found that due to varying frequencies of state changes during the agent’s interactions,some states are difficult to capture comprehensively only using random sampling.。To ensure a more comprehensivedistribution of states in the data and to facilitate better understanding by the language model, we employ a multi-roundrejection sampling approach to construct state set.。Let Sbe the set of states waiting to be sampled.。We perform multiplerounds of sampling on S, with Sgetirepresenting the set of states sampled in the i-th round, initially empty.。Next, wesample a state sfrom Swithout replacement and check whether shas any state values not present in Sgeti.。If it does,we accept it and add it to Sgeti, otherwise we reject it.。Once all states in Shave been sampled, one round is completed.Sgetiis the result of i-th round’s sampling, and Swill be reset for the next round.。This sampling method is employed toenhance the comprehensiveness of state coverage in all datasets except the Random Instruction dataset.•IS(State Instructions).。We aim to cover a broader range of state changes in the instructions to enhance the languagemodel’s understanding of various state transitions.. \n",
      "the piece 348\n",
      " \n",
      "licitly encoded within the neural network parameters.Instruction Datasets.。To cover a comprehensive range of instruction types and state distributions, we generated fourtypes of instruction sets, which, when combined with states sampled from the environment, result in four different datasets.These are the HI (Human Instruction) dataset, constructed based on human-annotated commands; the SI (State Instruction)dataset, built by reverse-generating commands based on state transitions specified by the intelligent agent; the AI (AgentInstruction) dataset, constructed by main kinds of instruction which can be complete by pre-trained Agent; and the RI(Random Instruction) dataset, generated through random sampling of agent state transitions and random commands.•IH(Human Instructions).。We generate open-ended instructions manually, while the corresponding states are sampledfrom the intelligent agent’s interaction logs.。These are combined and annotated using GPT-4 based on the promptingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationmethod previously described.。We found that due to varying frequencies of state changes during the agent’s interactions,some states are difficult to capture comprehensively only using random sampling.。To ensure a more comprehensivedistribution of states in the data and to facilitate better understanding by the language model, we employ a multi-roundrejection sampling approach to construct state set.。Let Sbe the set of states waiting to be sampled.。We perform multiplerounds of sampling on S, with Sgetirepresenting the set of states sampled in the i-th round, initially empty.。Next, wesample a state sfrom Swithout replacement and check whether shas any state values not present in Sgeti.。If it does,we accept it and add it to Sgeti, otherwise we reject it.。Once all states in Shave been sampled, one round is completed.Sgetiis the result of i-th round’s sampling, and Swill be reset for the next round.。This sampling method is employed toenhance the comprehensiveness of state coverage in all datasets except the Random Instruction dataset.•IS(State Instructions).。We aim to cover a broader range of state changes in the instructions to enhance the languagemodel’s understanding of various state transitions.。To achieve this, we design corresponding goals and instructions forall states.. \n",
      "the piece 349\n",
      " \n",
      "s, which, when combined with states sampled from the environment, result in four different datasets.These are the HI (Human Instruction) dataset, constructed based on human-annotated commands; the SI (State Instruction)dataset, built by reverse-generating commands based on state transitions specified by the intelligent agent; the AI (AgentInstruction) dataset, constructed by main kinds of instruction which can be complete by pre-trained Agent; and the RI(Random Instruction) dataset, generated through random sampling of agent state transitions and random commands.•IH(Human Instructions).。We generate open-ended instructions manually, while the corresponding states are sampledfrom the intelligent agent’s interaction logs.。These are combined and annotated using GPT-4 based on the promptingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationmethod previously described.。We found that due to varying frequencies of state changes during the agent’s interactions,some states are difficult to capture comprehensively only using random sampling.。To ensure a more comprehensivedistribution of states in the data and to facilitate better understanding by the language model, we employ a multi-roundrejection sampling approach to construct state set.。Let Sbe the set of states waiting to be sampled.。We perform multiplerounds of sampling on S, with Sgetirepresenting the set of states sampled in the i-th round, initially empty.。Next, wesample a state sfrom Swithout replacement and check whether shas any state values not present in Sgeti.。If it does,we accept it and add it to Sgeti, otherwise we reject it.。Once all states in Shave been sampled, one round is completed.Sgetiis the result of i-th round’s sampling, and Swill be reset for the next round.。This sampling method is employed toenhance the comprehensiveness of state coverage in all datasets except the Random Instruction dataset.•IS(State Instructions).。We aim to cover a broader range of state changes in the instructions to enhance the languagemodel’s understanding of various state transitions.。To achieve this, we design corresponding goals and instructions forall states.。Specifically, for each value of each state, we generate a series of instructions that require the correspondingstate and value.. \n",
      "the piece 350\n",
      " \n",
      "ilt by reverse-generating commands based on state transitions specified by the intelligent agent; the AI (AgentInstruction) dataset, constructed by main kinds of instruction which can be complete by pre-trained Agent; and the RI(Random Instruction) dataset, generated through random sampling of agent state transitions and random commands.•IH(Human Instructions).。We generate open-ended instructions manually, while the corresponding states are sampledfrom the intelligent agent’s interaction logs.。These are combined and annotated using GPT-4 based on the promptingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationmethod previously described.。We found that due to varying frequencies of state changes during the agent’s interactions,some states are difficult to capture comprehensively only using random sampling.。To ensure a more comprehensivedistribution of states in the data and to facilitate better understanding by the language model, we employ a multi-roundrejection sampling approach to construct state set.。Let Sbe the set of states waiting to be sampled.。We perform multiplerounds of sampling on S, with Sgetirepresenting the set of states sampled in the i-th round, initially empty.。Next, wesample a state sfrom Swithout replacement and check whether shas any state values not present in Sgeti.。If it does,we accept it and add it to Sgeti, otherwise we reject it.。Once all states in Shave been sampled, one round is completed.Sgetiis the result of i-th round’s sampling, and Swill be reset for the next round.。This sampling method is employed toenhance the comprehensiveness of state coverage in all datasets except the Random Instruction dataset.•IS(State Instructions).。We aim to cover a broader range of state changes in the instructions to enhance the languagemodel’s understanding of various state transitions.。To achieve this, we design corresponding goals and instructions forall states.。Specifically, for each value of each state, we generate a series of instructions that require the correspondingstate and value.。These are then annotated using GPT-4 based on the prompting methods previously described.. \n",
      "the piece 351\n",
      " \n",
      "s of state changes during the agent’s interactions,some states are difficult to capture comprehensively only using random sampling.。To ensure a more comprehensivedistribution of states in the data and to facilitate better understanding by the language model, we employ a multi-roundrejection sampling approach to construct state set.。Let Sbe the set of states waiting to be sampled.。We perform multiplerounds of sampling on S, with Sgetirepresenting the set of states sampled in the i-th round, initially empty.。Next, wesample a state sfrom Swithout replacement and check whether shas any state values not present in Sgeti.。If it does,we accept it and add it to Sgeti, otherwise we reject it.。Once all states in Shave been sampled, one round is completed.Sgetiis the result of i-th round’s sampling, and Swill be reset for the next round.。This sampling method is employed toenhance the comprehensiveness of state coverage in all datasets except the Random Instruction dataset.•IS(State Instructions).。We aim to cover a broader range of state changes in the instructions to enhance the languagemodel’s understanding of various state transitions.。To achieve this, we design corresponding goals and instructions forall states.。Specifically, for each value of each state, we generate a series of instructions that require the correspondingstate and value.。These are then annotated using GPT-4 based on the prompting methods previously described.。Theannotated results are checked; if they do not have corresponding states and values, manual annotation and modificationare performed to include the relevant states.•IA(Agent Instructions).. \n",
      "the piece 352\n",
      " \n",
      "nsure a more comprehensivedistribution of states in the data and to facilitate better understanding by the language model, we employ a multi-roundrejection sampling approach to construct state set.。Let Sbe the set of states waiting to be sampled.。We perform multiplerounds of sampling on S, with Sgetirepresenting the set of states sampled in the i-th round, initially empty.。Next, wesample a state sfrom Swithout replacement and check whether shas any state values not present in Sgeti.。If it does,we accept it and add it to Sgeti, otherwise we reject it.。Once all states in Shave been sampled, one round is completed.Sgetiis the result of i-th round’s sampling, and Swill be reset for the next round.。This sampling method is employed toenhance the comprehensiveness of state coverage in all datasets except the Random Instruction dataset.•IS(State Instructions).。We aim to cover a broader range of state changes in the instructions to enhance the languagemodel’s understanding of various state transitions.。To achieve this, we design corresponding goals and instructions forall states.。Specifically, for each value of each state, we generate a series of instructions that require the correspondingstate and value.。These are then annotated using GPT-4 based on the prompting methods previously described.。Theannotated results are checked; if they do not have corresponding states and values, manual annotation and modificationare performed to include the relevant states.•IA(Agent Instructions).。We aim to initially align the planning capabilities of the language model with the pre-trainedabilities of an intelligent agent based on reinforcement learning policies.. \n",
      "the piece 353\n",
      " \n",
      " construct state set.。Let Sbe the set of states waiting to be sampled.。We perform multiplerounds of sampling on S, with Sgetirepresenting the set of states sampled in the i-th round, initially empty.。Next, wesample a state sfrom Swithout replacement and check whether shas any state values not present in Sgeti.。If it does,we accept it and add it to Sgeti, otherwise we reject it.。Once all states in Shave been sampled, one round is completed.Sgetiis the result of i-th round’s sampling, and Swill be reset for the next round.。This sampling method is employed toenhance the comprehensiveness of state coverage in all datasets except the Random Instruction dataset.•IS(State Instructions).。We aim to cover a broader range of state changes in the instructions to enhance the languagemodel’s understanding of various state transitions.。To achieve this, we design corresponding goals and instructions forall states.。Specifically, for each value of each state, we generate a series of instructions that require the correspondingstate and value.。These are then annotated using GPT-4 based on the prompting methods previously described.。Theannotated results are checked; if they do not have corresponding states and values, manual annotation and modificationare performed to include the relevant states.•IA(Agent Instructions).。We aim to initially align the planning capabilities of the language model with the pre-trainedabilities of an intelligent agent based on reinforcement learning policies.。To do so, we generate potential correspondinginstructions based on actual state changes in agent interactions.. \n",
      "the piece 354\n",
      " \n",
      "-th round, initially empty.。Next, wesample a state sfrom Swithout replacement and check whether shas any state values not present in Sgeti.。If it does,we accept it and add it to Sgeti, otherwise we reject it.。Once all states in Shave been sampled, one round is completed.Sgetiis the result of i-th round’s sampling, and Swill be reset for the next round.。This sampling method is employed toenhance the comprehensiveness of state coverage in all datasets except the Random Instruction dataset.•IS(State Instructions).。We aim to cover a broader range of state changes in the instructions to enhance the languagemodel’s understanding of various state transitions.。To achieve this, we design corresponding goals and instructions forall states.。Specifically, for each value of each state, we generate a series of instructions that require the correspondingstate and value.。These are then annotated using GPT-4 based on the prompting methods previously described.。Theannotated results are checked; if they do not have corresponding states and values, manual annotation and modificationare performed to include the relevant states.•IA(Agent Instructions).。We aim to initially align the planning capabilities of the language model with the pre-trainedabilities of an intelligent agent based on reinforcement learning policies.。To do so, we generate potential correspondinginstructions based on actual state changes in agent interactions.。Specifically, we first sample a series of agent state pairsat 5-second intervals.. \n",
      "the piece 355\n",
      " \n",
      "t it.。Once all states in Shave been sampled, one round is completed.Sgetiis the result of i-th round’s sampling, and Swill be reset for the next round.。This sampling method is employed toenhance the comprehensiveness of state coverage in all datasets except the Random Instruction dataset.•IS(State Instructions).。We aim to cover a broader range of state changes in the instructions to enhance the languagemodel’s understanding of various state transitions.。To achieve this, we design corresponding goals and instructions forall states.。Specifically, for each value of each state, we generate a series of instructions that require the correspondingstate and value.。These are then annotated using GPT-4 based on the prompting methods previously described.。Theannotated results are checked; if they do not have corresponding states and values, manual annotation and modificationare performed to include the relevant states.•IA(Agent Instructions).。We aim to initially align the planning capabilities of the language model with the pre-trainedabilities of an intelligent agent based on reinforcement learning policies.。To do so, we generate potential correspondinginstructions based on actual state changes in agent interactions.。Specifically, we first sample a series of agent state pairsat 5-second intervals.。For a subset of these, we manually annotate possible corresponding instructions.. \n",
      "the piece 356\n",
      " \n",
      "ound is completed.Sgetiis the result of i-th round’s sampling, and Swill be reset for the next round.。This sampling method is employed toenhance the comprehensiveness of state coverage in all datasets except the Random Instruction dataset.•IS(State Instructions).。We aim to cover a broader range of state changes in the instructions to enhance the languagemodel’s understanding of various state transitions.。To achieve this, we design corresponding goals and instructions forall states.。Specifically, for each value of each state, we generate a series of instructions that require the correspondingstate and value.。These are then annotated using GPT-4 based on the prompting methods previously described.。Theannotated results are checked; if they do not have corresponding states and values, manual annotation and modificationare performed to include the relevant states.•IA(Agent Instructions).。We aim to initially align the planning capabilities of the language model with the pre-trainedabilities of an intelligent agent based on reinforcement learning policies.。To do so, we generate potential correspondinginstructions based on actual state changes in agent interactions.。Specifically, we first sample a series of agent state pairsat 5-second intervals.。For a subset of these, we manually annotate possible corresponding instructions.。We then usethese manual annotations as a knowledge base and employ the \"langchain\" method to use these examples to guidethe annotation of the remaining data using ChatGPT-3.5.. \n",
      "the piece 357\n",
      " \n",
      "oyed toenhance the comprehensiveness of state coverage in all datasets except the Random Instruction dataset.•IS(State Instructions).。We aim to cover a broader range of state changes in the instructions to enhance the languagemodel’s understanding of various state transitions.。To achieve this, we design corresponding goals and instructions forall states.。Specifically, for each value of each state, we generate a series of instructions that require the correspondingstate and value.。These are then annotated using GPT-4 based on the prompting methods previously described.。Theannotated results are checked; if they do not have corresponding states and values, manual annotation and modificationare performed to include the relevant states.•IA(Agent Instructions).。We aim to initially align the planning capabilities of the language model with the pre-trainedabilities of an intelligent agent based on reinforcement learning policies.。To do so, we generate potential correspondinginstructions based on actual state changes in agent interactions.。Specifically, we first sample a series of agent state pairsat 5-second intervals.。For a subset of these, we manually annotate possible corresponding instructions.。We then usethese manual annotations as a knowledge base and employ the \"langchain\" method to use these examples to guidethe annotation of the remaining data using ChatGPT-3.5.。Finally, we represent all the instructions as vectors usingOpenAI’s embedding API and perform clustering.. \n",
      "the piece 358\n",
      " \n",
      "State Instructions).。We aim to cover a broader range of state changes in the instructions to enhance the languagemodel’s understanding of various state transitions.。To achieve this, we design corresponding goals and instructions forall states.。Specifically, for each value of each state, we generate a series of instructions that require the correspondingstate and value.。These are then annotated using GPT-4 based on the prompting methods previously described.。Theannotated results are checked; if they do not have corresponding states and values, manual annotation and modificationare performed to include the relevant states.•IA(Agent Instructions).。We aim to initially align the planning capabilities of the language model with the pre-trainedabilities of an intelligent agent based on reinforcement learning policies.。To do so, we generate potential correspondinginstructions based on actual state changes in agent interactions.。Specifically, we first sample a series of agent state pairsat 5-second intervals.。For a subset of these, we manually annotate possible corresponding instructions.。We then usethese manual annotations as a knowledge base and employ the \"langchain\" method to use these examples to guidethe annotation of the remaining data using ChatGPT-3.5.。Finally, we represent all the instructions as vectors usingOpenAI’s embedding API and perform clustering.。We select the 14 most representative types of instructions and pairthem cyclically with two rounds of sampled states, ultimately constructing a dataset that better reflects the fundamentalexecution capabilities of the intelligent agent.•IR(Random Instructions).. \n",
      "the piece 359\n",
      " \n",
      "in the instructions to enhance the languagemodel’s understanding of various state transitions.。To achieve this, we design corresponding goals and instructions forall states.。Specifically, for each value of each state, we generate a series of instructions that require the correspondingstate and value.。These are then annotated using GPT-4 based on the prompting methods previously described.。Theannotated results are checked; if they do not have corresponding states and values, manual annotation and modificationare performed to include the relevant states.•IA(Agent Instructions).。We aim to initially align the planning capabilities of the language model with the pre-trainedabilities of an intelligent agent based on reinforcement learning policies.。To do so, we generate potential correspondinginstructions based on actual state changes in agent interactions.。Specifically, we first sample a series of agent state pairsat 5-second intervals.。For a subset of these, we manually annotate possible corresponding instructions.。We then usethese manual annotations as a knowledge base and employ the \"langchain\" method to use these examples to guidethe annotation of the remaining data using ChatGPT-3.5.。Finally, we represent all the instructions as vectors usingOpenAI’s embedding API and perform clustering.。We select the 14 most representative types of instructions and pairthem cyclically with two rounds of sampled states, ultimately constructing a dataset that better reflects the fundamentalexecution capabilities of the intelligent agent.•IR(Random Instructions).。This set is primarily designed to enrich the data distribution.. \n",
      "the piece 360\n",
      " \n",
      "nstructions forall states.。Specifically, for each value of each state, we generate a series of instructions that require the correspondingstate and value.。These are then annotated using GPT-4 based on the prompting methods previously described.。Theannotated results are checked; if they do not have corresponding states and values, manual annotation and modificationare performed to include the relevant states.•IA(Agent Instructions).。We aim to initially align the planning capabilities of the language model with the pre-trainedabilities of an intelligent agent based on reinforcement learning policies.。To do so, we generate potential correspondinginstructions based on actual state changes in agent interactions.。Specifically, we first sample a series of agent state pairsat 5-second intervals.。For a subset of these, we manually annotate possible corresponding instructions.。We then usethese manual annotations as a knowledge base and employ the \"langchain\" method to use these examples to guidethe annotation of the remaining data using ChatGPT-3.5.。Finally, we represent all the instructions as vectors usingOpenAI’s embedding API and perform clustering.。We select the 14 most representative types of instructions and pairthem cyclically with two rounds of sampled states, ultimately constructing a dataset that better reflects the fundamentalexecution capabilities of the intelligent agent.•IR(Random Instructions).。This set is primarily designed to enrich the data distribution.。It is constructed by randomlygenerating instructions and fully randomly sampling states, and then annotated using GPT-4 based on the promptingmethods previously described.The quantaty of the aforementioned four types of datasets is 507 for HI, 1098 for SI, 1441 for AI and 1382 for RI.. \n",
      "the piece 361\n",
      " \n",
      "e then annotated using GPT-4 based on the prompting methods previously described.。Theannotated results are checked; if they do not have corresponding states and values, manual annotation and modificationare performed to include the relevant states.•IA(Agent Instructions).。We aim to initially align the planning capabilities of the language model with the pre-trainedabilities of an intelligent agent based on reinforcement learning policies.。To do so, we generate potential correspondinginstructions based on actual state changes in agent interactions.。Specifically, we first sample a series of agent state pairsat 5-second intervals.。For a subset of these, we manually annotate possible corresponding instructions.。We then usethese manual annotations as a knowledge base and employ the \"langchain\" method to use these examples to guidethe annotation of the remaining data using ChatGPT-3.5.。Finally, we represent all the instructions as vectors usingOpenAI’s embedding API and perform clustering.。We select the 14 most representative types of instructions and pairthem cyclically with two rounds of sampled states, ultimately constructing a dataset that better reflects the fundamentalexecution capabilities of the intelligent agent.•IR(Random Instructions).。This set is primarily designed to enrich the data distribution.。It is constructed by randomlygenerating instructions and fully randomly sampling states, and then annotated using GPT-4 based on the promptingmethods previously described.The quantaty of the aforementioned four types of datasets is 507 for HI, 1098 for SI, 1441 for AI and 1382 for RI.。Moreover,the test dataset construct instructions that differ from those used in the training data, then utilize GPT-4 to generate draftlabels of goals and modified with manually filtered and annotated.. \n",
      "the piece 362\n",
      " \n",
      "ding states and values, manual annotation and modificationare performed to include the relevant states.•IA(Agent Instructions).。We aim to initially align the planning capabilities of the language model with the pre-trainedabilities of an intelligent agent based on reinforcement learning policies.。To do so, we generate potential correspondinginstructions based on actual state changes in agent interactions.。Specifically, we first sample a series of agent state pairsat 5-second intervals.。For a subset of these, we manually annotate possible corresponding instructions.。We then usethese manual annotations as a knowledge base and employ the \"langchain\" method to use these examples to guidethe annotation of the remaining data using ChatGPT-3.5.。Finally, we represent all the instructions as vectors usingOpenAI’s embedding API and perform clustering.。We select the 14 most representative types of instructions and pairthem cyclically with two rounds of sampled states, ultimately constructing a dataset that better reflects the fundamentalexecution capabilities of the intelligent agent.•IR(Random Instructions).。This set is primarily designed to enrich the data distribution.。It is constructed by randomlygenerating instructions and fully randomly sampling states, and then annotated using GPT-4 based on the promptingmethods previously described.The quantaty of the aforementioned four types of datasets is 507 for HI, 1098 for SI, 1441 for AI and 1382 for RI.。Moreover,the test dataset construct instructions that differ from those used in the training data, then utilize GPT-4 to generate draftlabels of goals and modified with manually filtered and annotated.。This test dataset used for evaluating the model’s ability toplan reasonably in response to instructions.. \n",
      "the piece 363\n",
      " \n",
      "de the relevant states.•IA(Agent Instructions).。We aim to initially align the planning capabilities of the language model with the pre-trainedabilities of an intelligent agent based on reinforcement learning policies.。To do so, we generate potential correspondinginstructions based on actual state changes in agent interactions.。Specifically, we first sample a series of agent state pairsat 5-second intervals.。For a subset of these, we manually annotate possible corresponding instructions.。We then usethese manual annotations as a knowledge base and employ the \"langchain\" method to use these examples to guidethe annotation of the remaining data using ChatGPT-3.5.。Finally, we represent all the instructions as vectors usingOpenAI’s embedding API and perform clustering.。We select the 14 most representative types of instructions and pairthem cyclically with two rounds of sampled states, ultimately constructing a dataset that better reflects the fundamentalexecution capabilities of the intelligent agent.•IR(Random Instructions).。This set is primarily designed to enrich the data distribution.。It is constructed by randomlygenerating instructions and fully randomly sampling states, and then annotated using GPT-4 based on the promptingmethods previously described.The quantaty of the aforementioned four types of datasets is 507 for HI, 1098 for SI, 1441 for AI and 1382 for RI.。Moreover,the test dataset construct instructions that differ from those used in the training data, then utilize GPT-4 to generate draftlabels of goals and modified with manually filtered and annotated.。This test dataset used for evaluating the model’s ability toplan reasonably in response to instructions.。And the size of dataset for each tuning step is 26,568 for CoT-assited fine-tuning,4,428 for supervised fine-tuning, and 4,994 for ensembling fine-tuning.F.. \n",
      "the piece 364\n",
      " \n",
      "e pre-trainedabilities of an intelligent agent based on reinforcement learning policies.。To do so, we generate potential correspondinginstructions based on actual state changes in agent interactions.。Specifically, we first sample a series of agent state pairsat 5-second intervals.。For a subset of these, we manually annotate possible corresponding instructions.。We then usethese manual annotations as a knowledge base and employ the \"langchain\" method to use these examples to guidethe annotation of the remaining data using ChatGPT-3.5.。Finally, we represent all the instructions as vectors usingOpenAI’s embedding API and perform clustering.。We select the 14 most representative types of instructions and pairthem cyclically with two rounds of sampled states, ultimately constructing a dataset that better reflects the fundamentalexecution capabilities of the intelligent agent.•IR(Random Instructions).。This set is primarily designed to enrich the data distribution.。It is constructed by randomlygenerating instructions and fully randomly sampling states, and then annotated using GPT-4 based on the promptingmethods previously described.The quantaty of the aforementioned four types of datasets is 507 for HI, 1098 for SI, 1441 for AI and 1382 for RI.。Moreover,the test dataset construct instructions that differ from those used in the training data, then utilize GPT-4 to generate draftlabels of goals and modified with manually filtered and annotated.。This test dataset used for evaluating the model’s ability toplan reasonably in response to instructions.。And the size of dataset for each tuning step is 26,568 for CoT-assited fine-tuning,4,428 for supervised fine-tuning, and 4,994 for ensembling fine-tuning.F.。Distributed Training FrameworkTo improve the training efficiency, we adopt a distributed training system, shown in Figure 7.. \n",
      "the piece 365\n",
      " \n",
      " do so, we generate potential correspondinginstructions based on actual state changes in agent interactions.。Specifically, we first sample a series of agent state pairsat 5-second intervals.。For a subset of these, we manually annotate possible corresponding instructions.。We then usethese manual annotations as a knowledge base and employ the \"langchain\" method to use these examples to guidethe annotation of the remaining data using ChatGPT-3.5.。Finally, we represent all the instructions as vectors usingOpenAI’s embedding API and perform clustering.。We select the 14 most representative types of instructions and pairthem cyclically with two rounds of sampled states, ultimately constructing a dataset that better reflects the fundamentalexecution capabilities of the intelligent agent.•IR(Random Instructions).。This set is primarily designed to enrich the data distribution.。It is constructed by randomlygenerating instructions and fully randomly sampling states, and then annotated using GPT-4 based on the promptingmethods previously described.The quantaty of the aforementioned four types of datasets is 507 for HI, 1098 for SI, 1441 for AI and 1382 for RI.。Moreover,the test dataset construct instructions that differ from those used in the training data, then utilize GPT-4 to generate draftlabels of goals and modified with manually filtered and annotated.。This test dataset used for evaluating the model’s ability toplan reasonably in response to instructions.。And the size of dataset for each tuning step is 26,568 for CoT-assited fine-tuning,4,428 for supervised fine-tuning, and 4,994 for ensembling fine-tuning.F.。Distributed Training FrameworkTo improve the training efficiency, we adopt a distributed training system, shown in Figure 7.。In this system, the Actors runover CPU nodes to collect training data, then send the collected data to the Learner which is deployed on a GPU node.. \n",
      "the piece 366\n",
      " \n",
      "or a subset of these, we manually annotate possible corresponding instructions.。We then usethese manual annotations as a knowledge base and employ the \"langchain\" method to use these examples to guidethe annotation of the remaining data using ChatGPT-3.5.。Finally, we represent all the instructions as vectors usingOpenAI’s embedding API and perform clustering.。We select the 14 most representative types of instructions and pairthem cyclically with two rounds of sampled states, ultimately constructing a dataset that better reflects the fundamentalexecution capabilities of the intelligent agent.•IR(Random Instructions).。This set is primarily designed to enrich the data distribution.。It is constructed by randomlygenerating instructions and fully randomly sampling states, and then annotated using GPT-4 based on the promptingmethods previously described.The quantaty of the aforementioned four types of datasets is 507 for HI, 1098 for SI, 1441 for AI and 1382 for RI.。Moreover,the test dataset construct instructions that differ from those used in the training data, then utilize GPT-4 to generate draftlabels of goals and modified with manually filtered and annotated.。This test dataset used for evaluating the model’s ability toplan reasonably in response to instructions.。And the size of dataset for each tuning step is 26,568 for CoT-assited fine-tuning,4,428 for supervised fine-tuning, and 4,994 for ensembling fine-tuning.F.。Distributed Training FrameworkTo improve the training efficiency, we adopt a distributed training system, shown in Figure 7.。In this system, the Actors runover CPU nodes to collect training data, then send the collected data to the Learner which is deployed on a GPU node.。Wefurther take a LLM server to enable multiple replicas of LLM for goal generation, which improve the throughput of rolloutwhen the RL training is switch to goal-conditioned cases.G.. \n",
      "the piece 367\n",
      " \n",
      "o use these examples to guidethe annotation of the remaining data using ChatGPT-3.5.。Finally, we represent all the instructions as vectors usingOpenAI’s embedding API and perform clustering.。We select the 14 most representative types of instructions and pairthem cyclically with two rounds of sampled states, ultimately constructing a dataset that better reflects the fundamentalexecution capabilities of the intelligent agent.•IR(Random Instructions).。This set is primarily designed to enrich the data distribution.。It is constructed by randomlygenerating instructions and fully randomly sampling states, and then annotated using GPT-4 based on the promptingmethods previously described.The quantaty of the aforementioned four types of datasets is 507 for HI, 1098 for SI, 1441 for AI and 1382 for RI.。Moreover,the test dataset construct instructions that differ from those used in the training data, then utilize GPT-4 to generate draftlabels of goals and modified with manually filtered and annotated.。This test dataset used for evaluating the model’s ability toplan reasonably in response to instructions.。And the size of dataset for each tuning step is 26,568 for CoT-assited fine-tuning,4,428 for supervised fine-tuning, and 4,994 for ensembling fine-tuning.F.。Distributed Training FrameworkTo improve the training efficiency, we adopt a distributed training system, shown in Figure 7.。In this system, the Actors runover CPU nodes to collect training data, then send the collected data to the Learner which is deployed on a GPU node.。Wefurther take a LLM server to enable multiple replicas of LLM for goal generation, which improve the throughput of rolloutwhen the RL training is switch to goal-conditioned cases.G.。Parameter SettingsSome of the hyper-parameters used in our experiment are illustrated in Table 11 and other dynamic hyper-parameters areintroduced their corresponding parts.H.. \n",
      "the piece 368\n",
      " \n",
      "he instructions as vectors usingOpenAI’s embedding API and perform clustering.。We select the 14 most representative types of instructions and pairthem cyclically with two rounds of sampled states, ultimately constructing a dataset that better reflects the fundamentalexecution capabilities of the intelligent agent.•IR(Random Instructions).。This set is primarily designed to enrich the data distribution.。It is constructed by randomlygenerating instructions and fully randomly sampling states, and then annotated using GPT-4 based on the promptingmethods previously described.The quantaty of the aforementioned four types of datasets is 507 for HI, 1098 for SI, 1441 for AI and 1382 for RI.。Moreover,the test dataset construct instructions that differ from those used in the training data, then utilize GPT-4 to generate draftlabels of goals and modified with manually filtered and annotated.。This test dataset used for evaluating the model’s ability toplan reasonably in response to instructions.。And the size of dataset for each tuning step is 26,568 for CoT-assited fine-tuning,4,428 for supervised fine-tuning, and 4,994 for ensembling fine-tuning.F.。Distributed Training FrameworkTo improve the training efficiency, we adopt a distributed training system, shown in Figure 7.。In this system, the Actors runover CPU nodes to collect training data, then send the collected data to the Learner which is deployed on a GPU node.。Wefurther take a LLM server to enable multiple replicas of LLM for goal generation, which improve the throughput of rolloutwhen the RL training is switch to goal-conditioned cases.G.。Parameter SettingsSome of the hyper-parameters used in our experiment are illustrated in Table 11 and other dynamic hyper-parameters areintroduced their corresponding parts.H.。AlgorithmsAlgorithm 3 lists the pseudo-code of goal-conditioned RL procedures in Stage I, where Rbt,RoatandRgtrepresent thediscounted basic return, obstacle avoidance return and goal-reaching return from time step ttill the termination, respectively.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationLearning tasksLearner LeagueTraining outcomes+-Model parameters --+ Redis ,.---' --------..., 'I,..)\"'----------- \"\\ 'IlLLM Replica !,I V\\ I / \\ I '\"------------ ./I II II Goals / I ;'State ab�tractions ,;--1 , ActionsI / ---------1 /;___ --��====���\\\\\\Checkpoints \\ ActorsRollout outcomesModel parameters --+ EnvironmentsFigure 7: This training system has four key parts: Actor, Learner, League and LLM replicas.. \n",
      "the piece 369\n",
      " \n",
      "elect the 14 most representative types of instructions and pairthem cyclically with two rounds of sampled states, ultimately constructing a dataset that better reflects the fundamentalexecution capabilities of the intelligent agent.•IR(Random Instructions).。This set is primarily designed to enrich the data distribution.。It is constructed by randomlygenerating instructions and fully randomly sampling states, and then annotated using GPT-4 based on the promptingmethods previously described.The quantaty of the aforementioned four types of datasets is 507 for HI, 1098 for SI, 1441 for AI and 1382 for RI.。Moreover,the test dataset construct instructions that differ from those used in the training data, then utilize GPT-4 to generate draftlabels of goals and modified with manually filtered and annotated.。This test dataset used for evaluating the model’s ability toplan reasonably in response to instructions.。And the size of dataset for each tuning step is 26,568 for CoT-assited fine-tuning,4,428 for supervised fine-tuning, and 4,994 for ensembling fine-tuning.F.。Distributed Training FrameworkTo improve the training efficiency, we adopt a distributed training system, shown in Figure 7.。In this system, the Actors runover CPU nodes to collect training data, then send the collected data to the Learner which is deployed on a GPU node.。Wefurther take a LLM server to enable multiple replicas of LLM for goal generation, which improve the throughput of rolloutwhen the RL training is switch to goal-conditioned cases.G.。Parameter SettingsSome of the hyper-parameters used in our experiment are illustrated in Table 11 and other dynamic hyper-parameters areintroduced their corresponding parts.H.。AlgorithmsAlgorithm 3 lists the pseudo-code of goal-conditioned RL procedures in Stage I, where Rbt,RoatandRgtrepresent thediscounted basic return, obstacle avoidance return and goal-reaching return from time step ttill the termination, respectively.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationLearning tasksLearner LeagueTraining outcomes+-Model parameters --+ Redis ,.---' --------..., 'I,..)\"'----------- \"\\ 'IlLLM Replica !,I V\\ I / \\ I '\"------------ ./I II II Goals / I ;'State ab�tractions ,;--1 , ActionsI / ---------1 /;___ --��====���\\\\\\Checkpoints \\ ActorsRollout outcomesModel parameters --+ EnvironmentsFigure 7: This training system has four key parts: Actor, Learner, League and LLM replicas.。Actors are responsible fordata collection, the Learner trains the policy model using this data, the League coordinates the overall training process anddisplays results, and the LLM Replicas handle goal generation and distribute them to downstream workers.Algorithm 3 GOAL-CONDITIONED REINFORCEMENT LEARNING1:Input: θparameterizes policy πandζ={ζ1, ζ2, ζ3}parameterizes value heads, goal generators GrndandGop2:fork=1, 2, ... do3: Reset environment with returned initial state s0∼P(s)4: Sample a goal: g∼(Grnd∪Gop)(s0,∆t,∆V)5: Run policy πθkin environment until be terminated6: Actors collect trajectories Dτand send them to the Learner7: Update the θktoθk+1with Equation (6)8: Update ζbymax ζEs∼Dτh∥Rbt−Vbζ1(st)∥2+∥Roat−Voaζ2(st)∥2+1gt̸=∅∥Rgt−Vgζ3(st)∥2i9:end forThe computation of each can be expressed as follows:Rbt=TXi=tγrb(si, ai), Roat=TXi=tγroa(si, ai), Rgt=TXi=tγrg(si, ai, g).. \n",
      "the piece 370\n",
      " \n",
      "h two rounds of sampled states, ultimately constructing a dataset that better reflects the fundamentalexecution capabilities of the intelligent agent.•IR(Random Instructions).。This set is primarily designed to enrich the data distribution.。It is constructed by randomlygenerating instructions and fully randomly sampling states, and then annotated using GPT-4 based on the promptingmethods previously described.The quantaty of the aforementioned four types of datasets is 507 for HI, 1098 for SI, 1441 for AI and 1382 for RI.。Moreover,the test dataset construct instructions that differ from those used in the training data, then utilize GPT-4 to generate draftlabels of goals and modified with manually filtered and annotated.。This test dataset used for evaluating the model’s ability toplan reasonably in response to instructions.。And the size of dataset for each tuning step is 26,568 for CoT-assited fine-tuning,4,428 for supervised fine-tuning, and 4,994 for ensembling fine-tuning.F.。Distributed Training FrameworkTo improve the training efficiency, we adopt a distributed training system, shown in Figure 7.。In this system, the Actors runover CPU nodes to collect training data, then send the collected data to the Learner which is deployed on a GPU node.。Wefurther take a LLM server to enable multiple replicas of LLM for goal generation, which improve the throughput of rolloutwhen the RL training is switch to goal-conditioned cases.G.。Parameter SettingsSome of the hyper-parameters used in our experiment are illustrated in Table 11 and other dynamic hyper-parameters areintroduced their corresponding parts.H.。AlgorithmsAlgorithm 3 lists the pseudo-code of goal-conditioned RL procedures in Stage I, where Rbt,RoatandRgtrepresent thediscounted basic return, obstacle avoidance return and goal-reaching return from time step ttill the termination, respectively.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationLearning tasksLearner LeagueTraining outcomes+-Model parameters --+ Redis ,.---' --------..., 'I,..)\"'----------- \"\\ 'IlLLM Replica !,I V\\ I / \\ I '\"------------ ./I II II Goals / I ;'State ab�tractions ,;--1 , ActionsI / ---------1 /;___ --��====���\\\\\\Checkpoints \\ ActorsRollout outcomesModel parameters --+ EnvironmentsFigure 7: This training system has four key parts: Actor, Learner, League and LLM replicas.。Actors are responsible fordata collection, the Learner trains the policy model using this data, the League coordinates the overall training process anddisplays results, and the LLM Replicas handle goal generation and distribute them to downstream workers.Algorithm 3 GOAL-CONDITIONED REINFORCEMENT LEARNING1:Input: θparameterizes policy πandζ={ζ1, ζ2, ζ3}parameterizes value heads, goal generators GrndandGop2:fork=1, 2, ... do3: Reset environment with returned initial state s0∼P(s)4: Sample a goal: g∼(Grnd∪Gop)(s0,∆t,∆V)5: Run policy πθkin environment until be terminated6: Actors collect trajectories Dτand send them to the Learner7: Update the θktoθk+1with Equation (6)8: Update ζbymax ζEs∼Dτh∥Rbt−Vbζ1(st)∥2+∥Roat−Voaζ2(st)∥2+1gt̸=∅∥Rgt−Vgζ3(st)∥2i9:end forThe computation of each can be expressed as follows:Rbt=TXi=tγrb(si, ai), Roat=TXi=tγroa(si, ai), Rgt=TXi=tγrg(si, ai, g).。(9)As for J(πg,θ)in Equation (6), we follow the computation of policy loss in PPO to express it asJ(πg,θ) = min\u0012πg,θ(s, a)πg,θk(s, a)Aπg,θk(s, a), g(ϵ, Aπg,θk(s, a))\u0013,where (s, a)∼ D τ, g(ϵ, A) =((1 +ϵ)A, A ≥0(1−ϵ)A, A > 0.. \n",
      "the piece 371\n",
      " \n",
      "his set is primarily designed to enrich the data distribution.。It is constructed by randomlygenerating instructions and fully randomly sampling states, and then annotated using GPT-4 based on the promptingmethods previously described.The quantaty of the aforementioned four types of datasets is 507 for HI, 1098 for SI, 1441 for AI and 1382 for RI.。Moreover,the test dataset construct instructions that differ from those used in the training data, then utilize GPT-4 to generate draftlabels of goals and modified with manually filtered and annotated.。This test dataset used for evaluating the model’s ability toplan reasonably in response to instructions.。And the size of dataset for each tuning step is 26,568 for CoT-assited fine-tuning,4,428 for supervised fine-tuning, and 4,994 for ensembling fine-tuning.F.。Distributed Training FrameworkTo improve the training efficiency, we adopt a distributed training system, shown in Figure 7.。In this system, the Actors runover CPU nodes to collect training data, then send the collected data to the Learner which is deployed on a GPU node.。Wefurther take a LLM server to enable multiple replicas of LLM for goal generation, which improve the throughput of rolloutwhen the RL training is switch to goal-conditioned cases.G.。Parameter SettingsSome of the hyper-parameters used in our experiment are illustrated in Table 11 and other dynamic hyper-parameters areintroduced their corresponding parts.H.。AlgorithmsAlgorithm 3 lists the pseudo-code of goal-conditioned RL procedures in Stage I, where Rbt,RoatandRgtrepresent thediscounted basic return, obstacle avoidance return and goal-reaching return from time step ttill the termination, respectively.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationLearning tasksLearner LeagueTraining outcomes+-Model parameters --+ Redis ,.---' --------..., 'I,..)\"'----------- \"\\ 'IlLLM Replica !,I V\\ I / \\ I '\"------------ ./I II II Goals / I ;'State ab�tractions ,;--1 , ActionsI / ---------1 /;___ --��====���\\\\\\Checkpoints \\ ActorsRollout outcomesModel parameters --+ EnvironmentsFigure 7: This training system has four key parts: Actor, Learner, League and LLM replicas.。Actors are responsible fordata collection, the Learner trains the policy model using this data, the League coordinates the overall training process anddisplays results, and the LLM Replicas handle goal generation and distribute them to downstream workers.Algorithm 3 GOAL-CONDITIONED REINFORCEMENT LEARNING1:Input: θparameterizes policy πandζ={ζ1, ζ2, ζ3}parameterizes value heads, goal generators GrndandGop2:fork=1, 2, ... do3: Reset environment with returned initial state s0∼P(s)4: Sample a goal: g∼(Grnd∪Gop)(s0,∆t,∆V)5: Run policy πθkin environment until be terminated6: Actors collect trajectories Dτand send them to the Learner7: Update the θktoθk+1with Equation (6)8: Update ζbymax ζEs∼Dτh∥Rbt−Vbζ1(st)∥2+∥Roat−Voaζ2(st)∥2+1gt̸=∅∥Rgt−Vgζ3(st)∥2i9:end forThe computation of each can be expressed as follows:Rbt=TXi=tγrb(si, ai), Roat=TXi=tγroa(si, ai), Rgt=TXi=tγrg(si, ai, g).。(9)As for J(πg,θ)in Equation (6), we follow the computation of policy loss in PPO to express it asJ(πg,θ) = min\u0012πg,θ(s, a)πg,θk(s, a)Aπg,θk(s, a), g(ϵ, Aπg,θk(s, a))\u0013,where (s, a)∼ D τ, g(ϵ, A) =((1 +ϵ)A, A ≥0(1−ϵ)A, A > 0.。(10)Aπ(s, a)indicates the advantage is computed under the condition of policy π.I.. \n",
      "the piece 372\n",
      " \n",
      "ructions and fully randomly sampling states, and then annotated using GPT-4 based on the promptingmethods previously described.The quantaty of the aforementioned four types of datasets is 507 for HI, 1098 for SI, 1441 for AI and 1382 for RI.。Moreover,the test dataset construct instructions that differ from those used in the training data, then utilize GPT-4 to generate draftlabels of goals and modified with manually filtered and annotated.。This test dataset used for evaluating the model’s ability toplan reasonably in response to instructions.。And the size of dataset for each tuning step is 26,568 for CoT-assited fine-tuning,4,428 for supervised fine-tuning, and 4,994 for ensembling fine-tuning.F.。Distributed Training FrameworkTo improve the training efficiency, we adopt a distributed training system, shown in Figure 7.。In this system, the Actors runover CPU nodes to collect training data, then send the collected data to the Learner which is deployed on a GPU node.。Wefurther take a LLM server to enable multiple replicas of LLM for goal generation, which improve the throughput of rolloutwhen the RL training is switch to goal-conditioned cases.G.。Parameter SettingsSome of the hyper-parameters used in our experiment are illustrated in Table 11 and other dynamic hyper-parameters areintroduced their corresponding parts.H.。AlgorithmsAlgorithm 3 lists the pseudo-code of goal-conditioned RL procedures in Stage I, where Rbt,RoatandRgtrepresent thediscounted basic return, obstacle avoidance return and goal-reaching return from time step ttill the termination, respectively.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationLearning tasksLearner LeagueTraining outcomes+-Model parameters --+ Redis ,.---' --------..., 'I,..)\"'----------- \"\\ 'IlLLM Replica !,I V\\ I / \\ I '\"------------ ./I II II Goals / I ;'State ab�tractions ,;--1 , ActionsI / ---------1 /;___ --��====���\\\\\\Checkpoints \\ ActorsRollout outcomesModel parameters --+ EnvironmentsFigure 7: This training system has four key parts: Actor, Learner, League and LLM replicas.。Actors are responsible fordata collection, the Learner trains the policy model using this data, the League coordinates the overall training process anddisplays results, and the LLM Replicas handle goal generation and distribute them to downstream workers.Algorithm 3 GOAL-CONDITIONED REINFORCEMENT LEARNING1:Input: θparameterizes policy πandζ={ζ1, ζ2, ζ3}parameterizes value heads, goal generators GrndandGop2:fork=1, 2, ... do3: Reset environment with returned initial state s0∼P(s)4: Sample a goal: g∼(Grnd∪Gop)(s0,∆t,∆V)5: Run policy πθkin environment until be terminated6: Actors collect trajectories Dτand send them to the Learner7: Update the θktoθk+1with Equation (6)8: Update ζbymax ζEs∼Dτh∥Rbt−Vbζ1(st)∥2+∥Roat−Voaζ2(st)∥2+1gt̸=∅∥Rgt−Vgζ3(st)∥2i9:end forThe computation of each can be expressed as follows:Rbt=TXi=tγrb(si, ai), Roat=TXi=tγroa(si, ai), Rgt=TXi=tγrg(si, ai, g).。(9)As for J(πg,θ)in Equation (6), we follow the computation of policy loss in PPO to express it asJ(πg,θ) = min\u0012πg,θ(s, a)πg,θk(s, a)Aπg,θk(s, a), g(ϵ, Aπg,θk(s, a))\u0013,where (s, a)∼ D τ, g(ϵ, A) =((1 +ϵ)A, A ≥0(1−ϵ)A, A > 0.。(10)Aπ(s, a)indicates the advantage is computed under the condition of policy π.I.。Open-ended Goal GenerationInspired by Hindsight Experience Replay (HER) (Andrychowicz et al., 2017), we adopt a similar method to utilizethe collected trajectories for learning a goal generator Gopwhich accepts a state as input.. \n",
      "the piece 373\n",
      " \n",
      "aset construct instructions that differ from those used in the training data, then utilize GPT-4 to generate draftlabels of goals and modified with manually filtered and annotated.。This test dataset used for evaluating the model’s ability toplan reasonably in response to instructions.。And the size of dataset for each tuning step is 26,568 for CoT-assited fine-tuning,4,428 for supervised fine-tuning, and 4,994 for ensembling fine-tuning.F.。Distributed Training FrameworkTo improve the training efficiency, we adopt a distributed training system, shown in Figure 7.。In this system, the Actors runover CPU nodes to collect training data, then send the collected data to the Learner which is deployed on a GPU node.。Wefurther take a LLM server to enable multiple replicas of LLM for goal generation, which improve the throughput of rolloutwhen the RL training is switch to goal-conditioned cases.G.。Parameter SettingsSome of the hyper-parameters used in our experiment are illustrated in Table 11 and other dynamic hyper-parameters areintroduced their corresponding parts.H.。AlgorithmsAlgorithm 3 lists the pseudo-code of goal-conditioned RL procedures in Stage I, where Rbt,RoatandRgtrepresent thediscounted basic return, obstacle avoidance return and goal-reaching return from time step ttill the termination, respectively.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationLearning tasksLearner LeagueTraining outcomes+-Model parameters --+ Redis ,.---' --------..., 'I,..)\"'----------- \"\\ 'IlLLM Replica !,I V\\ I / \\ I '\"------------ ./I II II Goals / I ;'State ab�tractions ,;--1 , ActionsI / ---------1 /;___ --��====���\\\\\\Checkpoints \\ ActorsRollout outcomesModel parameters --+ EnvironmentsFigure 7: This training system has four key parts: Actor, Learner, League and LLM replicas.。Actors are responsible fordata collection, the Learner trains the policy model using this data, the League coordinates the overall training process anddisplays results, and the LLM Replicas handle goal generation and distribute them to downstream workers.Algorithm 3 GOAL-CONDITIONED REINFORCEMENT LEARNING1:Input: θparameterizes policy πandζ={ζ1, ζ2, ζ3}parameterizes value heads, goal generators GrndandGop2:fork=1, 2, ... do3: Reset environment with returned initial state s0∼P(s)4: Sample a goal: g∼(Grnd∪Gop)(s0,∆t,∆V)5: Run policy πθkin environment until be terminated6: Actors collect trajectories Dτand send them to the Learner7: Update the θktoθk+1with Equation (6)8: Update ζbymax ζEs∼Dτh∥Rbt−Vbζ1(st)∥2+∥Roat−Voaζ2(st)∥2+1gt̸=∅∥Rgt−Vgζ3(st)∥2i9:end forThe computation of each can be expressed as follows:Rbt=TXi=tγrb(si, ai), Roat=TXi=tγroa(si, ai), Rgt=TXi=tγrg(si, ai, g).。(9)As for J(πg,θ)in Equation (6), we follow the computation of policy loss in PPO to express it asJ(πg,θ) = min\u0012πg,θ(s, a)πg,θk(s, a)Aπg,θk(s, a), g(ϵ, Aπg,θk(s, a))\u0013,where (s, a)∼ D τ, g(ϵ, A) =((1 +ϵ)A, A ≥0(1−ϵ)A, A > 0.。(10)Aπ(s, a)indicates the advantage is computed under the condition of policy π.I.。Open-ended Goal GenerationInspired by Hindsight Experience Replay (HER) (Andrychowicz et al., 2017), we adopt a similar method to utilizethe collected trajectories for learning a goal generator Gopwhich accepts a state as input.。We conclude its train-ing in two steps: (1) constructing (s, g)pairs with collected trajectories as illustrated in Figure 11(b); (2) supervisedtraining Gopwith the above pairs and an MSE loss between the labeled goals and predicted goals.. \n",
      "the piece 374\n",
      " \n",
      "aining data, then utilize GPT-4 to generate draftlabels of goals and modified with manually filtered and annotated.。This test dataset used for evaluating the model’s ability toplan reasonably in response to instructions.。And the size of dataset for each tuning step is 26,568 for CoT-assited fine-tuning,4,428 for supervised fine-tuning, and 4,994 for ensembling fine-tuning.F.。Distributed Training FrameworkTo improve the training efficiency, we adopt a distributed training system, shown in Figure 7.。In this system, the Actors runover CPU nodes to collect training data, then send the collected data to the Learner which is deployed on a GPU node.。Wefurther take a LLM server to enable multiple replicas of LLM for goal generation, which improve the throughput of rolloutwhen the RL training is switch to goal-conditioned cases.G.。Parameter SettingsSome of the hyper-parameters used in our experiment are illustrated in Table 11 and other dynamic hyper-parameters areintroduced their corresponding parts.H.。AlgorithmsAlgorithm 3 lists the pseudo-code of goal-conditioned RL procedures in Stage I, where Rbt,RoatandRgtrepresent thediscounted basic return, obstacle avoidance return and goal-reaching return from time step ttill the termination, respectively.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationLearning tasksLearner LeagueTraining outcomes+-Model parameters --+ Redis ,.---' --------..., 'I,..)\"'----------- \"\\ 'IlLLM Replica !,I V\\ I / \\ I '\"------------ ./I II II Goals / I ;'State ab�tractions ,;--1 , ActionsI / ---------1 /;___ --��====���\\\\\\Checkpoints \\ ActorsRollout outcomesModel parameters --+ EnvironmentsFigure 7: This training system has four key parts: Actor, Learner, League and LLM replicas.。Actors are responsible fordata collection, the Learner trains the policy model using this data, the League coordinates the overall training process anddisplays results, and the LLM Replicas handle goal generation and distribute them to downstream workers.Algorithm 3 GOAL-CONDITIONED REINFORCEMENT LEARNING1:Input: θparameterizes policy πandζ={ζ1, ζ2, ζ3}parameterizes value heads, goal generators GrndandGop2:fork=1, 2, ... do3: Reset environment with returned initial state s0∼P(s)4: Sample a goal: g∼(Grnd∪Gop)(s0,∆t,∆V)5: Run policy πθkin environment until be terminated6: Actors collect trajectories Dτand send them to the Learner7: Update the θktoθk+1with Equation (6)8: Update ζbymax ζEs∼Dτh∥Rbt−Vbζ1(st)∥2+∥Roat−Voaζ2(st)∥2+1gt̸=∅∥Rgt−Vgζ3(st)∥2i9:end forThe computation of each can be expressed as follows:Rbt=TXi=tγrb(si, ai), Roat=TXi=tγroa(si, ai), Rgt=TXi=tγrg(si, ai, g).。(9)As for J(πg,θ)in Equation (6), we follow the computation of policy loss in PPO to express it asJ(πg,θ) = min\u0012πg,θ(s, a)πg,θk(s, a)Aπg,θk(s, a), g(ϵ, Aπg,θk(s, a))\u0013,where (s, a)∼ D τ, g(ϵ, A) =((1 +ϵ)A, A ≥0(1−ϵ)A, A > 0.。(10)Aπ(s, a)indicates the advantage is computed under the condition of policy π.I.。Open-ended Goal GenerationInspired by Hindsight Experience Replay (HER) (Andrychowicz et al., 2017), we adopt a similar method to utilizethe collected trajectories for learning a goal generator Gopwhich accepts a state as input.。We conclude its train-ing in two steps: (1) constructing (s, g)pairs with collected trajectories as illustrated in Figure 11(b); (2) supervisedtraining Gopwith the above pairs and an MSE loss between the labeled goals and predicted goals.。For step (1),we split trajectories into many segments with length of 200 timesteps.. \n",
      "the piece 375\n",
      " \n",
      "ited fine-tuning,4,428 for supervised fine-tuning, and 4,994 for ensembling fine-tuning.F.。Distributed Training FrameworkTo improve the training efficiency, we adopt a distributed training system, shown in Figure 7.。In this system, the Actors runover CPU nodes to collect training data, then send the collected data to the Learner which is deployed on a GPU node.。Wefurther take a LLM server to enable multiple replicas of LLM for goal generation, which improve the throughput of rolloutwhen the RL training is switch to goal-conditioned cases.G.。Parameter SettingsSome of the hyper-parameters used in our experiment are illustrated in Table 11 and other dynamic hyper-parameters areintroduced their corresponding parts.H.。AlgorithmsAlgorithm 3 lists the pseudo-code of goal-conditioned RL procedures in Stage I, where Rbt,RoatandRgtrepresent thediscounted basic return, obstacle avoidance return and goal-reaching return from time step ttill the termination, respectively.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationLearning tasksLearner LeagueTraining outcomes+-Model parameters --+ Redis ,.---' --------..., 'I,..)\"'----------- \"\\ 'IlLLM Replica !,I V\\ I / \\ I '\"------------ ./I II II Goals / I ;'State ab�tractions ,;--1 , ActionsI / ---------1 /;___ --��====���\\\\\\Checkpoints \\ ActorsRollout outcomesModel parameters --+ EnvironmentsFigure 7: This training system has four key parts: Actor, Learner, League and LLM replicas.。Actors are responsible fordata collection, the Learner trains the policy model using this data, the League coordinates the overall training process anddisplays results, and the LLM Replicas handle goal generation and distribute them to downstream workers.Algorithm 3 GOAL-CONDITIONED REINFORCEMENT LEARNING1:Input: θparameterizes policy πandζ={ζ1, ζ2, ζ3}parameterizes value heads, goal generators GrndandGop2:fork=1, 2, ... do3: Reset environment with returned initial state s0∼P(s)4: Sample a goal: g∼(Grnd∪Gop)(s0,∆t,∆V)5: Run policy πθkin environment until be terminated6: Actors collect trajectories Dτand send them to the Learner7: Update the θktoθk+1with Equation (6)8: Update ζbymax ζEs∼Dτh∥Rbt−Vbζ1(st)∥2+∥Roat−Voaζ2(st)∥2+1gt̸=∅∥Rgt−Vgζ3(st)∥2i9:end forThe computation of each can be expressed as follows:Rbt=TXi=tγrb(si, ai), Roat=TXi=tγroa(si, ai), Rgt=TXi=tγrg(si, ai, g).。(9)As for J(πg,θ)in Equation (6), we follow the computation of policy loss in PPO to express it asJ(πg,θ) = min\u0012πg,θ(s, a)πg,θk(s, a)Aπg,θk(s, a), g(ϵ, Aπg,θk(s, a))\u0013,where (s, a)∼ D τ, g(ϵ, A) =((1 +ϵ)A, A ≥0(1−ϵ)A, A > 0.。(10)Aπ(s, a)indicates the advantage is computed under the condition of policy π.I.。Open-ended Goal GenerationInspired by Hindsight Experience Replay (HER) (Andrychowicz et al., 2017), we adopt a similar method to utilizethe collected trajectories for learning a goal generator Gopwhich accepts a state as input.。We conclude its train-ing in two steps: (1) constructing (s, g)pairs with collected trajectories as illustrated in Figure 11(b); (2) supervisedtraining Gopwith the above pairs and an MSE loss between the labeled goals and predicted goals.。For step (1),we split trajectories into many segments with length of 200 timesteps.。Then, we randomly sample a state sfromthe first 150 steps and sample a state s′from the last 20 steps to derive a goal g=PROJ(s′), with a distributionproportional to their basic value Vbasic(s′).. \n",
      "the piece 376\n",
      " \n",
      "in Figure 7.。In this system, the Actors runover CPU nodes to collect training data, then send the collected data to the Learner which is deployed on a GPU node.。Wefurther take a LLM server to enable multiple replicas of LLM for goal generation, which improve the throughput of rolloutwhen the RL training is switch to goal-conditioned cases.G.。Parameter SettingsSome of the hyper-parameters used in our experiment are illustrated in Table 11 and other dynamic hyper-parameters areintroduced their corresponding parts.H.。AlgorithmsAlgorithm 3 lists the pseudo-code of goal-conditioned RL procedures in Stage I, where Rbt,RoatandRgtrepresent thediscounted basic return, obstacle avoidance return and goal-reaching return from time step ttill the termination, respectively.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationLearning tasksLearner LeagueTraining outcomes+-Model parameters --+ Redis ,.---' --------..., 'I,..)\"'----------- \"\\ 'IlLLM Replica !,I V\\ I / \\ I '\"------------ ./I II II Goals / I ;'State ab�tractions ,;--1 , ActionsI / ---------1 /;___ --��====���\\\\\\Checkpoints \\ ActorsRollout outcomesModel parameters --+ EnvironmentsFigure 7: This training system has four key parts: Actor, Learner, League and LLM replicas.。Actors are responsible fordata collection, the Learner trains the policy model using this data, the League coordinates the overall training process anddisplays results, and the LLM Replicas handle goal generation and distribute them to downstream workers.Algorithm 3 GOAL-CONDITIONED REINFORCEMENT LEARNING1:Input: θparameterizes policy πandζ={ζ1, ζ2, ζ3}parameterizes value heads, goal generators GrndandGop2:fork=1, 2, ... do3: Reset environment with returned initial state s0∼P(s)4: Sample a goal: g∼(Grnd∪Gop)(s0,∆t,∆V)5: Run policy πθkin environment until be terminated6: Actors collect trajectories Dτand send them to the Learner7: Update the θktoθk+1with Equation (6)8: Update ζbymax ζEs∼Dτh∥Rbt−Vbζ1(st)∥2+∥Roat−Voaζ2(st)∥2+1gt̸=∅∥Rgt−Vgζ3(st)∥2i9:end forThe computation of each can be expressed as follows:Rbt=TXi=tγrb(si, ai), Roat=TXi=tγroa(si, ai), Rgt=TXi=tγrg(si, ai, g).。(9)As for J(πg,θ)in Equation (6), we follow the computation of policy loss in PPO to express it asJ(πg,θ) = min\u0012πg,θ(s, a)πg,θk(s, a)Aπg,θk(s, a), g(ϵ, Aπg,θk(s, a))\u0013,where (s, a)∼ D τ, g(ϵ, A) =((1 +ϵ)A, A ≥0(1−ϵ)A, A > 0.。(10)Aπ(s, a)indicates the advantage is computed under the condition of policy π.I.。Open-ended Goal GenerationInspired by Hindsight Experience Replay (HER) (Andrychowicz et al., 2017), we adopt a similar method to utilizethe collected trajectories for learning a goal generator Gopwhich accepts a state as input.。We conclude its train-ing in two steps: (1) constructing (s, g)pairs with collected trajectories as illustrated in Figure 11(b); (2) supervisedtraining Gopwith the above pairs and an MSE loss between the labeled goals and predicted goals.。For step (1),we split trajectories into many segments with length of 200 timesteps.。Then, we randomly sample a state sfromthe first 150 steps and sample a state s′from the last 20 steps to derive a goal g=PROJ(s′), with a distributionproportional to their basic value Vbasic(s′).。For step (2), we train Gopwith s,∆t,Vbasic(s)andVbasic(s′)as in-put to generate goals, where ∆tthe time slot of goal completion, Vbasic(s′)andVbasic(s)the basic state value.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationCoFT... ... ... ...StatesCoTQuestionsCoTResponses  HI      RI    SI     AIChatGLM LoRA-FT+tuning... ... ... ...SFTStatesGoalsInstructions  HI      RI    SI     AIChatGLM LoRA-FT+tuningEFTStateResponse 1ChatGLMLoRA-FTLoRA-CK1LoRA-CK2+......Response 2 Response NEnsemble R esponse generatingSFTvotingInstructionChatGLM LoRA-FT +LoRA-CT RL ModelENVRewardsStateGoal Completion RewardExamination RewardFormat RewardInstruction ResponseExam SetInteractionRewardComplete statusPPO TuningFormat recognitionselectingCo-TrainingStateFigure 8: Overview of the training framework with LLM.. \n",
      "the piece 377\n",
      " \n",
      "d data to the Learner which is deployed on a GPU node.。Wefurther take a LLM server to enable multiple replicas of LLM for goal generation, which improve the throughput of rolloutwhen the RL training is switch to goal-conditioned cases.G.。Parameter SettingsSome of the hyper-parameters used in our experiment are illustrated in Table 11 and other dynamic hyper-parameters areintroduced their corresponding parts.H.。AlgorithmsAlgorithm 3 lists the pseudo-code of goal-conditioned RL procedures in Stage I, where Rbt,RoatandRgtrepresent thediscounted basic return, obstacle avoidance return and goal-reaching return from time step ttill the termination, respectively.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationLearning tasksLearner LeagueTraining outcomes+-Model parameters --+ Redis ,.---' --------..., 'I,..)\"'----------- \"\\ 'IlLLM Replica !,I V\\ I / \\ I '\"------------ ./I II II Goals / I ;'State ab�tractions ,;--1 , ActionsI / ---------1 /;___ --��====���\\\\\\Checkpoints \\ ActorsRollout outcomesModel parameters --+ EnvironmentsFigure 7: This training system has four key parts: Actor, Learner, League and LLM replicas.。Actors are responsible fordata collection, the Learner trains the policy model using this data, the League coordinates the overall training process anddisplays results, and the LLM Replicas handle goal generation and distribute them to downstream workers.Algorithm 3 GOAL-CONDITIONED REINFORCEMENT LEARNING1:Input: θparameterizes policy πandζ={ζ1, ζ2, ζ3}parameterizes value heads, goal generators GrndandGop2:fork=1, 2, ... do3: Reset environment with returned initial state s0∼P(s)4: Sample a goal: g∼(Grnd∪Gop)(s0,∆t,∆V)5: Run policy πθkin environment until be terminated6: Actors collect trajectories Dτand send them to the Learner7: Update the θktoθk+1with Equation (6)8: Update ζbymax ζEs∼Dτh∥Rbt−Vbζ1(st)∥2+∥Roat−Voaζ2(st)∥2+1gt̸=∅∥Rgt−Vgζ3(st)∥2i9:end forThe computation of each can be expressed as follows:Rbt=TXi=tγrb(si, ai), Roat=TXi=tγroa(si, ai), Rgt=TXi=tγrg(si, ai, g).。(9)As for J(πg,θ)in Equation (6), we follow the computation of policy loss in PPO to express it asJ(πg,θ) = min\u0012πg,θ(s, a)πg,θk(s, a)Aπg,θk(s, a), g(ϵ, Aπg,θk(s, a))\u0013,where (s, a)∼ D τ, g(ϵ, A) =((1 +ϵ)A, A ≥0(1−ϵ)A, A > 0.。(10)Aπ(s, a)indicates the advantage is computed under the condition of policy π.I.。Open-ended Goal GenerationInspired by Hindsight Experience Replay (HER) (Andrychowicz et al., 2017), we adopt a similar method to utilizethe collected trajectories for learning a goal generator Gopwhich accepts a state as input.。We conclude its train-ing in two steps: (1) constructing (s, g)pairs with collected trajectories as illustrated in Figure 11(b); (2) supervisedtraining Gopwith the above pairs and an MSE loss between the labeled goals and predicted goals.。For step (1),we split trajectories into many segments with length of 200 timesteps.。Then, we randomly sample a state sfromthe first 150 steps and sample a state s′from the last 20 steps to derive a goal g=PROJ(s′), with a distributionproportional to their basic value Vbasic(s′).。For step (2), we train Gopwith s,∆t,Vbasic(s)andVbasic(s′)as in-put to generate goals, where ∆tthe time slot of goal completion, Vbasic(s′)andVbasic(s)the basic state value.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationCoFT... ... ... ...StatesCoTQuestionsCoTResponses  HI      RI    SI     AIChatGLM LoRA-FT+tuning... ... ... ...SFTStatesGoalsInstructions  HI      RI    SI     AIChatGLM LoRA-FT+tuningEFTStateResponse 1ChatGLMLoRA-FTLoRA-CK1LoRA-CK2+......Response 2 Response NEnsemble R esponse generatingSFTvotingInstructionChatGLM LoRA-FT +LoRA-CT RL ModelENVRewardsStateGoal Completion RewardExamination RewardFormat RewardInstruction ResponseExam SetInteractionRewardComplete statusPPO TuningFormat recognitionselectingCo-TrainingStateFigure 8: Overview of the training framework with LLM.。This training framework has three kinds of LLM tuning approaches:CoFT (Chain of Thoughts assisted Fine-Tuning), SFT (Supervised Fine-Tuning), EFT (Ensemble Fine-Tuning); and oneLLM-RL co-training approach.PPO clip eps 0.2Optimizer AdamLearning rate 0.0001Batch size 20480Number of CPUs 5120 (AMD EPYC 7H12 64-Core)Number of GPUs 2 (A100)γ(basic ) 0.995γ(oa) 0.92γ(goal) 0.993λ 0.95Entropy coefficient 0.025Unroll length 20Sample max use times 3Gradient clip threshold 10Table 11: Parameter settings for RL.Validation.. \n",
      "the piece 378\n",
      " \n",
      "hroughput of rolloutwhen the RL training is switch to goal-conditioned cases.G.。Parameter SettingsSome of the hyper-parameters used in our experiment are illustrated in Table 11 and other dynamic hyper-parameters areintroduced their corresponding parts.H.。AlgorithmsAlgorithm 3 lists the pseudo-code of goal-conditioned RL procedures in Stage I, where Rbt,RoatandRgtrepresent thediscounted basic return, obstacle avoidance return and goal-reaching return from time step ttill the termination, respectively.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationLearning tasksLearner LeagueTraining outcomes+-Model parameters --+ Redis ,.---' --------..., 'I,..)\"'----------- \"\\ 'IlLLM Replica !,I V\\ I / \\ I '\"------------ ./I II II Goals / I ;'State ab�tractions ,;--1 , ActionsI / ---------1 /;___ --��====���\\\\\\Checkpoints \\ ActorsRollout outcomesModel parameters --+ EnvironmentsFigure 7: This training system has four key parts: Actor, Learner, League and LLM replicas.。Actors are responsible fordata collection, the Learner trains the policy model using this data, the League coordinates the overall training process anddisplays results, and the LLM Replicas handle goal generation and distribute them to downstream workers.Algorithm 3 GOAL-CONDITIONED REINFORCEMENT LEARNING1:Input: θparameterizes policy πandζ={ζ1, ζ2, ζ3}parameterizes value heads, goal generators GrndandGop2:fork=1, 2, ... do3: Reset environment with returned initial state s0∼P(s)4: Sample a goal: g∼(Grnd∪Gop)(s0,∆t,∆V)5: Run policy πθkin environment until be terminated6: Actors collect trajectories Dτand send them to the Learner7: Update the θktoθk+1with Equation (6)8: Update ζbymax ζEs∼Dτh∥Rbt−Vbζ1(st)∥2+∥Roat−Voaζ2(st)∥2+1gt̸=∅∥Rgt−Vgζ3(st)∥2i9:end forThe computation of each can be expressed as follows:Rbt=TXi=tγrb(si, ai), Roat=TXi=tγroa(si, ai), Rgt=TXi=tγrg(si, ai, g).。(9)As for J(πg,θ)in Equation (6), we follow the computation of policy loss in PPO to express it asJ(πg,θ) = min\u0012πg,θ(s, a)πg,θk(s, a)Aπg,θk(s, a), g(ϵ, Aπg,θk(s, a))\u0013,where (s, a)∼ D τ, g(ϵ, A) =((1 +ϵ)A, A ≥0(1−ϵ)A, A > 0.。(10)Aπ(s, a)indicates the advantage is computed under the condition of policy π.I.。Open-ended Goal GenerationInspired by Hindsight Experience Replay (HER) (Andrychowicz et al., 2017), we adopt a similar method to utilizethe collected trajectories for learning a goal generator Gopwhich accepts a state as input.。We conclude its train-ing in two steps: (1) constructing (s, g)pairs with collected trajectories as illustrated in Figure 11(b); (2) supervisedtraining Gopwith the above pairs and an MSE loss between the labeled goals and predicted goals.。For step (1),we split trajectories into many segments with length of 200 timesteps.。Then, we randomly sample a state sfromthe first 150 steps and sample a state s′from the last 20 steps to derive a goal g=PROJ(s′), with a distributionproportional to their basic value Vbasic(s′).。For step (2), we train Gopwith s,∆t,Vbasic(s)andVbasic(s′)as in-put to generate goals, where ∆tthe time slot of goal completion, Vbasic(s′)andVbasic(s)the basic state value.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationCoFT... ... ... ...StatesCoTQuestionsCoTResponses  HI      RI    SI     AIChatGLM LoRA-FT+tuning... ... ... ...SFTStatesGoalsInstructions  HI      RI    SI     AIChatGLM LoRA-FT+tuningEFTStateResponse 1ChatGLMLoRA-FTLoRA-CK1LoRA-CK2+......Response 2 Response NEnsemble R esponse generatingSFTvotingInstructionChatGLM LoRA-FT +LoRA-CT RL ModelENVRewardsStateGoal Completion RewardExamination RewardFormat RewardInstruction ResponseExam SetInteractionRewardComplete statusPPO TuningFormat recognitionselectingCo-TrainingStateFigure 8: Overview of the training framework with LLM.。This training framework has three kinds of LLM tuning approaches:CoFT (Chain of Thoughts assisted Fine-Tuning), SFT (Supervised Fine-Tuning), EFT (Ensemble Fine-Tuning); and oneLLM-RL co-training approach.PPO clip eps 0.2Optimizer AdamLearning rate 0.0001Batch size 20480Number of CPUs 5120 (AMD EPYC 7H12 64-Core)Number of GPUs 2 (A100)γ(basic ) 0.995γ(oa) 0.92γ(goal) 0.993λ 0.95Entropy coefficient 0.025Unroll length 20Sample max use times 3Gradient clip threshold 10Table 11: Parameter settings for RL.Validation.。To validate the efficiency of Gop, we conduct a com-parison between generated goals and oracles w.r.t distribution, asillustrated in Figure 10.. \n",
      "the piece 379\n",
      " \n",
      " used in our experiment are illustrated in Table 11 and other dynamic hyper-parameters areintroduced their corresponding parts.H.。AlgorithmsAlgorithm 3 lists the pseudo-code of goal-conditioned RL procedures in Stage I, where Rbt,RoatandRgtrepresent thediscounted basic return, obstacle avoidance return and goal-reaching return from time step ttill the termination, respectively.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationLearning tasksLearner LeagueTraining outcomes+-Model parameters --+ Redis ,.---' --------..., 'I,..)\"'----------- \"\\ 'IlLLM Replica !,I V\\ I / \\ I '\"------------ ./I II II Goals / I ;'State ab�tractions ,;--1 , ActionsI / ---------1 /;___ --��====���\\\\\\Checkpoints \\ ActorsRollout outcomesModel parameters --+ EnvironmentsFigure 7: This training system has four key parts: Actor, Learner, League and LLM replicas.。Actors are responsible fordata collection, the Learner trains the policy model using this data, the League coordinates the overall training process anddisplays results, and the LLM Replicas handle goal generation and distribute them to downstream workers.Algorithm 3 GOAL-CONDITIONED REINFORCEMENT LEARNING1:Input: θparameterizes policy πandζ={ζ1, ζ2, ζ3}parameterizes value heads, goal generators GrndandGop2:fork=1, 2, ... do3: Reset environment with returned initial state s0∼P(s)4: Sample a goal: g∼(Grnd∪Gop)(s0,∆t,∆V)5: Run policy πθkin environment until be terminated6: Actors collect trajectories Dτand send them to the Learner7: Update the θktoθk+1with Equation (6)8: Update ζbymax ζEs∼Dτh∥Rbt−Vbζ1(st)∥2+∥Roat−Voaζ2(st)∥2+1gt̸=∅∥Rgt−Vgζ3(st)∥2i9:end forThe computation of each can be expressed as follows:Rbt=TXi=tγrb(si, ai), Roat=TXi=tγroa(si, ai), Rgt=TXi=tγrg(si, ai, g).。(9)As for J(πg,θ)in Equation (6), we follow the computation of policy loss in PPO to express it asJ(πg,θ) = min\u0012πg,θ(s, a)πg,θk(s, a)Aπg,θk(s, a), g(ϵ, Aπg,θk(s, a))\u0013,where (s, a)∼ D τ, g(ϵ, A) =((1 +ϵ)A, A ≥0(1−ϵ)A, A > 0.。(10)Aπ(s, a)indicates the advantage is computed under the condition of policy π.I.。Open-ended Goal GenerationInspired by Hindsight Experience Replay (HER) (Andrychowicz et al., 2017), we adopt a similar method to utilizethe collected trajectories for learning a goal generator Gopwhich accepts a state as input.。We conclude its train-ing in two steps: (1) constructing (s, g)pairs with collected trajectories as illustrated in Figure 11(b); (2) supervisedtraining Gopwith the above pairs and an MSE loss between the labeled goals and predicted goals.。For step (1),we split trajectories into many segments with length of 200 timesteps.。Then, we randomly sample a state sfromthe first 150 steps and sample a state s′from the last 20 steps to derive a goal g=PROJ(s′), with a distributionproportional to their basic value Vbasic(s′).。For step (2), we train Gopwith s,∆t,Vbasic(s)andVbasic(s′)as in-put to generate goals, where ∆tthe time slot of goal completion, Vbasic(s′)andVbasic(s)the basic state value.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationCoFT... ... ... ...StatesCoTQuestionsCoTResponses  HI      RI    SI     AIChatGLM LoRA-FT+tuning... ... ... ...SFTStatesGoalsInstructions  HI      RI    SI     AIChatGLM LoRA-FT+tuningEFTStateResponse 1ChatGLMLoRA-FTLoRA-CK1LoRA-CK2+......Response 2 Response NEnsemble R esponse generatingSFTvotingInstructionChatGLM LoRA-FT +LoRA-CT RL ModelENVRewardsStateGoal Completion RewardExamination RewardFormat RewardInstruction ResponseExam SetInteractionRewardComplete statusPPO TuningFormat recognitionselectingCo-TrainingStateFigure 8: Overview of the training framework with LLM.。This training framework has three kinds of LLM tuning approaches:CoFT (Chain of Thoughts assisted Fine-Tuning), SFT (Supervised Fine-Tuning), EFT (Ensemble Fine-Tuning); and oneLLM-RL co-training approach.PPO clip eps 0.2Optimizer AdamLearning rate 0.0001Batch size 20480Number of CPUs 5120 (AMD EPYC 7H12 64-Core)Number of GPUs 2 (A100)γ(basic ) 0.995γ(oa) 0.92γ(goal) 0.993λ 0.95Entropy coefficient 0.025Unroll length 20Sample max use times 3Gradient clip threshold 10Table 11: Parameter settings for RL.Validation.。To validate the efficiency of Gop, we conduct a com-parison between generated goals and oracles w.r.t distribution, asillustrated in Figure 10.。Empirically, we first construct a test datasetof goals (we label them as Oracles) with trajectories that are sampledwith multiple checkpoints of non-goal policy, to ensure the diversity.Then, the tuples of (s,∆t,∆V)that corresponding to the oracles areused to generate goal predictions (we label them as Prediction) withGop.. \n",
      "the piece 380\n",
      " \n",
      " 3 lists the pseudo-code of goal-conditioned RL procedures in Stage I, where Rbt,RoatandRgtrepresent thediscounted basic return, obstacle avoidance return and goal-reaching return from time step ttill the termination, respectively.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationLearning tasksLearner LeagueTraining outcomes+-Model parameters --+ Redis ,.---' --------..., 'I,..)\"'----------- \"\\ 'IlLLM Replica !,I V\\ I / \\ I '\"------------ ./I II II Goals / I ;'State ab�tractions ,;--1 , ActionsI / ---------1 /;___ --��====���\\\\\\Checkpoints \\ ActorsRollout outcomesModel parameters --+ EnvironmentsFigure 7: This training system has four key parts: Actor, Learner, League and LLM replicas.。Actors are responsible fordata collection, the Learner trains the policy model using this data, the League coordinates the overall training process anddisplays results, and the LLM Replicas handle goal generation and distribute them to downstream workers.Algorithm 3 GOAL-CONDITIONED REINFORCEMENT LEARNING1:Input: θparameterizes policy πandζ={ζ1, ζ2, ζ3}parameterizes value heads, goal generators GrndandGop2:fork=1, 2, ... do3: Reset environment with returned initial state s0∼P(s)4: Sample a goal: g∼(Grnd∪Gop)(s0,∆t,∆V)5: Run policy πθkin environment until be terminated6: Actors collect trajectories Dτand send them to the Learner7: Update the θktoθk+1with Equation (6)8: Update ζbymax ζEs∼Dτh∥Rbt−Vbζ1(st)∥2+∥Roat−Voaζ2(st)∥2+1gt̸=∅∥Rgt−Vgζ3(st)∥2i9:end forThe computation of each can be expressed as follows:Rbt=TXi=tγrb(si, ai), Roat=TXi=tγroa(si, ai), Rgt=TXi=tγrg(si, ai, g).。(9)As for J(πg,θ)in Equation (6), we follow the computation of policy loss in PPO to express it asJ(πg,θ) = min\u0012πg,θ(s, a)πg,θk(s, a)Aπg,θk(s, a), g(ϵ, Aπg,θk(s, a))\u0013,where (s, a)∼ D τ, g(ϵ, A) =((1 +ϵ)A, A ≥0(1−ϵ)A, A > 0.。(10)Aπ(s, a)indicates the advantage is computed under the condition of policy π.I.。Open-ended Goal GenerationInspired by Hindsight Experience Replay (HER) (Andrychowicz et al., 2017), we adopt a similar method to utilizethe collected trajectories for learning a goal generator Gopwhich accepts a state as input.。We conclude its train-ing in two steps: (1) constructing (s, g)pairs with collected trajectories as illustrated in Figure 11(b); (2) supervisedtraining Gopwith the above pairs and an MSE loss between the labeled goals and predicted goals.。For step (1),we split trajectories into many segments with length of 200 timesteps.。Then, we randomly sample a state sfromthe first 150 steps and sample a state s′from the last 20 steps to derive a goal g=PROJ(s′), with a distributionproportional to their basic value Vbasic(s′).。For step (2), we train Gopwith s,∆t,Vbasic(s)andVbasic(s′)as in-put to generate goals, where ∆tthe time slot of goal completion, Vbasic(s′)andVbasic(s)the basic state value.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationCoFT... ... ... ...StatesCoTQuestionsCoTResponses  HI      RI    SI     AIChatGLM LoRA-FT+tuning... ... ... ...SFTStatesGoalsInstructions  HI      RI    SI     AIChatGLM LoRA-FT+tuningEFTStateResponse 1ChatGLMLoRA-FTLoRA-CK1LoRA-CK2+......Response 2 Response NEnsemble R esponse generatingSFTvotingInstructionChatGLM LoRA-FT +LoRA-CT RL ModelENVRewardsStateGoal Completion RewardExamination RewardFormat RewardInstruction ResponseExam SetInteractionRewardComplete statusPPO TuningFormat recognitionselectingCo-TrainingStateFigure 8: Overview of the training framework with LLM.。This training framework has three kinds of LLM tuning approaches:CoFT (Chain of Thoughts assisted Fine-Tuning), SFT (Supervised Fine-Tuning), EFT (Ensemble Fine-Tuning); and oneLLM-RL co-training approach.PPO clip eps 0.2Optimizer AdamLearning rate 0.0001Batch size 20480Number of CPUs 5120 (AMD EPYC 7H12 64-Core)Number of GPUs 2 (A100)γ(basic ) 0.995γ(oa) 0.92γ(goal) 0.993λ 0.95Entropy coefficient 0.025Unroll length 20Sample max use times 3Gradient clip threshold 10Table 11: Parameter settings for RL.Validation.。To validate the efficiency of Gop, we conduct a com-parison between generated goals and oracles w.r.t distribution, asillustrated in Figure 10.。Empirically, we first construct a test datasetof goals (we label them as Oracles) with trajectories that are sampledwith multiple checkpoints of non-goal policy, to ensure the diversity.Then, the tuples of (s,∆t,∆V)that corresponding to the oracles areused to generate goal predictions (we label them as Prediction) withGop.。To visualize the goal distribution, we leverage TSNE (Van derMaaten & Hinton, 2008) to shrink the goal dimension from 68 to 1.The results in Figure 10 show that the distribution of Prediction wellmatches the distribution of Oracles.J.. \n",
      "the piece 381\n",
      " \n",
      " time step ttill the termination, respectively.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationLearning tasksLearner LeagueTraining outcomes+-Model parameters --+ Redis ,.---' --------..., 'I,..)\"'----------- \"\\ 'IlLLM Replica !,I V\\ I / \\ I '\"------------ ./I II II Goals / I ;'State ab�tractions ,;--1 , ActionsI / ---------1 /;___ --��====���\\\\\\Checkpoints \\ ActorsRollout outcomesModel parameters --+ EnvironmentsFigure 7: This training system has four key parts: Actor, Learner, League and LLM replicas.。Actors are responsible fordata collection, the Learner trains the policy model using this data, the League coordinates the overall training process anddisplays results, and the LLM Replicas handle goal generation and distribute them to downstream workers.Algorithm 3 GOAL-CONDITIONED REINFORCEMENT LEARNING1:Input: θparameterizes policy πandζ={ζ1, ζ2, ζ3}parameterizes value heads, goal generators GrndandGop2:fork=1, 2, ... do3: Reset environment with returned initial state s0∼P(s)4: Sample a goal: g∼(Grnd∪Gop)(s0,∆t,∆V)5: Run policy πθkin environment until be terminated6: Actors collect trajectories Dτand send them to the Learner7: Update the θktoθk+1with Equation (6)8: Update ζbymax ζEs∼Dτh∥Rbt−Vbζ1(st)∥2+∥Roat−Voaζ2(st)∥2+1gt̸=∅∥Rgt−Vgζ3(st)∥2i9:end forThe computation of each can be expressed as follows:Rbt=TXi=tγrb(si, ai), Roat=TXi=tγroa(si, ai), Rgt=TXi=tγrg(si, ai, g).。(9)As for J(πg,θ)in Equation (6), we follow the computation of policy loss in PPO to express it asJ(πg,θ) = min\u0012πg,θ(s, a)πg,θk(s, a)Aπg,θk(s, a), g(ϵ, Aπg,θk(s, a))\u0013,where (s, a)∼ D τ, g(ϵ, A) =((1 +ϵ)A, A ≥0(1−ϵ)A, A > 0.。(10)Aπ(s, a)indicates the advantage is computed under the condition of policy π.I.。Open-ended Goal GenerationInspired by Hindsight Experience Replay (HER) (Andrychowicz et al., 2017), we adopt a similar method to utilizethe collected trajectories for learning a goal generator Gopwhich accepts a state as input.。We conclude its train-ing in two steps: (1) constructing (s, g)pairs with collected trajectories as illustrated in Figure 11(b); (2) supervisedtraining Gopwith the above pairs and an MSE loss between the labeled goals and predicted goals.。For step (1),we split trajectories into many segments with length of 200 timesteps.。Then, we randomly sample a state sfromthe first 150 steps and sample a state s′from the last 20 steps to derive a goal g=PROJ(s′), with a distributionproportional to their basic value Vbasic(s′).。For step (2), we train Gopwith s,∆t,Vbasic(s)andVbasic(s′)as in-put to generate goals, where ∆tthe time slot of goal completion, Vbasic(s′)andVbasic(s)the basic state value.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationCoFT... ... ... ...StatesCoTQuestionsCoTResponses  HI      RI    SI     AIChatGLM LoRA-FT+tuning... ... ... ...SFTStatesGoalsInstructions  HI      RI    SI     AIChatGLM LoRA-FT+tuningEFTStateResponse 1ChatGLMLoRA-FTLoRA-CK1LoRA-CK2+......Response 2 Response NEnsemble R esponse generatingSFTvotingInstructionChatGLM LoRA-FT +LoRA-CT RL ModelENVRewardsStateGoal Completion RewardExamination RewardFormat RewardInstruction ResponseExam SetInteractionRewardComplete statusPPO TuningFormat recognitionselectingCo-TrainingStateFigure 8: Overview of the training framework with LLM.。This training framework has three kinds of LLM tuning approaches:CoFT (Chain of Thoughts assisted Fine-Tuning), SFT (Supervised Fine-Tuning), EFT (Ensemble Fine-Tuning); and oneLLM-RL co-training approach.PPO clip eps 0.2Optimizer AdamLearning rate 0.0001Batch size 20480Number of CPUs 5120 (AMD EPYC 7H12 64-Core)Number of GPUs 2 (A100)γ(basic ) 0.995γ(oa) 0.92γ(goal) 0.993λ 0.95Entropy coefficient 0.025Unroll length 20Sample max use times 3Gradient clip threshold 10Table 11: Parameter settings for RL.Validation.。To validate the efficiency of Gop, we conduct a com-parison between generated goals and oracles w.r.t distribution, asillustrated in Figure 10.。Empirically, we first construct a test datasetof goals (we label them as Oracles) with trajectories that are sampledwith multiple checkpoints of non-goal policy, to ensure the diversity.Then, the tuples of (s,∆t,∆V)that corresponding to the oracles areused to generate goal predictions (we label them as Prediction) withGop.。To visualize the goal distribution, we leverage TSNE (Van derMaaten & Hinton, 2008) to shrink the goal dimension from 68 to 1.The results in Figure 10 show that the distribution of Prediction wellmatches the distribution of Oracles.J.。Ablation Study on LLMsThe Impact of Lora Rank.. \n",
      "the piece 382\n",
      " \n",
      "l parameters --+ Redis ,.---' --------..., 'I,..)\"'----------- \"\\ 'IlLLM Replica !,I V\\ I / \\ I '\"------------ ./I II II Goals / I ;'State ab�tractions ,;--1 , ActionsI / ---------1 /;___ --��====���\\\\\\Checkpoints \\ ActorsRollout outcomesModel parameters --+ EnvironmentsFigure 7: This training system has four key parts: Actor, Learner, League and LLM replicas.。Actors are responsible fordata collection, the Learner trains the policy model using this data, the League coordinates the overall training process anddisplays results, and the LLM Replicas handle goal generation and distribute them to downstream workers.Algorithm 3 GOAL-CONDITIONED REINFORCEMENT LEARNING1:Input: θparameterizes policy πandζ={ζ1, ζ2, ζ3}parameterizes value heads, goal generators GrndandGop2:fork=1, 2, ... do3: Reset environment with returned initial state s0∼P(s)4: Sample a goal: g∼(Grnd∪Gop)(s0,∆t,∆V)5: Run policy πθkin environment until be terminated6: Actors collect trajectories Dτand send them to the Learner7: Update the θktoθk+1with Equation (6)8: Update ζbymax ζEs∼Dτh∥Rbt−Vbζ1(st)∥2+∥Roat−Voaζ2(st)∥2+1gt̸=∅∥Rgt−Vgζ3(st)∥2i9:end forThe computation of each can be expressed as follows:Rbt=TXi=tγrb(si, ai), Roat=TXi=tγroa(si, ai), Rgt=TXi=tγrg(si, ai, g).。(9)As for J(πg,θ)in Equation (6), we follow the computation of policy loss in PPO to express it asJ(πg,θ) = min\u0012πg,θ(s, a)πg,θk(s, a)Aπg,θk(s, a), g(ϵ, Aπg,θk(s, a))\u0013,where (s, a)∼ D τ, g(ϵ, A) =((1 +ϵ)A, A ≥0(1−ϵ)A, A > 0.。(10)Aπ(s, a)indicates the advantage is computed under the condition of policy π.I.。Open-ended Goal GenerationInspired by Hindsight Experience Replay (HER) (Andrychowicz et al., 2017), we adopt a similar method to utilizethe collected trajectories for learning a goal generator Gopwhich accepts a state as input.。We conclude its train-ing in two steps: (1) constructing (s, g)pairs with collected trajectories as illustrated in Figure 11(b); (2) supervisedtraining Gopwith the above pairs and an MSE loss between the labeled goals and predicted goals.。For step (1),we split trajectories into many segments with length of 200 timesteps.。Then, we randomly sample a state sfromthe first 150 steps and sample a state s′from the last 20 steps to derive a goal g=PROJ(s′), with a distributionproportional to their basic value Vbasic(s′).。For step (2), we train Gopwith s,∆t,Vbasic(s)andVbasic(s′)as in-put to generate goals, where ∆tthe time slot of goal completion, Vbasic(s′)andVbasic(s)the basic state value.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationCoFT... ... ... ...StatesCoTQuestionsCoTResponses  HI      RI    SI     AIChatGLM LoRA-FT+tuning... ... ... ...SFTStatesGoalsInstructions  HI      RI    SI     AIChatGLM LoRA-FT+tuningEFTStateResponse 1ChatGLMLoRA-FTLoRA-CK1LoRA-CK2+......Response 2 Response NEnsemble R esponse generatingSFTvotingInstructionChatGLM LoRA-FT +LoRA-CT RL ModelENVRewardsStateGoal Completion RewardExamination RewardFormat RewardInstruction ResponseExam SetInteractionRewardComplete statusPPO TuningFormat recognitionselectingCo-TrainingStateFigure 8: Overview of the training framework with LLM.。This training framework has three kinds of LLM tuning approaches:CoFT (Chain of Thoughts assisted Fine-Tuning), SFT (Supervised Fine-Tuning), EFT (Ensemble Fine-Tuning); and oneLLM-RL co-training approach.PPO clip eps 0.2Optimizer AdamLearning rate 0.0001Batch size 20480Number of CPUs 5120 (AMD EPYC 7H12 64-Core)Number of GPUs 2 (A100)γ(basic ) 0.995γ(oa) 0.92γ(goal) 0.993λ 0.95Entropy coefficient 0.025Unroll length 20Sample max use times 3Gradient clip threshold 10Table 11: Parameter settings for RL.Validation.。To validate the efficiency of Gop, we conduct a com-parison between generated goals and oracles w.r.t distribution, asillustrated in Figure 10.。Empirically, we first construct a test datasetof goals (we label them as Oracles) with trajectories that are sampledwith multiple checkpoints of non-goal policy, to ensure the diversity.Then, the tuples of (s,∆t,∆V)that corresponding to the oracles areused to generate goal predictions (we label them as Prediction) withGop.。To visualize the goal distribution, we leverage TSNE (Van derMaaten & Hinton, 2008) to shrink the goal dimension from 68 to 1.The results in Figure 10 show that the distribution of Prediction wellmatches the distribution of Oracles.J.。Ablation Study on LLMsThe Impact of Lora Rank.。We evaluate the impact of the rank parameter on performance during LoRA fine-tuning oflarge language model neural networks.. \n",
      "the piece 383\n",
      " \n",
      " goal generators GrndandGop2:fork=1, 2, ... do3: Reset environment with returned initial state s0∼P(s)4: Sample a goal: g∼(Grnd∪Gop)(s0,∆t,∆V)5: Run policy πθkin environment until be terminated6: Actors collect trajectories Dτand send them to the Learner7: Update the θktoθk+1with Equation (6)8: Update ζbymax ζEs∼Dτh∥Rbt−Vbζ1(st)∥2+∥Roat−Voaζ2(st)∥2+1gt̸=∅∥Rgt−Vgζ3(st)∥2i9:end forThe computation of each can be expressed as follows:Rbt=TXi=tγrb(si, ai), Roat=TXi=tγroa(si, ai), Rgt=TXi=tγrg(si, ai, g).。(9)As for J(πg,θ)in Equation (6), we follow the computation of policy loss in PPO to express it asJ(πg,θ) = min\u0012πg,θ(s, a)πg,θk(s, a)Aπg,θk(s, a), g(ϵ, Aπg,θk(s, a))\u0013,where (s, a)∼ D τ, g(ϵ, A) =((1 +ϵ)A, A ≥0(1−ϵ)A, A > 0.。(10)Aπ(s, a)indicates the advantage is computed under the condition of policy π.I.。Open-ended Goal GenerationInspired by Hindsight Experience Replay (HER) (Andrychowicz et al., 2017), we adopt a similar method to utilizethe collected trajectories for learning a goal generator Gopwhich accepts a state as input.。We conclude its train-ing in two steps: (1) constructing (s, g)pairs with collected trajectories as illustrated in Figure 11(b); (2) supervisedtraining Gopwith the above pairs and an MSE loss between the labeled goals and predicted goals.。For step (1),we split trajectories into many segments with length of 200 timesteps.。Then, we randomly sample a state sfromthe first 150 steps and sample a state s′from the last 20 steps to derive a goal g=PROJ(s′), with a distributionproportional to their basic value Vbasic(s′).。For step (2), we train Gopwith s,∆t,Vbasic(s)andVbasic(s′)as in-put to generate goals, where ∆tthe time slot of goal completion, Vbasic(s′)andVbasic(s)the basic state value.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationCoFT... ... ... ...StatesCoTQuestionsCoTResponses  HI      RI    SI     AIChatGLM LoRA-FT+tuning... ... ... ...SFTStatesGoalsInstructions  HI      RI    SI     AIChatGLM LoRA-FT+tuningEFTStateResponse 1ChatGLMLoRA-FTLoRA-CK1LoRA-CK2+......Response 2 Response NEnsemble R esponse generatingSFTvotingInstructionChatGLM LoRA-FT +LoRA-CT RL ModelENVRewardsStateGoal Completion RewardExamination RewardFormat RewardInstruction ResponseExam SetInteractionRewardComplete statusPPO TuningFormat recognitionselectingCo-TrainingStateFigure 8: Overview of the training framework with LLM.。This training framework has three kinds of LLM tuning approaches:CoFT (Chain of Thoughts assisted Fine-Tuning), SFT (Supervised Fine-Tuning), EFT (Ensemble Fine-Tuning); and oneLLM-RL co-training approach.PPO clip eps 0.2Optimizer AdamLearning rate 0.0001Batch size 20480Number of CPUs 5120 (AMD EPYC 7H12 64-Core)Number of GPUs 2 (A100)γ(basic ) 0.995γ(oa) 0.92γ(goal) 0.993λ 0.95Entropy coefficient 0.025Unroll length 20Sample max use times 3Gradient clip threshold 10Table 11: Parameter settings for RL.Validation.。To validate the efficiency of Gop, we conduct a com-parison between generated goals and oracles w.r.t distribution, asillustrated in Figure 10.。Empirically, we first construct a test datasetof goals (we label them as Oracles) with trajectories that are sampledwith multiple checkpoints of non-goal policy, to ensure the diversity.Then, the tuples of (s,∆t,∆V)that corresponding to the oracles areused to generate goal predictions (we label them as Prediction) withGop.。To visualize the goal distribution, we leverage TSNE (Van derMaaten & Hinton, 2008) to shrink the goal dimension from 68 to 1.The results in Figure 10 show that the distribution of Prediction wellmatches the distribution of Oracles.J.。Ablation Study on LLMsThe Impact of Lora Rank.。We evaluate the impact of the rank parameter on performance during LoRA fine-tuning oflarge language model neural networks.。Generally speaking, the larger the rank parameter, the more comprehensive andthorough the fine-tuning of the neural network, but the corresponding training time and model footprint will be larger.Rank PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)8 0.544 0.672 0.482 0.608 0.502 0.629 0.060 0.12416 0.550 0.673 0.487 0.601 0.507 0.626 0.070 0.12432 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.15964 0.547 0.675 0.501 0.616 0.519 0.635 0.070 0.124128 0.552 0.684 0.507 0.626 0.524 0.645 0.075 0.134Table 12: Evaluation on lora rank.The experimental results are shown in Table 12.. \n",
      "the piece 384\n",
      " \n",
      "drychowicz et al., 2017), we adopt a similar method to utilizethe collected trajectories for learning a goal generator Gopwhich accepts a state as input.。We conclude its train-ing in two steps: (1) constructing (s, g)pairs with collected trajectories as illustrated in Figure 11(b); (2) supervisedtraining Gopwith the above pairs and an MSE loss between the labeled goals and predicted goals.。For step (1),we split trajectories into many segments with length of 200 timesteps.。Then, we randomly sample a state sfromthe first 150 steps and sample a state s′from the last 20 steps to derive a goal g=PROJ(s′), with a distributionproportional to their basic value Vbasic(s′).。For step (2), we train Gopwith s,∆t,Vbasic(s)andVbasic(s′)as in-put to generate goals, where ∆tthe time slot of goal completion, Vbasic(s′)andVbasic(s)the basic state value.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationCoFT... ... ... ...StatesCoTQuestionsCoTResponses  HI      RI    SI     AIChatGLM LoRA-FT+tuning... ... ... ...SFTStatesGoalsInstructions  HI      RI    SI     AIChatGLM LoRA-FT+tuningEFTStateResponse 1ChatGLMLoRA-FTLoRA-CK1LoRA-CK2+......Response 2 Response NEnsemble R esponse generatingSFTvotingInstructionChatGLM LoRA-FT +LoRA-CT RL ModelENVRewardsStateGoal Completion RewardExamination RewardFormat RewardInstruction ResponseExam SetInteractionRewardComplete statusPPO TuningFormat recognitionselectingCo-TrainingStateFigure 8: Overview of the training framework with LLM.。This training framework has three kinds of LLM tuning approaches:CoFT (Chain of Thoughts assisted Fine-Tuning), SFT (Supervised Fine-Tuning), EFT (Ensemble Fine-Tuning); and oneLLM-RL co-training approach.PPO clip eps 0.2Optimizer AdamLearning rate 0.0001Batch size 20480Number of CPUs 5120 (AMD EPYC 7H12 64-Core)Number of GPUs 2 (A100)γ(basic ) 0.995γ(oa) 0.92γ(goal) 0.993λ 0.95Entropy coefficient 0.025Unroll length 20Sample max use times 3Gradient clip threshold 10Table 11: Parameter settings for RL.Validation.。To validate the efficiency of Gop, we conduct a com-parison between generated goals and oracles w.r.t distribution, asillustrated in Figure 10.。Empirically, we first construct a test datasetof goals (we label them as Oracles) with trajectories that are sampledwith multiple checkpoints of non-goal policy, to ensure the diversity.Then, the tuples of (s,∆t,∆V)that corresponding to the oracles areused to generate goal predictions (we label them as Prediction) withGop.。To visualize the goal distribution, we leverage TSNE (Van derMaaten & Hinton, 2008) to shrink the goal dimension from 68 to 1.The results in Figure 10 show that the distribution of Prediction wellmatches the distribution of Oracles.J.。Ablation Study on LLMsThe Impact of Lora Rank.。We evaluate the impact of the rank parameter on performance during LoRA fine-tuning oflarge language model neural networks.。Generally speaking, the larger the rank parameter, the more comprehensive andthorough the fine-tuning of the neural network, but the corresponding training time and model footprint will be larger.Rank PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)8 0.544 0.672 0.482 0.608 0.502 0.629 0.060 0.12416 0.550 0.673 0.487 0.601 0.507 0.626 0.070 0.12432 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.15964 0.547 0.675 0.501 0.616 0.519 0.635 0.070 0.124128 0.552 0.684 0.507 0.626 0.524 0.645 0.075 0.134Table 12: Evaluation on lora rank.The experimental results are shown in Table 12.。Thesize of lora rank has little impact on model performanceindicators, but a large rank will cause the model trainingtime and the size of the saved parameter file to increasedramatically.The Impact of Lora Target.. \n",
      "the piece 385\n",
      " \n",
      "th collected trajectories as illustrated in Figure 11(b); (2) supervisedtraining Gopwith the above pairs and an MSE loss between the labeled goals and predicted goals.。For step (1),we split trajectories into many segments with length of 200 timesteps.。Then, we randomly sample a state sfromthe first 150 steps and sample a state s′from the last 20 steps to derive a goal g=PROJ(s′), with a distributionproportional to their basic value Vbasic(s′).。For step (2), we train Gopwith s,∆t,Vbasic(s)andVbasic(s′)as in-put to generate goals, where ∆tthe time slot of goal completion, Vbasic(s′)andVbasic(s)the basic state value.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationCoFT... ... ... ...StatesCoTQuestionsCoTResponses  HI      RI    SI     AIChatGLM LoRA-FT+tuning... ... ... ...SFTStatesGoalsInstructions  HI      RI    SI     AIChatGLM LoRA-FT+tuningEFTStateResponse 1ChatGLMLoRA-FTLoRA-CK1LoRA-CK2+......Response 2 Response NEnsemble R esponse generatingSFTvotingInstructionChatGLM LoRA-FT +LoRA-CT RL ModelENVRewardsStateGoal Completion RewardExamination RewardFormat RewardInstruction ResponseExam SetInteractionRewardComplete statusPPO TuningFormat recognitionselectingCo-TrainingStateFigure 8: Overview of the training framework with LLM.。This training framework has three kinds of LLM tuning approaches:CoFT (Chain of Thoughts assisted Fine-Tuning), SFT (Supervised Fine-Tuning), EFT (Ensemble Fine-Tuning); and oneLLM-RL co-training approach.PPO clip eps 0.2Optimizer AdamLearning rate 0.0001Batch size 20480Number of CPUs 5120 (AMD EPYC 7H12 64-Core)Number of GPUs 2 (A100)γ(basic ) 0.995γ(oa) 0.92γ(goal) 0.993λ 0.95Entropy coefficient 0.025Unroll length 20Sample max use times 3Gradient clip threshold 10Table 11: Parameter settings for RL.Validation.。To validate the efficiency of Gop, we conduct a com-parison between generated goals and oracles w.r.t distribution, asillustrated in Figure 10.。Empirically, we first construct a test datasetof goals (we label them as Oracles) with trajectories that are sampledwith multiple checkpoints of non-goal policy, to ensure the diversity.Then, the tuples of (s,∆t,∆V)that corresponding to the oracles areused to generate goal predictions (we label them as Prediction) withGop.。To visualize the goal distribution, we leverage TSNE (Van derMaaten & Hinton, 2008) to shrink the goal dimension from 68 to 1.The results in Figure 10 show that the distribution of Prediction wellmatches the distribution of Oracles.J.。Ablation Study on LLMsThe Impact of Lora Rank.。We evaluate the impact of the rank parameter on performance during LoRA fine-tuning oflarge language model neural networks.。Generally speaking, the larger the rank parameter, the more comprehensive andthorough the fine-tuning of the neural network, but the corresponding training time and model footprint will be larger.Rank PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)8 0.544 0.672 0.482 0.608 0.502 0.629 0.060 0.12416 0.550 0.673 0.487 0.601 0.507 0.626 0.070 0.12432 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.15964 0.547 0.675 0.501 0.616 0.519 0.635 0.070 0.124128 0.552 0.684 0.507 0.626 0.524 0.645 0.075 0.134Table 12: Evaluation on lora rank.The experimental results are shown in Table 12.。Thesize of lora rank has little impact on model performanceindicators, but a large rank will cause the model trainingtime and the size of the saved parameter file to increasedramatically.The Impact of Lora Target.。We next verified whichneural networks in fine-tuning the ChatGLM-6B largelanguage model can achieve the best performance.. \n",
      "the piece 386\n",
      " \n",
      "with the above pairs and an MSE loss between the labeled goals and predicted goals.。For step (1),we split trajectories into many segments with length of 200 timesteps.。Then, we randomly sample a state sfromthe first 150 steps and sample a state s′from the last 20 steps to derive a goal g=PROJ(s′), with a distributionproportional to their basic value Vbasic(s′).。For step (2), we train Gopwith s,∆t,Vbasic(s)andVbasic(s′)as in-put to generate goals, where ∆tthe time slot of goal completion, Vbasic(s′)andVbasic(s)the basic state value.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationCoFT... ... ... ...StatesCoTQuestionsCoTResponses  HI      RI    SI     AIChatGLM LoRA-FT+tuning... ... ... ...SFTStatesGoalsInstructions  HI      RI    SI     AIChatGLM LoRA-FT+tuningEFTStateResponse 1ChatGLMLoRA-FTLoRA-CK1LoRA-CK2+......Response 2 Response NEnsemble R esponse generatingSFTvotingInstructionChatGLM LoRA-FT +LoRA-CT RL ModelENVRewardsStateGoal Completion RewardExamination RewardFormat RewardInstruction ResponseExam SetInteractionRewardComplete statusPPO TuningFormat recognitionselectingCo-TrainingStateFigure 8: Overview of the training framework with LLM.。This training framework has three kinds of LLM tuning approaches:CoFT (Chain of Thoughts assisted Fine-Tuning), SFT (Supervised Fine-Tuning), EFT (Ensemble Fine-Tuning); and oneLLM-RL co-training approach.PPO clip eps 0.2Optimizer AdamLearning rate 0.0001Batch size 20480Number of CPUs 5120 (AMD EPYC 7H12 64-Core)Number of GPUs 2 (A100)γ(basic ) 0.995γ(oa) 0.92γ(goal) 0.993λ 0.95Entropy coefficient 0.025Unroll length 20Sample max use times 3Gradient clip threshold 10Table 11: Parameter settings for RL.Validation.。To validate the efficiency of Gop, we conduct a com-parison between generated goals and oracles w.r.t distribution, asillustrated in Figure 10.。Empirically, we first construct a test datasetof goals (we label them as Oracles) with trajectories that are sampledwith multiple checkpoints of non-goal policy, to ensure the diversity.Then, the tuples of (s,∆t,∆V)that corresponding to the oracles areused to generate goal predictions (we label them as Prediction) withGop.。To visualize the goal distribution, we leverage TSNE (Van derMaaten & Hinton, 2008) to shrink the goal dimension from 68 to 1.The results in Figure 10 show that the distribution of Prediction wellmatches the distribution of Oracles.J.。Ablation Study on LLMsThe Impact of Lora Rank.。We evaluate the impact of the rank parameter on performance during LoRA fine-tuning oflarge language model neural networks.。Generally speaking, the larger the rank parameter, the more comprehensive andthorough the fine-tuning of the neural network, but the corresponding training time and model footprint will be larger.Rank PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)8 0.544 0.672 0.482 0.608 0.502 0.629 0.060 0.12416 0.550 0.673 0.487 0.601 0.507 0.626 0.070 0.12432 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.15964 0.547 0.675 0.501 0.616 0.519 0.635 0.070 0.124128 0.552 0.684 0.507 0.626 0.524 0.645 0.075 0.134Table 12: Evaluation on lora rank.The experimental results are shown in Table 12.。Thesize of lora rank has little impact on model performanceindicators, but a large rank will cause the model trainingtime and the size of the saved parameter file to increasedramatically.The Impact of Lora Target.。We next verified whichneural networks in fine-tuning the ChatGLM-6B largelanguage model can achieve the best performance.。TheBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(a) Altitude map (0.8m) (b) Altitude map (4m) (c) Aerial view map (0.4m)Figure 9: Illustration of BEV features in observation space.. \n",
      "the piece 387\n",
      " \n",
      "sample a state s′from the last 20 steps to derive a goal g=PROJ(s′), with a distributionproportional to their basic value Vbasic(s′).。For step (2), we train Gopwith s,∆t,Vbasic(s)andVbasic(s′)as in-put to generate goals, where ∆tthe time slot of goal completion, Vbasic(s′)andVbasic(s)the basic state value.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationCoFT... ... ... ...StatesCoTQuestionsCoTResponses  HI      RI    SI     AIChatGLM LoRA-FT+tuning... ... ... ...SFTStatesGoalsInstructions  HI      RI    SI     AIChatGLM LoRA-FT+tuningEFTStateResponse 1ChatGLMLoRA-FTLoRA-CK1LoRA-CK2+......Response 2 Response NEnsemble R esponse generatingSFTvotingInstructionChatGLM LoRA-FT +LoRA-CT RL ModelENVRewardsStateGoal Completion RewardExamination RewardFormat RewardInstruction ResponseExam SetInteractionRewardComplete statusPPO TuningFormat recognitionselectingCo-TrainingStateFigure 8: Overview of the training framework with LLM.。This training framework has three kinds of LLM tuning approaches:CoFT (Chain of Thoughts assisted Fine-Tuning), SFT (Supervised Fine-Tuning), EFT (Ensemble Fine-Tuning); and oneLLM-RL co-training approach.PPO clip eps 0.2Optimizer AdamLearning rate 0.0001Batch size 20480Number of CPUs 5120 (AMD EPYC 7H12 64-Core)Number of GPUs 2 (A100)γ(basic ) 0.995γ(oa) 0.92γ(goal) 0.993λ 0.95Entropy coefficient 0.025Unroll length 20Sample max use times 3Gradient clip threshold 10Table 11: Parameter settings for RL.Validation.。To validate the efficiency of Gop, we conduct a com-parison between generated goals and oracles w.r.t distribution, asillustrated in Figure 10.。Empirically, we first construct a test datasetof goals (we label them as Oracles) with trajectories that are sampledwith multiple checkpoints of non-goal policy, to ensure the diversity.Then, the tuples of (s,∆t,∆V)that corresponding to the oracles areused to generate goal predictions (we label them as Prediction) withGop.。To visualize the goal distribution, we leverage TSNE (Van derMaaten & Hinton, 2008) to shrink the goal dimension from 68 to 1.The results in Figure 10 show that the distribution of Prediction wellmatches the distribution of Oracles.J.。Ablation Study on LLMsThe Impact of Lora Rank.。We evaluate the impact of the rank parameter on performance during LoRA fine-tuning oflarge language model neural networks.。Generally speaking, the larger the rank parameter, the more comprehensive andthorough the fine-tuning of the neural network, but the corresponding training time and model footprint will be larger.Rank PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)8 0.544 0.672 0.482 0.608 0.502 0.629 0.060 0.12416 0.550 0.673 0.487 0.601 0.507 0.626 0.070 0.12432 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.15964 0.547 0.675 0.501 0.616 0.519 0.635 0.070 0.124128 0.552 0.684 0.507 0.626 0.524 0.645 0.075 0.134Table 12: Evaluation on lora rank.The experimental results are shown in Table 12.。Thesize of lora rank has little impact on model performanceindicators, but a large rank will cause the model trainingtime and the size of the saved parameter file to increasedramatically.The Impact of Lora Target.。We next verified whichneural networks in fine-tuning the ChatGLM-6B largelanguage model can achieve the best performance.。TheBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(a) Altitude map (0.8m) (b) Altitude map (4m) (c) Aerial view map (0.4m)Figure 9: Illustration of BEV features in observation space.。(a) and (b) are the altitude maps where bright areas are higherthan dark areas.. \n",
      "the piece 388\n",
      " \n",
      "ot of goal completion, Vbasic(s′)andVbasic(s)the basic state value.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationCoFT... ... ... ...StatesCoTQuestionsCoTResponses  HI      RI    SI     AIChatGLM LoRA-FT+tuning... ... ... ...SFTStatesGoalsInstructions  HI      RI    SI     AIChatGLM LoRA-FT+tuningEFTStateResponse 1ChatGLMLoRA-FTLoRA-CK1LoRA-CK2+......Response 2 Response NEnsemble R esponse generatingSFTvotingInstructionChatGLM LoRA-FT +LoRA-CT RL ModelENVRewardsStateGoal Completion RewardExamination RewardFormat RewardInstruction ResponseExam SetInteractionRewardComplete statusPPO TuningFormat recognitionselectingCo-TrainingStateFigure 8: Overview of the training framework with LLM.。This training framework has three kinds of LLM tuning approaches:CoFT (Chain of Thoughts assisted Fine-Tuning), SFT (Supervised Fine-Tuning), EFT (Ensemble Fine-Tuning); and oneLLM-RL co-training approach.PPO clip eps 0.2Optimizer AdamLearning rate 0.0001Batch size 20480Number of CPUs 5120 (AMD EPYC 7H12 64-Core)Number of GPUs 2 (A100)γ(basic ) 0.995γ(oa) 0.92γ(goal) 0.993λ 0.95Entropy coefficient 0.025Unroll length 20Sample max use times 3Gradient clip threshold 10Table 11: Parameter settings for RL.Validation.。To validate the efficiency of Gop, we conduct a com-parison between generated goals and oracles w.r.t distribution, asillustrated in Figure 10.。Empirically, we first construct a test datasetof goals (we label them as Oracles) with trajectories that are sampledwith multiple checkpoints of non-goal policy, to ensure the diversity.Then, the tuples of (s,∆t,∆V)that corresponding to the oracles areused to generate goal predictions (we label them as Prediction) withGop.。To visualize the goal distribution, we leverage TSNE (Van derMaaten & Hinton, 2008) to shrink the goal dimension from 68 to 1.The results in Figure 10 show that the distribution of Prediction wellmatches the distribution of Oracles.J.。Ablation Study on LLMsThe Impact of Lora Rank.。We evaluate the impact of the rank parameter on performance during LoRA fine-tuning oflarge language model neural networks.。Generally speaking, the larger the rank parameter, the more comprehensive andthorough the fine-tuning of the neural network, but the corresponding training time and model footprint will be larger.Rank PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)8 0.544 0.672 0.482 0.608 0.502 0.629 0.060 0.12416 0.550 0.673 0.487 0.601 0.507 0.626 0.070 0.12432 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.15964 0.547 0.675 0.501 0.616 0.519 0.635 0.070 0.124128 0.552 0.684 0.507 0.626 0.524 0.645 0.075 0.134Table 12: Evaluation on lora rank.The experimental results are shown in Table 12.。Thesize of lora rank has little impact on model performanceindicators, but a large rank will cause the model trainingtime and the size of the saved parameter file to increasedramatically.The Impact of Lora Target.。We next verified whichneural networks in fine-tuning the ChatGLM-6B largelanguage model can achieve the best performance.。TheBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(a) Altitude map (0.8m) (b) Altitude map (4m) (c) Aerial view map (0.4m)Figure 9: Illustration of BEV features in observation space.。(a) and (b) are the altitude maps where bright areas are higherthan dark areas.。(c) is the aerial view map where the disconnected areas are windows or doors.. \n",
      "the piece 389\n",
      " \n",
      "d Embodied Agent via Language-Policy Bidirectional AdaptationCoFT... ... ... ...StatesCoTQuestionsCoTResponses  HI      RI    SI     AIChatGLM LoRA-FT+tuning... ... ... ...SFTStatesGoalsInstructions  HI      RI    SI     AIChatGLM LoRA-FT+tuningEFTStateResponse 1ChatGLMLoRA-FTLoRA-CK1LoRA-CK2+......Response 2 Response NEnsemble R esponse generatingSFTvotingInstructionChatGLM LoRA-FT +LoRA-CT RL ModelENVRewardsStateGoal Completion RewardExamination RewardFormat RewardInstruction ResponseExam SetInteractionRewardComplete statusPPO TuningFormat recognitionselectingCo-TrainingStateFigure 8: Overview of the training framework with LLM.。This training framework has three kinds of LLM tuning approaches:CoFT (Chain of Thoughts assisted Fine-Tuning), SFT (Supervised Fine-Tuning), EFT (Ensemble Fine-Tuning); and oneLLM-RL co-training approach.PPO clip eps 0.2Optimizer AdamLearning rate 0.0001Batch size 20480Number of CPUs 5120 (AMD EPYC 7H12 64-Core)Number of GPUs 2 (A100)γ(basic ) 0.995γ(oa) 0.92γ(goal) 0.993λ 0.95Entropy coefficient 0.025Unroll length 20Sample max use times 3Gradient clip threshold 10Table 11: Parameter settings for RL.Validation.。To validate the efficiency of Gop, we conduct a com-parison between generated goals and oracles w.r.t distribution, asillustrated in Figure 10.。Empirically, we first construct a test datasetof goals (we label them as Oracles) with trajectories that are sampledwith multiple checkpoints of non-goal policy, to ensure the diversity.Then, the tuples of (s,∆t,∆V)that corresponding to the oracles areused to generate goal predictions (we label them as Prediction) withGop.。To visualize the goal distribution, we leverage TSNE (Van derMaaten & Hinton, 2008) to shrink the goal dimension from 68 to 1.The results in Figure 10 show that the distribution of Prediction wellmatches the distribution of Oracles.J.。Ablation Study on LLMsThe Impact of Lora Rank.。We evaluate the impact of the rank parameter on performance during LoRA fine-tuning oflarge language model neural networks.。Generally speaking, the larger the rank parameter, the more comprehensive andthorough the fine-tuning of the neural network, but the corresponding training time and model footprint will be larger.Rank PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)8 0.544 0.672 0.482 0.608 0.502 0.629 0.060 0.12416 0.550 0.673 0.487 0.601 0.507 0.626 0.070 0.12432 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.15964 0.547 0.675 0.501 0.616 0.519 0.635 0.070 0.124128 0.552 0.684 0.507 0.626 0.524 0.645 0.075 0.134Table 12: Evaluation on lora rank.The experimental results are shown in Table 12.。Thesize of lora rank has little impact on model performanceindicators, but a large rank will cause the model trainingtime and the size of the saved parameter file to increasedramatically.The Impact of Lora Target.。We next verified whichneural networks in fine-tuning the ChatGLM-6B largelanguage model can achieve the best performance.。TheBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(a) Altitude map (0.8m) (b) Altitude map (4m) (c) Aerial view map (0.4m)Figure 9: Illustration of BEV features in observation space.。(a) and (b) are the altitude maps where bright areas are higherthan dark areas.。(c) is the aerial view map where the disconnected areas are windows or doors.。One pixel in (a), (b) and (c)denotes 0.8 meter, 4 meters and 0.4 meter respectively.. \n",
      "the piece 390\n",
      " \n",
      "s  HI      RI    SI     AIChatGLM LoRA-FT+tuningEFTStateResponse 1ChatGLMLoRA-FTLoRA-CK1LoRA-CK2+......Response 2 Response NEnsemble R esponse generatingSFTvotingInstructionChatGLM LoRA-FT +LoRA-CT RL ModelENVRewardsStateGoal Completion RewardExamination RewardFormat RewardInstruction ResponseExam SetInteractionRewardComplete statusPPO TuningFormat recognitionselectingCo-TrainingStateFigure 8: Overview of the training framework with LLM.。This training framework has three kinds of LLM tuning approaches:CoFT (Chain of Thoughts assisted Fine-Tuning), SFT (Supervised Fine-Tuning), EFT (Ensemble Fine-Tuning); and oneLLM-RL co-training approach.PPO clip eps 0.2Optimizer AdamLearning rate 0.0001Batch size 20480Number of CPUs 5120 (AMD EPYC 7H12 64-Core)Number of GPUs 2 (A100)γ(basic ) 0.995γ(oa) 0.92γ(goal) 0.993λ 0.95Entropy coefficient 0.025Unroll length 20Sample max use times 3Gradient clip threshold 10Table 11: Parameter settings for RL.Validation.。To validate the efficiency of Gop, we conduct a com-parison between generated goals and oracles w.r.t distribution, asillustrated in Figure 10.。Empirically, we first construct a test datasetof goals (we label them as Oracles) with trajectories that are sampledwith multiple checkpoints of non-goal policy, to ensure the diversity.Then, the tuples of (s,∆t,∆V)that corresponding to the oracles areused to generate goal predictions (we label them as Prediction) withGop.。To visualize the goal distribution, we leverage TSNE (Van derMaaten & Hinton, 2008) to shrink the goal dimension from 68 to 1.The results in Figure 10 show that the distribution of Prediction wellmatches the distribution of Oracles.J.。Ablation Study on LLMsThe Impact of Lora Rank.。We evaluate the impact of the rank parameter on performance during LoRA fine-tuning oflarge language model neural networks.。Generally speaking, the larger the rank parameter, the more comprehensive andthorough the fine-tuning of the neural network, but the corresponding training time and model footprint will be larger.Rank PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)8 0.544 0.672 0.482 0.608 0.502 0.629 0.060 0.12416 0.550 0.673 0.487 0.601 0.507 0.626 0.070 0.12432 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.15964 0.547 0.675 0.501 0.616 0.519 0.635 0.070 0.124128 0.552 0.684 0.507 0.626 0.524 0.645 0.075 0.134Table 12: Evaluation on lora rank.The experimental results are shown in Table 12.。Thesize of lora rank has little impact on model performanceindicators, but a large rank will cause the model trainingtime and the size of the saved parameter file to increasedramatically.The Impact of Lora Target.。We next verified whichneural networks in fine-tuning the ChatGLM-6B largelanguage model can achieve the best performance.。TheBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(a) Altitude map (0.8m) (b) Altitude map (4m) (c) Aerial view map (0.4m)Figure 9: Illustration of BEV features in observation space.。(a) and (b) are the altitude maps where bright areas are higherthan dark areas.。(c) is the aerial view map where the disconnected areas are windows or doors.。One pixel in (a), (b) and (c)denotes 0.8 meter, 4 meters and 0.4 meter respectively.。The small yellow blocks represent player positions and small blueblocks represent enemy positions.V15T5075100125150175200225250Goal ProjectionOraclesPrediction(a) Corresponding to (∆T,∆V)50 75 100 125 150 175 200 225 250T050100Goal ProjectionOraclesPrediction (b) Corresponding to ∆T0 5 10 15V050100Goal ProjectionOraclesPrediction (c) Corresponding to ∆VFigure 10: Distribution comparison between real goals (Oracles) and goals generated by Gop(Prediction).. \n",
      "the piece 391\n",
      " \n",
      "coefficient 0.025Unroll length 20Sample max use times 3Gradient clip threshold 10Table 11: Parameter settings for RL.Validation.。To validate the efficiency of Gop, we conduct a com-parison between generated goals and oracles w.r.t distribution, asillustrated in Figure 10.。Empirically, we first construct a test datasetof goals (we label them as Oracles) with trajectories that are sampledwith multiple checkpoints of non-goal policy, to ensure the diversity.Then, the tuples of (s,∆t,∆V)that corresponding to the oracles areused to generate goal predictions (we label them as Prediction) withGop.。To visualize the goal distribution, we leverage TSNE (Van derMaaten & Hinton, 2008) to shrink the goal dimension from 68 to 1.The results in Figure 10 show that the distribution of Prediction wellmatches the distribution of Oracles.J.。Ablation Study on LLMsThe Impact of Lora Rank.。We evaluate the impact of the rank parameter on performance during LoRA fine-tuning oflarge language model neural networks.。Generally speaking, the larger the rank parameter, the more comprehensive andthorough the fine-tuning of the neural network, but the corresponding training time and model footprint will be larger.Rank PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)8 0.544 0.672 0.482 0.608 0.502 0.629 0.060 0.12416 0.550 0.673 0.487 0.601 0.507 0.626 0.070 0.12432 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.15964 0.547 0.675 0.501 0.616 0.519 0.635 0.070 0.124128 0.552 0.684 0.507 0.626 0.524 0.645 0.075 0.134Table 12: Evaluation on lora rank.The experimental results are shown in Table 12.。Thesize of lora rank has little impact on model performanceindicators, but a large rank will cause the model trainingtime and the size of the saved parameter file to increasedramatically.The Impact of Lora Target.。We next verified whichneural networks in fine-tuning the ChatGLM-6B largelanguage model can achieve the best performance.。TheBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(a) Altitude map (0.8m) (b) Altitude map (4m) (c) Aerial view map (0.4m)Figure 9: Illustration of BEV features in observation space.。(a) and (b) are the altitude maps where bright areas are higherthan dark areas.。(c) is the aerial view map where the disconnected areas are windows or doors.。One pixel in (a), (b) and (c)denotes 0.8 meter, 4 meters and 0.4 meter respectively.。The small yellow blocks represent player positions and small blueblocks represent enemy positions.V15T5075100125150175200225250Goal ProjectionOraclesPrediction(a) Corresponding to (∆T,∆V)50 75 100 125 150 175 200 225 250T050100Goal ProjectionOraclesPrediction (b) Corresponding to ∆T0 5 10 15V050100Goal ProjectionOraclesPrediction (c) Corresponding to ∆VFigure 10: Distribution comparison between real goals (Oracles) and goals generated by Gop(Prediction).。The illustrationshows that Gopgenerates goals that follows the real distribution, indicating good generalization on open-ended goalgeneration.experimental results are shown in Table 13.. \n",
      "the piece 392\n",
      " \n",
      "es areused to generate goal predictions (we label them as Prediction) withGop.。To visualize the goal distribution, we leverage TSNE (Van derMaaten & Hinton, 2008) to shrink the goal dimension from 68 to 1.The results in Figure 10 show that the distribution of Prediction wellmatches the distribution of Oracles.J.。Ablation Study on LLMsThe Impact of Lora Rank.。We evaluate the impact of the rank parameter on performance during LoRA fine-tuning oflarge language model neural networks.。Generally speaking, the larger the rank parameter, the more comprehensive andthorough the fine-tuning of the neural network, but the corresponding training time and model footprint will be larger.Rank PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)8 0.544 0.672 0.482 0.608 0.502 0.629 0.060 0.12416 0.550 0.673 0.487 0.601 0.507 0.626 0.070 0.12432 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.15964 0.547 0.675 0.501 0.616 0.519 0.635 0.070 0.124128 0.552 0.684 0.507 0.626 0.524 0.645 0.075 0.134Table 12: Evaluation on lora rank.The experimental results are shown in Table 12.。Thesize of lora rank has little impact on model performanceindicators, but a large rank will cause the model trainingtime and the size of the saved parameter file to increasedramatically.The Impact of Lora Target.。We next verified whichneural networks in fine-tuning the ChatGLM-6B largelanguage model can achieve the best performance.。TheBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(a) Altitude map (0.8m) (b) Altitude map (4m) (c) Aerial view map (0.4m)Figure 9: Illustration of BEV features in observation space.。(a) and (b) are the altitude maps where bright areas are higherthan dark areas.。(c) is the aerial view map where the disconnected areas are windows or doors.。One pixel in (a), (b) and (c)denotes 0.8 meter, 4 meters and 0.4 meter respectively.。The small yellow blocks represent player positions and small blueblocks represent enemy positions.V15T5075100125150175200225250Goal ProjectionOraclesPrediction(a) Corresponding to (∆T,∆V)50 75 100 125 150 175 200 225 250T050100Goal ProjectionOraclesPrediction (b) Corresponding to ∆T0 5 10 15V050100Goal ProjectionOraclesPrediction (c) Corresponding to ∆VFigure 10: Distribution comparison between real goals (Oracles) and goals generated by Gop(Prediction).。The illustrationshows that Gopgenerates goals that follows the real distribution, indicating good generalization on open-ended goalgeneration.experimental results are shown in Table 13.。It is worth noting that only fine-tuning the MLP network without fine-tuning theattention network can achieve the best training results.. \n",
      "the piece 393\n",
      " \n",
      "n & Hinton, 2008) to shrink the goal dimension from 68 to 1.The results in Figure 10 show that the distribution of Prediction wellmatches the distribution of Oracles.J.。Ablation Study on LLMsThe Impact of Lora Rank.。We evaluate the impact of the rank parameter on performance during LoRA fine-tuning oflarge language model neural networks.。Generally speaking, the larger the rank parameter, the more comprehensive andthorough the fine-tuning of the neural network, but the corresponding training time and model footprint will be larger.Rank PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)8 0.544 0.672 0.482 0.608 0.502 0.629 0.060 0.12416 0.550 0.673 0.487 0.601 0.507 0.626 0.070 0.12432 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.15964 0.547 0.675 0.501 0.616 0.519 0.635 0.070 0.124128 0.552 0.684 0.507 0.626 0.524 0.645 0.075 0.134Table 12: Evaluation on lora rank.The experimental results are shown in Table 12.。Thesize of lora rank has little impact on model performanceindicators, but a large rank will cause the model trainingtime and the size of the saved parameter file to increasedramatically.The Impact of Lora Target.。We next verified whichneural networks in fine-tuning the ChatGLM-6B largelanguage model can achieve the best performance.。TheBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(a) Altitude map (0.8m) (b) Altitude map (4m) (c) Aerial view map (0.4m)Figure 9: Illustration of BEV features in observation space.。(a) and (b) are the altitude maps where bright areas are higherthan dark areas.。(c) is the aerial view map where the disconnected areas are windows or doors.。One pixel in (a), (b) and (c)denotes 0.8 meter, 4 meters and 0.4 meter respectively.。The small yellow blocks represent player positions and small blueblocks represent enemy positions.V15T5075100125150175200225250Goal ProjectionOraclesPrediction(a) Corresponding to (∆T,∆V)50 75 100 125 150 175 200 225 250T050100Goal ProjectionOraclesPrediction (b) Corresponding to ∆T0 5 10 15V050100Goal ProjectionOraclesPrediction (c) Corresponding to ∆VFigure 10: Distribution comparison between real goals (Oracles) and goals generated by Gop(Prediction).。The illustrationshows that Gopgenerates goals that follows the real distribution, indicating good generalization on open-ended goalgeneration.experimental results are shown in Table 13.。It is worth noting that only fine-tuning the MLP network without fine-tuning theattention network can achieve the best training results.。Although generally speaking, the mainstream fine-tuning task oflarge language models is to fine-tune the attention layer network, but that task usually focuses more on answer semantics.. \n",
      "the piece 394\n",
      " \n",
      "ral networks.。Generally speaking, the larger the rank parameter, the more comprehensive andthorough the fine-tuning of the neural network, but the corresponding training time and model footprint will be larger.Rank PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)8 0.544 0.672 0.482 0.608 0.502 0.629 0.060 0.12416 0.550 0.673 0.487 0.601 0.507 0.626 0.070 0.12432 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.15964 0.547 0.675 0.501 0.616 0.519 0.635 0.070 0.124128 0.552 0.684 0.507 0.626 0.524 0.645 0.075 0.134Table 12: Evaluation on lora rank.The experimental results are shown in Table 12.。Thesize of lora rank has little impact on model performanceindicators, but a large rank will cause the model trainingtime and the size of the saved parameter file to increasedramatically.The Impact of Lora Target.。We next verified whichneural networks in fine-tuning the ChatGLM-6B largelanguage model can achieve the best performance.。TheBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(a) Altitude map (0.8m) (b) Altitude map (4m) (c) Aerial view map (0.4m)Figure 9: Illustration of BEV features in observation space.。(a) and (b) are the altitude maps where bright areas are higherthan dark areas.。(c) is the aerial view map where the disconnected areas are windows or doors.。One pixel in (a), (b) and (c)denotes 0.8 meter, 4 meters and 0.4 meter respectively.。The small yellow blocks represent player positions and small blueblocks represent enemy positions.V15T5075100125150175200225250Goal ProjectionOraclesPrediction(a) Corresponding to (∆T,∆V)50 75 100 125 150 175 200 225 250T050100Goal ProjectionOraclesPrediction (b) Corresponding to ∆T0 5 10 15V050100Goal ProjectionOraclesPrediction (c) Corresponding to ∆VFigure 10: Distribution comparison between real goals (Oracles) and goals generated by Gop(Prediction).。The illustrationshows that Gopgenerates goals that follows the real distribution, indicating good generalization on open-ended goalgeneration.experimental results are shown in Table 13.。It is worth noting that only fine-tuning the MLP network without fine-tuning theattention network can achieve the best training results.。Although generally speaking, the mainstream fine-tuning task oflarge language models is to fine-tune the attention layer network, but that task usually focuses more on answer semantics.。Inour task, we pay more attention to the format to meet the metastate parsing requirements, so fine-tuning the MLP networkcan achieve better results.The Impact of Dataset Scale.. \n",
      "the piece 395\n",
      " \n",
      "oice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)8 0.544 0.672 0.482 0.608 0.502 0.629 0.060 0.12416 0.550 0.673 0.487 0.601 0.507 0.626 0.070 0.12432 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.15964 0.547 0.675 0.501 0.616 0.519 0.635 0.070 0.124128 0.552 0.684 0.507 0.626 0.524 0.645 0.075 0.134Table 12: Evaluation on lora rank.The experimental results are shown in Table 12.。Thesize of lora rank has little impact on model performanceindicators, but a large rank will cause the model trainingtime and the size of the saved parameter file to increasedramatically.The Impact of Lora Target.。We next verified whichneural networks in fine-tuning the ChatGLM-6B largelanguage model can achieve the best performance.。TheBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(a) Altitude map (0.8m) (b) Altitude map (4m) (c) Aerial view map (0.4m)Figure 9: Illustration of BEV features in observation space.。(a) and (b) are the altitude maps where bright areas are higherthan dark areas.。(c) is the aerial view map where the disconnected areas are windows or doors.。One pixel in (a), (b) and (c)denotes 0.8 meter, 4 meters and 0.4 meter respectively.。The small yellow blocks represent player positions and small blueblocks represent enemy positions.V15T5075100125150175200225250Goal ProjectionOraclesPrediction(a) Corresponding to (∆T,∆V)50 75 100 125 150 175 200 225 250T050100Goal ProjectionOraclesPrediction (b) Corresponding to ∆T0 5 10 15V050100Goal ProjectionOraclesPrediction (c) Corresponding to ∆VFigure 10: Distribution comparison between real goals (Oracles) and goals generated by Gop(Prediction).。The illustrationshows that Gopgenerates goals that follows the real distribution, indicating good generalization on open-ended goalgeneration.experimental results are shown in Table 13.。It is worth noting that only fine-tuning the MLP network without fine-tuning theattention network can achieve the best training results.。Although generally speaking, the mainstream fine-tuning task oflarge language models is to fine-tune the attention layer network, but that task usually focuses more on answer semantics.。Inour task, we pay more attention to the format to meet the metastate parsing requirements, so fine-tuning the MLP networkcan achieve better results.The Impact of Dataset Scale.。We conduct experiments of various models with four percentages of fine-tuning train set,i.e., 100%, 30%, 10%, 3%, on the goal generation task.. \n",
      "the piece 396\n",
      " \n",
      "urate(Choice)8 0.544 0.672 0.482 0.608 0.502 0.629 0.060 0.12416 0.550 0.673 0.487 0.601 0.507 0.626 0.070 0.12432 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.15964 0.547 0.675 0.501 0.616 0.519 0.635 0.070 0.124128 0.552 0.684 0.507 0.626 0.524 0.645 0.075 0.134Table 12: Evaluation on lora rank.The experimental results are shown in Table 12.。Thesize of lora rank has little impact on model performanceindicators, but a large rank will cause the model trainingtime and the size of the saved parameter file to increasedramatically.The Impact of Lora Target.。We next verified whichneural networks in fine-tuning the ChatGLM-6B largelanguage model can achieve the best performance.。TheBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(a) Altitude map (0.8m) (b) Altitude map (4m) (c) Aerial view map (0.4m)Figure 9: Illustration of BEV features in observation space.。(a) and (b) are the altitude maps where bright areas are higherthan dark areas.。(c) is the aerial view map where the disconnected areas are windows or doors.。One pixel in (a), (b) and (c)denotes 0.8 meter, 4 meters and 0.4 meter respectively.。The small yellow blocks represent player positions and small blueblocks represent enemy positions.V15T5075100125150175200225250Goal ProjectionOraclesPrediction(a) Corresponding to (∆T,∆V)50 75 100 125 150 175 200 225 250T050100Goal ProjectionOraclesPrediction (b) Corresponding to ∆T0 5 10 15V050100Goal ProjectionOraclesPrediction (c) Corresponding to ∆VFigure 10: Distribution comparison between real goals (Oracles) and goals generated by Gop(Prediction).。The illustrationshows that Gopgenerates goals that follows the real distribution, indicating good generalization on open-ended goalgeneration.experimental results are shown in Table 13.。It is worth noting that only fine-tuning the MLP network without fine-tuning theattention network can achieve the best training results.。Although generally speaking, the mainstream fine-tuning task oflarge language models is to fine-tune the attention layer network, but that task usually focuses more on answer semantics.。Inour task, we pay more attention to the format to meet the metastate parsing requirements, so fine-tuning the MLP networkcan achieve better results.The Impact of Dataset Scale.。We conduct experiments of various models with four percentages of fine-tuning train set,i.e., 100%, 30%, 10%, 3%, on the goal generation task.。The results are shown in Table 15.. \n",
      "the piece 397\n",
      " \n",
      "5 0.505 0.621 0.529 0.652 0.065 0.15964 0.547 0.675 0.501 0.616 0.519 0.635 0.070 0.124128 0.552 0.684 0.507 0.626 0.524 0.645 0.075 0.134Table 12: Evaluation on lora rank.The experimental results are shown in Table 12.。Thesize of lora rank has little impact on model performanceindicators, but a large rank will cause the model trainingtime and the size of the saved parameter file to increasedramatically.The Impact of Lora Target.。We next verified whichneural networks in fine-tuning the ChatGLM-6B largelanguage model can achieve the best performance.。TheBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(a) Altitude map (0.8m) (b) Altitude map (4m) (c) Aerial view map (0.4m)Figure 9: Illustration of BEV features in observation space.。(a) and (b) are the altitude maps where bright areas are higherthan dark areas.。(c) is the aerial view map where the disconnected areas are windows or doors.。One pixel in (a), (b) and (c)denotes 0.8 meter, 4 meters and 0.4 meter respectively.。The small yellow blocks represent player positions and small blueblocks represent enemy positions.V15T5075100125150175200225250Goal ProjectionOraclesPrediction(a) Corresponding to (∆T,∆V)50 75 100 125 150 175 200 225 250T050100Goal ProjectionOraclesPrediction (b) Corresponding to ∆T0 5 10 15V050100Goal ProjectionOraclesPrediction (c) Corresponding to ∆VFigure 10: Distribution comparison between real goals (Oracles) and goals generated by Gop(Prediction).。The illustrationshows that Gopgenerates goals that follows the real distribution, indicating good generalization on open-ended goalgeneration.experimental results are shown in Table 13.。It is worth noting that only fine-tuning the MLP network without fine-tuning theattention network can achieve the best training results.。Although generally speaking, the mainstream fine-tuning task oflarge language models is to fine-tune the attention layer network, but that task usually focuses more on answer semantics.。Inour task, we pay more attention to the format to meet the metastate parsing requirements, so fine-tuning the MLP networkcan achieve better results.The Impact of Dataset Scale.。We conduct experiments of various models with four percentages of fine-tuning train set,i.e., 100%, 30%, 10%, 3%, on the goal generation task.。The results are shown in Table 15.。It can be seen that as the amountof data gradually decreases, the performance of various training indicators gradually deteriorates under various settings.However, the smaller the amount of data, the greater the improvement brought by pre-training of our proposed CoFT method.The results show that the CoFT method we proposed can effectively collect and expand the chain of thought data related tothe final goal, thereby avoiding overfitting of the training set in the case of small data.K.. \n",
      "the piece 398\n",
      " \n",
      "directional Adaptation(a) Altitude map (0.8m) (b) Altitude map (4m) (c) Aerial view map (0.4m)Figure 9: Illustration of BEV features in observation space.。(a) and (b) are the altitude maps where bright areas are higherthan dark areas.。(c) is the aerial view map where the disconnected areas are windows or doors.。One pixel in (a), (b) and (c)denotes 0.8 meter, 4 meters and 0.4 meter respectively.。The small yellow blocks represent player positions and small blueblocks represent enemy positions.V15T5075100125150175200225250Goal ProjectionOraclesPrediction(a) Corresponding to (∆T,∆V)50 75 100 125 150 175 200 225 250T050100Goal ProjectionOraclesPrediction (b) Corresponding to ∆T0 5 10 15V050100Goal ProjectionOraclesPrediction (c) Corresponding to ∆VFigure 10: Distribution comparison between real goals (Oracles) and goals generated by Gop(Prediction).。The illustrationshows that Gopgenerates goals that follows the real distribution, indicating good generalization on open-ended goalgeneration.experimental results are shown in Table 13.。It is worth noting that only fine-tuning the MLP network without fine-tuning theattention network can achieve the best training results.。Although generally speaking, the mainstream fine-tuning task oflarge language models is to fine-tune the attention layer network, but that task usually focuses more on answer semantics.。Inour task, we pay more attention to the format to meet the metastate parsing requirements, so fine-tuning the MLP networkcan achieve better results.The Impact of Dataset Scale.。We conduct experiments of various models with four percentages of fine-tuning train set,i.e., 100%, 30%, 10%, 3%, on the goal generation task.。The results are shown in Table 15.。It can be seen that as the amountof data gradually decreases, the performance of various training indicators gradually deteriorates under various settings.However, the smaller the amount of data, the greater the improvement brought by pre-training of our proposed CoFT method.The results show that the CoFT method we proposed can effectively collect and expand the chain of thought data related tothe final goal, thereby avoiding overfitting of the training set in the case of small data.K.。Reward Functions for Co-TrainingAgent Feedback Rewards.. \n",
      "the piece 399\n",
      " \n",
      "herthan dark areas.。(c) is the aerial view map where the disconnected areas are windows or doors.。One pixel in (a), (b) and (c)denotes 0.8 meter, 4 meters and 0.4 meter respectively.。The small yellow blocks represent player positions and small blueblocks represent enemy positions.V15T5075100125150175200225250Goal ProjectionOraclesPrediction(a) Corresponding to (∆T,∆V)50 75 100 125 150 175 200 225 250T050100Goal ProjectionOraclesPrediction (b) Corresponding to ∆T0 5 10 15V050100Goal ProjectionOraclesPrediction (c) Corresponding to ∆VFigure 10: Distribution comparison between real goals (Oracles) and goals generated by Gop(Prediction).。The illustrationshows that Gopgenerates goals that follows the real distribution, indicating good generalization on open-ended goalgeneration.experimental results are shown in Table 13.。It is worth noting that only fine-tuning the MLP network without fine-tuning theattention network can achieve the best training results.。Although generally speaking, the mainstream fine-tuning task oflarge language models is to fine-tune the attention layer network, but that task usually focuses more on answer semantics.。Inour task, we pay more attention to the format to meet the metastate parsing requirements, so fine-tuning the MLP networkcan achieve better results.The Impact of Dataset Scale.。We conduct experiments of various models with four percentages of fine-tuning train set,i.e., 100%, 30%, 10%, 3%, on the goal generation task.。The results are shown in Table 15.。It can be seen that as the amountof data gradually decreases, the performance of various training indicators gradually deteriorates under various settings.However, the smaller the amount of data, the greater the improvement brought by pre-training of our proposed CoFT method.The results show that the CoFT method we proposed can effectively collect and expand the chain of thought data related tothe final goal, thereby avoiding overfitting of the training set in the case of small data.K.。Reward Functions for Co-TrainingAgent Feedback Rewards.。The calculation of the agent feedback reward is multifaceted, aiming to reflect the degree ofcompletion as feedback for the training of the LLM.. \n",
      "the piece 400\n",
      " \n",
      " (c)denotes 0.8 meter, 4 meters and 0.4 meter respectively.。The small yellow blocks represent player positions and small blueblocks represent enemy positions.V15T5075100125150175200225250Goal ProjectionOraclesPrediction(a) Corresponding to (∆T,∆V)50 75 100 125 150 175 200 225 250T050100Goal ProjectionOraclesPrediction (b) Corresponding to ∆T0 5 10 15V050100Goal ProjectionOraclesPrediction (c) Corresponding to ∆VFigure 10: Distribution comparison between real goals (Oracles) and goals generated by Gop(Prediction).。The illustrationshows that Gopgenerates goals that follows the real distribution, indicating good generalization on open-ended goalgeneration.experimental results are shown in Table 13.。It is worth noting that only fine-tuning the MLP network without fine-tuning theattention network can achieve the best training results.。Although generally speaking, the mainstream fine-tuning task oflarge language models is to fine-tune the attention layer network, but that task usually focuses more on answer semantics.。Inour task, we pay more attention to the format to meet the metastate parsing requirements, so fine-tuning the MLP networkcan achieve better results.The Impact of Dataset Scale.。We conduct experiments of various models with four percentages of fine-tuning train set,i.e., 100%, 30%, 10%, 3%, on the goal generation task.。The results are shown in Table 15.。It can be seen that as the amountof data gradually decreases, the performance of various training indicators gradually deteriorates under various settings.However, the smaller the amount of data, the greater the improvement brought by pre-training of our proposed CoFT method.The results show that the CoFT method we proposed can effectively collect and expand the chain of thought data related tothe final goal, thereby avoiding overfitting of the training set in the case of small data.K.。Reward Functions for Co-TrainingAgent Feedback Rewards.。The calculation of the agent feedback reward is multifaceted, aiming to reflect the degree ofcompletion as feedback for the training of the LLM.。Specifically, three aspects are considered to satisfy the requirements,and the total agent feedback reward is given by the sum of them:•rfg- Minimal Distance to a Goal When Satisfying Environment Termination .. \n",
      "the piece 401\n",
      " \n",
      "ion(a) Corresponding to (∆T,∆V)50 75 100 125 150 175 200 225 250T050100Goal ProjectionOraclesPrediction (b) Corresponding to ∆T0 5 10 15V050100Goal ProjectionOraclesPrediction (c) Corresponding to ∆VFigure 10: Distribution comparison between real goals (Oracles) and goals generated by Gop(Prediction).。The illustrationshows that Gopgenerates goals that follows the real distribution, indicating good generalization on open-ended goalgeneration.experimental results are shown in Table 13.。It is worth noting that only fine-tuning the MLP network without fine-tuning theattention network can achieve the best training results.。Although generally speaking, the mainstream fine-tuning task oflarge language models is to fine-tune the attention layer network, but that task usually focuses more on answer semantics.。Inour task, we pay more attention to the format to meet the metastate parsing requirements, so fine-tuning the MLP networkcan achieve better results.The Impact of Dataset Scale.。We conduct experiments of various models with four percentages of fine-tuning train set,i.e., 100%, 30%, 10%, 3%, on the goal generation task.。The results are shown in Table 15.。It can be seen that as the amountof data gradually decreases, the performance of various training indicators gradually deteriorates under various settings.However, the smaller the amount of data, the greater the improvement brought by pre-training of our proposed CoFT method.The results show that the CoFT method we proposed can effectively collect and expand the chain of thought data related tothe final goal, thereby avoiding overfitting of the training set in the case of small data.K.。Reward Functions for Co-TrainingAgent Feedback Rewards.。The calculation of the agent feedback reward is multifaceted, aiming to reflect the degree ofcompletion as feedback for the training of the LLM.。Specifically, three aspects are considered to satisfy the requirements,and the total agent feedback reward is given by the sum of them:•rfg- Minimal Distance to a Goal When Satisfying Environment Termination .。As depicted by Equation (11), theBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub goal        g4   g8   g12   g16   g20 Time in loopsL1    L2    L3    L4    L5    L6    L70.0250.0500.0750.1000.1250.1500.1750.2000.225Possibility(a)(b)Figure 11: (a) Sub-goal distribution during co-training.. \n",
      "the piece 402\n",
      " \n",
      "ctionOraclesPrediction (b) Corresponding to ∆T0 5 10 15V050100Goal ProjectionOraclesPrediction (c) Corresponding to ∆VFigure 10: Distribution comparison between real goals (Oracles) and goals generated by Gop(Prediction).。The illustrationshows that Gopgenerates goals that follows the real distribution, indicating good generalization on open-ended goalgeneration.experimental results are shown in Table 13.。It is worth noting that only fine-tuning the MLP network without fine-tuning theattention network can achieve the best training results.。Although generally speaking, the mainstream fine-tuning task oflarge language models is to fine-tune the attention layer network, but that task usually focuses more on answer semantics.。Inour task, we pay more attention to the format to meet the metastate parsing requirements, so fine-tuning the MLP networkcan achieve better results.The Impact of Dataset Scale.。We conduct experiments of various models with four percentages of fine-tuning train set,i.e., 100%, 30%, 10%, 3%, on the goal generation task.。The results are shown in Table 15.。It can be seen that as the amountof data gradually decreases, the performance of various training indicators gradually deteriorates under various settings.However, the smaller the amount of data, the greater the improvement brought by pre-training of our proposed CoFT method.The results show that the CoFT method we proposed can effectively collect and expand the chain of thought data related tothe final goal, thereby avoiding overfitting of the training set in the case of small data.K.。Reward Functions for Co-TrainingAgent Feedback Rewards.。The calculation of the agent feedback reward is multifaceted, aiming to reflect the degree ofcompletion as feedback for the training of the LLM.。Specifically, three aspects are considered to satisfy the requirements,and the total agent feedback reward is given by the sum of them:•rfg- Minimal Distance to a Goal When Satisfying Environment Termination .。As depicted by Equation (11), theBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub goal        g4   g8   g12   g16   g20 Time in loopsL1    L2    L3    L4    L5    L6    L70.0250.0500.0750.1000.1250.1500.1750.2000.225Possibility(a)(b)Figure 11: (a) Sub-goal distribution during co-training.。The 20 most frequently occurring goal meta states are filteredout and displayed.. \n",
      "the piece 403\n",
      " \n",
      "aclesPrediction (c) Corresponding to ∆VFigure 10: Distribution comparison between real goals (Oracles) and goals generated by Gop(Prediction).。The illustrationshows that Gopgenerates goals that follows the real distribution, indicating good generalization on open-ended goalgeneration.experimental results are shown in Table 13.。It is worth noting that only fine-tuning the MLP network without fine-tuning theattention network can achieve the best training results.。Although generally speaking, the mainstream fine-tuning task oflarge language models is to fine-tune the attention layer network, but that task usually focuses more on answer semantics.。Inour task, we pay more attention to the format to meet the metastate parsing requirements, so fine-tuning the MLP networkcan achieve better results.The Impact of Dataset Scale.。We conduct experiments of various models with four percentages of fine-tuning train set,i.e., 100%, 30%, 10%, 3%, on the goal generation task.。The results are shown in Table 15.。It can be seen that as the amountof data gradually decreases, the performance of various training indicators gradually deteriorates under various settings.However, the smaller the amount of data, the greater the improvement brought by pre-training of our proposed CoFT method.The results show that the CoFT method we proposed can effectively collect and expand the chain of thought data related tothe final goal, thereby avoiding overfitting of the training set in the case of small data.K.。Reward Functions for Co-TrainingAgent Feedback Rewards.。The calculation of the agent feedback reward is multifaceted, aiming to reflect the degree ofcompletion as feedback for the training of the LLM.。Specifically, three aspects are considered to satisfy the requirements,and the total agent feedback reward is given by the sum of them:•rfg- Minimal Distance to a Goal When Satisfying Environment Termination .。As depicted by Equation (11), theBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub goal        g4   g8   g12   g16   g20 Time in loopsL1    L2    L3    L4    L5    L6    L70.0250.0500.0750.1000.1250.1500.1750.2000.225Possibility(a)(b)Figure 11: (a) Sub-goal distribution during co-training.。The 20 most frequently occurring goal meta states are filteredout and displayed.。The vertical axis represents the probability of the state being output by the language model; (b) For acollected trajectory segment with length k= 200 , we firstly estimate the basic value for the last k−j+ 1states (herej= 20 ) and select one state as the goal with the probability proportional to their values.agent progressively reduces the distance between the initial state and the goal, scaling it by the magnitude of the initialstate-goal difference:Rfg=TXt=1∥|g−Proj (st−1)| − |g−Proj (st)||g−Proj (s0)|+ϵ∥1,where ϵ=1e-6 (11)•rfkeep- Reward Indicating How Long the Goal Can Be Kept .. \n",
      "the piece 404\n",
      " \n",
      " goals (Oracles) and goals generated by Gop(Prediction).。The illustrationshows that Gopgenerates goals that follows the real distribution, indicating good generalization on open-ended goalgeneration.experimental results are shown in Table 13.。It is worth noting that only fine-tuning the MLP network without fine-tuning theattention network can achieve the best training results.。Although generally speaking, the mainstream fine-tuning task oflarge language models is to fine-tune the attention layer network, but that task usually focuses more on answer semantics.。Inour task, we pay more attention to the format to meet the metastate parsing requirements, so fine-tuning the MLP networkcan achieve better results.The Impact of Dataset Scale.。We conduct experiments of various models with four percentages of fine-tuning train set,i.e., 100%, 30%, 10%, 3%, on the goal generation task.。The results are shown in Table 15.。It can be seen that as the amountof data gradually decreases, the performance of various training indicators gradually deteriorates under various settings.However, the smaller the amount of data, the greater the improvement brought by pre-training of our proposed CoFT method.The results show that the CoFT method we proposed can effectively collect and expand the chain of thought data related tothe final goal, thereby avoiding overfitting of the training set in the case of small data.K.。Reward Functions for Co-TrainingAgent Feedback Rewards.。The calculation of the agent feedback reward is multifaceted, aiming to reflect the degree ofcompletion as feedback for the training of the LLM.。Specifically, three aspects are considered to satisfy the requirements,and the total agent feedback reward is given by the sum of them:•rfg- Minimal Distance to a Goal When Satisfying Environment Termination .。As depicted by Equation (11), theBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub goal        g4   g8   g12   g16   g20 Time in loopsL1    L2    L3    L4    L5    L6    L70.0250.0500.0750.1000.1250.1500.1750.2000.225Possibility(a)(b)Figure 11: (a) Sub-goal distribution during co-training.。The 20 most frequently occurring goal meta states are filteredout and displayed.。The vertical axis represents the probability of the state being output by the language model; (b) For acollected trajectory segment with length k= 200 , we firstly estimate the basic value for the last k−j+ 1states (herej= 20 ) and select one state as the goal with the probability proportional to their values.agent progressively reduces the distance between the initial state and the goal, scaling it by the magnitude of the initialstate-goal difference:Rfg=TXt=1∥|g−Proj (st−1)| − |g−Proj (st)||g−Proj (s0)|+ϵ∥1,where ϵ=1e-6 (11)•rfkeep- Reward Indicating How Long the Goal Can Be Kept .。As depicted by Equation (12), upon accomplishing thegoal, the agent receives a reward proportional to the cumulative number of steps taken to sustain the goal state, scaledby the count of distinct sub-goals between the initial state s0and the goal g, i.e.n(g∩PROJ(s0)):Rfkeep=n(g∩PROJ(s0))·TXt=01g∩PROJ(st)̸=∅ (12)•rfrnd- Reward Indicating Whether the Generated Goal is Reachable for the Current State .. \n",
      "the piece 405\n",
      " \n",
      "dels is to fine-tune the attention layer network, but that task usually focuses more on answer semantics.。Inour task, we pay more attention to the format to meet the metastate parsing requirements, so fine-tuning the MLP networkcan achieve better results.The Impact of Dataset Scale.。We conduct experiments of various models with four percentages of fine-tuning train set,i.e., 100%, 30%, 10%, 3%, on the goal generation task.。The results are shown in Table 15.。It can be seen that as the amountof data gradually decreases, the performance of various training indicators gradually deteriorates under various settings.However, the smaller the amount of data, the greater the improvement brought by pre-training of our proposed CoFT method.The results show that the CoFT method we proposed can effectively collect and expand the chain of thought data related tothe final goal, thereby avoiding overfitting of the training set in the case of small data.K.。Reward Functions for Co-TrainingAgent Feedback Rewards.。The calculation of the agent feedback reward is multifaceted, aiming to reflect the degree ofcompletion as feedback for the training of the LLM.。Specifically, three aspects are considered to satisfy the requirements,and the total agent feedback reward is given by the sum of them:•rfg- Minimal Distance to a Goal When Satisfying Environment Termination .。As depicted by Equation (11), theBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub goal        g4   g8   g12   g16   g20 Time in loopsL1    L2    L3    L4    L5    L6    L70.0250.0500.0750.1000.1250.1500.1750.2000.225Possibility(a)(b)Figure 11: (a) Sub-goal distribution during co-training.。The 20 most frequently occurring goal meta states are filteredout and displayed.。The vertical axis represents the probability of the state being output by the language model; (b) For acollected trajectory segment with length k= 200 , we firstly estimate the basic value for the last k−j+ 1states (herej= 20 ) and select one state as the goal with the probability proportional to their values.agent progressively reduces the distance between the initial state and the goal, scaling it by the magnitude of the initialstate-goal difference:Rfg=TXt=1∥|g−Proj (st−1)| − |g−Proj (st)||g−Proj (s0)|+ϵ∥1,where ϵ=1e-6 (11)•rfkeep- Reward Indicating How Long the Goal Can Be Kept .。As depicted by Equation (12), upon accomplishing thegoal, the agent receives a reward proportional to the cumulative number of steps taken to sustain the goal state, scaledby the count of distinct sub-goals between the initial state s0and the goal g, i.e.n(g∩PROJ(s0)):Rfkeep=n(g∩PROJ(s0))·TXt=01g∩PROJ(st)̸=∅ (12)•rfrnd- Reward Indicating Whether the Generated Goal is Reachable for the Current State .。RND (Burda et al.,2018; Nikulin et al., 2023; Zhang et al., 2021; Du et al., 2023) is an effective method to measure the visiting frequencyof states or transitions in RL, where higher a RND score (reward), the more frequent a state is visited.. \n",
      "the piece 406\n",
      " \n",
      "uirements, so fine-tuning the MLP networkcan achieve better results.The Impact of Dataset Scale.。We conduct experiments of various models with four percentages of fine-tuning train set,i.e., 100%, 30%, 10%, 3%, on the goal generation task.。The results are shown in Table 15.。It can be seen that as the amountof data gradually decreases, the performance of various training indicators gradually deteriorates under various settings.However, the smaller the amount of data, the greater the improvement brought by pre-training of our proposed CoFT method.The results show that the CoFT method we proposed can effectively collect and expand the chain of thought data related tothe final goal, thereby avoiding overfitting of the training set in the case of small data.K.。Reward Functions for Co-TrainingAgent Feedback Rewards.。The calculation of the agent feedback reward is multifaceted, aiming to reflect the degree ofcompletion as feedback for the training of the LLM.。Specifically, three aspects are considered to satisfy the requirements,and the total agent feedback reward is given by the sum of them:•rfg- Minimal Distance to a Goal When Satisfying Environment Termination .。As depicted by Equation (11), theBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub goal        g4   g8   g12   g16   g20 Time in loopsL1    L2    L3    L4    L5    L6    L70.0250.0500.0750.1000.1250.1500.1750.2000.225Possibility(a)(b)Figure 11: (a) Sub-goal distribution during co-training.。The 20 most frequently occurring goal meta states are filteredout and displayed.。The vertical axis represents the probability of the state being output by the language model; (b) For acollected trajectory segment with length k= 200 , we firstly estimate the basic value for the last k−j+ 1states (herej= 20 ) and select one state as the goal with the probability proportional to their values.agent progressively reduces the distance between the initial state and the goal, scaling it by the magnitude of the initialstate-goal difference:Rfg=TXt=1∥|g−Proj (st−1)| − |g−Proj (st)||g−Proj (s0)|+ϵ∥1,where ϵ=1e-6 (11)•rfkeep- Reward Indicating How Long the Goal Can Be Kept .。As depicted by Equation (12), upon accomplishing thegoal, the agent receives a reward proportional to the cumulative number of steps taken to sustain the goal state, scaledby the count of distinct sub-goals between the initial state s0and the goal g, i.e.n(g∩PROJ(s0)):Rfkeep=n(g∩PROJ(s0))·TXt=01g∩PROJ(st)̸=∅ (12)•rfrnd- Reward Indicating Whether the Generated Goal is Reachable for the Current State .。RND (Burda et al.,2018; Nikulin et al., 2023; Zhang et al., 2021; Du et al., 2023) is an effective method to measure the visiting frequencyof states or transitions in RL, where higher a RND score (reward), the more frequent a state is visited.。Thus, we canleverage such a method to quantify how novel a state is:Rfrnd=−TXt=0∥φ(E(st, g))−φ⋆(E(st, g))∥, (13)where φ⋆a target network which shares the same architecture as the RND predictor but the network is non-trainable.Thus, we express RfasRf=Rfg+Rfkeep+Rfrnd.Examination Reward Function.. \n",
      "the piece 407\n",
      " \n",
      "with four percentages of fine-tuning train set,i.e., 100%, 30%, 10%, 3%, on the goal generation task.。The results are shown in Table 15.。It can be seen that as the amountof data gradually decreases, the performance of various training indicators gradually deteriorates under various settings.However, the smaller the amount of data, the greater the improvement brought by pre-training of our proposed CoFT method.The results show that the CoFT method we proposed can effectively collect and expand the chain of thought data related tothe final goal, thereby avoiding overfitting of the training set in the case of small data.K.。Reward Functions for Co-TrainingAgent Feedback Rewards.。The calculation of the agent feedback reward is multifaceted, aiming to reflect the degree ofcompletion as feedback for the training of the LLM.。Specifically, three aspects are considered to satisfy the requirements,and the total agent feedback reward is given by the sum of them:•rfg- Minimal Distance to a Goal When Satisfying Environment Termination .。As depicted by Equation (11), theBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub goal        g4   g8   g12   g16   g20 Time in loopsL1    L2    L3    L4    L5    L6    L70.0250.0500.0750.1000.1250.1500.1750.2000.225Possibility(a)(b)Figure 11: (a) Sub-goal distribution during co-training.。The 20 most frequently occurring goal meta states are filteredout and displayed.。The vertical axis represents the probability of the state being output by the language model; (b) For acollected trajectory segment with length k= 200 , we firstly estimate the basic value for the last k−j+ 1states (herej= 20 ) and select one state as the goal with the probability proportional to their values.agent progressively reduces the distance between the initial state and the goal, scaling it by the magnitude of the initialstate-goal difference:Rfg=TXt=1∥|g−Proj (st−1)| − |g−Proj (st)||g−Proj (s0)|+ϵ∥1,where ϵ=1e-6 (11)•rfkeep- Reward Indicating How Long the Goal Can Be Kept .。As depicted by Equation (12), upon accomplishing thegoal, the agent receives a reward proportional to the cumulative number of steps taken to sustain the goal state, scaledby the count of distinct sub-goals between the initial state s0and the goal g, i.e.n(g∩PROJ(s0)):Rfkeep=n(g∩PROJ(s0))·TXt=01g∩PROJ(st)̸=∅ (12)•rfrnd- Reward Indicating Whether the Generated Goal is Reachable for the Current State .。RND (Burda et al.,2018; Nikulin et al., 2023; Zhang et al., 2021; Du et al., 2023) is an effective method to measure the visiting frequencyof states or transitions in RL, where higher a RND score (reward), the more frequent a state is visited.。Thus, we canleverage such a method to quantify how novel a state is:Rfrnd=−TXt=0∥φ(E(st, g))−φ⋆(E(st, g))∥, (13)where φ⋆a target network which shares the same architecture as the RND predictor but the network is non-trainable.Thus, we express RfasRf=Rfg+Rfkeep+Rfrnd.Examination Reward Function.。The examination reward function is introduced as an intrinsic signal to encourage theLLM to generate goals with essential sub-goals.. \n",
      "the piece 408\n",
      " \n",
      " decreases, the performance of various training indicators gradually deteriorates under various settings.However, the smaller the amount of data, the greater the improvement brought by pre-training of our proposed CoFT method.The results show that the CoFT method we proposed can effectively collect and expand the chain of thought data related tothe final goal, thereby avoiding overfitting of the training set in the case of small data.K.。Reward Functions for Co-TrainingAgent Feedback Rewards.。The calculation of the agent feedback reward is multifaceted, aiming to reflect the degree ofcompletion as feedback for the training of the LLM.。Specifically, three aspects are considered to satisfy the requirements,and the total agent feedback reward is given by the sum of them:•rfg- Minimal Distance to a Goal When Satisfying Environment Termination .。As depicted by Equation (11), theBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub goal        g4   g8   g12   g16   g20 Time in loopsL1    L2    L3    L4    L5    L6    L70.0250.0500.0750.1000.1250.1500.1750.2000.225Possibility(a)(b)Figure 11: (a) Sub-goal distribution during co-training.。The 20 most frequently occurring goal meta states are filteredout and displayed.。The vertical axis represents the probability of the state being output by the language model; (b) For acollected trajectory segment with length k= 200 , we firstly estimate the basic value for the last k−j+ 1states (herej= 20 ) and select one state as the goal with the probability proportional to their values.agent progressively reduces the distance between the initial state and the goal, scaling it by the magnitude of the initialstate-goal difference:Rfg=TXt=1∥|g−Proj (st−1)| − |g−Proj (st)||g−Proj (s0)|+ϵ∥1,where ϵ=1e-6 (11)•rfkeep- Reward Indicating How Long the Goal Can Be Kept .。As depicted by Equation (12), upon accomplishing thegoal, the agent receives a reward proportional to the cumulative number of steps taken to sustain the goal state, scaledby the count of distinct sub-goals between the initial state s0and the goal g, i.e.n(g∩PROJ(s0)):Rfkeep=n(g∩PROJ(s0))·TXt=01g∩PROJ(st)̸=∅ (12)•rfrnd- Reward Indicating Whether the Generated Goal is Reachable for the Current State .。RND (Burda et al.,2018; Nikulin et al., 2023; Zhang et al., 2021; Du et al., 2023) is an effective method to measure the visiting frequencyof states or transitions in RL, where higher a RND score (reward), the more frequent a state is visited.。Thus, we canleverage such a method to quantify how novel a state is:Rfrnd=−TXt=0∥φ(E(st, g))−φ⋆(E(st, g))∥, (13)where φ⋆a target network which shares the same architecture as the RND predictor but the network is non-trainable.Thus, we express RfasRf=Rfg+Rfkeep+Rfrnd.Examination Reward Function.。The examination reward function is introduced as an intrinsic signal to encourage theLLM to generate goals with essential sub-goals.。We use the SI dataset as the examination set IS.. \n",
      "the piece 409\n",
      " \n",
      "ht by pre-training of our proposed CoFT method.The results show that the CoFT method we proposed can effectively collect and expand the chain of thought data related tothe final goal, thereby avoiding overfitting of the training set in the case of small data.K.。Reward Functions for Co-TrainingAgent Feedback Rewards.。The calculation of the agent feedback reward is multifaceted, aiming to reflect the degree ofcompletion as feedback for the training of the LLM.。Specifically, three aspects are considered to satisfy the requirements,and the total agent feedback reward is given by the sum of them:•rfg- Minimal Distance to a Goal When Satisfying Environment Termination .。As depicted by Equation (11), theBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub goal        g4   g8   g12   g16   g20 Time in loopsL1    L2    L3    L4    L5    L6    L70.0250.0500.0750.1000.1250.1500.1750.2000.225Possibility(a)(b)Figure 11: (a) Sub-goal distribution during co-training.。The 20 most frequently occurring goal meta states are filteredout and displayed.。The vertical axis represents the probability of the state being output by the language model; (b) For acollected trajectory segment with length k= 200 , we firstly estimate the basic value for the last k−j+ 1states (herej= 20 ) and select one state as the goal with the probability proportional to their values.agent progressively reduces the distance between the initial state and the goal, scaling it by the magnitude of the initialstate-goal difference:Rfg=TXt=1∥|g−Proj (st−1)| − |g−Proj (st)||g−Proj (s0)|+ϵ∥1,where ϵ=1e-6 (11)•rfkeep- Reward Indicating How Long the Goal Can Be Kept .。As depicted by Equation (12), upon accomplishing thegoal, the agent receives a reward proportional to the cumulative number of steps taken to sustain the goal state, scaledby the count of distinct sub-goals between the initial state s0and the goal g, i.e.n(g∩PROJ(s0)):Rfkeep=n(g∩PROJ(s0))·TXt=01g∩PROJ(st)̸=∅ (12)•rfrnd- Reward Indicating Whether the Generated Goal is Reachable for the Current State .。RND (Burda et al.,2018; Nikulin et al., 2023; Zhang et al., 2021; Du et al., 2023) is an effective method to measure the visiting frequencyof states or transitions in RL, where higher a RND score (reward), the more frequent a state is visited.。Thus, we canleverage such a method to quantify how novel a state is:Rfrnd=−TXt=0∥φ(E(st, g))−φ⋆(E(st, g))∥, (13)where φ⋆a target network which shares the same architecture as the RND predictor but the network is non-trainable.Thus, we express RfasRf=Rfg+Rfkeep+Rfrnd.Examination Reward Function.。The examination reward function is introduced as an intrinsic signal to encourage theLLM to generate goals with essential sub-goals.。We use the SI dataset as the examination set IS.。For each training iteration,a batch of instructions Itrain is randomly sampled from the full instruction dataset I, and corresponding goals garegenerated.. \n",
      "the piece 410\n",
      " \n",
      " thought data related tothe final goal, thereby avoiding overfitting of the training set in the case of small data.K.。Reward Functions for Co-TrainingAgent Feedback Rewards.。The calculation of the agent feedback reward is multifaceted, aiming to reflect the degree ofcompletion as feedback for the training of the LLM.。Specifically, three aspects are considered to satisfy the requirements,and the total agent feedback reward is given by the sum of them:•rfg- Minimal Distance to a Goal When Satisfying Environment Termination .。As depicted by Equation (11), theBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub goal        g4   g8   g12   g16   g20 Time in loopsL1    L2    L3    L4    L5    L6    L70.0250.0500.0750.1000.1250.1500.1750.2000.225Possibility(a)(b)Figure 11: (a) Sub-goal distribution during co-training.。The 20 most frequently occurring goal meta states are filteredout and displayed.。The vertical axis represents the probability of the state being output by the language model; (b) For acollected trajectory segment with length k= 200 , we firstly estimate the basic value for the last k−j+ 1states (herej= 20 ) and select one state as the goal with the probability proportional to their values.agent progressively reduces the distance between the initial state and the goal, scaling it by the magnitude of the initialstate-goal difference:Rfg=TXt=1∥|g−Proj (st−1)| − |g−Proj (st)||g−Proj (s0)|+ϵ∥1,where ϵ=1e-6 (11)•rfkeep- Reward Indicating How Long the Goal Can Be Kept .。As depicted by Equation (12), upon accomplishing thegoal, the agent receives a reward proportional to the cumulative number of steps taken to sustain the goal state, scaledby the count of distinct sub-goals between the initial state s0and the goal g, i.e.n(g∩PROJ(s0)):Rfkeep=n(g∩PROJ(s0))·TXt=01g∩PROJ(st)̸=∅ (12)•rfrnd- Reward Indicating Whether the Generated Goal is Reachable for the Current State .。RND (Burda et al.,2018; Nikulin et al., 2023; Zhang et al., 2021; Du et al., 2023) is an effective method to measure the visiting frequencyof states or transitions in RL, where higher a RND score (reward), the more frequent a state is visited.。Thus, we canleverage such a method to quantify how novel a state is:Rfrnd=−TXt=0∥φ(E(st, g))−φ⋆(E(st, g))∥, (13)where φ⋆a target network which shares the same architecture as the RND predictor but the network is non-trainable.Thus, we express RfasRf=Rfg+Rfkeep+Rfrnd.Examination Reward Function.。The examination reward function is introduced as an intrinsic signal to encourage theLLM to generate goals with essential sub-goals.。We use the SI dataset as the examination set IS.。For each training iteration,a batch of instructions Itrain is randomly sampled from the full instruction dataset I, and corresponding goals garegenerated.。After the agent finishes its rollout, the examination reward for each batch is computed based on the intersectionI∩=IS∩ Itrain .. \n",
      "the piece 411\n",
      " \n",
      "al, thereby avoiding overfitting of the training set in the case of small data.K.。Reward Functions for Co-TrainingAgent Feedback Rewards.。The calculation of the agent feedback reward is multifaceted, aiming to reflect the degree ofcompletion as feedback for the training of the LLM.。Specifically, three aspects are considered to satisfy the requirements,and the total agent feedback reward is given by the sum of them:•rfg- Minimal Distance to a Goal When Satisfying Environment Termination .。As depicted by Equation (11), theBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub goal        g4   g8   g12   g16   g20 Time in loopsL1    L2    L3    L4    L5    L6    L70.0250.0500.0750.1000.1250.1500.1750.2000.225Possibility(a)(b)Figure 11: (a) Sub-goal distribution during co-training.。The 20 most frequently occurring goal meta states are filteredout and displayed.。The vertical axis represents the probability of the state being output by the language model; (b) For acollected trajectory segment with length k= 200 , we firstly estimate the basic value for the last k−j+ 1states (herej= 20 ) and select one state as the goal with the probability proportional to their values.agent progressively reduces the distance between the initial state and the goal, scaling it by the magnitude of the initialstate-goal difference:Rfg=TXt=1∥|g−Proj (st−1)| − |g−Proj (st)||g−Proj (s0)|+ϵ∥1,where ϵ=1e-6 (11)•rfkeep- Reward Indicating How Long the Goal Can Be Kept .。As depicted by Equation (12), upon accomplishing thegoal, the agent receives a reward proportional to the cumulative number of steps taken to sustain the goal state, scaledby the count of distinct sub-goals between the initial state s0and the goal g, i.e.n(g∩PROJ(s0)):Rfkeep=n(g∩PROJ(s0))·TXt=01g∩PROJ(st)̸=∅ (12)•rfrnd- Reward Indicating Whether the Generated Goal is Reachable for the Current State .。RND (Burda et al.,2018; Nikulin et al., 2023; Zhang et al., 2021; Du et al., 2023) is an effective method to measure the visiting frequencyof states or transitions in RL, where higher a RND score (reward), the more frequent a state is visited.。Thus, we canleverage such a method to quantify how novel a state is:Rfrnd=−TXt=0∥φ(E(st, g))−φ⋆(E(st, g))∥, (13)where φ⋆a target network which shares the same architecture as the RND predictor but the network is non-trainable.Thus, we express RfasRf=Rfg+Rfkeep+Rfrnd.Examination Reward Function.。The examination reward function is introduced as an intrinsic signal to encourage theLLM to generate goals with essential sub-goals.。We use the SI dataset as the examination set IS.。For each training iteration,a batch of instructions Itrain is randomly sampled from the full instruction dataset I, and corresponding goals garegenerated.。After the agent finishes its rollout, the examination reward for each batch is computed based on the intersectionI∩=IS∩ Itrain .。For non-empty I∩, an examination reward for each instruction in I∩is computed as:Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationre(ι, g, g sub) =(+2 gsub∈g−2otherwise,∀ι∈ I∩ (14)Then, Reis calculated as Re=1|I∩|Pι∈I∩re(ι, g, g sub|g=Gllm(s, ι)).Formatting Reward Function.. \n",
      "the piece 412\n",
      " \n",
      "。As depicted by Equation (11), theBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub goal        g4   g8   g12   g16   g20 Time in loopsL1    L2    L3    L4    L5    L6    L70.0250.0500.0750.1000.1250.1500.1750.2000.225Possibility(a)(b)Figure 11: (a) Sub-goal distribution during co-training.。The 20 most frequently occurring goal meta states are filteredout and displayed.。The vertical axis represents the probability of the state being output by the language model; (b) For acollected trajectory segment with length k= 200 , we firstly estimate the basic value for the last k−j+ 1states (herej= 20 ) and select one state as the goal with the probability proportional to their values.agent progressively reduces the distance between the initial state and the goal, scaling it by the magnitude of the initialstate-goal difference:Rfg=TXt=1∥|g−Proj (st−1)| − |g−Proj (st)||g−Proj (s0)|+ϵ∥1,where ϵ=1e-6 (11)•rfkeep- Reward Indicating How Long the Goal Can Be Kept .。As depicted by Equation (12), upon accomplishing thegoal, the agent receives a reward proportional to the cumulative number of steps taken to sustain the goal state, scaledby the count of distinct sub-goals between the initial state s0and the goal g, i.e.n(g∩PROJ(s0)):Rfkeep=n(g∩PROJ(s0))·TXt=01g∩PROJ(st)̸=∅ (12)•rfrnd- Reward Indicating Whether the Generated Goal is Reachable for the Current State .。RND (Burda et al.,2018; Nikulin et al., 2023; Zhang et al., 2021; Du et al., 2023) is an effective method to measure the visiting frequencyof states or transitions in RL, where higher a RND score (reward), the more frequent a state is visited.。Thus, we canleverage such a method to quantify how novel a state is:Rfrnd=−TXt=0∥φ(E(st, g))−φ⋆(E(st, g))∥, (13)where φ⋆a target network which shares the same architecture as the RND predictor but the network is non-trainable.Thus, we express RfasRf=Rfg+Rfkeep+Rfrnd.Examination Reward Function.。The examination reward function is introduced as an intrinsic signal to encourage theLLM to generate goals with essential sub-goals.。We use the SI dataset as the examination set IS.。For each training iteration,a batch of instructions Itrain is randomly sampled from the full instruction dataset I, and corresponding goals garegenerated.。After the agent finishes its rollout, the examination reward for each batch is computed based on the intersectionI∩=IS∩ Itrain .。For non-empty I∩, an examination reward for each instruction in I∩is computed as:Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationre(ι, g, g sub) =(+2 gsub∈g−2otherwise,∀ι∈ I∩ (14)Then, Reis calculated as Re=1|I∩|Pι∈I∩re(ι, g, g sub|g=Gllm(s, ι)).Formatting Reward Function.。The formatting reward Rmfor each generated goal is calculated by computing an editdistance, utilizing the Wagner-Fischer algorithm (Wagner & Fischer, 1974).With the defined reward functions, RLAF is applied with a reward function R=Rf+Re+Rmand Proximal PolicyOptimization (PPO) for each data point in a batch.Figure 12: Implementation of the RND predictor network.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.. \n",
      "the piece 413\n",
      " \n",
      "odied Agent via Language-Policy Bidirectional AdaptationSub goal        g4   g8   g12   g16   g20 Time in loopsL1    L2    L3    L4    L5    L6    L70.0250.0500.0750.1000.1250.1500.1750.2000.225Possibility(a)(b)Figure 11: (a) Sub-goal distribution during co-training.。The 20 most frequently occurring goal meta states are filteredout and displayed.。The vertical axis represents the probability of the state being output by the language model; (b) For acollected trajectory segment with length k= 200 , we firstly estimate the basic value for the last k−j+ 1states (herej= 20 ) and select one state as the goal with the probability proportional to their values.agent progressively reduces the distance between the initial state and the goal, scaling it by the magnitude of the initialstate-goal difference:Rfg=TXt=1∥|g−Proj (st−1)| − |g−Proj (st)||g−Proj (s0)|+ϵ∥1,where ϵ=1e-6 (11)•rfkeep- Reward Indicating How Long the Goal Can Be Kept .。As depicted by Equation (12), upon accomplishing thegoal, the agent receives a reward proportional to the cumulative number of steps taken to sustain the goal state, scaledby the count of distinct sub-goals between the initial state s0and the goal g, i.e.n(g∩PROJ(s0)):Rfkeep=n(g∩PROJ(s0))·TXt=01g∩PROJ(st)̸=∅ (12)•rfrnd- Reward Indicating Whether the Generated Goal is Reachable for the Current State .。RND (Burda et al.,2018; Nikulin et al., 2023; Zhang et al., 2021; Du et al., 2023) is an effective method to measure the visiting frequencyof states or transitions in RL, where higher a RND score (reward), the more frequent a state is visited.。Thus, we canleverage such a method to quantify how novel a state is:Rfrnd=−TXt=0∥φ(E(st, g))−φ⋆(E(st, g))∥, (13)where φ⋆a target network which shares the same architecture as the RND predictor but the network is non-trainable.Thus, we express RfasRf=Rfg+Rfkeep+Rfrnd.Examination Reward Function.。The examination reward function is introduced as an intrinsic signal to encourage theLLM to generate goals with essential sub-goals.。We use the SI dataset as the examination set IS.。For each training iteration,a batch of instructions Itrain is randomly sampled from the full instruction dataset I, and corresponding goals garegenerated.。After the agent finishes its rollout, the examination reward for each batch is computed based on the intersectionI∩=IS∩ Itrain .。For non-empty I∩, an examination reward for each instruction in I∩is computed as:Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationre(ι, g, g sub) =(+2 gsub∈g−2otherwise,∀ι∈ I∩ (14)Then, Reis calculated as Re=1|I∩|Pι∈I∩re(ι, g, g sub|g=Gllm(s, ι)).Formatting Reward Function.。The formatting reward Rmfor each generated goal is calculated by computing an editdistance, utilizing the Wagner-Fischer algorithm (Wagner & Fischer, 1974).With the defined reward functions, RLAF is applied with a reward function R=Rf+Re+Rmand Proximal PolicyOptimization (PPO) for each data point in a batch.Figure 12: Implementation of the RND predictor network.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.。Analyze the verbal orders of teammates and players, what do you want to do?. \n",
      "the piece 414\n",
      " \n",
      " L70.0250.0500.0750.1000.1250.1500.1750.2000.225Possibility(a)(b)Figure 11: (a) Sub-goal distribution during co-training.。The 20 most frequently occurring goal meta states are filteredout and displayed.。The vertical axis represents the probability of the state being output by the language model; (b) For acollected trajectory segment with length k= 200 , we firstly estimate the basic value for the last k−j+ 1states (herej= 20 ) and select one state as the goal with the probability proportional to their values.agent progressively reduces the distance between the initial state and the goal, scaling it by the magnitude of the initialstate-goal difference:Rfg=TXt=1∥|g−Proj (st−1)| − |g−Proj (st)||g−Proj (s0)|+ϵ∥1,where ϵ=1e-6 (11)•rfkeep- Reward Indicating How Long the Goal Can Be Kept .。As depicted by Equation (12), upon accomplishing thegoal, the agent receives a reward proportional to the cumulative number of steps taken to sustain the goal state, scaledby the count of distinct sub-goals between the initial state s0and the goal g, i.e.n(g∩PROJ(s0)):Rfkeep=n(g∩PROJ(s0))·TXt=01g∩PROJ(st)̸=∅ (12)•rfrnd- Reward Indicating Whether the Generated Goal is Reachable for the Current State .。RND (Burda et al.,2018; Nikulin et al., 2023; Zhang et al., 2021; Du et al., 2023) is an effective method to measure the visiting frequencyof states or transitions in RL, where higher a RND score (reward), the more frequent a state is visited.。Thus, we canleverage such a method to quantify how novel a state is:Rfrnd=−TXt=0∥φ(E(st, g))−φ⋆(E(st, g))∥, (13)where φ⋆a target network which shares the same architecture as the RND predictor but the network is non-trainable.Thus, we express RfasRf=Rfg+Rfkeep+Rfrnd.Examination Reward Function.。The examination reward function is introduced as an intrinsic signal to encourage theLLM to generate goals with essential sub-goals.。We use the SI dataset as the examination set IS.。For each training iteration,a batch of instructions Itrain is randomly sampled from the full instruction dataset I, and corresponding goals garegenerated.。After the agent finishes its rollout, the examination reward for each batch is computed based on the intersectionI∩=IS∩ Itrain .。For non-empty I∩, an examination reward for each instruction in I∩is computed as:Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationre(ι, g, g sub) =(+2 gsub∈g−2otherwise,∀ι∈ I∩ (14)Then, Reis calculated as Re=1|I∩|Pι∈I∩re(ι, g, g sub|g=Gllm(s, ι)).Formatting Reward Function.。The formatting reward Rmfor each generated goal is calculated by computing an editdistance, utilizing the Wagner-Fischer algorithm (Wagner & Fischer, 1974).With the defined reward functions, RLAF is applied with a reward function R=Rf+Re+Rmand Proximal PolicyOptimization (PPO) for each data point in a batch.Figure 12: Implementation of the RND predictor network.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is ‘You should lie in wait’, which means teammate player wantsthe agent to set up an ambush or take a hiding position.2.. \n",
      "the piece 415\n",
      " \n",
      "ical axis represents the probability of the state being output by the language model; (b) For acollected trajectory segment with length k= 200 , we firstly estimate the basic value for the last k−j+ 1states (herej= 20 ) and select one state as the goal with the probability proportional to their values.agent progressively reduces the distance between the initial state and the goal, scaling it by the magnitude of the initialstate-goal difference:Rfg=TXt=1∥|g−Proj (st−1)| − |g−Proj (st)||g−Proj (s0)|+ϵ∥1,where ϵ=1e-6 (11)•rfkeep- Reward Indicating How Long the Goal Can Be Kept .。As depicted by Equation (12), upon accomplishing thegoal, the agent receives a reward proportional to the cumulative number of steps taken to sustain the goal state, scaledby the count of distinct sub-goals between the initial state s0and the goal g, i.e.n(g∩PROJ(s0)):Rfkeep=n(g∩PROJ(s0))·TXt=01g∩PROJ(st)̸=∅ (12)•rfrnd- Reward Indicating Whether the Generated Goal is Reachable for the Current State .。RND (Burda et al.,2018; Nikulin et al., 2023; Zhang et al., 2021; Du et al., 2023) is an effective method to measure the visiting frequencyof states or transitions in RL, where higher a RND score (reward), the more frequent a state is visited.。Thus, we canleverage such a method to quantify how novel a state is:Rfrnd=−TXt=0∥φ(E(st, g))−φ⋆(E(st, g))∥, (13)where φ⋆a target network which shares the same architecture as the RND predictor but the network is non-trainable.Thus, we express RfasRf=Rfg+Rfkeep+Rfrnd.Examination Reward Function.。The examination reward function is introduced as an intrinsic signal to encourage theLLM to generate goals with essential sub-goals.。We use the SI dataset as the examination set IS.。For each training iteration,a batch of instructions Itrain is randomly sampled from the full instruction dataset I, and corresponding goals garegenerated.。After the agent finishes its rollout, the examination reward for each batch is computed based on the intersectionI∩=IS∩ Itrain .。For non-empty I∩, an examination reward for each instruction in I∩is computed as:Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationre(ι, g, g sub) =(+2 gsub∈g−2otherwise,∀ι∈ I∩ (14)Then, Reis calculated as Re=1|I∩|Pι∈I∩re(ι, g, g sub|g=Gllm(s, ι)).Formatting Reward Function.。The formatting reward Rmfor each generated goal is calculated by computing an editdistance, utilizing the Wagner-Fischer algorithm (Wagner & Fischer, 1974).With the defined reward functions, RLAF is applied with a reward function R=Rf+Re+Rmand Proximal PolicyOptimization (PPO) for each data point in a batch.Figure 12: Implementation of the RND predictor network.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is ‘You should lie in wait’, which means teammate player wantsthe agent to set up an ambush or take a hiding position.2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.. \n",
      "the piece 416\n",
      " \n",
      "duces the distance between the initial state and the goal, scaling it by the magnitude of the initialstate-goal difference:Rfg=TXt=1∥|g−Proj (st−1)| − |g−Proj (st)||g−Proj (s0)|+ϵ∥1,where ϵ=1e-6 (11)•rfkeep- Reward Indicating How Long the Goal Can Be Kept .。As depicted by Equation (12), upon accomplishing thegoal, the agent receives a reward proportional to the cumulative number of steps taken to sustain the goal state, scaledby the count of distinct sub-goals between the initial state s0and the goal g, i.e.n(g∩PROJ(s0)):Rfkeep=n(g∩PROJ(s0))·TXt=01g∩PROJ(st)̸=∅ (12)•rfrnd- Reward Indicating Whether the Generated Goal is Reachable for the Current State .。RND (Burda et al.,2018; Nikulin et al., 2023; Zhang et al., 2021; Du et al., 2023) is an effective method to measure the visiting frequencyof states or transitions in RL, where higher a RND score (reward), the more frequent a state is visited.。Thus, we canleverage such a method to quantify how novel a state is:Rfrnd=−TXt=0∥φ(E(st, g))−φ⋆(E(st, g))∥, (13)where φ⋆a target network which shares the same architecture as the RND predictor but the network is non-trainable.Thus, we express RfasRf=Rfg+Rfkeep+Rfrnd.Examination Reward Function.。The examination reward function is introduced as an intrinsic signal to encourage theLLM to generate goals with essential sub-goals.。We use the SI dataset as the examination set IS.。For each training iteration,a batch of instructions Itrain is randomly sampled from the full instruction dataset I, and corresponding goals garegenerated.。After the agent finishes its rollout, the examination reward for each batch is computed based on the intersectionI∩=IS∩ Itrain .。For non-empty I∩, an examination reward for each instruction in I∩is computed as:Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationre(ι, g, g sub) =(+2 gsub∈g−2otherwise,∀ι∈ I∩ (14)Then, Reis calculated as Re=1|I∩|Pι∈I∩re(ι, g, g sub|g=Gllm(s, ι)).Formatting Reward Function.。The formatting reward Rmfor each generated goal is calculated by computing an editdistance, utilizing the Wagner-Fischer algorithm (Wagner & Fischer, 1974).With the defined reward functions, RLAF is applied with a reward function R=Rf+Re+Rmand Proximal PolicyOptimization (PPO) for each data point in a batch.Figure 12: Implementation of the RND predictor network.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is ‘You should lie in wait’, which means teammate player wantsthe agent to set up an ambush or take a hiding position.2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan.According to the teammate‘S command:2.1.. \n",
      "the piece 417\n",
      " \n",
      "tude of the initialstate-goal difference:Rfg=TXt=1∥|g−Proj (st−1)| − |g−Proj (st)||g−Proj (s0)|+ϵ∥1,where ϵ=1e-6 (11)•rfkeep- Reward Indicating How Long the Goal Can Be Kept .。As depicted by Equation (12), upon accomplishing thegoal, the agent receives a reward proportional to the cumulative number of steps taken to sustain the goal state, scaledby the count of distinct sub-goals between the initial state s0and the goal g, i.e.n(g∩PROJ(s0)):Rfkeep=n(g∩PROJ(s0))·TXt=01g∩PROJ(st)̸=∅ (12)•rfrnd- Reward Indicating Whether the Generated Goal is Reachable for the Current State .。RND (Burda et al.,2018; Nikulin et al., 2023; Zhang et al., 2021; Du et al., 2023) is an effective method to measure the visiting frequencyof states or transitions in RL, where higher a RND score (reward), the more frequent a state is visited.。Thus, we canleverage such a method to quantify how novel a state is:Rfrnd=−TXt=0∥φ(E(st, g))−φ⋆(E(st, g))∥, (13)where φ⋆a target network which shares the same architecture as the RND predictor but the network is non-trainable.Thus, we express RfasRf=Rfg+Rfkeep+Rfrnd.Examination Reward Function.。The examination reward function is introduced as an intrinsic signal to encourage theLLM to generate goals with essential sub-goals.。We use the SI dataset as the examination set IS.。For each training iteration,a batch of instructions Itrain is randomly sampled from the full instruction dataset I, and corresponding goals garegenerated.。After the agent finishes its rollout, the examination reward for each batch is computed based on the intersectionI∩=IS∩ Itrain .。For non-empty I∩, an examination reward for each instruction in I∩is computed as:Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationre(ι, g, g sub) =(+2 gsub∈g−2otherwise,∀ι∈ I∩ (14)Then, Reis calculated as Re=1|I∩|Pι∈I∩re(ι, g, g sub|g=Gllm(s, ι)).Formatting Reward Function.。The formatting reward Rmfor each generated goal is calculated by computing an editdistance, utilizing the Wagner-Fischer algorithm (Wagner & Fischer, 1974).With the defined reward functions, RLAF is applied with a reward function R=Rf+Re+Rmand Proximal PolicyOptimization (PPO) for each data point in a batch.Figure 12: Implementation of the RND predictor network.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is ‘You should lie in wait’, which means teammate player wantsthe agent to set up an ambush or take a hiding position.2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan.According to the teammate‘S command:2.1.。Need to hide: ‘Whether prone position’, ‘Whether crouch position’2.2.. \n",
      "the piece 418\n",
      " \n",
      "t al.,2018; Nikulin et al., 2023; Zhang et al., 2021; Du et al., 2023) is an effective method to measure the visiting frequencyof states or transitions in RL, where higher a RND score (reward), the more frequent a state is visited.。Thus, we canleverage such a method to quantify how novel a state is:Rfrnd=−TXt=0∥φ(E(st, g))−φ⋆(E(st, g))∥, (13)where φ⋆a target network which shares the same architecture as the RND predictor but the network is non-trainable.Thus, we express RfasRf=Rfg+Rfkeep+Rfrnd.Examination Reward Function.。The examination reward function is introduced as an intrinsic signal to encourage theLLM to generate goals with essential sub-goals.。We use the SI dataset as the examination set IS.。For each training iteration,a batch of instructions Itrain is randomly sampled from the full instruction dataset I, and corresponding goals garegenerated.。After the agent finishes its rollout, the examination reward for each batch is computed based on the intersectionI∩=IS∩ Itrain .。For non-empty I∩, an examination reward for each instruction in I∩is computed as:Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationre(ι, g, g sub) =(+2 gsub∈g−2otherwise,∀ι∈ I∩ (14)Then, Reis calculated as Re=1|I∩|Pι∈I∩re(ι, g, g sub|g=Gllm(s, ι)).Formatting Reward Function.。The formatting reward Rmfor each generated goal is calculated by computing an editdistance, utilizing the Wagner-Fischer algorithm (Wagner & Fischer, 1974).With the defined reward functions, RLAF is applied with a reward function R=Rf+Re+Rmand Proximal PolicyOptimization (PPO) for each data point in a batch.Figure 12: Implementation of the RND predictor network.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is ‘You should lie in wait’, which means teammate player wantsthe agent to set up an ambush or take a hiding position.2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan.According to the teammate‘S command:2.1.。Need to hide: ‘Whether prone position’, ‘Whether crouch position’2.2.。Reduce movement to stay unnoticed: ‘Average velocity’, ‘Length of distance moved’2.3.. \n",
      "the piece 419\n",
      " \n",
      "s the RND predictor but the network is non-trainable.Thus, we express RfasRf=Rfg+Rfkeep+Rfrnd.Examination Reward Function.。The examination reward function is introduced as an intrinsic signal to encourage theLLM to generate goals with essential sub-goals.。We use the SI dataset as the examination set IS.。For each training iteration,a batch of instructions Itrain is randomly sampled from the full instruction dataset I, and corresponding goals garegenerated.。After the agent finishes its rollout, the examination reward for each batch is computed based on the intersectionI∩=IS∩ Itrain .。For non-empty I∩, an examination reward for each instruction in I∩is computed as:Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationre(ι, g, g sub) =(+2 gsub∈g−2otherwise,∀ι∈ I∩ (14)Then, Reis calculated as Re=1|I∩|Pι∈I∩re(ι, g, g sub|g=Gllm(s, ι)).Formatting Reward Function.。The formatting reward Rmfor each generated goal is calculated by computing an editdistance, utilizing the Wagner-Fischer algorithm (Wagner & Fischer, 1974).With the defined reward functions, RLAF is applied with a reward function R=Rf+Re+Rmand Proximal PolicyOptimization (PPO) for each data point in a batch.Figure 12: Implementation of the RND predictor network.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is ‘You should lie in wait’, which means teammate player wantsthe agent to set up an ambush or take a hiding position.2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan.According to the teammate‘S command:2.1.。Need to hide: ‘Whether prone position’, ‘Whether crouch position’2.2.。Reduce movement to stay unnoticed: ‘Average velocity’, ‘Length of distance moved’2.3.。Ensure readiness for potential attack: ‘Whether hold a gun’3.. \n",
      "the piece 420\n",
      " \n",
      "sub-goals.。We use the SI dataset as the examination set IS.。For each training iteration,a batch of instructions Itrain is randomly sampled from the full instruction dataset I, and corresponding goals garegenerated.。After the agent finishes its rollout, the examination reward for each batch is computed based on the intersectionI∩=IS∩ Itrain .。For non-empty I∩, an examination reward for each instruction in I∩is computed as:Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationre(ι, g, g sub) =(+2 gsub∈g−2otherwise,∀ι∈ I∩ (14)Then, Reis calculated as Re=1|I∩|Pι∈I∩re(ι, g, g sub|g=Gllm(s, ι)).Formatting Reward Function.。The formatting reward Rmfor each generated goal is calculated by computing an editdistance, utilizing the Wagner-Fischer algorithm (Wagner & Fischer, 1974).With the defined reward functions, RLAF is applied with a reward function R=Rf+Re+Rmand Proximal PolicyOptimization (PPO) for each data point in a batch.Figure 12: Implementation of the RND predictor network.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is ‘You should lie in wait’, which means teammate player wantsthe agent to set up an ambush or take a hiding position.2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan.According to the teammate‘S command:2.1.。Need to hide: ‘Whether prone position’, ‘Whether crouch position’2.2.。Reduce movement to stay unnoticed: ‘Average velocity’, ‘Length of distance moved’2.3.。Ensure readiness for potential attack: ‘Whether hold a gun’3.。Plan how these key states need to be adjusted.According to the teammate‘S command:3.1.. \n",
      "the piece 421\n",
      " \n",
      "puted based on the intersectionI∩=IS∩ Itrain .。For non-empty I∩, an examination reward for each instruction in I∩is computed as:Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationre(ι, g, g sub) =(+2 gsub∈g−2otherwise,∀ι∈ I∩ (14)Then, Reis calculated as Re=1|I∩|Pι∈I∩re(ι, g, g sub|g=Gllm(s, ι)).Formatting Reward Function.。The formatting reward Rmfor each generated goal is calculated by computing an editdistance, utilizing the Wagner-Fischer algorithm (Wagner & Fischer, 1974).With the defined reward functions, RLAF is applied with a reward function R=Rf+Re+Rmand Proximal PolicyOptimization (PPO) for each data point in a batch.Figure 12: Implementation of the RND predictor network.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is ‘You should lie in wait’, which means teammate player wantsthe agent to set up an ambush or take a hiding position.2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan.According to the teammate‘S command:2.1.。Need to hide: ‘Whether prone position’, ‘Whether crouch position’2.2.。Reduce movement to stay unnoticed: ‘Average velocity’, ‘Length of distance moved’2.3.。Ensure readiness for potential attack: ‘Whether hold a gun’3.。Plan how these key states need to be adjusted.According to the teammate‘S command:3.1.。‘Whether prone position’: Need to hide: ‘False’ -> ‘True’3.2.. \n",
      "the piece 422\n",
      " \n",
      "ng Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationre(ι, g, g sub) =(+2 gsub∈g−2otherwise,∀ι∈ I∩ (14)Then, Reis calculated as Re=1|I∩|Pι∈I∩re(ι, g, g sub|g=Gllm(s, ι)).Formatting Reward Function.。The formatting reward Rmfor each generated goal is calculated by computing an editdistance, utilizing the Wagner-Fischer algorithm (Wagner & Fischer, 1974).With the defined reward functions, RLAF is applied with a reward function R=Rf+Re+Rmand Proximal PolicyOptimization (PPO) for each data point in a batch.Figure 12: Implementation of the RND predictor network.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is ‘You should lie in wait’, which means teammate player wantsthe agent to set up an ambush or take a hiding position.2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan.According to the teammate‘S command:2.1.。Need to hide: ‘Whether prone position’, ‘Whether crouch position’2.2.。Reduce movement to stay unnoticed: ‘Average velocity’, ‘Length of distance moved’2.3.。Ensure readiness for potential attack: ‘Whether hold a gun’3.。Plan how these key states need to be adjusted.According to the teammate‘S command:3.1.。‘Whether prone position’: Need to hide: ‘False’ -> ‘True’3.2.。‘Whether crouch position’: Alternative hiding posture if not prone: ‘False’ -> ‘True’3.3.. \n",
      "the piece 423\n",
      " \n",
      "idirectional Adaptationre(ι, g, g sub) =(+2 gsub∈g−2otherwise,∀ι∈ I∩ (14)Then, Reis calculated as Re=1|I∩|Pι∈I∩re(ι, g, g sub|g=Gllm(s, ι)).Formatting Reward Function.。The formatting reward Rmfor each generated goal is calculated by computing an editdistance, utilizing the Wagner-Fischer algorithm (Wagner & Fischer, 1974).With the defined reward functions, RLAF is applied with a reward function R=Rf+Re+Rmand Proximal PolicyOptimization (PPO) for each data point in a batch.Figure 12: Implementation of the RND predictor network.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is ‘You should lie in wait’, which means teammate player wantsthe agent to set up an ambush or take a hiding position.2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan.According to the teammate‘S command:2.1.。Need to hide: ‘Whether prone position’, ‘Whether crouch position’2.2.。Reduce movement to stay unnoticed: ‘Average velocity’, ‘Length of distance moved’2.3.。Ensure readiness for potential attack: ‘Whether hold a gun’3.。Plan how these key states need to be adjusted.According to the teammate‘S command:3.1.。‘Whether prone position’: Need to hide: ‘False’ -> ‘True’3.2.。‘Whether crouch position’: Alternative hiding posture if not prone: ‘False’ -> ‘True’3.3.。‘Average velocity’: Reduce movement: ‘Fast’ -> ‘Static’3.4.. \n",
      "the piece 424\n",
      " \n",
      "d Function.。The formatting reward Rmfor each generated goal is calculated by computing an editdistance, utilizing the Wagner-Fischer algorithm (Wagner & Fischer, 1974).With the defined reward functions, RLAF is applied with a reward function R=Rf+Re+Rmand Proximal PolicyOptimization (PPO) for each data point in a batch.Figure 12: Implementation of the RND predictor network.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is ‘You should lie in wait’, which means teammate player wantsthe agent to set up an ambush or take a hiding position.2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan.According to the teammate‘S command:2.1.。Need to hide: ‘Whether prone position’, ‘Whether crouch position’2.2.。Reduce movement to stay unnoticed: ‘Average velocity’, ‘Length of distance moved’2.3.。Ensure readiness for potential attack: ‘Whether hold a gun’3.。Plan how these key states need to be adjusted.According to the teammate‘S command:3.1.。‘Whether prone position’: Need to hide: ‘False’ -> ‘True’3.2.。‘Whether crouch position’: Alternative hiding posture if not prone: ‘False’ -> ‘True’3.3.。‘Average velocity’: Reduce movement: ‘Fast’ -> ‘Static’3.4.。‘Length of distance moved’: Reduce movement: ‘Short’ -> ‘No movement’3.5.. \n",
      "the piece 425\n",
      " \n",
      "er algorithm (Wagner & Fischer, 1974).With the defined reward functions, RLAF is applied with a reward function R=Rf+Re+Rmand Proximal PolicyOptimization (PPO) for each data point in a batch.Figure 12: Implementation of the RND predictor network.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is ‘You should lie in wait’, which means teammate player wantsthe agent to set up an ambush or take a hiding position.2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan.According to the teammate‘S command:2.1.。Need to hide: ‘Whether prone position’, ‘Whether crouch position’2.2.。Reduce movement to stay unnoticed: ‘Average velocity’, ‘Length of distance moved’2.3.。Ensure readiness for potential attack: ‘Whether hold a gun’3.。Plan how these key states need to be adjusted.According to the teammate‘S command:3.1.。‘Whether prone position’: Need to hide: ‘False’ -> ‘True’3.2.。‘Whether crouch position’: Alternative hiding posture if not prone: ‘False’ -> ‘True’3.3.。‘Average velocity’: Reduce movement: ‘Fast’ -> ‘Static’3.4.。‘Length of distance moved’: Reduce movement: ‘Short’ -> ‘No movement’3.5.。‘Whether hold a gun’: Ensure readiness for potential attack: ‘True’ -> ‘True’4.. \n",
      "the piece 426\n",
      " \n",
      "ectional Adaptation1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is ‘You should lie in wait’, which means teammate player wantsthe agent to set up an ambush or take a hiding position.2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan.According to the teammate‘S command:2.1.。Need to hide: ‘Whether prone position’, ‘Whether crouch position’2.2.。Reduce movement to stay unnoticed: ‘Average velocity’, ‘Length of distance moved’2.3.。Ensure readiness for potential attack: ‘Whether hold a gun’3.。Plan how these key states need to be adjusted.According to the teammate‘S command:3.1.。‘Whether prone position’: Need to hide: ‘False’ -> ‘True’3.2.。‘Whether crouch position’: Alternative hiding posture if not prone: ‘False’ -> ‘True’3.3.。‘Average velocity’: Reduce movement: ‘Fast’ -> ‘Static’3.4.。‘Length of distance moved’: Reduce movement: ‘Short’ -> ‘No movement’3.5.。‘Whether hold a gun’: Ensure readiness for potential attack: ‘True’ -> ‘True’4.。Modify the adjustment that may be wrong, and refer to the Rules to analyze which state adjustments mayconflict, repeat or be unnecessary, and output the modified adjustment plan:According to the states adjustments in 3:4.1.. \n",
      "the piece 427\n",
      " \n",
      "mmate player.。The agentsin the unselected states will adjust themselves to complete your plan.According to the teammate‘S command:2.1.。Need to hide: ‘Whether prone position’, ‘Whether crouch position’2.2.。Reduce movement to stay unnoticed: ‘Average velocity’, ‘Length of distance moved’2.3.。Ensure readiness for potential attack: ‘Whether hold a gun’3.。Plan how these key states need to be adjusted.According to the teammate‘S command:3.1.。‘Whether prone position’: Need to hide: ‘False’ -> ‘True’3.2.。‘Whether crouch position’: Alternative hiding posture if not prone: ‘False’ -> ‘True’3.3.。‘Average velocity’: Reduce movement: ‘Fast’ -> ‘Static’3.4.。‘Length of distance moved’: Reduce movement: ‘Short’ -> ‘No movement’3.5.。‘Whether hold a gun’: Ensure readiness for potential attack: ‘True’ -> ‘True’4.。Modify the adjustment that may be wrong, and refer to the Rules to analyze which state adjustments mayconflict, repeat or be unnecessary, and output the modified adjustment plan:According to the states adjustments in 3:4.1.。‘Whether prone position’: ‘False’ -> ‘True’4.2.. \n",
      "the piece 428\n",
      " \n",
      "mplete your plan.According to the teammate‘S command:2.1.。Need to hide: ‘Whether prone position’, ‘Whether crouch position’2.2.。Reduce movement to stay unnoticed: ‘Average velocity’, ‘Length of distance moved’2.3.。Ensure readiness for potential attack: ‘Whether hold a gun’3.。Plan how these key states need to be adjusted.According to the teammate‘S command:3.1.。‘Whether prone position’: Need to hide: ‘False’ -> ‘True’3.2.。‘Whether crouch position’: Alternative hiding posture if not prone: ‘False’ -> ‘True’3.3.。‘Average velocity’: Reduce movement: ‘Fast’ -> ‘Static’3.4.。‘Length of distance moved’: Reduce movement: ‘Short’ -> ‘No movement’3.5.。‘Whether hold a gun’: Ensure readiness for potential attack: ‘True’ -> ‘True’4.。Modify the adjustment that may be wrong, and refer to the Rules to analyze which state adjustments mayconflict, repeat or be unnecessary, and output the modified adjustment plan:According to the states adjustments in 3:4.1.。‘Whether prone position’: ‘False’ -> ‘True’4.2.。‘Average velocity’: ‘Fast’ -> ‘Static’4.3.. \n",
      "the piece 429\n",
      " \n",
      " gun’3.。Plan how these key states need to be adjusted.According to the teammate‘S command:3.1.。‘Whether prone position’: Need to hide: ‘False’ -> ‘True’3.2.。‘Whether crouch position’: Alternative hiding posture if not prone: ‘False’ -> ‘True’3.3.。‘Average velocity’: Reduce movement: ‘Fast’ -> ‘Static’3.4.。‘Length of distance moved’: Reduce movement: ‘Short’ -> ‘No movement’3.5.。‘Whether hold a gun’: Ensure readiness for potential attack: ‘True’ -> ‘True’4.。Modify the adjustment that may be wrong, and refer to the Rules to analyze which state adjustments mayconflict, repeat or be unnecessary, and output the modified adjustment plan:According to the states adjustments in 3:4.1.。‘Whether prone position’: ‘False’ -> ‘True’4.2.。‘Average velocity’: ‘Fast’ -> ‘Static’4.3.。‘Length of distance moved’: ‘Short’ -> ‘No movement’4.4.. \n",
      "the piece 430\n",
      " \n",
      "hether prone position’: Need to hide: ‘False’ -> ‘True’3.2.。‘Whether crouch position’: Alternative hiding posture if not prone: ‘False’ -> ‘True’3.3.。‘Average velocity’: Reduce movement: ‘Fast’ -> ‘Static’3.4.。‘Length of distance moved’: Reduce movement: ‘Short’ -> ‘No movement’3.5.。‘Whether hold a gun’: Ensure readiness for potential attack: ‘True’ -> ‘True’4.。Modify the adjustment that may be wrong, and refer to the Rules to analyze which state adjustments mayconflict, repeat or be unnecessary, and output the modified adjustment plan:According to the states adjustments in 3:4.1.。‘Whether prone position’: ‘False’ -> ‘True’4.2.。‘Average velocity’: ‘Fast’ -> ‘Static’4.3.。‘Length of distance moved’: ‘Short’ -> ‘No movement’4.4.。‘Whether hold a gun’: ‘True’ -> ‘True’(Removed ‘Whether crouch position’ as it conflicts with ‘Whether prone position’ and prone position givesbetter cover for an ambush.)5.. \n",
      "the piece 431\n",
      " \n",
      "rone: ‘False’ -> ‘True’3.3.。‘Average velocity’: Reduce movement: ‘Fast’ -> ‘Static’3.4.。‘Length of distance moved’: Reduce movement: ‘Short’ -> ‘No movement’3.5.。‘Whether hold a gun’: Ensure readiness for potential attack: ‘True’ -> ‘True’4.。Modify the adjustment that may be wrong, and refer to the Rules to analyze which state adjustments mayconflict, repeat or be unnecessary, and output the modified adjustment plan:According to the states adjustments in 3:4.1.。‘Whether prone position’: ‘False’ -> ‘True’4.2.。‘Average velocity’: ‘Fast’ -> ‘Static’4.3.。‘Length of distance moved’: ‘Short’ -> ‘No movement’4.4.。‘Whether hold a gun’: ‘True’ -> ‘True’(Removed ‘Whether crouch position’ as it conflicts with ‘Whether prone position’ and prone position givesbetter cover for an ambush.)5.。According to the analyze and the planing of the verbal command, further analyze the behavior tendencyrequired in the adjustment process.Mobile: Need to stop all movement: 0%Offense: Must be ready to attack if necessary: 20%Waiting: The main goal is to set up an ambush and wait: 70%Supplies: No need for supplies during this phase: 0%Scouting: Need to observe the surroundings, but not actively scout: 10%6.. \n",
      "the piece 432\n",
      " \n",
      " -> ‘Static’3.4.。‘Length of distance moved’: Reduce movement: ‘Short’ -> ‘No movement’3.5.。‘Whether hold a gun’: Ensure readiness for potential attack: ‘True’ -> ‘True’4.。Modify the adjustment that may be wrong, and refer to the Rules to analyze which state adjustments mayconflict, repeat or be unnecessary, and output the modified adjustment plan:According to the states adjustments in 3:4.1.。‘Whether prone position’: ‘False’ -> ‘True’4.2.。‘Average velocity’: ‘Fast’ -> ‘Static’4.3.。‘Length of distance moved’: ‘Short’ -> ‘No movement’4.4.。‘Whether hold a gun’: ‘True’ -> ‘True’(Removed ‘Whether crouch position’ as it conflicts with ‘Whether prone position’ and prone position givesbetter cover for an ambush.)5.。According to the analyze and the planing of the verbal command, further analyze the behavior tendencyrequired in the adjustment process.Mobile: Need to stop all movement: 0%Offense: Must be ready to attack if necessary: 20%Waiting: The main goal is to set up an ambush and wait: 70%Supplies: No need for supplies during this phase: 0%Scouting: Need to observe the surroundings, but not actively scout: 10%6.。Analyze how long the current command needs to be kept.According to the command of the teammate, taking a hiding position or setting up an ambush is usually donewith the intention of waiting for an opportune moment to strike or observe the enemy‘S movement: The currentcommand needs to be kept by ‘long term’.Table 9: Chain of thought response from GPT4.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.Only select the most relevant and necessary states for planning, and the unplanned states will be adjusted bythe agent itself2.. \n",
      "the piece 433\n",
      " \n",
      ".5.。‘Whether hold a gun’: Ensure readiness for potential attack: ‘True’ -> ‘True’4.。Modify the adjustment that may be wrong, and refer to the Rules to analyze which state adjustments mayconflict, repeat or be unnecessary, and output the modified adjustment plan:According to the states adjustments in 3:4.1.。‘Whether prone position’: ‘False’ -> ‘True’4.2.。‘Average velocity’: ‘Fast’ -> ‘Static’4.3.。‘Length of distance moved’: ‘Short’ -> ‘No movement’4.4.。‘Whether hold a gun’: ‘True’ -> ‘True’(Removed ‘Whether crouch position’ as it conflicts with ‘Whether prone position’ and prone position givesbetter cover for an ambush.)5.。According to the analyze and the planing of the verbal command, further analyze the behavior tendencyrequired in the adjustment process.Mobile: Need to stop all movement: 0%Offense: Must be ready to attack if necessary: 20%Waiting: The main goal is to set up an ambush and wait: 70%Supplies: No need for supplies during this phase: 0%Scouting: Need to observe the surroundings, but not actively scout: 10%6.。Analyze how long the current command needs to be kept.According to the command of the teammate, taking a hiding position or setting up an ambush is usually donewith the intention of waiting for an opportune moment to strike or observe the enemy‘S movement: The currentcommand needs to be kept by ‘long term’.Table 9: Chain of thought response from GPT4.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.Only select the most relevant and necessary states for planning, and the unplanned states will be adjusted bythe agent itself2.。[Choose 1, Choose 2, ...] indicates the values that can be selected for the state.. \n",
      "the piece 434\n",
      " \n",
      ": ‘True’ -> ‘True’4.。Modify the adjustment that may be wrong, and refer to the Rules to analyze which state adjustments mayconflict, repeat or be unnecessary, and output the modified adjustment plan:According to the states adjustments in 3:4.1.。‘Whether prone position’: ‘False’ -> ‘True’4.2.。‘Average velocity’: ‘Fast’ -> ‘Static’4.3.。‘Length of distance moved’: ‘Short’ -> ‘No movement’4.4.。‘Whether hold a gun’: ‘True’ -> ‘True’(Removed ‘Whether crouch position’ as it conflicts with ‘Whether prone position’ and prone position givesbetter cover for an ambush.)5.。According to the analyze and the planing of the verbal command, further analyze the behavior tendencyrequired in the adjustment process.Mobile: Need to stop all movement: 0%Offense: Must be ready to attack if necessary: 20%Waiting: The main goal is to set up an ambush and wait: 70%Supplies: No need for supplies during this phase: 0%Scouting: Need to observe the surroundings, but not actively scout: 10%6.。Analyze how long the current command needs to be kept.According to the command of the teammate, taking a hiding position or setting up an ambush is usually donewith the intention of waiting for an opportune moment to strike or observe the enemy‘S movement: The currentcommand needs to be kept by ‘long term’.Table 9: Chain of thought response from GPT4.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.Only select the most relevant and necessary states for planning, and the unplanned states will be adjusted bythe agent itself2.。[Choose 1, Choose 2, ...] indicates the values that can be selected for the state.。When you plan, you can onlychoose the value of the state from it, and do not invent new value not listed in [Choice1, Choice2, ...].3.The selected state can change the current value or maintain the current value.. \n",
      "the piece 435\n",
      " \n",
      "analyze which state adjustments mayconflict, repeat or be unnecessary, and output the modified adjustment plan:According to the states adjustments in 3:4.1.。‘Whether prone position’: ‘False’ -> ‘True’4.2.。‘Average velocity’: ‘Fast’ -> ‘Static’4.3.。‘Length of distance moved’: ‘Short’ -> ‘No movement’4.4.。‘Whether hold a gun’: ‘True’ -> ‘True’(Removed ‘Whether crouch position’ as it conflicts with ‘Whether prone position’ and prone position givesbetter cover for an ambush.)5.。According to the analyze and the planing of the verbal command, further analyze the behavior tendencyrequired in the adjustment process.Mobile: Need to stop all movement: 0%Offense: Must be ready to attack if necessary: 20%Waiting: The main goal is to set up an ambush and wait: 70%Supplies: No need for supplies during this phase: 0%Scouting: Need to observe the surroundings, but not actively scout: 10%6.。Analyze how long the current command needs to be kept.According to the command of the teammate, taking a hiding position or setting up an ambush is usually donewith the intention of waiting for an opportune moment to strike or observe the enemy‘S movement: The currentcommand needs to be kept by ‘long term’.Table 9: Chain of thought response from GPT4.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.Only select the most relevant and necessary states for planning, and the unplanned states will be adjusted bythe agent itself2.。[Choose 1, Choose 2, ...] indicates the values that can be selected for the state.。When you plan, you can onlychoose the value of the state from it, and do not invent new value not listed in [Choice1, Choice2, ...].3.The selected state can change the current value or maintain the current value.。The agent will try to achieveand maintain the value of the state you choose after you give the plan.4.Agents don’t voluntarily discard items (for example guns, bullets, medical kits) unless items are reduced orset as False in your plan, so there is no need to keep them, only to choose when making changes.5.Do not plan and adjust the states of teammates and enemies, they can move freely and cannot be controlled.6.Avoid conflicts of states planing.. \n",
      "the piece 436\n",
      " \n",
      "essary, and output the modified adjustment plan:According to the states adjustments in 3:4.1.。‘Whether prone position’: ‘False’ -> ‘True’4.2.。‘Average velocity’: ‘Fast’ -> ‘Static’4.3.。‘Length of distance moved’: ‘Short’ -> ‘No movement’4.4.。‘Whether hold a gun’: ‘True’ -> ‘True’(Removed ‘Whether crouch position’ as it conflicts with ‘Whether prone position’ and prone position givesbetter cover for an ambush.)5.。According to the analyze and the planing of the verbal command, further analyze the behavior tendencyrequired in the adjustment process.Mobile: Need to stop all movement: 0%Offense: Must be ready to attack if necessary: 20%Waiting: The main goal is to set up an ambush and wait: 70%Supplies: No need for supplies during this phase: 0%Scouting: Need to observe the surroundings, but not actively scout: 10%6.。Analyze how long the current command needs to be kept.According to the command of the teammate, taking a hiding position or setting up an ambush is usually donewith the intention of waiting for an opportune moment to strike or observe the enemy‘S movement: The currentcommand needs to be kept by ‘long term’.Table 9: Chain of thought response from GPT4.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.Only select the most relevant and necessary states for planning, and the unplanned states will be adjusted bythe agent itself2.。[Choose 1, Choose 2, ...] indicates the values that can be selected for the state.。When you plan, you can onlychoose the value of the state from it, and do not invent new value not listed in [Choice1, Choice2, ...].3.The selected state can change the current value or maintain the current value.。The agent will try to achieveand maintain the value of the state you choose after you give the plan.4.Agents don’t voluntarily discard items (for example guns, bullets, medical kits) unless items are reduced orset as False in your plan, so there is no need to keep them, only to choose when making changes.5.Do not plan and adjust the states of teammates and enemies, they can move freely and cannot be controlled.6.Avoid conflicts of states planing.。For example, agent unable to move quickly when lying down, and unable tosee enemies when length of distance from agent to enemy is far away.7.Avoid the repetition of states planing.. \n",
      "the piece 437\n",
      " \n",
      "1.。‘Whether prone position’: ‘False’ -> ‘True’4.2.。‘Average velocity’: ‘Fast’ -> ‘Static’4.3.。‘Length of distance moved’: ‘Short’ -> ‘No movement’4.4.。‘Whether hold a gun’: ‘True’ -> ‘True’(Removed ‘Whether crouch position’ as it conflicts with ‘Whether prone position’ and prone position givesbetter cover for an ambush.)5.。According to the analyze and the planing of the verbal command, further analyze the behavior tendencyrequired in the adjustment process.Mobile: Need to stop all movement: 0%Offense: Must be ready to attack if necessary: 20%Waiting: The main goal is to set up an ambush and wait: 70%Supplies: No need for supplies during this phase: 0%Scouting: Need to observe the surroundings, but not actively scout: 10%6.。Analyze how long the current command needs to be kept.According to the command of the teammate, taking a hiding position or setting up an ambush is usually donewith the intention of waiting for an opportune moment to strike or observe the enemy‘S movement: The currentcommand needs to be kept by ‘long term’.Table 9: Chain of thought response from GPT4.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.Only select the most relevant and necessary states for planning, and the unplanned states will be adjusted bythe agent itself2.。[Choose 1, Choose 2, ...] indicates the values that can be selected for the state.。When you plan, you can onlychoose the value of the state from it, and do not invent new value not listed in [Choice1, Choice2, ...].3.The selected state can change the current value or maintain the current value.。The agent will try to achieveand maintain the value of the state you choose after you give the plan.4.Agents don’t voluntarily discard items (for example guns, bullets, medical kits) unless items are reduced orset as False in your plan, so there is no need to keep them, only to choose when making changes.5.Do not plan and adjust the states of teammates and enemies, they can move freely and cannot be controlled.6.Avoid conflicts of states planing.。For example, agent unable to move quickly when lying down, and unable tosee enemies when length of distance from agent to enemy is far away.7.Avoid the repetition of states planing.。For example, if the Average velocity has been adjusted to be Fast, thereis no need to adjust the Whether prone position to False, because the agent can automatically adjust state to fitoverlapping meanings.8.When it is necessary to refer to enemy or teammate information for planing, describe the specific state valueduring analysis.Table 10: Rule prompt for GPT4.Dataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)Attention 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.159Mlp 0.549 0.664 0.482 0.587 0.514 0.620 0.065 0.134All 0.529 0.642 0.471 0.581 0.485 0.596 0.069 0.119Table 13: Evaluation on LoRA target.Symbol Sub-goal Classg1Average velocityg2Horizontal direction of movementg3Whether seen enemyg4Whether hold a gung5Whether prone positiong6Length of distance movedg7Length of distance from agent to teammateg8Distance with nearest enemyg9Whether seen by enemyg10Damage to enemyg11Whether have bulletsg12Horizontal direction of viewg13Whether follow with the movement direction of teammateg14Whether crouch positiong15Whether have a gung16Whether have medical kitsg17Whether to restore healthg18Health levelg19Whether knock down enemyg20Whether target the same enemy as teammateTable 14: Top 20 sub-goals ranked by frequency.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationDataset SizeTrainingMethodPrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)100%CoFT 51.85% 64.84% 46.68% 57.91% 49.13% 61.18%CoFT→SFT 55.48% 68.52% 50.48% 62.10% 52.86% 65.15%SFT 54.70% 65.20% 49.00% 60.20% 51.70% 63.20%Improve Rate 1.42% 5.09% 3.02% 3.16% 2.25% 3.09%30%CoFT 49.43% 62.66% 44.45% 56.29% 46.81% 59.30%CoFT→SFT 49.92% 62.59% 45.51% 57.65% 47.61% 60.02%SFT 46.12% 60.96% 33.68% 45.39% 38.93% 52.03%Improve Rate 8.25% 2.68% 35.11% 27.01% 22.30% 15.34%10%CoFT 45.58% 60.84% 41.77% 53.92% 43.59% 57.17%CoFT→SFT 48.06% 61.01% 43.15% 54.31% 45.47% 57.47%SFT 42.08% 55.31% 30.86% 41.45% 35.61% 47.39%Improve Rate 14.20% 10.32% 39.80% 31.03% 27.69% 21.28%3%CoFT 39.42% 58.35% 34.28% 50.10% 36.67% 53.91%CoFT→SFT 41.61% 60.40% 34.55% 50.32% 37.75% 54.90%SFT 17.66% 38.28% 13.33% 29.47% 15.20% 33.30%Improve Rate 135.52% 57.78% 159.15% 70.79% 148.43% 64.88%Table 15: Language model performance evaluation with different sizes of fine-tuning training set.. \n",
      "the piece 438\n",
      " \n",
      "elocity’: ‘Fast’ -> ‘Static’4.3.。‘Length of distance moved’: ‘Short’ -> ‘No movement’4.4.。‘Whether hold a gun’: ‘True’ -> ‘True’(Removed ‘Whether crouch position’ as it conflicts with ‘Whether prone position’ and prone position givesbetter cover for an ambush.)5.。According to the analyze and the planing of the verbal command, further analyze the behavior tendencyrequired in the adjustment process.Mobile: Need to stop all movement: 0%Offense: Must be ready to attack if necessary: 20%Waiting: The main goal is to set up an ambush and wait: 70%Supplies: No need for supplies during this phase: 0%Scouting: Need to observe the surroundings, but not actively scout: 10%6.。Analyze how long the current command needs to be kept.According to the command of the teammate, taking a hiding position or setting up an ambush is usually donewith the intention of waiting for an opportune moment to strike or observe the enemy‘S movement: The currentcommand needs to be kept by ‘long term’.Table 9: Chain of thought response from GPT4.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.Only select the most relevant and necessary states for planning, and the unplanned states will be adjusted bythe agent itself2.。[Choose 1, Choose 2, ...] indicates the values that can be selected for the state.。When you plan, you can onlychoose the value of the state from it, and do not invent new value not listed in [Choice1, Choice2, ...].3.The selected state can change the current value or maintain the current value.。The agent will try to achieveand maintain the value of the state you choose after you give the plan.4.Agents don’t voluntarily discard items (for example guns, bullets, medical kits) unless items are reduced orset as False in your plan, so there is no need to keep them, only to choose when making changes.5.Do not plan and adjust the states of teammates and enemies, they can move freely and cannot be controlled.6.Avoid conflicts of states planing.。For example, agent unable to move quickly when lying down, and unable tosee enemies when length of distance from agent to enemy is far away.7.Avoid the repetition of states planing.。For example, if the Average velocity has been adjusted to be Fast, thereis no need to adjust the Whether prone position to False, because the agent can automatically adjust state to fitoverlapping meanings.8.When it is necessary to refer to enemy or teammate information for planing, describe the specific state valueduring analysis.Table 10: Rule prompt for GPT4.Dataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)Attention 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.159Mlp 0.549 0.664 0.482 0.587 0.514 0.620 0.065 0.134All 0.529 0.642 0.471 0.581 0.485 0.596 0.069 0.119Table 13: Evaluation on LoRA target.Symbol Sub-goal Classg1Average velocityg2Horizontal direction of movementg3Whether seen enemyg4Whether hold a gung5Whether prone positiong6Length of distance movedg7Length of distance from agent to teammateg8Distance with nearest enemyg9Whether seen by enemyg10Damage to enemyg11Whether have bulletsg12Horizontal direction of viewg13Whether follow with the movement direction of teammateg14Whether crouch positiong15Whether have a gung16Whether have medical kitsg17Whether to restore healthg18Health levelg19Whether knock down enemyg20Whether target the same enemy as teammateTable 14: Top 20 sub-goals ranked by frequency.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationDataset SizeTrainingMethodPrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)100%CoFT 51.85% 64.84% 46.68% 57.91% 49.13% 61.18%CoFT→SFT 55.48% 68.52% 50.48% 62.10% 52.86% 65.15%SFT 54.70% 65.20% 49.00% 60.20% 51.70% 63.20%Improve Rate 1.42% 5.09% 3.02% 3.16% 2.25% 3.09%30%CoFT 49.43% 62.66% 44.45% 56.29% 46.81% 59.30%CoFT→SFT 49.92% 62.59% 45.51% 57.65% 47.61% 60.02%SFT 46.12% 60.96% 33.68% 45.39% 38.93% 52.03%Improve Rate 8.25% 2.68% 35.11% 27.01% 22.30% 15.34%10%CoFT 45.58% 60.84% 41.77% 53.92% 43.59% 57.17%CoFT→SFT 48.06% 61.01% 43.15% 54.31% 45.47% 57.47%SFT 42.08% 55.31% 30.86% 41.45% 35.61% 47.39%Improve Rate 14.20% 10.32% 39.80% 31.03% 27.69% 21.28%3%CoFT 39.42% 58.35% 34.28% 50.10% 36.67% 53.91%CoFT→SFT 41.61% 60.40% 34.55% 50.32% 37.75% 54.90%SFT 17.66% 38.28% 13.33% 29.47% 15.20% 33.30%Improve Rate 135.52% 57.78% 159.15% 70.79% 148.43% 64.88%Table 15: Language model performance evaluation with different sizes of fine-tuning training set.。The underlined “ImproveRate” values represent the improvement percentage of the “CoFT →SFT” method relative to “SFT” method.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationIn order to complete the command ‘You should lie in wait’, let us plan the states of the agent step by step usingthe following template:1.. \n",
      "the piece 439\n",
      " \n",
      " movement’4.4.。‘Whether hold a gun’: ‘True’ -> ‘True’(Removed ‘Whether crouch position’ as it conflicts with ‘Whether prone position’ and prone position givesbetter cover for an ambush.)5.。According to the analyze and the planing of the verbal command, further analyze the behavior tendencyrequired in the adjustment process.Mobile: Need to stop all movement: 0%Offense: Must be ready to attack if necessary: 20%Waiting: The main goal is to set up an ambush and wait: 70%Supplies: No need for supplies during this phase: 0%Scouting: Need to observe the surroundings, but not actively scout: 10%6.。Analyze how long the current command needs to be kept.According to the command of the teammate, taking a hiding position or setting up an ambush is usually donewith the intention of waiting for an opportune moment to strike or observe the enemy‘S movement: The currentcommand needs to be kept by ‘long term’.Table 9: Chain of thought response from GPT4.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.Only select the most relevant and necessary states for planning, and the unplanned states will be adjusted bythe agent itself2.。[Choose 1, Choose 2, ...] indicates the values that can be selected for the state.。When you plan, you can onlychoose the value of the state from it, and do not invent new value not listed in [Choice1, Choice2, ...].3.The selected state can change the current value or maintain the current value.。The agent will try to achieveand maintain the value of the state you choose after you give the plan.4.Agents don’t voluntarily discard items (for example guns, bullets, medical kits) unless items are reduced orset as False in your plan, so there is no need to keep them, only to choose when making changes.5.Do not plan and adjust the states of teammates and enemies, they can move freely and cannot be controlled.6.Avoid conflicts of states planing.。For example, agent unable to move quickly when lying down, and unable tosee enemies when length of distance from agent to enemy is far away.7.Avoid the repetition of states planing.。For example, if the Average velocity has been adjusted to be Fast, thereis no need to adjust the Whether prone position to False, because the agent can automatically adjust state to fitoverlapping meanings.8.When it is necessary to refer to enemy or teammate information for planing, describe the specific state valueduring analysis.Table 10: Rule prompt for GPT4.Dataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)Attention 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.159Mlp 0.549 0.664 0.482 0.587 0.514 0.620 0.065 0.134All 0.529 0.642 0.471 0.581 0.485 0.596 0.069 0.119Table 13: Evaluation on LoRA target.Symbol Sub-goal Classg1Average velocityg2Horizontal direction of movementg3Whether seen enemyg4Whether hold a gung5Whether prone positiong6Length of distance movedg7Length of distance from agent to teammateg8Distance with nearest enemyg9Whether seen by enemyg10Damage to enemyg11Whether have bulletsg12Horizontal direction of viewg13Whether follow with the movement direction of teammateg14Whether crouch positiong15Whether have a gung16Whether have medical kitsg17Whether to restore healthg18Health levelg19Whether knock down enemyg20Whether target the same enemy as teammateTable 14: Top 20 sub-goals ranked by frequency.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationDataset SizeTrainingMethodPrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)100%CoFT 51.85% 64.84% 46.68% 57.91% 49.13% 61.18%CoFT→SFT 55.48% 68.52% 50.48% 62.10% 52.86% 65.15%SFT 54.70% 65.20% 49.00% 60.20% 51.70% 63.20%Improve Rate 1.42% 5.09% 3.02% 3.16% 2.25% 3.09%30%CoFT 49.43% 62.66% 44.45% 56.29% 46.81% 59.30%CoFT→SFT 49.92% 62.59% 45.51% 57.65% 47.61% 60.02%SFT 46.12% 60.96% 33.68% 45.39% 38.93% 52.03%Improve Rate 8.25% 2.68% 35.11% 27.01% 22.30% 15.34%10%CoFT 45.58% 60.84% 41.77% 53.92% 43.59% 57.17%CoFT→SFT 48.06% 61.01% 43.15% 54.31% 45.47% 57.47%SFT 42.08% 55.31% 30.86% 41.45% 35.61% 47.39%Improve Rate 14.20% 10.32% 39.80% 31.03% 27.69% 21.28%3%CoFT 39.42% 58.35% 34.28% 50.10% 36.67% 53.91%CoFT→SFT 41.61% 60.40% 34.55% 50.32% 37.75% 54.90%SFT 17.66% 38.28% 13.33% 29.47% 15.20% 33.30%Improve Rate 135.52% 57.78% 159.15% 70.79% 148.43% 64.88%Table 15: Language model performance evaluation with different sizes of fine-tuning training set.。The underlined “ImproveRate” values represent the improvement percentage of the “CoFT →SFT” method relative to “SFT” method.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationIn order to complete the command ‘You should lie in wait’, let us plan the states of the agent step by step usingthe following template:1.。Analyze the verbal orders of teammates and players, what do you want to do?. \n",
      "the piece 440\n",
      " \n",
      "ition’ as it conflicts with ‘Whether prone position’ and prone position givesbetter cover for an ambush.)5.。According to the analyze and the planing of the verbal command, further analyze the behavior tendencyrequired in the adjustment process.Mobile: Need to stop all movement: 0%Offense: Must be ready to attack if necessary: 20%Waiting: The main goal is to set up an ambush and wait: 70%Supplies: No need for supplies during this phase: 0%Scouting: Need to observe the surroundings, but not actively scout: 10%6.。Analyze how long the current command needs to be kept.According to the command of the teammate, taking a hiding position or setting up an ambush is usually donewith the intention of waiting for an opportune moment to strike or observe the enemy‘S movement: The currentcommand needs to be kept by ‘long term’.Table 9: Chain of thought response from GPT4.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.Only select the most relevant and necessary states for planning, and the unplanned states will be adjusted bythe agent itself2.。[Choose 1, Choose 2, ...] indicates the values that can be selected for the state.。When you plan, you can onlychoose the value of the state from it, and do not invent new value not listed in [Choice1, Choice2, ...].3.The selected state can change the current value or maintain the current value.。The agent will try to achieveand maintain the value of the state you choose after you give the plan.4.Agents don’t voluntarily discard items (for example guns, bullets, medical kits) unless items are reduced orset as False in your plan, so there is no need to keep them, only to choose when making changes.5.Do not plan and adjust the states of teammates and enemies, they can move freely and cannot be controlled.6.Avoid conflicts of states planing.。For example, agent unable to move quickly when lying down, and unable tosee enemies when length of distance from agent to enemy is far away.7.Avoid the repetition of states planing.。For example, if the Average velocity has been adjusted to be Fast, thereis no need to adjust the Whether prone position to False, because the agent can automatically adjust state to fitoverlapping meanings.8.When it is necessary to refer to enemy or teammate information for planing, describe the specific state valueduring analysis.Table 10: Rule prompt for GPT4.Dataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)Attention 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.159Mlp 0.549 0.664 0.482 0.587 0.514 0.620 0.065 0.134All 0.529 0.642 0.471 0.581 0.485 0.596 0.069 0.119Table 13: Evaluation on LoRA target.Symbol Sub-goal Classg1Average velocityg2Horizontal direction of movementg3Whether seen enemyg4Whether hold a gung5Whether prone positiong6Length of distance movedg7Length of distance from agent to teammateg8Distance with nearest enemyg9Whether seen by enemyg10Damage to enemyg11Whether have bulletsg12Horizontal direction of viewg13Whether follow with the movement direction of teammateg14Whether crouch positiong15Whether have a gung16Whether have medical kitsg17Whether to restore healthg18Health levelg19Whether knock down enemyg20Whether target the same enemy as teammateTable 14: Top 20 sub-goals ranked by frequency.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationDataset SizeTrainingMethodPrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)100%CoFT 51.85% 64.84% 46.68% 57.91% 49.13% 61.18%CoFT→SFT 55.48% 68.52% 50.48% 62.10% 52.86% 65.15%SFT 54.70% 65.20% 49.00% 60.20% 51.70% 63.20%Improve Rate 1.42% 5.09% 3.02% 3.16% 2.25% 3.09%30%CoFT 49.43% 62.66% 44.45% 56.29% 46.81% 59.30%CoFT→SFT 49.92% 62.59% 45.51% 57.65% 47.61% 60.02%SFT 46.12% 60.96% 33.68% 45.39% 38.93% 52.03%Improve Rate 8.25% 2.68% 35.11% 27.01% 22.30% 15.34%10%CoFT 45.58% 60.84% 41.77% 53.92% 43.59% 57.17%CoFT→SFT 48.06% 61.01% 43.15% 54.31% 45.47% 57.47%SFT 42.08% 55.31% 30.86% 41.45% 35.61% 47.39%Improve Rate 14.20% 10.32% 39.80% 31.03% 27.69% 21.28%3%CoFT 39.42% 58.35% 34.28% 50.10% 36.67% 53.91%CoFT→SFT 41.61% 60.40% 34.55% 50.32% 37.75% 54.90%SFT 17.66% 38.28% 13.33% 29.47% 15.20% 33.30%Improve Rate 135.52% 57.78% 159.15% 70.79% 148.43% 64.88%Table 15: Language model performance evaluation with different sizes of fine-tuning training set.。The underlined “ImproveRate” values represent the improvement percentage of the “CoFT →SFT” method relative to “SFT” method.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationIn order to complete the command ‘You should lie in wait’, let us plan the states of the agent step by step usingthe following template:1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is [Command], which means teammate player wants the agent...2.. \n",
      "the piece 441\n",
      " \n",
      "adjustment process.Mobile: Need to stop all movement: 0%Offense: Must be ready to attack if necessary: 20%Waiting: The main goal is to set up an ambush and wait: 70%Supplies: No need for supplies during this phase: 0%Scouting: Need to observe the surroundings, but not actively scout: 10%6.。Analyze how long the current command needs to be kept.According to the command of the teammate, taking a hiding position or setting up an ambush is usually donewith the intention of waiting for an opportune moment to strike or observe the enemy‘S movement: The currentcommand needs to be kept by ‘long term’.Table 9: Chain of thought response from GPT4.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.Only select the most relevant and necessary states for planning, and the unplanned states will be adjusted bythe agent itself2.。[Choose 1, Choose 2, ...] indicates the values that can be selected for the state.。When you plan, you can onlychoose the value of the state from it, and do not invent new value not listed in [Choice1, Choice2, ...].3.The selected state can change the current value or maintain the current value.。The agent will try to achieveand maintain the value of the state you choose after you give the plan.4.Agents don’t voluntarily discard items (for example guns, bullets, medical kits) unless items are reduced orset as False in your plan, so there is no need to keep them, only to choose when making changes.5.Do not plan and adjust the states of teammates and enemies, they can move freely and cannot be controlled.6.Avoid conflicts of states planing.。For example, agent unable to move quickly when lying down, and unable tosee enemies when length of distance from agent to enemy is far away.7.Avoid the repetition of states planing.。For example, if the Average velocity has been adjusted to be Fast, thereis no need to adjust the Whether prone position to False, because the agent can automatically adjust state to fitoverlapping meanings.8.When it is necessary to refer to enemy or teammate information for planing, describe the specific state valueduring analysis.Table 10: Rule prompt for GPT4.Dataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)Attention 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.159Mlp 0.549 0.664 0.482 0.587 0.514 0.620 0.065 0.134All 0.529 0.642 0.471 0.581 0.485 0.596 0.069 0.119Table 13: Evaluation on LoRA target.Symbol Sub-goal Classg1Average velocityg2Horizontal direction of movementg3Whether seen enemyg4Whether hold a gung5Whether prone positiong6Length of distance movedg7Length of distance from agent to teammateg8Distance with nearest enemyg9Whether seen by enemyg10Damage to enemyg11Whether have bulletsg12Horizontal direction of viewg13Whether follow with the movement direction of teammateg14Whether crouch positiong15Whether have a gung16Whether have medical kitsg17Whether to restore healthg18Health levelg19Whether knock down enemyg20Whether target the same enemy as teammateTable 14: Top 20 sub-goals ranked by frequency.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationDataset SizeTrainingMethodPrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)100%CoFT 51.85% 64.84% 46.68% 57.91% 49.13% 61.18%CoFT→SFT 55.48% 68.52% 50.48% 62.10% 52.86% 65.15%SFT 54.70% 65.20% 49.00% 60.20% 51.70% 63.20%Improve Rate 1.42% 5.09% 3.02% 3.16% 2.25% 3.09%30%CoFT 49.43% 62.66% 44.45% 56.29% 46.81% 59.30%CoFT→SFT 49.92% 62.59% 45.51% 57.65% 47.61% 60.02%SFT 46.12% 60.96% 33.68% 45.39% 38.93% 52.03%Improve Rate 8.25% 2.68% 35.11% 27.01% 22.30% 15.34%10%CoFT 45.58% 60.84% 41.77% 53.92% 43.59% 57.17%CoFT→SFT 48.06% 61.01% 43.15% 54.31% 45.47% 57.47%SFT 42.08% 55.31% 30.86% 41.45% 35.61% 47.39%Improve Rate 14.20% 10.32% 39.80% 31.03% 27.69% 21.28%3%CoFT 39.42% 58.35% 34.28% 50.10% 36.67% 53.91%CoFT→SFT 41.61% 60.40% 34.55% 50.32% 37.75% 54.90%SFT 17.66% 38.28% 13.33% 29.47% 15.20% 33.30%Improve Rate 135.52% 57.78% 159.15% 70.79% 148.43% 64.88%Table 15: Language model performance evaluation with different sizes of fine-tuning training set.。The underlined “ImproveRate” values represent the improvement percentage of the “CoFT →SFT” method relative to “SFT” method.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationIn order to complete the command ‘You should lie in wait’, let us plan the states of the agent step by step usingthe following template:1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is [Command], which means teammate player wants the agent...2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.. \n",
      "the piece 442\n",
      " \n",
      "ent: 0%Offense: Must be ready to attack if necessary: 20%Waiting: The main goal is to set up an ambush and wait: 70%Supplies: No need for supplies during this phase: 0%Scouting: Need to observe the surroundings, but not actively scout: 10%6.。Analyze how long the current command needs to be kept.According to the command of the teammate, taking a hiding position or setting up an ambush is usually donewith the intention of waiting for an opportune moment to strike or observe the enemy‘S movement: The currentcommand needs to be kept by ‘long term’.Table 9: Chain of thought response from GPT4.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.Only select the most relevant and necessary states for planning, and the unplanned states will be adjusted bythe agent itself2.。[Choose 1, Choose 2, ...] indicates the values that can be selected for the state.。When you plan, you can onlychoose the value of the state from it, and do not invent new value not listed in [Choice1, Choice2, ...].3.The selected state can change the current value or maintain the current value.。The agent will try to achieveand maintain the value of the state you choose after you give the plan.4.Agents don’t voluntarily discard items (for example guns, bullets, medical kits) unless items are reduced orset as False in your plan, so there is no need to keep them, only to choose when making changes.5.Do not plan and adjust the states of teammates and enemies, they can move freely and cannot be controlled.6.Avoid conflicts of states planing.。For example, agent unable to move quickly when lying down, and unable tosee enemies when length of distance from agent to enemy is far away.7.Avoid the repetition of states planing.。For example, if the Average velocity has been adjusted to be Fast, thereis no need to adjust the Whether prone position to False, because the agent can automatically adjust state to fitoverlapping meanings.8.When it is necessary to refer to enemy or teammate information for planing, describe the specific state valueduring analysis.Table 10: Rule prompt for GPT4.Dataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)Attention 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.159Mlp 0.549 0.664 0.482 0.587 0.514 0.620 0.065 0.134All 0.529 0.642 0.471 0.581 0.485 0.596 0.069 0.119Table 13: Evaluation on LoRA target.Symbol Sub-goal Classg1Average velocityg2Horizontal direction of movementg3Whether seen enemyg4Whether hold a gung5Whether prone positiong6Length of distance movedg7Length of distance from agent to teammateg8Distance with nearest enemyg9Whether seen by enemyg10Damage to enemyg11Whether have bulletsg12Horizontal direction of viewg13Whether follow with the movement direction of teammateg14Whether crouch positiong15Whether have a gung16Whether have medical kitsg17Whether to restore healthg18Health levelg19Whether knock down enemyg20Whether target the same enemy as teammateTable 14: Top 20 sub-goals ranked by frequency.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationDataset SizeTrainingMethodPrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)100%CoFT 51.85% 64.84% 46.68% 57.91% 49.13% 61.18%CoFT→SFT 55.48% 68.52% 50.48% 62.10% 52.86% 65.15%SFT 54.70% 65.20% 49.00% 60.20% 51.70% 63.20%Improve Rate 1.42% 5.09% 3.02% 3.16% 2.25% 3.09%30%CoFT 49.43% 62.66% 44.45% 56.29% 46.81% 59.30%CoFT→SFT 49.92% 62.59% 45.51% 57.65% 47.61% 60.02%SFT 46.12% 60.96% 33.68% 45.39% 38.93% 52.03%Improve Rate 8.25% 2.68% 35.11% 27.01% 22.30% 15.34%10%CoFT 45.58% 60.84% 41.77% 53.92% 43.59% 57.17%CoFT→SFT 48.06% 61.01% 43.15% 54.31% 45.47% 57.47%SFT 42.08% 55.31% 30.86% 41.45% 35.61% 47.39%Improve Rate 14.20% 10.32% 39.80% 31.03% 27.69% 21.28%3%CoFT 39.42% 58.35% 34.28% 50.10% 36.67% 53.91%CoFT→SFT 41.61% 60.40% 34.55% 50.32% 37.75% 54.90%SFT 17.66% 38.28% 13.33% 29.47% 15.20% 33.30%Improve Rate 135.52% 57.78% 159.15% 70.79% 148.43% 64.88%Table 15: Language model performance evaluation with different sizes of fine-tuning training set.。The underlined “ImproveRate” values represent the improvement percentage of the “CoFT →SFT” method relative to “SFT” method.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationIn order to complete the command ‘You should lie in wait’, let us plan the states of the agent step by step usingthe following template:1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is [Command], which means teammate player wants the agent...2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan (analyze the reason first, then select keystates one by one as few as possible and as important as possible according to the degree of importance)?According to the teammate’s command:2.1.. \n",
      "the piece 443\n",
      " \n",
      "ecessary: 20%Waiting: The main goal is to set up an ambush and wait: 70%Supplies: No need for supplies during this phase: 0%Scouting: Need to observe the surroundings, but not actively scout: 10%6.。Analyze how long the current command needs to be kept.According to the command of the teammate, taking a hiding position or setting up an ambush is usually donewith the intention of waiting for an opportune moment to strike or observe the enemy‘S movement: The currentcommand needs to be kept by ‘long term’.Table 9: Chain of thought response from GPT4.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.Only select the most relevant and necessary states for planning, and the unplanned states will be adjusted bythe agent itself2.。[Choose 1, Choose 2, ...] indicates the values that can be selected for the state.。When you plan, you can onlychoose the value of the state from it, and do not invent new value not listed in [Choice1, Choice2, ...].3.The selected state can change the current value or maintain the current value.。The agent will try to achieveand maintain the value of the state you choose after you give the plan.4.Agents don’t voluntarily discard items (for example guns, bullets, medical kits) unless items are reduced orset as False in your plan, so there is no need to keep them, only to choose when making changes.5.Do not plan and adjust the states of teammates and enemies, they can move freely and cannot be controlled.6.Avoid conflicts of states planing.。For example, agent unable to move quickly when lying down, and unable tosee enemies when length of distance from agent to enemy is far away.7.Avoid the repetition of states planing.。For example, if the Average velocity has been adjusted to be Fast, thereis no need to adjust the Whether prone position to False, because the agent can automatically adjust state to fitoverlapping meanings.8.When it is necessary to refer to enemy or teammate information for planing, describe the specific state valueduring analysis.Table 10: Rule prompt for GPT4.Dataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)Attention 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.159Mlp 0.549 0.664 0.482 0.587 0.514 0.620 0.065 0.134All 0.529 0.642 0.471 0.581 0.485 0.596 0.069 0.119Table 13: Evaluation on LoRA target.Symbol Sub-goal Classg1Average velocityg2Horizontal direction of movementg3Whether seen enemyg4Whether hold a gung5Whether prone positiong6Length of distance movedg7Length of distance from agent to teammateg8Distance with nearest enemyg9Whether seen by enemyg10Damage to enemyg11Whether have bulletsg12Horizontal direction of viewg13Whether follow with the movement direction of teammateg14Whether crouch positiong15Whether have a gung16Whether have medical kitsg17Whether to restore healthg18Health levelg19Whether knock down enemyg20Whether target the same enemy as teammateTable 14: Top 20 sub-goals ranked by frequency.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationDataset SizeTrainingMethodPrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)100%CoFT 51.85% 64.84% 46.68% 57.91% 49.13% 61.18%CoFT→SFT 55.48% 68.52% 50.48% 62.10% 52.86% 65.15%SFT 54.70% 65.20% 49.00% 60.20% 51.70% 63.20%Improve Rate 1.42% 5.09% 3.02% 3.16% 2.25% 3.09%30%CoFT 49.43% 62.66% 44.45% 56.29% 46.81% 59.30%CoFT→SFT 49.92% 62.59% 45.51% 57.65% 47.61% 60.02%SFT 46.12% 60.96% 33.68% 45.39% 38.93% 52.03%Improve Rate 8.25% 2.68% 35.11% 27.01% 22.30% 15.34%10%CoFT 45.58% 60.84% 41.77% 53.92% 43.59% 57.17%CoFT→SFT 48.06% 61.01% 43.15% 54.31% 45.47% 57.47%SFT 42.08% 55.31% 30.86% 41.45% 35.61% 47.39%Improve Rate 14.20% 10.32% 39.80% 31.03% 27.69% 21.28%3%CoFT 39.42% 58.35% 34.28% 50.10% 36.67% 53.91%CoFT→SFT 41.61% 60.40% 34.55% 50.32% 37.75% 54.90%SFT 17.66% 38.28% 13.33% 29.47% 15.20% 33.30%Improve Rate 135.52% 57.78% 159.15% 70.79% 148.43% 64.88%Table 15: Language model performance evaluation with different sizes of fine-tuning training set.。The underlined “ImproveRate” values represent the improvement percentage of the “CoFT →SFT” method relative to “SFT” method.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationIn order to complete the command ‘You should lie in wait’, let us plan the states of the agent step by step usingthe following template:1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is [Command], which means teammate player wants the agent...2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan (analyze the reason first, then select keystates one by one as few as possible and as important as possible according to the degree of importance)?According to the teammate’s command:2.1.。[Reason1]: [State1]2.2.. \n",
      "the piece 444\n",
      " \n",
      " and wait: 70%Supplies: No need for supplies during this phase: 0%Scouting: Need to observe the surroundings, but not actively scout: 10%6.。Analyze how long the current command needs to be kept.According to the command of the teammate, taking a hiding position or setting up an ambush is usually donewith the intention of waiting for an opportune moment to strike or observe the enemy‘S movement: The currentcommand needs to be kept by ‘long term’.Table 9: Chain of thought response from GPT4.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.Only select the most relevant and necessary states for planning, and the unplanned states will be adjusted bythe agent itself2.。[Choose 1, Choose 2, ...] indicates the values that can be selected for the state.。When you plan, you can onlychoose the value of the state from it, and do not invent new value not listed in [Choice1, Choice2, ...].3.The selected state can change the current value or maintain the current value.。The agent will try to achieveand maintain the value of the state you choose after you give the plan.4.Agents don’t voluntarily discard items (for example guns, bullets, medical kits) unless items are reduced orset as False in your plan, so there is no need to keep them, only to choose when making changes.5.Do not plan and adjust the states of teammates and enemies, they can move freely and cannot be controlled.6.Avoid conflicts of states planing.。For example, agent unable to move quickly when lying down, and unable tosee enemies when length of distance from agent to enemy is far away.7.Avoid the repetition of states planing.。For example, if the Average velocity has been adjusted to be Fast, thereis no need to adjust the Whether prone position to False, because the agent can automatically adjust state to fitoverlapping meanings.8.When it is necessary to refer to enemy or teammate information for planing, describe the specific state valueduring analysis.Table 10: Rule prompt for GPT4.Dataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)Attention 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.159Mlp 0.549 0.664 0.482 0.587 0.514 0.620 0.065 0.134All 0.529 0.642 0.471 0.581 0.485 0.596 0.069 0.119Table 13: Evaluation on LoRA target.Symbol Sub-goal Classg1Average velocityg2Horizontal direction of movementg3Whether seen enemyg4Whether hold a gung5Whether prone positiong6Length of distance movedg7Length of distance from agent to teammateg8Distance with nearest enemyg9Whether seen by enemyg10Damage to enemyg11Whether have bulletsg12Horizontal direction of viewg13Whether follow with the movement direction of teammateg14Whether crouch positiong15Whether have a gung16Whether have medical kitsg17Whether to restore healthg18Health levelg19Whether knock down enemyg20Whether target the same enemy as teammateTable 14: Top 20 sub-goals ranked by frequency.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationDataset SizeTrainingMethodPrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)100%CoFT 51.85% 64.84% 46.68% 57.91% 49.13% 61.18%CoFT→SFT 55.48% 68.52% 50.48% 62.10% 52.86% 65.15%SFT 54.70% 65.20% 49.00% 60.20% 51.70% 63.20%Improve Rate 1.42% 5.09% 3.02% 3.16% 2.25% 3.09%30%CoFT 49.43% 62.66% 44.45% 56.29% 46.81% 59.30%CoFT→SFT 49.92% 62.59% 45.51% 57.65% 47.61% 60.02%SFT 46.12% 60.96% 33.68% 45.39% 38.93% 52.03%Improve Rate 8.25% 2.68% 35.11% 27.01% 22.30% 15.34%10%CoFT 45.58% 60.84% 41.77% 53.92% 43.59% 57.17%CoFT→SFT 48.06% 61.01% 43.15% 54.31% 45.47% 57.47%SFT 42.08% 55.31% 30.86% 41.45% 35.61% 47.39%Improve Rate 14.20% 10.32% 39.80% 31.03% 27.69% 21.28%3%CoFT 39.42% 58.35% 34.28% 50.10% 36.67% 53.91%CoFT→SFT 41.61% 60.40% 34.55% 50.32% 37.75% 54.90%SFT 17.66% 38.28% 13.33% 29.47% 15.20% 33.30%Improve Rate 135.52% 57.78% 159.15% 70.79% 148.43% 64.88%Table 15: Language model performance evaluation with different sizes of fine-tuning training set.。The underlined “ImproveRate” values represent the improvement percentage of the “CoFT →SFT” method relative to “SFT” method.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationIn order to complete the command ‘You should lie in wait’, let us plan the states of the agent step by step usingthe following template:1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is [Command], which means teammate player wants the agent...2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan (analyze the reason first, then select keystates one by one as few as possible and as important as possible according to the degree of importance)?According to the teammate’s command:2.1.。[Reason1]: [State1]2.2.。[Reason2]: [State2]...3.. \n",
      "the piece 445\n",
      " \n",
      "d needs to be kept.According to the command of the teammate, taking a hiding position or setting up an ambush is usually donewith the intention of waiting for an opportune moment to strike or observe the enemy‘S movement: The currentcommand needs to be kept by ‘long term’.Table 9: Chain of thought response from GPT4.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.Only select the most relevant and necessary states for planning, and the unplanned states will be adjusted bythe agent itself2.。[Choose 1, Choose 2, ...] indicates the values that can be selected for the state.。When you plan, you can onlychoose the value of the state from it, and do not invent new value not listed in [Choice1, Choice2, ...].3.The selected state can change the current value or maintain the current value.。The agent will try to achieveand maintain the value of the state you choose after you give the plan.4.Agents don’t voluntarily discard items (for example guns, bullets, medical kits) unless items are reduced orset as False in your plan, so there is no need to keep them, only to choose when making changes.5.Do not plan and adjust the states of teammates and enemies, they can move freely and cannot be controlled.6.Avoid conflicts of states planing.。For example, agent unable to move quickly when lying down, and unable tosee enemies when length of distance from agent to enemy is far away.7.Avoid the repetition of states planing.。For example, if the Average velocity has been adjusted to be Fast, thereis no need to adjust the Whether prone position to False, because the agent can automatically adjust state to fitoverlapping meanings.8.When it is necessary to refer to enemy or teammate information for planing, describe the specific state valueduring analysis.Table 10: Rule prompt for GPT4.Dataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)Attention 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.159Mlp 0.549 0.664 0.482 0.587 0.514 0.620 0.065 0.134All 0.529 0.642 0.471 0.581 0.485 0.596 0.069 0.119Table 13: Evaluation on LoRA target.Symbol Sub-goal Classg1Average velocityg2Horizontal direction of movementg3Whether seen enemyg4Whether hold a gung5Whether prone positiong6Length of distance movedg7Length of distance from agent to teammateg8Distance with nearest enemyg9Whether seen by enemyg10Damage to enemyg11Whether have bulletsg12Horizontal direction of viewg13Whether follow with the movement direction of teammateg14Whether crouch positiong15Whether have a gung16Whether have medical kitsg17Whether to restore healthg18Health levelg19Whether knock down enemyg20Whether target the same enemy as teammateTable 14: Top 20 sub-goals ranked by frequency.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationDataset SizeTrainingMethodPrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)100%CoFT 51.85% 64.84% 46.68% 57.91% 49.13% 61.18%CoFT→SFT 55.48% 68.52% 50.48% 62.10% 52.86% 65.15%SFT 54.70% 65.20% 49.00% 60.20% 51.70% 63.20%Improve Rate 1.42% 5.09% 3.02% 3.16% 2.25% 3.09%30%CoFT 49.43% 62.66% 44.45% 56.29% 46.81% 59.30%CoFT→SFT 49.92% 62.59% 45.51% 57.65% 47.61% 60.02%SFT 46.12% 60.96% 33.68% 45.39% 38.93% 52.03%Improve Rate 8.25% 2.68% 35.11% 27.01% 22.30% 15.34%10%CoFT 45.58% 60.84% 41.77% 53.92% 43.59% 57.17%CoFT→SFT 48.06% 61.01% 43.15% 54.31% 45.47% 57.47%SFT 42.08% 55.31% 30.86% 41.45% 35.61% 47.39%Improve Rate 14.20% 10.32% 39.80% 31.03% 27.69% 21.28%3%CoFT 39.42% 58.35% 34.28% 50.10% 36.67% 53.91%CoFT→SFT 41.61% 60.40% 34.55% 50.32% 37.75% 54.90%SFT 17.66% 38.28% 13.33% 29.47% 15.20% 33.30%Improve Rate 135.52% 57.78% 159.15% 70.79% 148.43% 64.88%Table 15: Language model performance evaluation with different sizes of fine-tuning training set.。The underlined “ImproveRate” values represent the improvement percentage of the “CoFT →SFT” method relative to “SFT” method.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationIn order to complete the command ‘You should lie in wait’, let us plan the states of the agent step by step usingthe following template:1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is [Command], which means teammate player wants the agent...2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan (analyze the reason first, then select keystates one by one as few as possible and as important as possible according to the degree of importance)?According to the teammate’s command:2.1.。[Reason1]: [State1]2.2.。[Reason2]: [State2]...3.。Plan how these key states need to be adjusted (analyze the reason first, and then make adjustments one stateby one state, the state can be changed or remain the same, and must be selected from the value range of thegame state [Choice 1, Choice 2, ...])?According to the teammate’s command:3.1.. \n",
      "the piece 446\n",
      " \n",
      "t the most relevant and necessary states for planning, and the unplanned states will be adjusted bythe agent itself2.。[Choose 1, Choose 2, ...] indicates the values that can be selected for the state.。When you plan, you can onlychoose the value of the state from it, and do not invent new value not listed in [Choice1, Choice2, ...].3.The selected state can change the current value or maintain the current value.。The agent will try to achieveand maintain the value of the state you choose after you give the plan.4.Agents don’t voluntarily discard items (for example guns, bullets, medical kits) unless items are reduced orset as False in your plan, so there is no need to keep them, only to choose when making changes.5.Do not plan and adjust the states of teammates and enemies, they can move freely and cannot be controlled.6.Avoid conflicts of states planing.。For example, agent unable to move quickly when lying down, and unable tosee enemies when length of distance from agent to enemy is far away.7.Avoid the repetition of states planing.。For example, if the Average velocity has been adjusted to be Fast, thereis no need to adjust the Whether prone position to False, because the agent can automatically adjust state to fitoverlapping meanings.8.When it is necessary to refer to enemy or teammate information for planing, describe the specific state valueduring analysis.Table 10: Rule prompt for GPT4.Dataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)Attention 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.159Mlp 0.549 0.664 0.482 0.587 0.514 0.620 0.065 0.134All 0.529 0.642 0.471 0.581 0.485 0.596 0.069 0.119Table 13: Evaluation on LoRA target.Symbol Sub-goal Classg1Average velocityg2Horizontal direction of movementg3Whether seen enemyg4Whether hold a gung5Whether prone positiong6Length of distance movedg7Length of distance from agent to teammateg8Distance with nearest enemyg9Whether seen by enemyg10Damage to enemyg11Whether have bulletsg12Horizontal direction of viewg13Whether follow with the movement direction of teammateg14Whether crouch positiong15Whether have a gung16Whether have medical kitsg17Whether to restore healthg18Health levelg19Whether knock down enemyg20Whether target the same enemy as teammateTable 14: Top 20 sub-goals ranked by frequency.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationDataset SizeTrainingMethodPrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)100%CoFT 51.85% 64.84% 46.68% 57.91% 49.13% 61.18%CoFT→SFT 55.48% 68.52% 50.48% 62.10% 52.86% 65.15%SFT 54.70% 65.20% 49.00% 60.20% 51.70% 63.20%Improve Rate 1.42% 5.09% 3.02% 3.16% 2.25% 3.09%30%CoFT 49.43% 62.66% 44.45% 56.29% 46.81% 59.30%CoFT→SFT 49.92% 62.59% 45.51% 57.65% 47.61% 60.02%SFT 46.12% 60.96% 33.68% 45.39% 38.93% 52.03%Improve Rate 8.25% 2.68% 35.11% 27.01% 22.30% 15.34%10%CoFT 45.58% 60.84% 41.77% 53.92% 43.59% 57.17%CoFT→SFT 48.06% 61.01% 43.15% 54.31% 45.47% 57.47%SFT 42.08% 55.31% 30.86% 41.45% 35.61% 47.39%Improve Rate 14.20% 10.32% 39.80% 31.03% 27.69% 21.28%3%CoFT 39.42% 58.35% 34.28% 50.10% 36.67% 53.91%CoFT→SFT 41.61% 60.40% 34.55% 50.32% 37.75% 54.90%SFT 17.66% 38.28% 13.33% 29.47% 15.20% 33.30%Improve Rate 135.52% 57.78% 159.15% 70.79% 148.43% 64.88%Table 15: Language model performance evaluation with different sizes of fine-tuning training set.。The underlined “ImproveRate” values represent the improvement percentage of the “CoFT →SFT” method relative to “SFT” method.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationIn order to complete the command ‘You should lie in wait’, let us plan the states of the agent step by step usingthe following template:1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is [Command], which means teammate player wants the agent...2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan (analyze the reason first, then select keystates one by one as few as possible and as important as possible according to the degree of importance)?According to the teammate’s command:2.1.。[Reason1]: [State1]2.2.。[Reason2]: [State2]...3.。Plan how these key states need to be adjusted (analyze the reason first, and then make adjustments one stateby one state, the state can be changed or remain the same, and must be selected from the value range of thegame state [Choice 1, Choice 2, ...])?According to the teammate’s command:3.1.。[State1]: [Reason1]: [Current_value1] ->[Target_value2]3.2.. \n",
      "the piece 447\n",
      " \n",
      "mple guns, bullets, medical kits) unless items are reduced orset as False in your plan, so there is no need to keep them, only to choose when making changes.5.Do not plan and adjust the states of teammates and enemies, they can move freely and cannot be controlled.6.Avoid conflicts of states planing.。For example, agent unable to move quickly when lying down, and unable tosee enemies when length of distance from agent to enemy is far away.7.Avoid the repetition of states planing.。For example, if the Average velocity has been adjusted to be Fast, thereis no need to adjust the Whether prone position to False, because the agent can automatically adjust state to fitoverlapping meanings.8.When it is necessary to refer to enemy or teammate information for planing, describe the specific state valueduring analysis.Table 10: Rule prompt for GPT4.Dataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)Attention 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.159Mlp 0.549 0.664 0.482 0.587 0.514 0.620 0.065 0.134All 0.529 0.642 0.471 0.581 0.485 0.596 0.069 0.119Table 13: Evaluation on LoRA target.Symbol Sub-goal Classg1Average velocityg2Horizontal direction of movementg3Whether seen enemyg4Whether hold a gung5Whether prone positiong6Length of distance movedg7Length of distance from agent to teammateg8Distance with nearest enemyg9Whether seen by enemyg10Damage to enemyg11Whether have bulletsg12Horizontal direction of viewg13Whether follow with the movement direction of teammateg14Whether crouch positiong15Whether have a gung16Whether have medical kitsg17Whether to restore healthg18Health levelg19Whether knock down enemyg20Whether target the same enemy as teammateTable 14: Top 20 sub-goals ranked by frequency.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationDataset SizeTrainingMethodPrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)100%CoFT 51.85% 64.84% 46.68% 57.91% 49.13% 61.18%CoFT→SFT 55.48% 68.52% 50.48% 62.10% 52.86% 65.15%SFT 54.70% 65.20% 49.00% 60.20% 51.70% 63.20%Improve Rate 1.42% 5.09% 3.02% 3.16% 2.25% 3.09%30%CoFT 49.43% 62.66% 44.45% 56.29% 46.81% 59.30%CoFT→SFT 49.92% 62.59% 45.51% 57.65% 47.61% 60.02%SFT 46.12% 60.96% 33.68% 45.39% 38.93% 52.03%Improve Rate 8.25% 2.68% 35.11% 27.01% 22.30% 15.34%10%CoFT 45.58% 60.84% 41.77% 53.92% 43.59% 57.17%CoFT→SFT 48.06% 61.01% 43.15% 54.31% 45.47% 57.47%SFT 42.08% 55.31% 30.86% 41.45% 35.61% 47.39%Improve Rate 14.20% 10.32% 39.80% 31.03% 27.69% 21.28%3%CoFT 39.42% 58.35% 34.28% 50.10% 36.67% 53.91%CoFT→SFT 41.61% 60.40% 34.55% 50.32% 37.75% 54.90%SFT 17.66% 38.28% 13.33% 29.47% 15.20% 33.30%Improve Rate 135.52% 57.78% 159.15% 70.79% 148.43% 64.88%Table 15: Language model performance evaluation with different sizes of fine-tuning training set.。The underlined “ImproveRate” values represent the improvement percentage of the “CoFT →SFT” method relative to “SFT” method.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationIn order to complete the command ‘You should lie in wait’, let us plan the states of the agent step by step usingthe following template:1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is [Command], which means teammate player wants the agent...2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan (analyze the reason first, then select keystates one by one as few as possible and as important as possible according to the degree of importance)?According to the teammate’s command:2.1.。[Reason1]: [State1]2.2.。[Reason2]: [State2]...3.。Plan how these key states need to be adjusted (analyze the reason first, and then make adjustments one stateby one state, the state can be changed or remain the same, and must be selected from the value range of thegame state [Choice 1, Choice 2, ...])?According to the teammate’s command:3.1.。[State1]: [Reason1]: [Current_value1] ->[Target_value2]3.2.。[State2]: [Reason2]: [Current_value1] ->[Target_value2]...4.. \n",
      "the piece 448\n",
      " \n",
      "an, so there is no need to keep them, only to choose when making changes.5.Do not plan and adjust the states of teammates and enemies, they can move freely and cannot be controlled.6.Avoid conflicts of states planing.。For example, agent unable to move quickly when lying down, and unable tosee enemies when length of distance from agent to enemy is far away.7.Avoid the repetition of states planing.。For example, if the Average velocity has been adjusted to be Fast, thereis no need to adjust the Whether prone position to False, because the agent can automatically adjust state to fitoverlapping meanings.8.When it is necessary to refer to enemy or teammate information for planing, describe the specific state valueduring analysis.Table 10: Rule prompt for GPT4.Dataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)Attention 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.159Mlp 0.549 0.664 0.482 0.587 0.514 0.620 0.065 0.134All 0.529 0.642 0.471 0.581 0.485 0.596 0.069 0.119Table 13: Evaluation on LoRA target.Symbol Sub-goal Classg1Average velocityg2Horizontal direction of movementg3Whether seen enemyg4Whether hold a gung5Whether prone positiong6Length of distance movedg7Length of distance from agent to teammateg8Distance with nearest enemyg9Whether seen by enemyg10Damage to enemyg11Whether have bulletsg12Horizontal direction of viewg13Whether follow with the movement direction of teammateg14Whether crouch positiong15Whether have a gung16Whether have medical kitsg17Whether to restore healthg18Health levelg19Whether knock down enemyg20Whether target the same enemy as teammateTable 14: Top 20 sub-goals ranked by frequency.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationDataset SizeTrainingMethodPrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)100%CoFT 51.85% 64.84% 46.68% 57.91% 49.13% 61.18%CoFT→SFT 55.48% 68.52% 50.48% 62.10% 52.86% 65.15%SFT 54.70% 65.20% 49.00% 60.20% 51.70% 63.20%Improve Rate 1.42% 5.09% 3.02% 3.16% 2.25% 3.09%30%CoFT 49.43% 62.66% 44.45% 56.29% 46.81% 59.30%CoFT→SFT 49.92% 62.59% 45.51% 57.65% 47.61% 60.02%SFT 46.12% 60.96% 33.68% 45.39% 38.93% 52.03%Improve Rate 8.25% 2.68% 35.11% 27.01% 22.30% 15.34%10%CoFT 45.58% 60.84% 41.77% 53.92% 43.59% 57.17%CoFT→SFT 48.06% 61.01% 43.15% 54.31% 45.47% 57.47%SFT 42.08% 55.31% 30.86% 41.45% 35.61% 47.39%Improve Rate 14.20% 10.32% 39.80% 31.03% 27.69% 21.28%3%CoFT 39.42% 58.35% 34.28% 50.10% 36.67% 53.91%CoFT→SFT 41.61% 60.40% 34.55% 50.32% 37.75% 54.90%SFT 17.66% 38.28% 13.33% 29.47% 15.20% 33.30%Improve Rate 135.52% 57.78% 159.15% 70.79% 148.43% 64.88%Table 15: Language model performance evaluation with different sizes of fine-tuning training set.。The underlined “ImproveRate” values represent the improvement percentage of the “CoFT →SFT” method relative to “SFT” method.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationIn order to complete the command ‘You should lie in wait’, let us plan the states of the agent step by step usingthe following template:1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is [Command], which means teammate player wants the agent...2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan (analyze the reason first, then select keystates one by one as few as possible and as important as possible according to the degree of importance)?According to the teammate’s command:2.1.。[Reason1]: [State1]2.2.。[Reason2]: [State2]...3.。Plan how these key states need to be adjusted (analyze the reason first, and then make adjustments one stateby one state, the state can be changed or remain the same, and must be selected from the value range of thegame state [Choice 1, Choice 2, ...])?According to the teammate’s command:3.1.。[State1]: [Reason1]: [Current_value1] ->[Target_value2]3.2.。[State2]: [Reason2]: [Current_value1] ->[Target_value2]...4.。Modify the adjustment that may be wrong, and refer to the Rules to analyze which state adjustments mayconflict, repeat or be unnecessary, and output the modified adjustment plan: According to the states adjustmentsin 3...4.1.. \n",
      "the piece 449\n",
      " \n",
      "ng.。For example, agent unable to move quickly when lying down, and unable tosee enemies when length of distance from agent to enemy is far away.7.Avoid the repetition of states planing.。For example, if the Average velocity has been adjusted to be Fast, thereis no need to adjust the Whether prone position to False, because the agent can automatically adjust state to fitoverlapping meanings.8.When it is necessary to refer to enemy or teammate information for planing, describe the specific state valueduring analysis.Table 10: Rule prompt for GPT4.Dataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)Attention 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.159Mlp 0.549 0.664 0.482 0.587 0.514 0.620 0.065 0.134All 0.529 0.642 0.471 0.581 0.485 0.596 0.069 0.119Table 13: Evaluation on LoRA target.Symbol Sub-goal Classg1Average velocityg2Horizontal direction of movementg3Whether seen enemyg4Whether hold a gung5Whether prone positiong6Length of distance movedg7Length of distance from agent to teammateg8Distance with nearest enemyg9Whether seen by enemyg10Damage to enemyg11Whether have bulletsg12Horizontal direction of viewg13Whether follow with the movement direction of teammateg14Whether crouch positiong15Whether have a gung16Whether have medical kitsg17Whether to restore healthg18Health levelg19Whether knock down enemyg20Whether target the same enemy as teammateTable 14: Top 20 sub-goals ranked by frequency.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationDataset SizeTrainingMethodPrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)100%CoFT 51.85% 64.84% 46.68% 57.91% 49.13% 61.18%CoFT→SFT 55.48% 68.52% 50.48% 62.10% 52.86% 65.15%SFT 54.70% 65.20% 49.00% 60.20% 51.70% 63.20%Improve Rate 1.42% 5.09% 3.02% 3.16% 2.25% 3.09%30%CoFT 49.43% 62.66% 44.45% 56.29% 46.81% 59.30%CoFT→SFT 49.92% 62.59% 45.51% 57.65% 47.61% 60.02%SFT 46.12% 60.96% 33.68% 45.39% 38.93% 52.03%Improve Rate 8.25% 2.68% 35.11% 27.01% 22.30% 15.34%10%CoFT 45.58% 60.84% 41.77% 53.92% 43.59% 57.17%CoFT→SFT 48.06% 61.01% 43.15% 54.31% 45.47% 57.47%SFT 42.08% 55.31% 30.86% 41.45% 35.61% 47.39%Improve Rate 14.20% 10.32% 39.80% 31.03% 27.69% 21.28%3%CoFT 39.42% 58.35% 34.28% 50.10% 36.67% 53.91%CoFT→SFT 41.61% 60.40% 34.55% 50.32% 37.75% 54.90%SFT 17.66% 38.28% 13.33% 29.47% 15.20% 33.30%Improve Rate 135.52% 57.78% 159.15% 70.79% 148.43% 64.88%Table 15: Language model performance evaluation with different sizes of fine-tuning training set.。The underlined “ImproveRate” values represent the improvement percentage of the “CoFT →SFT” method relative to “SFT” method.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationIn order to complete the command ‘You should lie in wait’, let us plan the states of the agent step by step usingthe following template:1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is [Command], which means teammate player wants the agent...2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan (analyze the reason first, then select keystates one by one as few as possible and as important as possible according to the degree of importance)?According to the teammate’s command:2.1.。[Reason1]: [State1]2.2.。[Reason2]: [State2]...3.。Plan how these key states need to be adjusted (analyze the reason first, and then make adjustments one stateby one state, the state can be changed or remain the same, and must be selected from the value range of thegame state [Choice 1, Choice 2, ...])?According to the teammate’s command:3.1.。[State1]: [Reason1]: [Current_value1] ->[Target_value2]3.2.。[State2]: [Reason2]: [Current_value1] ->[Target_value2]...4.。Modify the adjustment that may be wrong, and refer to the Rules to analyze which state adjustments mayconflict, repeat or be unnecessary, and output the modified adjustment plan: According to the states adjustmentsin 3...4.1.。[State1]: [Current_value1] ->[Target_value2]4.2.. \n",
      "the piece 450\n",
      " \n",
      "tion for planing, describe the specific state valueduring analysis.Table 10: Rule prompt for GPT4.Dataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)Attention 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.159Mlp 0.549 0.664 0.482 0.587 0.514 0.620 0.065 0.134All 0.529 0.642 0.471 0.581 0.485 0.596 0.069 0.119Table 13: Evaluation on LoRA target.Symbol Sub-goal Classg1Average velocityg2Horizontal direction of movementg3Whether seen enemyg4Whether hold a gung5Whether prone positiong6Length of distance movedg7Length of distance from agent to teammateg8Distance with nearest enemyg9Whether seen by enemyg10Damage to enemyg11Whether have bulletsg12Horizontal direction of viewg13Whether follow with the movement direction of teammateg14Whether crouch positiong15Whether have a gung16Whether have medical kitsg17Whether to restore healthg18Health levelg19Whether knock down enemyg20Whether target the same enemy as teammateTable 14: Top 20 sub-goals ranked by frequency.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationDataset SizeTrainingMethodPrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)100%CoFT 51.85% 64.84% 46.68% 57.91% 49.13% 61.18%CoFT→SFT 55.48% 68.52% 50.48% 62.10% 52.86% 65.15%SFT 54.70% 65.20% 49.00% 60.20% 51.70% 63.20%Improve Rate 1.42% 5.09% 3.02% 3.16% 2.25% 3.09%30%CoFT 49.43% 62.66% 44.45% 56.29% 46.81% 59.30%CoFT→SFT 49.92% 62.59% 45.51% 57.65% 47.61% 60.02%SFT 46.12% 60.96% 33.68% 45.39% 38.93% 52.03%Improve Rate 8.25% 2.68% 35.11% 27.01% 22.30% 15.34%10%CoFT 45.58% 60.84% 41.77% 53.92% 43.59% 57.17%CoFT→SFT 48.06% 61.01% 43.15% 54.31% 45.47% 57.47%SFT 42.08% 55.31% 30.86% 41.45% 35.61% 47.39%Improve Rate 14.20% 10.32% 39.80% 31.03% 27.69% 21.28%3%CoFT 39.42% 58.35% 34.28% 50.10% 36.67% 53.91%CoFT→SFT 41.61% 60.40% 34.55% 50.32% 37.75% 54.90%SFT 17.66% 38.28% 13.33% 29.47% 15.20% 33.30%Improve Rate 135.52% 57.78% 159.15% 70.79% 148.43% 64.88%Table 15: Language model performance evaluation with different sizes of fine-tuning training set.。The underlined “ImproveRate” values represent the improvement percentage of the “CoFT →SFT” method relative to “SFT” method.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationIn order to complete the command ‘You should lie in wait’, let us plan the states of the agent step by step usingthe following template:1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is [Command], which means teammate player wants the agent...2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan (analyze the reason first, then select keystates one by one as few as possible and as important as possible according to the degree of importance)?According to the teammate’s command:2.1.。[Reason1]: [State1]2.2.。[Reason2]: [State2]...3.。Plan how these key states need to be adjusted (analyze the reason first, and then make adjustments one stateby one state, the state can be changed or remain the same, and must be selected from the value range of thegame state [Choice 1, Choice 2, ...])?According to the teammate’s command:3.1.。[State1]: [Reason1]: [Current_value1] ->[Target_value2]3.2.。[State2]: [Reason2]: [Current_value1] ->[Target_value2]...4.。Modify the adjustment that may be wrong, and refer to the Rules to analyze which state adjustments mayconflict, repeat or be unnecessary, and output the modified adjustment plan: According to the states adjustmentsin 3...4.1.。[State1]: [Current_value1] ->[Target_value2]4.2.。[State2]: [Current_value1] ->[Target_value2]...5.. \n",
      "the piece 451\n",
      " \n",
      "oice)Attention 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.159Mlp 0.549 0.664 0.482 0.587 0.514 0.620 0.065 0.134All 0.529 0.642 0.471 0.581 0.485 0.596 0.069 0.119Table 13: Evaluation on LoRA target.Symbol Sub-goal Classg1Average velocityg2Horizontal direction of movementg3Whether seen enemyg4Whether hold a gung5Whether prone positiong6Length of distance movedg7Length of distance from agent to teammateg8Distance with nearest enemyg9Whether seen by enemyg10Damage to enemyg11Whether have bulletsg12Horizontal direction of viewg13Whether follow with the movement direction of teammateg14Whether crouch positiong15Whether have a gung16Whether have medical kitsg17Whether to restore healthg18Health levelg19Whether knock down enemyg20Whether target the same enemy as teammateTable 14: Top 20 sub-goals ranked by frequency.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationDataset SizeTrainingMethodPrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)100%CoFT 51.85% 64.84% 46.68% 57.91% 49.13% 61.18%CoFT→SFT 55.48% 68.52% 50.48% 62.10% 52.86% 65.15%SFT 54.70% 65.20% 49.00% 60.20% 51.70% 63.20%Improve Rate 1.42% 5.09% 3.02% 3.16% 2.25% 3.09%30%CoFT 49.43% 62.66% 44.45% 56.29% 46.81% 59.30%CoFT→SFT 49.92% 62.59% 45.51% 57.65% 47.61% 60.02%SFT 46.12% 60.96% 33.68% 45.39% 38.93% 52.03%Improve Rate 8.25% 2.68% 35.11% 27.01% 22.30% 15.34%10%CoFT 45.58% 60.84% 41.77% 53.92% 43.59% 57.17%CoFT→SFT 48.06% 61.01% 43.15% 54.31% 45.47% 57.47%SFT 42.08% 55.31% 30.86% 41.45% 35.61% 47.39%Improve Rate 14.20% 10.32% 39.80% 31.03% 27.69% 21.28%3%CoFT 39.42% 58.35% 34.28% 50.10% 36.67% 53.91%CoFT→SFT 41.61% 60.40% 34.55% 50.32% 37.75% 54.90%SFT 17.66% 38.28% 13.33% 29.47% 15.20% 33.30%Improve Rate 135.52% 57.78% 159.15% 70.79% 148.43% 64.88%Table 15: Language model performance evaluation with different sizes of fine-tuning training set.。The underlined “ImproveRate” values represent the improvement percentage of the “CoFT →SFT” method relative to “SFT” method.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationIn order to complete the command ‘You should lie in wait’, let us plan the states of the agent step by step usingthe following template:1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is [Command], which means teammate player wants the agent...2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan (analyze the reason first, then select keystates one by one as few as possible and as important as possible according to the degree of importance)?According to the teammate’s command:2.1.。[Reason1]: [State1]2.2.。[Reason2]: [State2]...3.。Plan how these key states need to be adjusted (analyze the reason first, and then make adjustments one stateby one state, the state can be changed or remain the same, and must be selected from the value range of thegame state [Choice 1, Choice 2, ...])?According to the teammate’s command:3.1.。[State1]: [Reason1]: [Current_value1] ->[Target_value2]3.2.。[State2]: [Reason2]: [Current_value1] ->[Target_value2]...4.。Modify the adjustment that may be wrong, and refer to the Rules to analyze which state adjustments mayconflict, repeat or be unnecessary, and output the modified adjustment plan: According to the states adjustmentsin 3...4.1.。[State1]: [Current_value1] ->[Target_value2]4.2.。[State2]: [Current_value1] ->[Target_value2]...5.。According to the analyze and the planing of the verbal command, further analyze the behavior tendencyrequired in the adjustment process (the proportion of Mobile, Offense, Waiting, Supplies, Scouting, first analyzethe reason, and then calculate the percentage)Mobile: [Reason1]: [Percent1]Offense: [Reason2]: [Percent2]Waiting: [Reason3]: [Percent3]Supplies: [Reason4]: [Percent4]Scouting: [Reason5]: [Percent5]6.. \n",
      "the piece 452\n",
      " \n",
      "alysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is [Command], which means teammate player wants the agent...2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan (analyze the reason first, then select keystates one by one as few as possible and as important as possible according to the degree of importance)?According to the teammate’s command:2.1.。[Reason1]: [State1]2.2.。[Reason2]: [State2]...3.。Plan how these key states need to be adjusted (analyze the reason first, and then make adjustments one stateby one state, the state can be changed or remain the same, and must be selected from the value range of thegame state [Choice 1, Choice 2, ...])?According to the teammate’s command:3.1.。[State1]: [Reason1]: [Current_value1] ->[Target_value2]3.2.。[State2]: [Reason2]: [Current_value1] ->[Target_value2]...4.。Modify the adjustment that may be wrong, and refer to the Rules to analyze which state adjustments mayconflict, repeat or be unnecessary, and output the modified adjustment plan: According to the states adjustmentsin 3...4.1.。[State1]: [Current_value1] ->[Target_value2]4.2.。[State2]: [Current_value1] ->[Target_value2]...5.。According to the analyze and the planing of the verbal command, further analyze the behavior tendencyrequired in the adjustment process (the proportion of Mobile, Offense, Waiting, Supplies, Scouting, first analyzethe reason, and then calculate the percentage)Mobile: [Reason1]: [Percent1]Offense: [Reason2]: [Percent2]Waiting: [Reason3]: [Percent3]Supplies: [Reason4]: [Percent4]Scouting: [Reason5]: [Percent5]6.。Analyze how long the current command needs to be kept (for example, the command of ‘killing the enemy’needs to be kept for a ‘short term’, and the command of ‘pay attention to reconnaissance’ needs to be kept for a‘long term’.. \n",
      "the piece 453\n",
      " \n",
      "your plan (analyze the reason first, then select keystates one by one as few as possible and as important as possible according to the degree of importance)?According to the teammate’s command:2.1.。[Reason1]: [State1]2.2.。[Reason2]: [State2]...3.。Plan how these key states need to be adjusted (analyze the reason first, and then make adjustments one stateby one state, the state can be changed or remain the same, and must be selected from the value range of thegame state [Choice 1, Choice 2, ...])?According to the teammate’s command:3.1.。[State1]: [Reason1]: [Current_value1] ->[Target_value2]3.2.。[State2]: [Reason2]: [Current_value1] ->[Target_value2]...4.。Modify the adjustment that may be wrong, and refer to the Rules to analyze which state adjustments mayconflict, repeat or be unnecessary, and output the modified adjustment plan: According to the states adjustmentsin 3...4.1.。[State1]: [Current_value1] ->[Target_value2]4.2.。[State2]: [Current_value1] ->[Target_value2]...5.。According to the analyze and the planing of the verbal command, further analyze the behavior tendencyrequired in the adjustment process (the proportion of Mobile, Offense, Waiting, Supplies, Scouting, first analyzethe reason, and then calculate the percentage)Mobile: [Reason1]: [Percent1]Offense: [Reason2]: [Percent2]Waiting: [Reason3]: [Percent3]Supplies: [Reason4]: [Percent4]Scouting: [Reason5]: [Percent5]6.。Analyze how long the current command needs to be kept (for example, the command of ‘killing the enemy’needs to be kept for a ‘short term’, and the command of ‘pay attention to reconnaissance’ needs to be kept for a‘long term’.。First analyze the reason and then make a judgment).According to the command of the teammate, [Analysis]: The current command needs to be kept by ‘[XX term]’.If you see phrases like [Context] in answer template, replace the entire phrase according to the meaning of theContext, do not repeat the content; make analogy expansion for ‘...’; keep ‘:’; absolutely do not modify others intemplate.Table 16: Chain of thought prompt for GPT4.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationpromptsystembackgroundpromptWe have an agent and a player working together as a teammate in aPUBG game.. \n",
      "the piece 454\n",
      " \n",
      "as possible and as important as possible according to the degree of importance)?According to the teammate’s command:2.1.。[Reason1]: [State1]2.2.。[Reason2]: [State2]...3.。Plan how these key states need to be adjusted (analyze the reason first, and then make adjustments one stateby one state, the state can be changed or remain the same, and must be selected from the value range of thegame state [Choice 1, Choice 2, ...])?According to the teammate’s command:3.1.。[State1]: [Reason1]: [Current_value1] ->[Target_value2]3.2.。[State2]: [Reason2]: [Current_value1] ->[Target_value2]...4.。Modify the adjustment that may be wrong, and refer to the Rules to analyze which state adjustments mayconflict, repeat or be unnecessary, and output the modified adjustment plan: According to the states adjustmentsin 3...4.1.。[State1]: [Current_value1] ->[Target_value2]4.2.。[State2]: [Current_value1] ->[Target_value2]...5.。According to the analyze and the planing of the verbal command, further analyze the behavior tendencyrequired in the adjustment process (the proportion of Mobile, Offense, Waiting, Supplies, Scouting, first analyzethe reason, and then calculate the percentage)Mobile: [Reason1]: [Percent1]Offense: [Reason2]: [Percent2]Waiting: [Reason3]: [Percent3]Supplies: [Reason4]: [Percent4]Scouting: [Reason5]: [Percent5]6.。Analyze how long the current command needs to be kept (for example, the command of ‘killing the enemy’needs to be kept for a ‘short term’, and the command of ‘pay attention to reconnaissance’ needs to be kept for a‘long term’.。First analyze the reason and then make a judgment).According to the command of the teammate, [Analysis]: The current command needs to be kept by ‘[XX term]’.If you see phrases like [Context] in answer template, replace the entire phrase according to the meaning of theContext, do not repeat the content; make analogy expansion for ‘...’; keep ‘:’; absolutely do not modify others intemplate.Table 16: Chain of thought prompt for GPT4.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationpromptsystembackgroundpromptWe have an agent and a player working together as a teammate in aPUBG game.。We hope you can help the agent plan how the agent’sgame state should change, so as to complete the player’s command andhelp the player win the game.teammatestatepromptThe state of the agent’s teammates can be described as follows:{‘Length of distance moved’: ‘No movement’, ‘Average velocity’: ‘Slow’,‘Horizontal direction of movement’: ‘Southeast’, ‘Horizontaldirection of view’: ‘South’, ‘Pitch direction of view’: ‘Medium’,‘Health level’: ‘Empty’, ‘Whether to restore health’: ‘False’,‘Whether the health is damaged’: ‘False’, ‘Whether rescued teammate’:‘False’, ‘Whether prone position’: ‘False’, ‘Whether crouchposition’: ‘False’, ‘Whether have a gun’: ‘True’, ‘Whether hold agun’: ‘False’, ‘Whether have bullets’: ‘True’, ‘Whether have medicalkits’: ‘True’, ‘Whether be knocked down’: ‘False’, ‘Damage to enemy’:‘Zero’, ‘Whether knock down enemy’: ‘False’, ‘Whether seen enemy’:‘True’, ‘Number of enemies have ever seen’: 5, ‘Whether seen byenemy’: ‘True’, ‘Distance with nearest enemy’: ‘Nearby’, ‘Whethercloser with nearest enemy’: ‘False’, ‘ID of teammate player’: 2}enemystatepromptThe state of the enemy can be described as follows:{‘Horizontaldirection of movement of enemy’: ‘Southwest’, ‘Velocity of enemy’:‘Slow’, ‘Enemy’s position relative to agent’: ‘West’}selfstatepromptThe state of the agent can be described as follows:{‘Damage toenemy’: ‘Zero’, ‘Whether knock down enemy’: ‘False’, ‘Whether killenemy’: ‘False’, ‘Whether seen enemy’: ‘True’, ‘Whether seen byenemy’: ‘True’, ‘Number of enemies have ever seen’: 3, ‘Length ofdistance moved’: ‘Short’, ‘Average velocity’: ‘Fast’, ‘Horizontaldirection of movement’: ‘West’, ‘Horizontal direction of view’:‘NorthEast’, ‘Pitch direction of view’: ‘Medium’, ‘Health level’:‘Full’, ‘Whether to restore health’: ‘False’, ‘Whether the health isdamaged’: ‘False’, ‘Whether rescued teammate’: ‘False’, ‘Whether beknocked down’: ‘False’, ‘Whether prone position’: ‘False’, ‘Whetherhave a gun’: ‘True’, ‘Whether have bullets’: ‘True’, ‘Whether havemedical kits’: ‘True’, ‘Distance with nearest enemy’: ‘Nearby’,‘Whether closer with nearest enemy’: ‘True’, ‘Whether crouchposition’: ‘False’, ‘Whether hold a gun’: ‘True’, ‘Whether seen byteammate’: ‘True’, ‘Length of distance from agent to teammate’:‘Touch’, ‘Teammate’s position relative to agent’: ‘Southwest’,‘Whether follow with the views of teammate’: ‘False’, ‘Whethertarget the same enemy as teammate’: ‘False’, ‘Whether follow withthe movement direction of teammate’: ‘False’}commandpromptTeammate player gives you a verbal command based on the currentgame states:‘You should lie in wait.’questionpromptIn order to complete the command ‘You should lie in wait.’, howthe agent’s game state should change?responsegoalmeta-statepromptWhether prone position:TrueAverage velocity:StaticLength of distance moved:No movementWhether hold a gun:TrueTable 17: Example of prompt and response.. \n",
      "the piece 455\n",
      " \n",
      "Building Open-Ended Embodied Agent via Language-PolicyBidirectional AdaptationShaopeng Zhai* 1Jie Wang* 1Tianyi Zhang* 1Fuxian Huang* 1Qi Zhang* 1Ming Zhou* 1Jing Hou* 2 1Yu Qiao1Yu Liu1AbstractBuilding embodied agents on integrating LargeLanguage Models (LLMs) and ReinforcementLearning (RL) have revolutionized human-AI in-teraction: researchers can now leverage languageinstructions to plan decision-making for open-ended tasks.。However, existing research faceschallenges in meeting the requirement of open-endedness.. \n",
      "the piece 456\n",
      " \n",
      "However, existing research faceschallenges in meeting the requirement of open-endedness.。They typically either train LLM/RLmodels to adapt to a fixed counterpart, limiting ex-ploration of novel skills and hindering the efficacyof human-AI interaction.。To this end, we presentOpenPAL, a co-training framework comprisingtwo stages: (1) fine-tuning a pre-trained LLM totranslate human instructions into goals for plan-ning, and goal-conditioned training a policy fordecision-making; (2) co-training to align the LLMand policy, achieving instruction open-endedness.We conducted experiments using Contra, an open-ended FPS game, demonstrating that an agenttrained with OpenPAL not only comprehends ar-bitrary instructions but also exhibits efficient exe-cution.. \n",
      "the piece 457\n",
      " \n",
      "hey typically either train LLM/RLmodels to adapt to a fixed counterpart, limiting ex-ploration of novel skills and hindering the efficacyof human-AI interaction.。To this end, we presentOpenPAL, a co-training framework comprisingtwo stages: (1) fine-tuning a pre-trained LLM totranslate human instructions into goals for plan-ning, and goal-conditioned training a policy fordecision-making; (2) co-training to align the LLMand policy, achieving instruction open-endedness.We conducted experiments using Contra, an open-ended FPS game, demonstrating that an agenttrained with OpenPAL not only comprehends ar-bitrary instructions but also exhibits efficient exe-cution.。These results suggest that OpenPAL holdsthe potential to construct open-ended embodiedagents in practical scenarios.1.. \n",
      "the piece 458\n",
      " \n",
      " this end, we presentOpenPAL, a co-training framework comprisingtwo stages: (1) fine-tuning a pre-trained LLM totranslate human instructions into goals for plan-ning, and goal-conditioned training a policy fordecision-making; (2) co-training to align the LLMand policy, achieving instruction open-endedness.We conducted experiments using Contra, an open-ended FPS game, demonstrating that an agenttrained with OpenPAL not only comprehends ar-bitrary instructions but also exhibits efficient exe-cution.。These results suggest that OpenPAL holdsthe potential to construct open-ended embodiedagents in practical scenarios.1.。IntroductionWith the increasing prevalence of LLMs such as ChatGPT,researchers have progressively shifted their focus towardsLLM-centered principles, building embodied agents thatinteract with humans to tackle open-ended tasks (Khandel-wal et al., 2022; Huang et al., 2023).. \n",
      "the piece 459\n",
      " \n",
      "se results suggest that OpenPAL holdsthe potential to construct open-ended embodiedagents in practical scenarios.1.。IntroductionWith the increasing prevalence of LLMs such as ChatGPT,researchers have progressively shifted their focus towardsLLM-centered principles, building embodied agents thatinteract with humans to tackle open-ended tasks (Khandel-wal et al., 2022; Huang et al., 2023).。To achieve this target,we need to resolve the challenge of developing AI agentswith the ability to continuously learn new skills, which isrelated to a domain commonly referred to as open-endedlearning that is broadly categorized into two main factions:*Equal contribution1Shanghai AI Laboratory2Tongji University.. \n",
      "the piece 460\n",
      " \n",
      "oductionWith the increasing prevalence of LLMs such as ChatGPT,researchers have progressively shifted their focus towardsLLM-centered principles, building embodied agents thatinteract with humans to tackle open-ended tasks (Khandel-wal et al., 2022; Huang et al., 2023).。To achieve this target,we need to resolve the challenge of developing AI agentswith the ability to continuously learn new skills, which isrelated to a domain commonly referred to as open-endedlearning that is broadly categorized into two main factions:*Equal contribution1Shanghai AI Laboratory2Tongji University.。Correspondence to: Shaopeng Zhai<zhaishaopeng@pjlab.org.cn>, Yu Liu <liuyu@pjlab.org.cn>.Copyright 2024 by the author(s).. \n",
      "the piece 461\n",
      " \n",
      "hieve this target,we need to resolve the challenge of developing AI agentswith the ability to continuously learn new skills, which isrelated to a domain commonly referred to as open-endedlearning that is broadly categorized into two main factions:*Equal contribution1Shanghai AI Laboratory2Tongji University.。Correspondence to: Shaopeng Zhai<zhaishaopeng@pjlab.org.cn>, Yu Liu <liuyu@pjlab.org.cn>.Copyright 2024 by the author(s).。In progress.。(1) pre-training LLMs to translate human-instructions intosub-tasks, for open-ended planning (Wang et al., 2023a;Ouyang et al., 2022), and (2) curriculum RL for open-endedcontrol (Team et al., 2021; Balduzzi et al., 2019).For pre-trained LLMs, particularly those with closed sourcearchitectures, focus on resolving planning with generalknowledge acquired during the pre-training stage (Wanget al., 2023a).. \n",
      "the piece 462\n",
      " \n",
      "pondence to: Shaopeng Zhai<zhaishaopeng@pjlab.org.cn>, Yu Liu <liuyu@pjlab.org.cn>.Copyright 2024 by the author(s).。In progress.。(1) pre-training LLMs to translate human-instructions intosub-tasks, for open-ended planning (Wang et al., 2023a;Ouyang et al., 2022), and (2) curriculum RL for open-endedcontrol (Team et al., 2021; Balduzzi et al., 2019).For pre-trained LLMs, particularly those with closed sourcearchitectures, focus on resolving planning with generalknowledge acquired during the pre-training stage (Wanget al., 2023a).。However, they share shortcomings like re-lying on task-oriented and hand-crafted prompting, strug-gling to comprehend interactions in special contexts suchas games and be incompetent for high real-time require-ments due to inefficient model computation.. \n",
      "the piece 463\n",
      " \n",
      "ress.。(1) pre-training LLMs to translate human-instructions intosub-tasks, for open-ended planning (Wang et al., 2023a;Ouyang et al., 2022), and (2) curriculum RL for open-endedcontrol (Team et al., 2021; Balduzzi et al., 2019).For pre-trained LLMs, particularly those with closed sourcearchitectures, focus on resolving planning with generalknowledge acquired during the pre-training stage (Wanget al., 2023a).。However, they share shortcomings like re-lying on task-oriented and hand-crafted prompting, strug-gling to comprehend interactions in special contexts suchas games and be incompetent for high real-time require-ments due to inefficient model computation.。In contrast,curriculum RL conducts open-ended learning in an end-to-end manner, developing in diverse methodologies such aspopulation-based RL (Team et al., 2021), goal-conditionedRL (GCRL) (Liu et al., 2022) and etc.. \n",
      "the piece 464\n",
      " \n",
      "training LLMs to translate human-instructions intosub-tasks, for open-ended planning (Wang et al., 2023a;Ouyang et al., 2022), and (2) curriculum RL for open-endedcontrol (Team et al., 2021; Balduzzi et al., 2019).For pre-trained LLMs, particularly those with closed sourcearchitectures, focus on resolving planning with generalknowledge acquired during the pre-training stage (Wanget al., 2023a).。However, they share shortcomings like re-lying on task-oriented and hand-crafted prompting, strug-gling to comprehend interactions in special contexts suchas games and be incompetent for high real-time require-ments due to inefficient model computation.。In contrast,curriculum RL conducts open-ended learning in an end-to-end manner, developing in diverse methodologies such aspopulation-based RL (Team et al., 2021), goal-conditionedRL (GCRL) (Liu et al., 2022) and etc.。Despite RL excelsin learning novel skills compared to rule-based control, itlacks the direct capability for interaction with humans.. \n",
      "the piece 465\n",
      " \n",
      "they share shortcomings like re-lying on task-oriented and hand-crafted prompting, strug-gling to comprehend interactions in special contexts suchas games and be incompetent for high real-time require-ments due to inefficient model computation.。In contrast,curriculum RL conducts open-ended learning in an end-to-end manner, developing in diverse methodologies such aspopulation-based RL (Team et al., 2021), goal-conditionedRL (GCRL) (Liu et al., 2022) and etc.。Despite RL excelsin learning novel skills compared to rule-based control, itlacks the direct capability for interaction with humans.。Toleverage advantages from both sides, i.e., being capable ofinteracting human and AI in solving real-time tasks towardsopen-endedness, an ideal implementation is to integratingLLM for planning and RL policy for decision making.. \n",
      "the piece 466\n",
      " \n",
      "t,curriculum RL conducts open-ended learning in an end-to-end manner, developing in diverse methodologies such aspopulation-based RL (Team et al., 2021), goal-conditionedRL (GCRL) (Liu et al., 2022) and etc.。Despite RL excelsin learning novel skills compared to rule-based control, itlacks the direct capability for interaction with humans.。Toleverage advantages from both sides, i.e., being capable ofinteracting human and AI in solving real-time tasks towardsopen-endedness, an ideal implementation is to integratingLLM for planning and RL policy for decision making.。How-ever, existing studies in this domain have focused on im-proving training efficiency or reducing interaction costs byeither independently training the RL policy or LLM (Huet al., 2023; Du et al., 2023) to adapt the other, resulting inoverfitting and failing to explore novel skills in specific envi-ronments that necessitate specialized knowledge and fallingshort of achieving true open-endedness.. \n",
      "the piece 467\n",
      " \n",
      "excelsin learning novel skills compared to rule-based control, itlacks the direct capability for interaction with humans.。Toleverage advantages from both sides, i.e., being capable ofinteracting human and AI in solving real-time tasks towardsopen-endedness, an ideal implementation is to integratingLLM for planning and RL policy for decision making.。How-ever, existing studies in this domain have focused on im-proving training efficiency or reducing interaction costs byeither independently training the RL policy or LLM (Huet al., 2023; Du et al., 2023) to adapt the other, resulting inoverfitting and failing to explore novel skills in specific envi-ronments that necessitate specialized knowledge and fallingshort of achieving true open-endedness.。Despite previouswork resolve this issue with LLM-based re-planning (Wanget al., 2023a;b), it is inefficient for high-dimensional tasksand the re-planning is still in the existing range of strength.To address the above challenge, we propose a co-trainingframework, OpenPAL, strunctured as a two-stage learningprocess to implemente bi-directional adaptation.. \n",
      "the piece 468\n",
      " \n",
      "dvantages from both sides, i.e., being capable ofinteracting human and AI in solving real-time tasks towardsopen-endedness, an ideal implementation is to integratingLLM for planning and RL policy for decision making.。How-ever, existing studies in this domain have focused on im-proving training efficiency or reducing interaction costs byeither independently training the RL policy or LLM (Huet al., 2023; Du et al., 2023) to adapt the other, resulting inoverfitting and failing to explore novel skills in specific envi-ronments that necessitate specialized knowledge and fallingshort of achieving true open-endedness.。Despite previouswork resolve this issue with LLM-based re-planning (Wanget al., 2023a;b), it is inefficient for high-dimensional tasksand the re-planning is still in the existing range of strength.To address the above challenge, we propose a co-trainingframework, OpenPAL, strunctured as a two-stage learningprocess to implemente bi-directional adaptation.。This de-sign enables the RL policy continuously explore novel skillswhiling align the LLM and the policy towards achievinginstruction open-endedness.. \n",
      "the piece 469\n",
      " \n",
      "sting studies in this domain have focused on im-proving training efficiency or reducing interaction costs byeither independently training the RL policy or LLM (Huet al., 2023; Du et al., 2023) to adapt the other, resulting inoverfitting and failing to explore novel skills in specific envi-ronments that necessitate specialized knowledge and fallingshort of achieving true open-endedness.。Despite previouswork resolve this issue with LLM-based re-planning (Wanget al., 2023a;b), it is inefficient for high-dimensional tasksand the re-planning is still in the existing range of strength.To address the above challenge, we propose a co-trainingframework, OpenPAL, strunctured as a two-stage learningprocess to implemente bi-directional adaptation.。This de-sign enables the RL policy continuously explore novel skillswhiling align the LLM and the policy towards achievinginstruction open-endedness.。In the first stage, we sepa-rately train a ChatGLM-6B (Du et al., 2022) as a planner(or goal generator) Gllmand policy πg, where Gllmgen-erates goals with given instructions and environment con-1arXiv:2401.00006v3  [cs.AI]  6 Feb 2024Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationOpen-ended EnvironmentsLLM Policy       Co-trainingMulti-step Fine-tuning Goal-conditioned RL“Find Them Out !”/gid00175Prepare to Ambush.”/gid00175Be Careful !”Human InstructionsObservations Agent Actions/gid00003/gid00036/gid00045/gid00031/gid00174/gid00046/gid00183/gid00032/gid00052/gid00032/gid00183/gid00049/gid00036/gid00032/gid00050/gid00006/gid00041/gid00049/gid00036/gid00045/gid00042/gid00041/gid00040/gid00032/gid00041/gid00047 /gid00002/gid00034/gid00032/gid00041/gid00047 /gid00016/gid00047/gid00035/gid00032/gid00045/gid00001/gid00043/gid00039/gid00028/gid00052/gid00032/gid00045/gid00046Goals PlanningRewards FeedbackGoal RewardsFigure 1: Overview of co-training in OpenPAL.. \n",
      "the piece 470\n",
      " \n",
      "uswork resolve this issue with LLM-based re-planning (Wanget al., 2023a;b), it is inefficient for high-dimensional tasksand the re-planning is still in the existing range of strength.To address the above challenge, we propose a co-trainingframework, OpenPAL, strunctured as a two-stage learningprocess to implemente bi-directional adaptation.。This de-sign enables the RL policy continuously explore novel skillswhiling align the LLM and the policy towards achievinginstruction open-endedness.。In the first stage, we sepa-rately train a ChatGLM-6B (Du et al., 2022) as a planner(or goal generator) Gllmand policy πg, where Gllmgen-erates goals with given instructions and environment con-1arXiv:2401.00006v3  [cs.AI]  6 Feb 2024Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationOpen-ended EnvironmentsLLM Policy       Co-trainingMulti-step Fine-tuning Goal-conditioned RL“Find Them Out !”/gid00175Prepare to Ambush.”/gid00175Be Careful !”Human InstructionsObservations Agent Actions/gid00003/gid00036/gid00045/gid00031/gid00174/gid00046/gid00183/gid00032/gid00052/gid00032/gid00183/gid00049/gid00036/gid00032/gid00050/gid00006/gid00041/gid00049/gid00036/gid00045/gid00042/gid00041/gid00040/gid00032/gid00041/gid00047 /gid00002/gid00034/gid00032/gid00041/gid00047 /gid00016/gid00047/gid00035/gid00032/gid00045/gid00001/gid00043/gid00039/gid00028/gid00052/gid00032/gid00045/gid00046Goals PlanningRewards FeedbackGoal RewardsFigure 1: Overview of co-training in OpenPAL.。The Policy and LLM is pre-trained with multi-step fine-tuning and goal-conditioned RL, respectively.. \n",
      "the piece 471\n",
      " \n",
      "ables the RL policy continuously explore novel skillswhiling align the LLM and the policy towards achievinginstruction open-endedness.。In the first stage, we sepa-rately train a ChatGLM-6B (Du et al., 2022) as a planner(or goal generator) Gllmand policy πg, where Gllmgen-erates goals with given instructions and environment con-1arXiv:2401.00006v3  [cs.AI]  6 Feb 2024Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationOpen-ended EnvironmentsLLM Policy       Co-trainingMulti-step Fine-tuning Goal-conditioned RL“Find Them Out !”/gid00175Prepare to Ambush.”/gid00175Be Careful !”Human InstructionsObservations Agent Actions/gid00003/gid00036/gid00045/gid00031/gid00174/gid00046/gid00183/gid00032/gid00052/gid00032/gid00183/gid00049/gid00036/gid00032/gid00050/gid00006/gid00041/gid00049/gid00036/gid00045/gid00042/gid00041/gid00040/gid00032/gid00041/gid00047 /gid00002/gid00034/gid00032/gid00041/gid00047 /gid00016/gid00047/gid00035/gid00032/gid00045/gid00001/gid00043/gid00039/gid00028/gid00052/gid00032/gid00045/gid00046Goals PlanningRewards FeedbackGoal RewardsFigure 1: Overview of co-training in OpenPAL.。The Policy and LLM is pre-trained with multi-step fine-tuning and goal-conditioned RL, respectively.。Then, the co-training aligns them towards achieving instruction open-endedness.text, and πglearns to execute goals.. \n",
      "the piece 472\n",
      " \n",
      "ge, we sepa-rately train a ChatGLM-6B (Du et al., 2022) as a planner(or goal generator) Gllmand policy πg, where Gllmgen-erates goals with given instructions and environment con-1arXiv:2401.00006v3  [cs.AI]  6 Feb 2024Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationOpen-ended EnvironmentsLLM Policy       Co-trainingMulti-step Fine-tuning Goal-conditioned RL“Find Them Out !”/gid00175Prepare to Ambush.”/gid00175Be Careful !”Human InstructionsObservations Agent Actions/gid00003/gid00036/gid00045/gid00031/gid00174/gid00046/gid00183/gid00032/gid00052/gid00032/gid00183/gid00049/gid00036/gid00032/gid00050/gid00006/gid00041/gid00049/gid00036/gid00045/gid00042/gid00041/gid00040/gid00032/gid00041/gid00047 /gid00002/gid00034/gid00032/gid00041/gid00047 /gid00016/gid00047/gid00035/gid00032/gid00045/gid00001/gid00043/gid00039/gid00028/gid00052/gid00032/gid00045/gid00046Goals PlanningRewards FeedbackGoal RewardsFigure 1: Overview of co-training in OpenPAL.。The Policy and LLM is pre-trained with multi-step fine-tuning and goal-conditioned RL, respectively.。Then, the co-training aligns them towards achieving instruction open-endedness.text, and πglearns to execute goals.。To achieve that, wepropose multi-step fine-tuning a pre-trained LLM with GPT-4-generated instructions and goals, and open-ended goalgeneration to learn a goal-conditioned policy.. \n",
      "the piece 473\n",
      " \n",
      "M is pre-trained with multi-step fine-tuning and goal-conditioned RL, respectively.。Then, the co-training aligns them towards achieving instruction open-endedness.text, and πglearns to execute goals.。To achieve that, wepropose multi-step fine-tuning a pre-trained LLM with GPT-4-generated instructions and goals, and open-ended goalgeneration to learn a goal-conditioned policy.。In the secondstage, we implement co-training to align Gllm(planning)andπg(decision-making), as illustrated in Figure 1.。Thisaims to achieve instruction open-endedness, aligning theinstruction space with the open-ended goal space that theagent achieved.. \n",
      "the piece 474\n",
      " \n",
      "ing aligns them towards achieving instruction open-endedness.text, and πglearns to execute goals.。To achieve that, wepropose multi-step fine-tuning a pre-trained LLM with GPT-4-generated instructions and goals, and open-ended goalgeneration to learn a goal-conditioned policy.。In the secondstage, we implement co-training to align Gllm(planning)andπg(decision-making), as illustrated in Figure 1.。Thisaims to achieve instruction open-endedness, aligning theinstruction space with the open-ended goal space that theagent achieved.。Specifically, we implement the co-trainingas an interleaved execution of (1) Reinforcement Learn-ing with Agent Feedback (RLAF) for Gllmand (2) GCRLforπgwith goals generated by Gllm, where RLAF cen-ters around rewarding Gllmwith agent feedback and goalexecution.. \n",
      "the piece 475\n",
      " \n",
      "propose multi-step fine-tuning a pre-trained LLM with GPT-4-generated instructions and goals, and open-ended goalgeneration to learn a goal-conditioned policy.。In the secondstage, we implement co-training to align Gllm(planning)andπg(decision-making), as illustrated in Figure 1.。Thisaims to achieve instruction open-endedness, aligning theinstruction space with the open-ended goal space that theagent achieved.。Specifically, we implement the co-trainingas an interleaved execution of (1) Reinforcement Learn-ing with Agent Feedback (RLAF) for Gllmand (2) GCRLforπgwith goals generated by Gllm, where RLAF cen-ters around rewarding Gllmwith agent feedback and goalexecution.。This two-staged approach optimizes the LLMfor comprehending environment context under the consid-eration of decision-making, while concurrently enhancingdecision-making for goals aligned with human instructions.For evaluation, we employ Contra, an open-ended FPS game.The results demonstrate that OpenPAL achieves a high goalcompletion ratio for open-ended human-AI interaction.2.. \n",
      "the piece 476\n",
      " \n",
      "we implement co-training to align Gllm(planning)andπg(decision-making), as illustrated in Figure 1.。Thisaims to achieve instruction open-endedness, aligning theinstruction space with the open-ended goal space that theagent achieved.。Specifically, we implement the co-trainingas an interleaved execution of (1) Reinforcement Learn-ing with Agent Feedback (RLAF) for Gllmand (2) GCRLforπgwith goals generated by Gllm, where RLAF cen-ters around rewarding Gllmwith agent feedback and goalexecution.。This two-staged approach optimizes the LLMfor comprehending environment context under the consid-eration of decision-making, while concurrently enhancingdecision-making for goals aligned with human instructions.For evaluation, we employ Contra, an open-ended FPS game.The results demonstrate that OpenPAL achieves a high goalcompletion ratio for open-ended human-AI interaction.2.。BackgroundGoal-conditioned Reinforcement Learning.. \n",
      "the piece 477\n",
      " \n",
      "nstruction open-endedness, aligning theinstruction space with the open-ended goal space that theagent achieved.。Specifically, we implement the co-trainingas an interleaved execution of (1) Reinforcement Learn-ing with Agent Feedback (RLAF) for Gllmand (2) GCRLforπgwith goals generated by Gllm, where RLAF cen-ters around rewarding Gllmwith agent feedback and goalexecution.。This two-staged approach optimizes the LLMfor comprehending environment context under the consid-eration of decision-making, while concurrently enhancingdecision-making for goals aligned with human instructions.For evaluation, we employ Contra, an open-ended FPS game.The results demonstrate that OpenPAL achieves a high goalcompletion ratio for open-ended human-AI interaction.2.。BackgroundGoal-conditioned Reinforcement Learning.。Formally,GCRL could be formulated as a goal-augmented MarkovDecision Process M(Liu et al., 2022).. \n",
      "the piece 478\n",
      " \n",
      "ment the co-trainingas an interleaved execution of (1) Reinforcement Learn-ing with Agent Feedback (RLAF) for Gllmand (2) GCRLforπgwith goals generated by Gllm, where RLAF cen-ters around rewarding Gllmwith agent feedback and goalexecution.。This two-staged approach optimizes the LLMfor comprehending environment context under the consid-eration of decision-making, while concurrently enhancingdecision-making for goals aligned with human instructions.For evaluation, we employ Contra, an open-ended FPS game.The results demonstrate that OpenPAL achieves a high goalcompletion ratio for open-ended human-AI interaction.2.。BackgroundGoal-conditioned Reinforcement Learning.。Formally,GCRL could be formulated as a goal-augmented MarkovDecision Process M(Liu et al., 2022).。Denoting Matuple⟨S,A,G,P,R, γ⟩, where S,A,Gthe state, actionand goal spaces, respectively.. \n",
      "the piece 479\n",
      " \n",
      "h optimizes the LLMfor comprehending environment context under the consid-eration of decision-making, while concurrently enhancingdecision-making for goals aligned with human instructions.For evaluation, we employ Contra, an open-ended FPS game.The results demonstrate that OpenPAL achieves a high goalcompletion ratio for open-ended human-AI interaction.2.。BackgroundGoal-conditioned Reinforcement Learning.。Formally,GCRL could be formulated as a goal-augmented MarkovDecision Process M(Liu et al., 2022).。Denoting Matuple⟨S,A,G,P,R, γ⟩, where S,A,Gthe state, actionand goal spaces, respectively.。In general, Gis a projectionofS, i.e.,G=PROJ(S).Pdefines the state transitionprobabilities, i.e., P:S × A → ∆(S), where ∆(·)adistribution.. \n",
      "the piece 480\n",
      " \n",
      "ed Reinforcement Learning.。Formally,GCRL could be formulated as a goal-augmented MarkovDecision Process M(Liu et al., 2022).。Denoting Matuple⟨S,A,G,P,R, γ⟩, where S,A,Gthe state, actionand goal spaces, respectively.。In general, Gis a projectionofS, i.e.,G=PROJ(S).Pdefines the state transitionprobabilities, i.e., P:S × A → ∆(S), where ∆(·)adistribution.。R:S × A × G → Rdefines the rewardfunction r(s, a, g ).。At the beginning of an trajectory τ, agoalgis sampled from a distribution Pg, which generallydefines a task for the agent to execute.. \n",
      "the piece 481\n",
      " \n",
      "rmulated as a goal-augmented MarkovDecision Process M(Liu et al., 2022).。Denoting Matuple⟨S,A,G,P,R, γ⟩, where S,A,Gthe state, actionand goal spaces, respectively.。In general, Gis a projectionofS, i.e.,G=PROJ(S).Pdefines the state transitionprobabilities, i.e., P:S × A → ∆(S), where ∆(·)adistribution.。R:S × A × G → Rdefines the rewardfunction r(s, a, g ).。At the beginning of an trajectory τ, agoalgis sampled from a distribution Pg, which generallydefines a task for the agent to execute.。As for decisionmaking, πdenotes a policy as π:S × G → ∆(A), whichis a distribution over the action space.. \n",
      "the piece 482\n",
      " \n",
      ", γ⟩, where S,A,Gthe state, actionand goal spaces, respectively.。In general, Gis a projectionofS, i.e.,G=PROJ(S).Pdefines the state transitionprobabilities, i.e., P:S × A → ∆(S), where ∆(·)adistribution.。R:S × A × G → Rdefines the rewardfunction r(s, a, g ).。At the beginning of an trajectory τ, agoalgis sampled from a distribution Pg, which generallydefines a task for the agent to execute.。As for decisionmaking, πdenotes a policy as π:S × G → ∆(A), whichis a distribution over the action space.。To solve a M, orachieve open-endedness in other words, an agent with policyπneeds to maximize its accumulative reward over the goalspace as Eat∼π(·|st,g),(st,at)∼τ,g∼PghPTt=0γtr(st, at, g)i,where γ∈[0,1)discounts a reward at each time step toensure the convergence.. \n",
      "the piece 483\n",
      " \n",
      "nofS, i.e.,G=PROJ(S).Pdefines the state transitionprobabilities, i.e., P:S × A → ∆(S), where ∆(·)adistribution.。R:S × A × G → Rdefines the rewardfunction r(s, a, g ).。At the beginning of an trajectory τ, agoalgis sampled from a distribution Pg, which generallydefines a task for the agent to execute.。As for decisionmaking, πdenotes a policy as π:S × G → ∆(A), whichis a distribution over the action space.。To solve a M, orachieve open-endedness in other words, an agent with policyπneeds to maximize its accumulative reward over the goalspace as Eat∼π(·|st,g),(st,at)∼τ,g∼PghPTt=0γtr(st, at, g)i,where γ∈[0,1)discounts a reward at each time step toensure the convergence.。Normally, r(st, at, g)is binary asr(st, at, g) =(1PROJ(st+1) =g0otherwise.. \n",
      "the piece 484\n",
      " \n",
      "ewardfunction r(s, a, g ).。At the beginning of an trajectory τ, agoalgis sampled from a distribution Pg, which generallydefines a task for the agent to execute.。As for decisionmaking, πdenotes a policy as π:S × G → ∆(A), whichis a distribution over the action space.。To solve a M, orachieve open-endedness in other words, an agent with policyπneeds to maximize its accumulative reward over the goalspace as Eat∼π(·|st,g),(st,at)∼τ,g∼PghPTt=0γtr(st, at, g)i,where γ∈[0,1)discounts a reward at each time step toensure the convergence.。Normally, r(st, at, g)is binary asr(st, at, g) =(1PROJ(st+1) =g0otherwise.。(1)To approximatePTt=0γtr(st, at, g), GRL suggests us-ing the Universal Value Function Approximator (UVFA)V(s, g).. \n",
      "the piece 485\n",
      " \n",
      "tory τ, agoalgis sampled from a distribution Pg, which generallydefines a task for the agent to execute.。As for decisionmaking, πdenotes a policy as π:S × G → ∆(A), whichis a distribution over the action space.。To solve a M, orachieve open-endedness in other words, an agent with policyπneeds to maximize its accumulative reward over the goalspace as Eat∼π(·|st,g),(st,at)∼τ,g∼PghPTt=0γtr(st, at, g)i,where γ∈[0,1)discounts a reward at each time step toensure the convergence.。Normally, r(st, at, g)is binary asr(st, at, g) =(1PROJ(st+1) =g0otherwise.。(1)To approximatePTt=0γtr(st, at, g), GRL suggests us-ing the Universal Value Function Approximator (UVFA)V(s, g).。As for the solving of open-endedness, there arecritical challenges that need to be resolved, including butnot limited to: (1) Pgis agnostic to an agent, and (2) Gisuncountable and continuous.. \n",
      "the piece 486\n",
      " \n",
      "s a policy as π:S × G → ∆(A), whichis a distribution over the action space.。To solve a M, orachieve open-endedness in other words, an agent with policyπneeds to maximize its accumulative reward over the goalspace as Eat∼π(·|st,g),(st,at)∼τ,g∼PghPTt=0γtr(st, at, g)i,where γ∈[0,1)discounts a reward at each time step toensure the convergence.。Normally, r(st, at, g)is binary asr(st, at, g) =(1PROJ(st+1) =g0otherwise.。(1)To approximatePTt=0γtr(st, at, g), GRL suggests us-ing the Universal Value Function Approximator (UVFA)V(s, g).。As for the solving of open-endedness, there arecritical challenges that need to be resolved, including butnot limited to: (1) Pgis agnostic to an agent, and (2) Gisuncountable and continuous.。To resolve these issues, exist-ing research is centered on curriculum RL (Weng, 2020),i.e., automatically discovering novel goals from past learn-ing.. \n",
      "the piece 487\n",
      " \n",
      "dedness in other words, an agent with policyπneeds to maximize its accumulative reward over the goalspace as Eat∼π(·|st,g),(st,at)∼τ,g∼PghPTt=0γtr(st, at, g)i,where γ∈[0,1)discounts a reward at each time step toensure the convergence.。Normally, r(st, at, g)is binary asr(st, at, g) =(1PROJ(st+1) =g0otherwise.。(1)To approximatePTt=0γtr(st, at, g), GRL suggests us-ing the Universal Value Function Approximator (UVFA)V(s, g).。As for the solving of open-endedness, there arecritical challenges that need to be resolved, including butnot limited to: (1) Pgis agnostic to an agent, and (2) Gisuncountable and continuous.。To resolve these issues, exist-ing research is centered on curriculum RL (Weng, 2020),i.e., automatically discovering novel goals from past learn-ing.。Hindsight goal relabelling (Andrychowicz et al., 2017;Fang et al., 2019; Zheng et al., 2022; Schaul et al., 2015)implicitly implements curriculum learning by introducinga prioritized replay mechanism and performs high data ef-ficiency.. \n",
      "the piece 488\n",
      " \n",
      "asr(st, at, g) =(1PROJ(st+1) =g0otherwise.。(1)To approximatePTt=0γtr(st, at, g), GRL suggests us-ing the Universal Value Function Approximator (UVFA)V(s, g).。As for the solving of open-endedness, there arecritical challenges that need to be resolved, including butnot limited to: (1) Pgis agnostic to an agent, and (2) Gisuncountable and continuous.。To resolve these issues, exist-ing research is centered on curriculum RL (Weng, 2020),i.e., automatically discovering novel goals from past learn-ing.。Hindsight goal relabelling (Andrychowicz et al., 2017;Fang et al., 2019; Zheng et al., 2022; Schaul et al., 2015)implicitly implements curriculum learning by introducinga prioritized replay mechanism and performs high data ef-ficiency.。Despite numerous curriculum approaches, thesample inefficiency due to the setting of binary reward sig-nals (Equation (4)) hinders policy learning.. \n",
      "the piece 489\n",
      " \n",
      " g), GRL suggests us-ing the Universal Value Function Approximator (UVFA)V(s, g).。As for the solving of open-endedness, there arecritical challenges that need to be resolved, including butnot limited to: (1) Pgis agnostic to an agent, and (2) Gisuncountable and continuous.。To resolve these issues, exist-ing research is centered on curriculum RL (Weng, 2020),i.e., automatically discovering novel goals from past learn-ing.。Hindsight goal relabelling (Andrychowicz et al., 2017;Fang et al., 2019; Zheng et al., 2022; Schaul et al., 2015)implicitly implements curriculum learning by introducinga prioritized replay mechanism and performs high data ef-ficiency.。Despite numerous curriculum approaches, thesample inefficiency due to the setting of binary reward sig-nals (Equation (4)) hinders policy learning.。As a solution,existing research leverages reward shaping (Ng et al., 1999;Ecoffet et al., 2021; Ding et al., 2023; Trott et al., 2019)which is a straightforward and efficient idea.LLMs and Human-AI Interaction.. \n",
      "the piece 490\n",
      " \n",
      "ss, there arecritical challenges that need to be resolved, including butnot limited to: (1) Pgis agnostic to an agent, and (2) Gisuncountable and continuous.。To resolve these issues, exist-ing research is centered on curriculum RL (Weng, 2020),i.e., automatically discovering novel goals from past learn-ing.。Hindsight goal relabelling (Andrychowicz et al., 2017;Fang et al., 2019; Zheng et al., 2022; Schaul et al., 2015)implicitly implements curriculum learning by introducinga prioritized replay mechanism and performs high data ef-ficiency.。Despite numerous curriculum approaches, thesample inefficiency due to the setting of binary reward sig-nals (Equation (4)) hinders policy learning.。As a solution,existing research leverages reward shaping (Ng et al., 1999;Ecoffet et al., 2021; Ding et al., 2023; Trott et al., 2019)which is a straightforward and efficient idea.LLMs and Human-AI Interaction.。LLMs (Brown et al.,2020; OpenAI, 2023) a class of neural networks that ex-Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationecute in auto-regressive for text generation.. \n",
      "the piece 491\n",
      " \n",
      "research is centered on curriculum RL (Weng, 2020),i.e., automatically discovering novel goals from past learn-ing.。Hindsight goal relabelling (Andrychowicz et al., 2017;Fang et al., 2019; Zheng et al., 2022; Schaul et al., 2015)implicitly implements curriculum learning by introducinga prioritized replay mechanism and performs high data ef-ficiency.。Despite numerous curriculum approaches, thesample inefficiency due to the setting of binary reward sig-nals (Equation (4)) hinders policy learning.。As a solution,existing research leverages reward shaping (Ng et al., 1999;Ecoffet et al., 2021; Ding et al., 2023; Trott et al., 2019)which is a straightforward and efficient idea.LLMs and Human-AI Interaction.。LLMs (Brown et al.,2020; OpenAI, 2023) a class of neural networks that ex-Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationecute in auto-regressive for text generation.。Given a se-quence of text tokens with length tasx1:t= (x1, .. \n",
      "the piece 492\n",
      " \n",
      "wicz et al., 2017;Fang et al., 2019; Zheng et al., 2022; Schaul et al., 2015)implicitly implements curriculum learning by introducinga prioritized replay mechanism and performs high data ef-ficiency.。Despite numerous curriculum approaches, thesample inefficiency due to the setting of binary reward sig-nals (Equation (4)) hinders policy learning.。As a solution,existing research leverages reward shaping (Ng et al., 1999;Ecoffet et al., 2021; Ding et al., 2023; Trott et al., 2019)which is a straightforward and efficient idea.LLMs and Human-AI Interaction.。LLMs (Brown et al.,2020; OpenAI, 2023) a class of neural networks that ex-Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationecute in auto-regressive for text generation.。Given a se-quence of text tokens with length tasx1:t= (x1, .。.. \n",
      "the piece 493\n",
      " \n",
      "s, thesample inefficiency due to the setting of binary reward sig-nals (Equation (4)) hinders policy learning.。As a solution,existing research leverages reward shaping (Ng et al., 1999;Ecoffet et al., 2021; Ding et al., 2023; Trott et al., 2019)which is a straightforward and efficient idea.LLMs and Human-AI Interaction.。LLMs (Brown et al.,2020; OpenAI, 2023) a class of neural networks that ex-Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationecute in auto-regressive for text generation.。Given a se-quence of text tokens with length tasx1:t= (x1, .。.。.. \n",
      "the piece 494\n",
      " \n",
      "ges reward shaping (Ng et al., 1999;Ecoffet et al., 2021; Ding et al., 2023; Trott et al., 2019)which is a straightforward and efficient idea.LLMs and Human-AI Interaction.。LLMs (Brown et al.,2020; OpenAI, 2023) a class of neural networks that ex-Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationecute in auto-regressive for text generation.。Given a se-quence of text tokens with length tasx1:t= (x1, .。.。.。, x t),the generation of a next token xt+1could be formulatedas sampling from a probabilistic model P(·|x1:t).. \n",
      "the piece 495\n",
      " \n",
      "a class of neural networks that ex-Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationecute in auto-regressive for text generation.。Given a se-quence of text tokens with length tasx1:t= (x1, .。.。.。, x t),the generation of a next token xt+1could be formulatedas sampling from a probabilistic model P(·|x1:t).。As forthe training of LLMs, the target is equivalently to find aparameter set θLLMwhich satisfies the optimal generation,i.e.,θLLM= arg max θEx1:t,xt+1P(xt+1|x1:t;θ).。Beyondthe research of LLMs, it is attractive to leverage LLMsas an interface to interact human with agents (Jiang et al.,2023; Hermann et al., 2017).. \n",
      "the piece 496\n",
      " \n",
      "ngth tasx1:t= (x1, .。.。.。, x t),the generation of a next token xt+1could be formulatedas sampling from a probabilistic model P(·|x1:t).。As forthe training of LLMs, the target is equivalently to find aparameter set θLLMwhich satisfies the optimal generation,i.e.,θLLM= arg max θEx1:t,xt+1P(xt+1|x1:t;θ).。Beyondthe research of LLMs, it is attractive to leverage LLMsas an interface to interact human with agents (Jiang et al.,2023; Hermann et al., 2017).。We can roughly reformulatethe generation as xt+1∼P(·|ι, x1:t)for human-AI inter-action, where ιthe language instruction as a prompt, x1:tthe context.. \n",
      "the piece 497\n",
      " \n",
      " xt+1could be formulatedas sampling from a probabilistic model P(·|x1:t).。As forthe training of LLMs, the target is equivalently to find aparameter set θLLMwhich satisfies the optimal generation,i.e.,θLLM= arg max θEx1:t,xt+1P(xt+1|x1:t;θ).。Beyondthe research of LLMs, it is attractive to leverage LLMsas an interface to interact human with agents (Jiang et al.,2023; Hermann et al., 2017).。We can roughly reformulatethe generation as xt+1∼P(·|ι, x1:t)for human-AI inter-action, where ιthe language instruction as a prompt, x1:tthe context.。For the cases have low real-time requirements,xt+1is a control signal for decision making (Brohan et al.,2022; Zitkovich et al., 2023; Nakano et al., 2021).. \n",
      "the piece 498\n",
      " \n",
      "+1could be formulatedas sampling from a probabilistic model P(·|x1:t).。As forthe training of LLMs, the target is equivalently to find aparameter set θLLMwhich satisfies the optimal generation,i.e.,θLLM= arg max θEx1:t,xt+1P(xt+1|x1:t;θ).。Beyondthe research of LLMs, it is attractive to leverage LLMsas an interface to interact human with agents (Jiang et al.,2023; Hermann et al., 2017).。We can roughly reformulatethe generation as xt+1∼P(·|ι, x1:t)for human-AI inter-action, where ιthe language instruction as a prompt, x1:tthe context.。For the cases have low real-time requirements,xt+1is a control signal for decision making (Brohan et al.,2022; Zitkovich et al., 2023; Nakano et al., 2021).。Whilefor the cases have high real-time requirements, xt+1is agoal and will be fed to a controller to guide the decisionmaking (Wang et al., 2023b;a).. \n",
      "the piece 499\n",
      " \n",
      "ould be formulatedas sampling from a probabilistic model P(·|x1:t).。As forthe training of LLMs, the target is equivalently to find aparameter set θLLMwhich satisfies the optimal generation,i.e.,θLLM= arg max θEx1:t,xt+1P(xt+1|x1:t;θ).。Beyondthe research of LLMs, it is attractive to leverage LLMsas an interface to interact human with agents (Jiang et al.,2023; Hermann et al., 2017).。We can roughly reformulatethe generation as xt+1∼P(·|ι, x1:t)for human-AI inter-action, where ιthe language instruction as a prompt, x1:tthe context.。For the cases have low real-time requirements,xt+1is a control signal for decision making (Brohan et al.,2022; Zitkovich et al., 2023; Nakano et al., 2021).。Whilefor the cases have high real-time requirements, xt+1is agoal and will be fed to a controller to guide the decisionmaking (Wang et al., 2023b;a).。Our study falls within thelatter situation, building open-endeded embodied agents inthe cases with high real-time requirements.3.. \n",
      "the piece 500\n",
      " \n",
      "uivalently to find aparameter set θLLMwhich satisfies the optimal generation,i.e.,θLLM= arg max θEx1:t,xt+1P(xt+1|x1:t;θ).。Beyondthe research of LLMs, it is attractive to leverage LLMsas an interface to interact human with agents (Jiang et al.,2023; Hermann et al., 2017).。We can roughly reformulatethe generation as xt+1∼P(·|ι, x1:t)for human-AI inter-action, where ιthe language instruction as a prompt, x1:tthe context.。For the cases have low real-time requirements,xt+1is a control signal for decision making (Brohan et al.,2022; Zitkovich et al., 2023; Nakano et al., 2021).。Whilefor the cases have high real-time requirements, xt+1is agoal and will be fed to a controller to guide the decisionmaking (Wang et al., 2023b;a).。Our study falls within thelatter situation, building open-endeded embodied agents inthe cases with high real-time requirements.3.。The Contra: A Battle Royale FPS GameContra seamlessly merges the last-man-standing gameplaydynamics with the survival, exploration, and scavengingelements inherent in first-person shooting games (Gautamet al., 2021).. \n",
      "the piece 501\n",
      " \n",
      "to leverage LLMsas an interface to interact human with agents (Jiang et al.,2023; Hermann et al., 2017).。We can roughly reformulatethe generation as xt+1∼P(·|ι, x1:t)for human-AI inter-action, where ιthe language instruction as a prompt, x1:tthe context.。For the cases have low real-time requirements,xt+1is a control signal for decision making (Brohan et al.,2022; Zitkovich et al., 2023; Nakano et al., 2021).。Whilefor the cases have high real-time requirements, xt+1is agoal and will be fed to a controller to guide the decisionmaking (Wang et al., 2023b;a).。Our study falls within thelatter situation, building open-endeded embodied agents inthe cases with high real-time requirements.3.。The Contra: A Battle Royale FPS GameContra seamlessly merges the last-man-standing gameplaydynamics with the survival, exploration, and scavengingelements inherent in first-person shooting games (Gautamet al., 2021).。It unfolds with multiple hostile teams, neces-sitating players to collaborate with teammates, withstandadversaries, and strive to outlast others in the ever-changingarena.. \n",
      "the piece 502\n",
      " \n",
      "+1∼P(·|ι, x1:t)for human-AI inter-action, where ιthe language instruction as a prompt, x1:tthe context.。For the cases have low real-time requirements,xt+1is a control signal for decision making (Brohan et al.,2022; Zitkovich et al., 2023; Nakano et al., 2021).。Whilefor the cases have high real-time requirements, xt+1is agoal and will be fed to a controller to guide the decisionmaking (Wang et al., 2023b;a).。Our study falls within thelatter situation, building open-endeded embodied agents inthe cases with high real-time requirements.3.。The Contra: A Battle Royale FPS GameContra seamlessly merges the last-man-standing gameplaydynamics with the survival, exploration, and scavengingelements inherent in first-person shooting games (Gautamet al., 2021).。It unfolds with multiple hostile teams, neces-sitating players to collaborate with teammates, withstandadversaries, and strive to outlast others in the ever-changingarena.。The agent’s objectives encompass individual survivaland the elimination of encountered enemies.. \n",
      "the piece 503\n",
      " \n",
      "t+1is a control signal for decision making (Brohan et al.,2022; Zitkovich et al., 2023; Nakano et al., 2021).。Whilefor the cases have high real-time requirements, xt+1is agoal and will be fed to a controller to guide the decisionmaking (Wang et al., 2023b;a).。Our study falls within thelatter situation, building open-endeded embodied agents inthe cases with high real-time requirements.3.。The Contra: A Battle Royale FPS GameContra seamlessly merges the last-man-standing gameplaydynamics with the survival, exploration, and scavengingelements inherent in first-person shooting games (Gautamet al., 2021).。It unfolds with multiple hostile teams, neces-sitating players to collaborate with teammates, withstandadversaries, and strive to outlast others in the ever-changingarena.。The agent’s objectives encompass individual survivaland the elimination of encountered enemies.。An agent inContra mandates a sequential acquisition of skills, startingfrom fundamental abilities like running and item collec-tion.. \n",
      "the piece 504\n",
      " \n",
      "nts, xt+1is agoal and will be fed to a controller to guide the decisionmaking (Wang et al., 2023b;a).。Our study falls within thelatter situation, building open-endeded embodied agents inthe cases with high real-time requirements.3.。The Contra: A Battle Royale FPS GameContra seamlessly merges the last-man-standing gameplaydynamics with the survival, exploration, and scavengingelements inherent in first-person shooting games (Gautamet al., 2021).。It unfolds with multiple hostile teams, neces-sitating players to collaborate with teammates, withstandadversaries, and strive to outlast others in the ever-changingarena.。The agent’s objectives encompass individual survivaland the elimination of encountered enemies.。An agent inContra mandates a sequential acquisition of skills, startingfrom fundamental abilities like running and item collec-tion.。As the learning proceeds, an agent must master moreintricate skills such as evading enemy projectiles and co-ordinating tactics with teammates, an open-ended learningprocess.. \n",
      "the piece 505\n",
      " \n",
      "ing open-endeded embodied agents inthe cases with high real-time requirements.3.。The Contra: A Battle Royale FPS GameContra seamlessly merges the last-man-standing gameplaydynamics with the survival, exploration, and scavengingelements inherent in first-person shooting games (Gautamet al., 2021).。It unfolds with multiple hostile teams, neces-sitating players to collaborate with teammates, withstandadversaries, and strive to outlast others in the ever-changingarena.。The agent’s objectives encompass individual survivaland the elimination of encountered enemies.。An agent inContra mandates a sequential acquisition of skills, startingfrom fundamental abilities like running and item collec-tion.。As the learning proceeds, an agent must master moreintricate skills such as evading enemy projectiles and co-ordinating tactics with teammates, an open-ended learningprocess.。The primary rationale behind choosing Contra asour testbed lies in its possession of proprietary knowledgenot encompassed in general knowledge repositories.. \n",
      "the piece 506\n",
      " \n",
      "sly merges the last-man-standing gameplaydynamics with the survival, exploration, and scavengingelements inherent in first-person shooting games (Gautamet al., 2021).。It unfolds with multiple hostile teams, neces-sitating players to collaborate with teammates, withstandadversaries, and strive to outlast others in the ever-changingarena.。The agent’s objectives encompass individual survivaland the elimination of encountered enemies.。An agent inContra mandates a sequential acquisition of skills, startingfrom fundamental abilities like running and item collec-tion.。As the learning proceeds, an agent must master moreintricate skills such as evading enemy projectiles and co-ordinating tactics with teammates, an open-ended learningprocess.。The primary rationale behind choosing Contra asour testbed lies in its possession of proprietary knowledgenot encompassed in general knowledge repositories.。Con-sequently, we employ Reinforcement Learning (RL) forknowledge discovery, and co-training to align the LanguageModel (LLM) and RL in comprehending the environment.4.. \n",
      "the piece 507\n",
      " \n",
      "ing players to collaborate with teammates, withstandadversaries, and strive to outlast others in the ever-changingarena.。The agent’s objectives encompass individual survivaland the elimination of encountered enemies.。An agent inContra mandates a sequential acquisition of skills, startingfrom fundamental abilities like running and item collec-tion.。As the learning proceeds, an agent must master moreintricate skills such as evading enemy projectiles and co-ordinating tactics with teammates, an open-ended learningprocess.。The primary rationale behind choosing Contra asour testbed lies in its possession of proprietary knowledgenot encompassed in general knowledge repositories.。Con-sequently, we employ Reinforcement Learning (RL) forknowledge discovery, and co-training to align the LanguageModel (LLM) and RL in comprehending the environment.4.。A Co-training Framework: OpenPALConsidering the training in the context of open-ended learn-ing is extensive data-thirsty, we first introduce two criticalengineering designs to enhance training efficiency.. \n",
      "the piece 508\n",
      " \n",
      "and the elimination of encountered enemies.。An agent inContra mandates a sequential acquisition of skills, startingfrom fundamental abilities like running and item collec-tion.。As the learning proceeds, an agent must master moreintricate skills such as evading enemy projectiles and co-ordinating tactics with teammates, an open-ended learningprocess.。The primary rationale behind choosing Contra asour testbed lies in its possession of proprietary knowledgenot encompassed in general knowledge repositories.。Con-sequently, we employ Reinforcement Learning (RL) forknowledge discovery, and co-training to align the LanguageModel (LLM) and RL in comprehending the environment.4.。A Co-training Framework: OpenPALConsidering the training in the context of open-ended learn-ing is extensive data-thirsty, we first introduce two criticalengineering designs to enhance training efficiency.。Specif-ically, OpenPAL incorporates a distributed RL frameworkinspired by AlphaStar (Vinyals et al., 2019) with modi-fications, resulting in the formation of the Actor-League-Learner architecture.. \n",
      "the piece 509\n",
      " \n",
      "f skills, startingfrom fundamental abilities like running and item collec-tion.。As the learning proceeds, an agent must master moreintricate skills such as evading enemy projectiles and co-ordinating tactics with teammates, an open-ended learningprocess.。The primary rationale behind choosing Contra asour testbed lies in its possession of proprietary knowledgenot encompassed in general knowledge repositories.。Con-sequently, we employ Reinforcement Learning (RL) forknowledge discovery, and co-training to align the LanguageModel (LLM) and RL in comprehending the environment.4.。A Co-training Framework: OpenPALConsidering the training in the context of open-ended learn-ing is extensive data-thirsty, we first introduce two criticalengineering designs to enhance training efficiency.。Specif-ically, OpenPAL incorporates a distributed RL frameworkinspired by AlphaStar (Vinyals et al., 2019) with modi-fications, resulting in the formation of the Actor-League-Learner architecture.。In this architecture, the League is re-sponsible for distributing rollout tasks to a cluster of Actors(CPU nodes) for data collection and evaluation, while opti-mization tasks are delegated to the Learner (GPU node) forpolicy updates.. \n",
      "the piece 510\n",
      " \n",
      "ricate skills such as evading enemy projectiles and co-ordinating tactics with teammates, an open-ended learningprocess.。The primary rationale behind choosing Contra asour testbed lies in its possession of proprietary knowledgenot encompassed in general knowledge repositories.。Con-sequently, we employ Reinforcement Learning (RL) forknowledge discovery, and co-training to align the LanguageModel (LLM) and RL in comprehending the environment.4.。A Co-training Framework: OpenPALConsidering the training in the context of open-ended learn-ing is extensive data-thirsty, we first introduce two criticalengineering designs to enhance training efficiency.。Specif-ically, OpenPAL incorporates a distributed RL frameworkinspired by AlphaStar (Vinyals et al., 2019) with modi-fications, resulting in the formation of the Actor-League-Learner architecture.。In this architecture, the League is re-sponsible for distributing rollout tasks to a cluster of Actors(CPU nodes) for data collection and evaluation, while opti-mization tasks are delegated to the Learner (GPU node) forpolicy updates.。This distributed approach significantly en-hances rollout throughput, thereby improving overall train-ing efficiency.. \n",
      "the piece 511\n",
      " \n",
      "bed lies in its possession of proprietary knowledgenot encompassed in general knowledge repositories.。Con-sequently, we employ Reinforcement Learning (RL) forknowledge discovery, and co-training to align the LanguageModel (LLM) and RL in comprehending the environment.4.。A Co-training Framework: OpenPALConsidering the training in the context of open-ended learn-ing is extensive data-thirsty, we first introduce two criticalengineering designs to enhance training efficiency.。Specif-ically, OpenPAL incorporates a distributed RL frameworkinspired by AlphaStar (Vinyals et al., 2019) with modi-fications, resulting in the formation of the Actor-League-Learner architecture.。In this architecture, the League is re-sponsible for distributing rollout tasks to a cluster of Actors(CPU nodes) for data collection and evaluation, while opti-mization tasks are delegated to the Learner (GPU node) forpolicy updates.。This distributed approach significantly en-hances rollout throughput, thereby improving overall train-ing efficiency.。Another efficiency challenge stems from theiterative development of Contra.. \n",
      "the piece 512\n",
      " \n",
      "knowledge discovery, and co-training to align the LanguageModel (LLM) and RL in comprehending the environment.4.。A Co-training Framework: OpenPALConsidering the training in the context of open-ended learn-ing is extensive data-thirsty, we first introduce two criticalengineering designs to enhance training efficiency.。Specif-ically, OpenPAL incorporates a distributed RL frameworkinspired by AlphaStar (Vinyals et al., 2019) with modi-fications, resulting in the formation of the Actor-League-Learner architecture.。In this architecture, the League is re-sponsible for distributing rollout tasks to a cluster of Actors(CPU nodes) for data collection and evaluation, while opti-mization tasks are delegated to the Learner (GPU node) forpolicy updates.。This distributed approach significantly en-hances rollout throughput, thereby improving overall train-ing efficiency.。Another efficiency challenge stems from theiterative development of Contra.。During the period of ourresearch, Contra’s environmental attributes continuouslychange as the programming development.. \n",
      "the piece 513\n",
      " \n",
      "in the context of open-ended learn-ing is extensive data-thirsty, we first introduce two criticalengineering designs to enhance training efficiency.。Specif-ically, OpenPAL incorporates a distributed RL frameworkinspired by AlphaStar (Vinyals et al., 2019) with modi-fications, resulting in the formation of the Actor-League-Learner architecture.。In this architecture, the League is re-sponsible for distributing rollout tasks to a cluster of Actors(CPU nodes) for data collection and evaluation, while opti-mization tasks are delegated to the Learner (GPU node) forpolicy updates.。This distributed approach significantly en-hances rollout throughput, thereby improving overall train-ing efficiency.。Another efficiency challenge stems from theiterative development of Contra.。During the period of ourresearch, Contra’s environmental attributes continuouslychange as the programming development.。Thus, policyretraining would be necessary if there is no explicit inter-vention.. \n",
      "the piece 514\n",
      " \n",
      "workinspired by AlphaStar (Vinyals et al., 2019) with modi-fications, resulting in the formation of the Actor-League-Learner architecture.。In this architecture, the League is re-sponsible for distributing rollout tasks to a cluster of Actors(CPU nodes) for data collection and evaluation, while opti-mization tasks are delegated to the Learner (GPU node) forpolicy updates.。This distributed approach significantly en-hances rollout throughput, thereby improving overall train-ing efficiency.。Another efficiency challenge stems from theiterative development of Contra.。During the period of ourresearch, Contra’s environmental attributes continuouslychange as the programming development.。Thus, policyretraining would be necessary if there is no explicit inter-vention.。To reduce such an extra computation burden, weemploy surgery (Berner et al., 2019) to retain learned skillsat the lowest training cost, enabling adaptation to a changingobservation/goal space while ensuring compatibility withnetwork inputs.. \n",
      "the piece 515\n",
      " \n",
      "buting rollout tasks to a cluster of Actors(CPU nodes) for data collection and evaluation, while opti-mization tasks are delegated to the Learner (GPU node) forpolicy updates.。This distributed approach significantly en-hances rollout throughput, thereby improving overall train-ing efficiency.。Another efficiency challenge stems from theiterative development of Contra.。During the period of ourresearch, Contra’s environmental attributes continuouslychange as the programming development.。Thus, policyretraining would be necessary if there is no explicit inter-vention.。To reduce such an extra computation burden, weemploy surgery (Berner et al., 2019) to retain learned skillsat the lowest training cost, enabling adaptation to a changingobservation/goal space while ensuring compatibility withnetwork inputs.。Detailed information on the distributed RLframework can be found in Appendix F, and version changesare listed in Table 8.. \n",
      "the piece 516\n",
      " \n",
      "roughput, thereby improving overall train-ing efficiency.。Another efficiency challenge stems from theiterative development of Contra.。During the period of ourresearch, Contra’s environmental attributes continuouslychange as the programming development.。Thus, policyretraining would be necessary if there is no explicit inter-vention.。To reduce such an extra computation burden, weemploy surgery (Berner et al., 2019) to retain learned skillsat the lowest training cost, enabling adaptation to a changingobservation/goal space while ensuring compatibility withnetwork inputs.。Detailed information on the distributed RLframework can be found in Appendix F, and version changesare listed in Table 8.。In the following content, we will in-troduce OpenPAL in two stages, including the independenttraining at stage I (Section 4.1 ∼4.3) and the co-training atstage II (Section 4.4).4.1.. \n",
      "the piece 517\n",
      " \n",
      "ent of Contra.。During the period of ourresearch, Contra’s environmental attributes continuouslychange as the programming development.。Thus, policyretraining would be necessary if there is no explicit inter-vention.。To reduce such an extra computation burden, weemploy surgery (Berner et al., 2019) to retain learned skillsat the lowest training cost, enabling adaptation to a changingobservation/goal space while ensuring compatibility withnetwork inputs.。Detailed information on the distributed RLframework can be found in Appendix F, and version changesare listed in Table 8.。In the following content, we will in-troduce OpenPAL in two stages, including the independenttraining at stage I (Section 4.1 ∼4.3) and the co-training atstage II (Section 4.4).4.1.。Exploring Basic Skills via Non-goal RLIn the realm of GCRL, the prevalent approach involvescurriculum learning a goal-conditioned policy from scratch,learning goal execution while exploring goals.. \n",
      "the piece 518\n",
      " \n",
      "butes continuouslychange as the programming development.。Thus, policyretraining would be necessary if there is no explicit inter-vention.。To reduce such an extra computation burden, weemploy surgery (Berner et al., 2019) to retain learned skillsat the lowest training cost, enabling adaptation to a changingobservation/goal space while ensuring compatibility withnetwork inputs.。Detailed information on the distributed RLframework can be found in Appendix F, and version changesare listed in Table 8.。In the following content, we will in-troduce OpenPAL in two stages, including the independenttraining at stage I (Section 4.1 ∼4.3) and the co-training atstage II (Section 4.4).4.1.。Exploring Basic Skills via Non-goal RLIn the realm of GCRL, the prevalent approach involvescurriculum learning a goal-conditioned policy from scratch,learning goal execution while exploring goals.。However,it maybe inefficient for an agent to explore the whole goalspace when there is a lack of prior knowledge of the goalspace.. \n",
      "the piece 519\n",
      " \n",
      "it inter-vention.。To reduce such an extra computation burden, weemploy surgery (Berner et al., 2019) to retain learned skillsat the lowest training cost, enabling adaptation to a changingobservation/goal space while ensuring compatibility withnetwork inputs.。Detailed information on the distributed RLframework can be found in Appendix F, and version changesare listed in Table 8.。In the following content, we will in-troduce OpenPAL in two stages, including the independenttraining at stage I (Section 4.1 ∼4.3) and the co-training atstage II (Section 4.4).4.1.。Exploring Basic Skills via Non-goal RLIn the realm of GCRL, the prevalent approach involvescurriculum learning a goal-conditioned policy from scratch,learning goal execution while exploring goals.。However,it maybe inefficient for an agent to explore the whole goalspace when there is a lack of prior knowledge of the goalspace.。Thus, we opt for leveraging non-goal RL for basicskill learning before goal-conditioned learning.. \n",
      "the piece 520\n",
      " \n",
      "rner et al., 2019) to retain learned skillsat the lowest training cost, enabling adaptation to a changingobservation/goal space while ensuring compatibility withnetwork inputs.。Detailed information on the distributed RLframework can be found in Appendix F, and version changesare listed in Table 8.。In the following content, we will in-troduce OpenPAL in two stages, including the independenttraining at stage I (Section 4.1 ∼4.3) and the co-training atstage II (Section 4.4).4.1.。Exploring Basic Skills via Non-goal RLIn the realm of GCRL, the prevalent approach involvescurriculum learning a goal-conditioned policy from scratch,learning goal execution while exploring goals.。However,it maybe inefficient for an agent to explore the whole goalspace when there is a lack of prior knowledge of the goalspace.。Thus, we opt for leveraging non-goal RL for basicskill learning before goal-conditioned learning.。For theimplementation, we employ Proximal Policy Optimization(PPO) (Schulman et al., 2017) with fine-grained rewardshaping asr(s, a) =λ1rb(s, a) +λ2roa(s, a), (2)where rbfor skill learning which focuses on targeting theagent towards wining and surviving as long as possible, is alinear combination of diverse behavior factors, roaencour-ages the agent to avoid obstacles due to the agent is notsensitive to obstacles when navigating, and λ1,λ2the fac-tors to weight the contribution of each item.. \n",
      "the piece 521\n",
      " \n",
      "in Appendix F, and version changesare listed in Table 8.。In the following content, we will in-troduce OpenPAL in two stages, including the independenttraining at stage I (Section 4.1 ∼4.3) and the co-training atstage II (Section 4.4).4.1.。Exploring Basic Skills via Non-goal RLIn the realm of GCRL, the prevalent approach involvescurriculum learning a goal-conditioned policy from scratch,learning goal execution while exploring goals.。However,it maybe inefficient for an agent to explore the whole goalspace when there is a lack of prior knowledge of the goalspace.。Thus, we opt for leveraging non-goal RL for basicskill learning before goal-conditioned learning.。For theimplementation, we employ Proximal Policy Optimization(PPO) (Schulman et al., 2017) with fine-grained rewardshaping asr(s, a) =λ1rb(s, a) +λ2roa(s, a), (2)where rbfor skill learning which focuses on targeting theagent towards wining and surviving as long as possible, is alinear combination of diverse behavior factors, roaencour-ages the agent to avoid obstacles due to the agent is notsensitive to obstacles when navigating, and λ1,λ2the fac-tors to weight the contribution of each item.。The details ofthe reward construction are included in Table 5, Appendix A.Then, the value function for estimatingPTl=tγl−tr(sl, al)is implemented as a multi-head network and shares the back-bone of policy, i.e., V(st) =λ1Vb(st)+λ2Voa(st), whereVb(st)andVoa(st)approximatePTl=tγl−trb(sl, al)andPTl=tγl−troa(sl, al), respectively.4.2.. \n",
      "the piece 522\n",
      " \n",
      ", including the independenttraining at stage I (Section 4.1 ∼4.3) and the co-training atstage II (Section 4.4).4.1.。Exploring Basic Skills via Non-goal RLIn the realm of GCRL, the prevalent approach involvescurriculum learning a goal-conditioned policy from scratch,learning goal execution while exploring goals.。However,it maybe inefficient for an agent to explore the whole goalspace when there is a lack of prior knowledge of the goalspace.。Thus, we opt for leveraging non-goal RL for basicskill learning before goal-conditioned learning.。For theimplementation, we employ Proximal Policy Optimization(PPO) (Schulman et al., 2017) with fine-grained rewardshaping asr(s, a) =λ1rb(s, a) +λ2roa(s, a), (2)where rbfor skill learning which focuses on targeting theagent towards wining and surviving as long as possible, is alinear combination of diverse behavior factors, roaencour-ages the agent to avoid obstacles due to the agent is notsensitive to obstacles when navigating, and λ1,λ2the fac-tors to weight the contribution of each item.。The details ofthe reward construction are included in Table 5, Appendix A.Then, the value function for estimatingPTl=tγl−tr(sl, al)is implemented as a multi-head network and shares the back-bone of policy, i.e., V(st) =λ1Vb(st)+λ2Voa(st), whereVb(st)andVoa(st)approximatePTl=tγl−trb(sl, al)andPTl=tγl−troa(sl, al), respectively.4.2.。Learning a Goal-conditioned PolicyWe construct the goal space using various state attributeswhich can be determined and effected by interaction.. \n",
      "the piece 523\n",
      " \n",
      "valent approach involvescurriculum learning a goal-conditioned policy from scratch,learning goal execution while exploring goals.。However,it maybe inefficient for an agent to explore the whole goalspace when there is a lack of prior knowledge of the goalspace.。Thus, we opt for leveraging non-goal RL for basicskill learning before goal-conditioned learning.。For theimplementation, we employ Proximal Policy Optimization(PPO) (Schulman et al., 2017) with fine-grained rewardshaping asr(s, a) =λ1rb(s, a) +λ2roa(s, a), (2)where rbfor skill learning which focuses on targeting theagent towards wining and surviving as long as possible, is alinear combination of diverse behavior factors, roaencour-ages the agent to avoid obstacles due to the agent is notsensitive to obstacles when navigating, and λ1,λ2the fac-tors to weight the contribution of each item.。The details ofthe reward construction are included in Table 5, Appendix A.Then, the value function for estimatingPTl=tγl−tr(sl, al)is implemented as a multi-head network and shares the back-bone of policy, i.e., V(st) =λ1Vb(st)+λ2Voa(st), whereVb(st)andVoa(st)approximatePTl=tγl−trb(sl, al)andPTl=tγl−troa(sl, al), respectively.4.2.。Learning a Goal-conditioned PolicyWe construct the goal space using various state attributeswhich can be determined and effected by interaction.。Indetail, they are (1) agent private states that can be directlychanged by the agent or other players, such as firing, walk-ing, etc.. \n",
      "the piece 524\n",
      " \n",
      "pace when there is a lack of prior knowledge of the goalspace.。Thus, we opt for leveraging non-goal RL for basicskill learning before goal-conditioned learning.。For theimplementation, we employ Proximal Policy Optimization(PPO) (Schulman et al., 2017) with fine-grained rewardshaping asr(s, a) =λ1rb(s, a) +λ2roa(s, a), (2)where rbfor skill learning which focuses on targeting theagent towards wining and surviving as long as possible, is alinear combination of diverse behavior factors, roaencour-ages the agent to avoid obstacles due to the agent is notsensitive to obstacles when navigating, and λ1,λ2the fac-tors to weight the contribution of each item.。The details ofthe reward construction are included in Table 5, Appendix A.Then, the value function for estimatingPTl=tγl−tr(sl, al)is implemented as a multi-head network and shares the back-bone of policy, i.e., V(st) =λ1Vb(st)+λ2Voa(st), whereVb(st)andVoa(st)approximatePTl=tγl−trb(sl, al)andPTl=tγl−troa(sl, al), respectively.4.2.。Learning a Goal-conditioned PolicyWe construct the goal space using various state attributeswhich can be determined and effected by interaction.。Indetail, they are (1) agent private states that can be directlychanged by the agent or other players, such as firing, walk-ing, etc.。(2) enemies states that can be effected throughthe agent interactions, such as knock down an enemies; andBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(3)teammates states that can be effected by the interactionbetween the agent and its teammates.. \n",
      "the piece 525\n",
      " \n",
      "e goal-conditioned learning.。For theimplementation, we employ Proximal Policy Optimization(PPO) (Schulman et al., 2017) with fine-grained rewardshaping asr(s, a) =λ1rb(s, a) +λ2roa(s, a), (2)where rbfor skill learning which focuses on targeting theagent towards wining and surviving as long as possible, is alinear combination of diverse behavior factors, roaencour-ages the agent to avoid obstacles due to the agent is notsensitive to obstacles when navigating, and λ1,λ2the fac-tors to weight the contribution of each item.。The details ofthe reward construction are included in Table 5, Appendix A.Then, the value function for estimatingPTl=tγl−tr(sl, al)is implemented as a multi-head network and shares the back-bone of policy, i.e., V(st) =λ1Vb(st)+λ2Voa(st), whereVb(st)andVoa(st)approximatePTl=tγl−trb(sl, al)andPTl=tγl−troa(sl, al), respectively.4.2.。Learning a Goal-conditioned PolicyWe construct the goal space using various state attributeswhich can be determined and effected by interaction.。Indetail, they are (1) agent private states that can be directlychanged by the agent or other players, such as firing, walk-ing, etc.。(2) enemies states that can be effected throughthe agent interactions, such as knock down an enemies; andBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(3)teammates states that can be effected by the interactionbetween the agent and its teammates.。We summarize themin Table 4.. \n",
      "the piece 526\n",
      " \n",
      "hulman et al., 2017) with fine-grained rewardshaping asr(s, a) =λ1rb(s, a) +λ2roa(s, a), (2)where rbfor skill learning which focuses on targeting theagent towards wining and surviving as long as possible, is alinear combination of diverse behavior factors, roaencour-ages the agent to avoid obstacles due to the agent is notsensitive to obstacles when navigating, and λ1,λ2the fac-tors to weight the contribution of each item.。The details ofthe reward construction are included in Table 5, Appendix A.Then, the value function for estimatingPTl=tγl−tr(sl, al)is implemented as a multi-head network and shares the back-bone of policy, i.e., V(st) =λ1Vb(st)+λ2Voa(st), whereVb(st)andVoa(st)approximatePTl=tγl−trb(sl, al)andPTl=tγl−troa(sl, al), respectively.4.2.。Learning a Goal-conditioned PolicyWe construct the goal space using various state attributeswhich can be determined and effected by interaction.。Indetail, they are (1) agent private states that can be directlychanged by the agent or other players, such as firing, walk-ing, etc.。(2) enemies states that can be effected throughthe agent interactions, such as knock down an enemies; andBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(3)teammates states that can be effected by the interactionbetween the agent and its teammates.。We summarize themin Table 4.。With the above consideration, we further modeleach attribute as a sub-goal space Giwith multiple candi-dates that can be expressed as a set of normalized indices{j|Gi||j= 0, .. \n",
      "the piece 527\n",
      " \n",
      " A.Then, the value function for estimatingPTl=tγl−tr(sl, al)is implemented as a multi-head network and shares the back-bone of policy, i.e., V(st) =λ1Vb(st)+λ2Voa(st), whereVb(st)andVoa(st)approximatePTl=tγl−trb(sl, al)andPTl=tγl−troa(sl, al), respectively.4.2.。Learning a Goal-conditioned PolicyWe construct the goal space using various state attributeswhich can be determined and effected by interaction.。Indetail, they are (1) agent private states that can be directlychanged by the agent or other players, such as firing, walk-ing, etc.。(2) enemies states that can be effected throughthe agent interactions, such as knock down an enemies; andBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(3)teammates states that can be effected by the interactionbetween the agent and its teammates.。We summarize themin Table 4.。With the above consideration, we further modeleach attribute as a sub-goal space Giwith multiple candi-dates that can be expressed as a set of normalized indices{j|Gi||j= 0, .。.. \n",
      "the piece 528\n",
      " \n",
      "ous state attributeswhich can be determined and effected by interaction.。Indetail, they are (1) agent private states that can be directlychanged by the agent or other players, such as firing, walk-ing, etc.。(2) enemies states that can be effected throughthe agent interactions, such as knock down an enemies; andBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(3)teammates states that can be effected by the interactionbetween the agent and its teammates.。We summarize themin Table 4.。With the above consideration, we further modeleach attribute as a sub-goal space Giwith multiple candi-dates that can be expressed as a set of normalized indices{j|Gi||j= 0, .。.。.. \n",
      "the piece 529\n",
      " \n",
      "y the agent or other players, such as firing, walk-ing, etc.。(2) enemies states that can be effected throughthe agent interactions, such as knock down an enemies; andBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(3)teammates states that can be effected by the interactionbetween the agent and its teammates.。We summarize themin Table 4.。With the above consideration, we further modeleach attribute as a sub-goal space Giwith multiple candi-dates that can be expressed as a set of normalized indices{j|Gi||j= 0, .。.。.。,|Gi|}, where 0 an invalid attribute valueindicates the corresponding attribute not be selected.. \n",
      "the piece 530\n",
      " \n",
      "h as knock down an enemies; andBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(3)teammates states that can be effected by the interactionbetween the agent and its teammates.。We summarize themin Table 4.。With the above consideration, we further modeleach attribute as a sub-goal space Giwith multiple candi-dates that can be expressed as a set of normalized indices{j|Gi||j= 0, .。.。.。,|Gi|}, where 0 an invalid attribute valueindicates the corresponding attribute not be selected.。Forthe goal space, there are 68 sub-goal spaces that shape it asG= Π68i=1Gi.. \n",
      "the piece 531\n",
      " \n",
      "each attribute as a sub-goal space Giwith multiple candi-dates that can be expressed as a set of normalized indices{j|Gi||j= 0, .。.。.。,|Gi|}, where 0 an invalid attribute valueindicates the corresponding attribute not be selected.。Forthe goal space, there are 68 sub-goal spaces that shape it asG= Π68i=1Gi.。Obviously, gwhich comprises of more validsub-goals, the more difficult to complete for the policy.Open-ended Goal Generation.。Among existingGCRL research, hindsight goal relabelling and genera-tion (Andrychowicz et al., 2017; Ren et al., 2019) areeffective goal-conditioned learning methods that advantagefrom a free of goal prior, compared to explicit curriculum.However, there is a limitation of in-distribution goalexploration (Bai et al., 2019), i.e., policy learning andgoal exploration shares the same training dataset, whichis inefficient in exploration as the range of goals arelimited by the scale of samples.. \n",
      "the piece 532\n",
      " \n",
      "pace Giwith multiple candi-dates that can be expressed as a set of normalized indices{j|Gi||j= 0, .。.。.。,|Gi|}, where 0 an invalid attribute valueindicates the corresponding attribute not be selected.。Forthe goal space, there are 68 sub-goal spaces that shape it asG= Π68i=1Gi.。Obviously, gwhich comprises of more validsub-goals, the more difficult to complete for the policy.Open-ended Goal Generation.。Among existingGCRL research, hindsight goal relabelling and genera-tion (Andrychowicz et al., 2017; Ren et al., 2019) areeffective goal-conditioned learning methods that advantagefrom a free of goal prior, compared to explicit curriculum.However, there is a limitation of in-distribution goalexploration (Bai et al., 2019), i.e., policy learning andgoal exploration shares the same training dataset, whichis inefficient in exploration as the range of goals arelimited by the scale of samples.。Comparatively, if wecan model the goal distribution, we can not only achievedata efficiency akin to that of hindsight goal generation,but also progressively attain an open-ended goal spaceby adjusting the goal distribution.. \n",
      "the piece 533\n",
      " \n",
      "ribute not be selected.。Forthe goal space, there are 68 sub-goal spaces that shape it asG= Π68i=1Gi.。Obviously, gwhich comprises of more validsub-goals, the more difficult to complete for the policy.Open-ended Goal Generation.。Among existingGCRL research, hindsight goal relabelling and genera-tion (Andrychowicz et al., 2017; Ren et al., 2019) areeffective goal-conditioned learning methods that advantagefrom a free of goal prior, compared to explicit curriculum.However, there is a limitation of in-distribution goalexploration (Bai et al., 2019), i.e., policy learning andgoal exploration shares the same training dataset, whichis inefficient in exploration as the range of goals arelimited by the scale of samples.。Comparatively, if wecan model the goal distribution, we can not only achievedata efficiency akin to that of hindsight goal generation,but also progressively attain an open-ended goal spaceby adjusting the goal distribution.。Therefore, we train aneural-based goal generator Gopover a dataset of trajectorysegments Dτ={τ}explored by the well-trained policy π⋆from Section 4.1 as it is trained for exploring basic skills.We assume that a goal gcorresponding to a given initialstatescan be represented by a 3-tuple x= (s,∆t,∆V),where ∆tthe time slot required to achieve gstarting froms, and ∆Va vector of state values from stogwith theconsideration of representation.. \n",
      "the piece 534\n",
      " \n",
      "ute not be selected.。Forthe goal space, there are 68 sub-goal spaces that shape it asG= Π68i=1Gi.。Obviously, gwhich comprises of more validsub-goals, the more difficult to complete for the policy.Open-ended Goal Generation.。Among existingGCRL research, hindsight goal relabelling and genera-tion (Andrychowicz et al., 2017; Ren et al., 2019) areeffective goal-conditioned learning methods that advantagefrom a free of goal prior, compared to explicit curriculum.However, there is a limitation of in-distribution goalexploration (Bai et al., 2019), i.e., policy learning andgoal exploration shares the same training dataset, whichis inefficient in exploration as the range of goals arelimited by the scale of samples.。Comparatively, if wecan model the goal distribution, we can not only achievedata efficiency akin to that of hindsight goal generation,but also progressively attain an open-ended goal spaceby adjusting the goal distribution.。Therefore, we train aneural-based goal generator Gopover a dataset of trajectorysegments Dτ={τ}explored by the well-trained policy π⋆from Section 4.1 as it is trained for exploring basic skills.We assume that a goal gcorresponding to a given initialstatescan be represented by a 3-tuple x= (s,∆t,∆V),where ∆tthe time slot required to achieve gstarting froms, and ∆Va vector of state values from stogwith theconsideration of representation.。As a result, we train thegoal generator Gopto take input in the form of x, therebyallowing variations in ∆tand∆Vto yield different goalsfor a given state s. For an implementation, we firstlyconstruct a dataset Dx,g={(x, g)}fromDτ, where eachitem in Dx,gsatisfies:s∼UNIFORM (τ:150), (3)g=PROJ(s′), s′∼P(s′) =Vb(s′|s′∈τ−20:)Ps′∈τ−20:Vb(s′),∆t=the time slot from stos′,∆V=\u0002Vb(s), Vb(s′)\u0003.τ:150 the first 150 states of τ,τ−20:the last 20states.. \n",
      "the piece 535\n",
      " \n",
      " not be selected.。Forthe goal space, there are 68 sub-goal spaces that shape it asG= Π68i=1Gi.。Obviously, gwhich comprises of more validsub-goals, the more difficult to complete for the policy.Open-ended Goal Generation.。Among existingGCRL research, hindsight goal relabelling and genera-tion (Andrychowicz et al., 2017; Ren et al., 2019) areeffective goal-conditioned learning methods that advantagefrom a free of goal prior, compared to explicit curriculum.However, there is a limitation of in-distribution goalexploration (Bai et al., 2019), i.e., policy learning andgoal exploration shares the same training dataset, whichis inefficient in exploration as the range of goals arelimited by the scale of samples.。Comparatively, if wecan model the goal distribution, we can not only achievedata efficiency akin to that of hindsight goal generation,but also progressively attain an open-ended goal spaceby adjusting the goal distribution.。Therefore, we train aneural-based goal generator Gopover a dataset of trajectorysegments Dτ={τ}explored by the well-trained policy π⋆from Section 4.1 as it is trained for exploring basic skills.We assume that a goal gcorresponding to a given initialstatescan be represented by a 3-tuple x= (s,∆t,∆V),where ∆tthe time slot required to achieve gstarting froms, and ∆Va vector of state values from stogwith theconsideration of representation.。As a result, we train thegoal generator Gopto take input in the form of x, therebyallowing variations in ∆tand∆Vto yield different goalsfor a given state s. For an implementation, we firstlyconstruct a dataset Dx,g={(x, g)}fromDτ, where eachitem in Dx,gsatisfies:s∼UNIFORM (τ:150), (3)g=PROJ(s′), s′∼P(s′) =Vb(s′|s′∈τ−20:)Ps′∈τ−20:Vb(s′),∆t=the time slot from stos′,∆V=\u0002Vb(s), Vb(s′)\u0003.τ:150 the first 150 states of τ,τ−20:the last 20states.。Then we train Gopwith a MSE loss asminGopEDx,g[∥Gop(x)−g∥2].. \n",
      "the piece 536\n",
      " \n",
      "iously, gwhich comprises of more validsub-goals, the more difficult to complete for the policy.Open-ended Goal Generation.。Among existingGCRL research, hindsight goal relabelling and genera-tion (Andrychowicz et al., 2017; Ren et al., 2019) areeffective goal-conditioned learning methods that advantagefrom a free of goal prior, compared to explicit curriculum.However, there is a limitation of in-distribution goalexploration (Bai et al., 2019), i.e., policy learning andgoal exploration shares the same training dataset, whichis inefficient in exploration as the range of goals arelimited by the scale of samples.。Comparatively, if wecan model the goal distribution, we can not only achievedata efficiency akin to that of hindsight goal generation,but also progressively attain an open-ended goal spaceby adjusting the goal distribution.。Therefore, we train aneural-based goal generator Gopover a dataset of trajectorysegments Dτ={τ}explored by the well-trained policy π⋆from Section 4.1 as it is trained for exploring basic skills.We assume that a goal gcorresponding to a given initialstatescan be represented by a 3-tuple x= (s,∆t,∆V),where ∆tthe time slot required to achieve gstarting froms, and ∆Va vector of state values from stogwith theconsideration of representation.。As a result, we train thegoal generator Gopto take input in the form of x, therebyallowing variations in ∆tand∆Vto yield different goalsfor a given state s. For an implementation, we firstlyconstruct a dataset Dx,g={(x, g)}fromDτ, where eachitem in Dx,gsatisfies:s∼UNIFORM (τ:150), (3)g=PROJ(s′), s′∼P(s′) =Vb(s′|s′∈τ−20:)Ps′∈τ−20:Vb(s′),∆t=the time slot from stos′,∆V=\u0002Vb(s), Vb(s′)\u0003.τ:150 the first 150 states of τ,τ−20:the last 20states.。Then we train Gopwith a MSE loss asminGopEDx,g[∥Gop(x)−g∥2].。While varying ∆tand∆Vproduces diverse goals, it remains challenging to compre-hensively cover the entire goal space corresponding toa given state.. \n",
      "the piece 537\n",
      " \n",
      "e for the policy.Open-ended Goal Generation.。Among existingGCRL research, hindsight goal relabelling and genera-tion (Andrychowicz et al., 2017; Ren et al., 2019) areeffective goal-conditioned learning methods that advantagefrom a free of goal prior, compared to explicit curriculum.However, there is a limitation of in-distribution goalexploration (Bai et al., 2019), i.e., policy learning andgoal exploration shares the same training dataset, whichis inefficient in exploration as the range of goals arelimited by the scale of samples.。Comparatively, if wecan model the goal distribution, we can not only achievedata efficiency akin to that of hindsight goal generation,but also progressively attain an open-ended goal spaceby adjusting the goal distribution.。Therefore, we train aneural-based goal generator Gopover a dataset of trajectorysegments Dτ={τ}explored by the well-trained policy π⋆from Section 4.1 as it is trained for exploring basic skills.We assume that a goal gcorresponding to a given initialstatescan be represented by a 3-tuple x= (s,∆t,∆V),where ∆tthe time slot required to achieve gstarting froms, and ∆Va vector of state values from stogwith theconsideration of representation.。As a result, we train thegoal generator Gopto take input in the form of x, therebyallowing variations in ∆tand∆Vto yield different goalsfor a given state s. For an implementation, we firstlyconstruct a dataset Dx,g={(x, g)}fromDτ, where eachitem in Dx,gsatisfies:s∼UNIFORM (τ:150), (3)g=PROJ(s′), s′∼P(s′) =Vb(s′|s′∈τ−20:)Ps′∈τ−20:Vb(s′),∆t=the time slot from stos′,∆V=\u0002Vb(s), Vb(s′)\u0003.τ:150 the first 150 states of τ,τ−20:the last 20states.。Then we train Gopwith a MSE loss asminGopEDx,g[∥Gop(x)−g∥2].。While varying ∆tand∆Vproduces diverse goals, it remains challenging to compre-hensively cover the entire goal space corresponding toa given state.。As a supplement, we propose integratinggoal generation with a uniform sampler, denoted as Grnd,which randomly samples goals from the goal space Gus-ingGrnd= Π68i=1UNIFORM (Gi).. \n",
      "the piece 538\n",
      " \n",
      "icz et al., 2017; Ren et al., 2019) areeffective goal-conditioned learning methods that advantagefrom a free of goal prior, compared to explicit curriculum.However, there is a limitation of in-distribution goalexploration (Bai et al., 2019), i.e., policy learning andgoal exploration shares the same training dataset, whichis inefficient in exploration as the range of goals arelimited by the scale of samples.。Comparatively, if wecan model the goal distribution, we can not only achievedata efficiency akin to that of hindsight goal generation,but also progressively attain an open-ended goal spaceby adjusting the goal distribution.。Therefore, we train aneural-based goal generator Gopover a dataset of trajectorysegments Dτ={τ}explored by the well-trained policy π⋆from Section 4.1 as it is trained for exploring basic skills.We assume that a goal gcorresponding to a given initialstatescan be represented by a 3-tuple x= (s,∆t,∆V),where ∆tthe time slot required to achieve gstarting froms, and ∆Va vector of state values from stogwith theconsideration of representation.。As a result, we train thegoal generator Gopto take input in the form of x, therebyallowing variations in ∆tand∆Vto yield different goalsfor a given state s. For an implementation, we firstlyconstruct a dataset Dx,g={(x, g)}fromDτ, where eachitem in Dx,gsatisfies:s∼UNIFORM (τ:150), (3)g=PROJ(s′), s′∼P(s′) =Vb(s′|s′∈τ−20:)Ps′∈τ−20:Vb(s′),∆t=the time slot from stos′,∆V=\u0002Vb(s), Vb(s′)\u0003.τ:150 the first 150 states of τ,τ−20:the last 20states.。Then we train Gopwith a MSE loss asminGopEDx,g[∥Gop(x)−g∥2].。While varying ∆tand∆Vproduces diverse goals, it remains challenging to compre-hensively cover the entire goal space corresponding toa given state.。As a supplement, we propose integratinggoal generation with a uniform sampler, denoted as Grnd,which randomly samples goals from the goal space Gus-ingGrnd= Π68i=1UNIFORM (Gi).。This results in a goalgeneration g∼Grnd∪Gop.Intrinsic Reward Shaping.. \n",
      "the piece 539\n",
      " \n",
      "ficiency akin to that of hindsight goal generation,but also progressively attain an open-ended goal spaceby adjusting the goal distribution.。Therefore, we train aneural-based goal generator Gopover a dataset of trajectorysegments Dτ={τ}explored by the well-trained policy π⋆from Section 4.1 as it is trained for exploring basic skills.We assume that a goal gcorresponding to a given initialstatescan be represented by a 3-tuple x= (s,∆t,∆V),where ∆tthe time slot required to achieve gstarting froms, and ∆Va vector of state values from stogwith theconsideration of representation.。As a result, we train thegoal generator Gopto take input in the form of x, therebyallowing variations in ∆tand∆Vto yield different goalsfor a given state s. For an implementation, we firstlyconstruct a dataset Dx,g={(x, g)}fromDτ, where eachitem in Dx,gsatisfies:s∼UNIFORM (τ:150), (3)g=PROJ(s′), s′∼P(s′) =Vb(s′|s′∈τ−20:)Ps′∈τ−20:Vb(s′),∆t=the time slot from stos′,∆V=\u0002Vb(s), Vb(s′)\u0003.τ:150 the first 150 states of τ,τ−20:the last 20states.。Then we train Gopwith a MSE loss asminGopEDx,g[∥Gop(x)−g∥2].。While varying ∆tand∆Vproduces diverse goals, it remains challenging to compre-hensively cover the entire goal space corresponding toa given state.。As a supplement, we propose integratinggoal generation with a uniform sampler, denoted as Grnd,which randomly samples goals from the goal space Gus-ingGrnd= Π68i=1UNIFORM (Gi).。This results in a goalgeneration g∼Grnd∪Gop.Intrinsic Reward Shaping.。As introduced in the afore-mentioned, a critical challenge hinders the goal comple-tion is sparse rewarding.. \n",
      "the piece 540\n",
      " \n",
      "ents Dτ={τ}explored by the well-trained policy π⋆from Section 4.1 as it is trained for exploring basic skills.We assume that a goal gcorresponding to a given initialstatescan be represented by a 3-tuple x= (s,∆t,∆V),where ∆tthe time slot required to achieve gstarting froms, and ∆Va vector of state values from stogwith theconsideration of representation.。As a result, we train thegoal generator Gopto take input in the form of x, therebyallowing variations in ∆tand∆Vto yield different goalsfor a given state s. For an implementation, we firstlyconstruct a dataset Dx,g={(x, g)}fromDτ, where eachitem in Dx,gsatisfies:s∼UNIFORM (τ:150), (3)g=PROJ(s′), s′∼P(s′) =Vb(s′|s′∈τ−20:)Ps′∈τ−20:Vb(s′),∆t=the time slot from stos′,∆V=\u0002Vb(s), Vb(s′)\u0003.τ:150 the first 150 states of τ,τ−20:the last 20states.。Then we train Gopwith a MSE loss asminGopEDx,g[∥Gop(x)−g∥2].。While varying ∆tand∆Vproduces diverse goals, it remains challenging to compre-hensively cover the entire goal space corresponding toa given state.。As a supplement, we propose integratinggoal generation with a uniform sampler, denoted as Grnd,which randomly samples goals from the goal space Gus-ingGrnd= Π68i=1UNIFORM (Gi).。This results in a goalgeneration g∼Grnd∪Gop.Intrinsic Reward Shaping.。As introduced in the afore-mentioned, a critical challenge hinders the goal comple-tion is sparse rewarding.。To mitigate this, we extend thereward function in Equation (2) with an intrinsic rewardrg(st, at, g)that evaluates the degree of goal completion.Equation (4) shows the calculation of rg(st, at, g)as theEuclidean norm difference between two consecutive statesand a goal asrg(st, at, g) =∥g−PROJ(st−1)∥p−∥g−PROJ(st)∥p,(4)where ∥ · ∥pindicates the p-norm.. \n",
      "the piece 541\n",
      " \n",
      "owing variations in ∆tand∆Vto yield different goalsfor a given state s. For an implementation, we firstlyconstruct a dataset Dx,g={(x, g)}fromDτ, where eachitem in Dx,gsatisfies:s∼UNIFORM (τ:150), (3)g=PROJ(s′), s′∼P(s′) =Vb(s′|s′∈τ−20:)Ps′∈τ−20:Vb(s′),∆t=the time slot from stos′,∆V=\u0002Vb(s), Vb(s′)\u0003.τ:150 the first 150 states of τ,τ−20:the last 20states.。Then we train Gopwith a MSE loss asminGopEDx,g[∥Gop(x)−g∥2].。While varying ∆tand∆Vproduces diverse goals, it remains challenging to compre-hensively cover the entire goal space corresponding toa given state.。As a supplement, we propose integratinggoal generation with a uniform sampler, denoted as Grnd,which randomly samples goals from the goal space Gus-ingGrnd= Π68i=1UNIFORM (Gi).。This results in a goalgeneration g∼Grnd∪Gop.Intrinsic Reward Shaping.。As introduced in the afore-mentioned, a critical challenge hinders the goal comple-tion is sparse rewarding.。To mitigate this, we extend thereward function in Equation (2) with an intrinsic rewardrg(st, at, g)that evaluates the degree of goal completion.Equation (4) shows the calculation of rg(st, at, g)as theEuclidean norm difference between two consecutive statesand a goal asrg(st, at, g) =∥g−PROJ(st−1)∥p−∥g−PROJ(st)∥p,(4)where ∥ · ∥pindicates the p-norm.。This reward provides adenser reward signal at each time step to the agent about itsproximity to the goal, offering more nuanced informationthan a binary signal indicating whether it has reached thegoal or not.. \n",
      "the piece 542\n",
      " \n",
      "uces diverse goals, it remains challenging to compre-hensively cover the entire goal space corresponding toa given state.。As a supplement, we propose integratinggoal generation with a uniform sampler, denoted as Grnd,which randomly samples goals from the goal space Gus-ingGrnd= Π68i=1UNIFORM (Gi).。This results in a goalgeneration g∼Grnd∪Gop.Intrinsic Reward Shaping.。As introduced in the afore-mentioned, a critical challenge hinders the goal comple-tion is sparse rewarding.。To mitigate this, we extend thereward function in Equation (2) with an intrinsic rewardrg(st, at, g)that evaluates the degree of goal completion.Equation (4) shows the calculation of rg(st, at, g)as theEuclidean norm difference between two consecutive statesand a goal asrg(st, at, g) =∥g−PROJ(st−1)∥p−∥g−PROJ(st)∥p,(4)where ∥ · ∥pindicates the p-norm.。This reward provides adenser reward signal at each time step to the agent about itsproximity to the goal, offering more nuanced informationthan a binary signal indicating whether it has reached thegoal or not.。In our current implementation, we set p= 1.Thus, the reward function for GCRL is formulated asr(s, a, g ) =r(s, a) +λ3rg(s, a, g ), (5)where r(s, a)comes from Equation (2).. \n",
      "the piece 543\n",
      " \n",
      " cover the entire goal space corresponding toa given state.。As a supplement, we propose integratinggoal generation with a uniform sampler, denoted as Grnd,which randomly samples goals from the goal space Gus-ingGrnd= Π68i=1UNIFORM (Gi).。This results in a goalgeneration g∼Grnd∪Gop.Intrinsic Reward Shaping.。As introduced in the afore-mentioned, a critical challenge hinders the goal comple-tion is sparse rewarding.。To mitigate this, we extend thereward function in Equation (2) with an intrinsic rewardrg(st, at, g)that evaluates the degree of goal completion.Equation (4) shows the calculation of rg(st, at, g)as theEuclidean norm difference between two consecutive statesand a goal asrg(st, at, g) =∥g−PROJ(st−1)∥p−∥g−PROJ(st)∥p,(4)where ∥ · ∥pindicates the p-norm.。This reward provides adenser reward signal at each time step to the agent about itsproximity to the goal, offering more nuanced informationthan a binary signal indicating whether it has reached thegoal or not.。In our current implementation, we set p= 1.Thus, the reward function for GCRL is formulated asr(s, a, g ) =r(s, a) +λ3rg(s, a, g ), (5)where r(s, a)comes from Equation (2).。And for thevalue function corresponds to Equation (5), we extendthe multi-head V(st)with a new value head Vg(st, g)asV(st, g) =V(s) +λ3Vg(st, g), where Vg(st, g)approxi-matesPTi=tγi−trg(st, at, g).Avoiding Policy Degeneration.. \n",
      "the piece 544\n",
      " \n",
      "s Grnd,which randomly samples goals from the goal space Gus-ingGrnd= Π68i=1UNIFORM (Gi).。This results in a goalgeneration g∼Grnd∪Gop.Intrinsic Reward Shaping.。As introduced in the afore-mentioned, a critical challenge hinders the goal comple-tion is sparse rewarding.。To mitigate this, we extend thereward function in Equation (2) with an intrinsic rewardrg(st, at, g)that evaluates the degree of goal completion.Equation (4) shows the calculation of rg(st, at, g)as theEuclidean norm difference between two consecutive statesand a goal asrg(st, at, g) =∥g−PROJ(st−1)∥p−∥g−PROJ(st)∥p,(4)where ∥ · ∥pindicates the p-norm.。This reward provides adenser reward signal at each time step to the agent about itsproximity to the goal, offering more nuanced informationthan a binary signal indicating whether it has reached thegoal or not.。In our current implementation, we set p= 1.Thus, the reward function for GCRL is formulated asr(s, a, g ) =r(s, a) +λ3rg(s, a, g ), (5)where r(s, a)comes from Equation (2).。And for thevalue function corresponds to Equation (5), we extendthe multi-head V(st)with a new value head Vg(st, g)asV(st, g) =V(s) +λ3Vg(st, g), where Vg(st, g)approxi-matesPTi=tγi−trg(st, at, g).Avoiding Policy Degeneration.。Letπ⋆denote the well-trained policy from the non-goal reinforcement learningstep.. \n",
      "the piece 545\n",
      " \n",
      "e afore-mentioned, a critical challenge hinders the goal comple-tion is sparse rewarding.。To mitigate this, we extend thereward function in Equation (2) with an intrinsic rewardrg(st, at, g)that evaluates the degree of goal completion.Equation (4) shows the calculation of rg(st, at, g)as theEuclidean norm difference between two consecutive statesand a goal asrg(st, at, g) =∥g−PROJ(st−1)∥p−∥g−PROJ(st)∥p,(4)where ∥ · ∥pindicates the p-norm.。This reward provides adenser reward signal at each time step to the agent about itsproximity to the goal, offering more nuanced informationthan a binary signal indicating whether it has reached thegoal or not.。In our current implementation, we set p= 1.Thus, the reward function for GCRL is formulated asr(s, a, g ) =r(s, a) +λ3rg(s, a, g ), (5)where r(s, a)comes from Equation (2).。And for thevalue function corresponds to Equation (5), we extendthe multi-head V(st)with a new value head Vg(st, g)asV(st, g) =V(s) +λ3Vg(st, g), where Vg(st, g)approxi-matesPTi=tγi−trg(st, at, g).Avoiding Policy Degeneration.。Letπ⋆denote the well-trained policy from the non-goal reinforcement learningstep.。However, we have observed a performance degenera-tion on basic skill execution when continuing the trainingof the goal-conditioned policy πg,θstarting from π⋆.. \n",
      "the piece 546\n",
      " \n",
      " sparse rewarding.。To mitigate this, we extend thereward function in Equation (2) with an intrinsic rewardrg(st, at, g)that evaluates the degree of goal completion.Equation (4) shows the calculation of rg(st, at, g)as theEuclidean norm difference between two consecutive statesand a goal asrg(st, at, g) =∥g−PROJ(st−1)∥p−∥g−PROJ(st)∥p,(4)where ∥ · ∥pindicates the p-norm.。This reward provides adenser reward signal at each time step to the agent about itsproximity to the goal, offering more nuanced informationthan a binary signal indicating whether it has reached thegoal or not.。In our current implementation, we set p= 1.Thus, the reward function for GCRL is formulated asr(s, a, g ) =r(s, a) +λ3rg(s, a, g ), (5)where r(s, a)comes from Equation (2).。And for thevalue function corresponds to Equation (5), we extendthe multi-head V(st)with a new value head Vg(st, g)asV(st, g) =V(s) +λ3Vg(st, g), where Vg(st, g)approxi-matesPTi=tγi−trg(st, at, g).Avoiding Policy Degeneration.。Letπ⋆denote the well-trained policy from the non-goal reinforcement learningstep.。However, we have observed a performance degenera-tion on basic skill execution when continuing the trainingof the goal-conditioned policy πg,θstarting from π⋆.。Thisis attributed to two aspects: (1) catastrophic forgetting onthe basic skills as the goal-conditioned learning continues;(2) a change in the input of the policy network from sto(s, g), where gintroduces interference in decision-making,as the policy has not encountered goal inputs during non-goal-conditioned training.. \n",
      "the piece 547\n",
      " \n",
      "t, at, g)that evaluates the degree of goal completion.Equation (4) shows the calculation of rg(st, at, g)as theEuclidean norm difference between two consecutive statesand a goal asrg(st, at, g) =∥g−PROJ(st−1)∥p−∥g−PROJ(st)∥p,(4)where ∥ · ∥pindicates the p-norm.。This reward provides adenser reward signal at each time step to the agent about itsproximity to the goal, offering more nuanced informationthan a binary signal indicating whether it has reached thegoal or not.。In our current implementation, we set p= 1.Thus, the reward function for GCRL is formulated asr(s, a, g ) =r(s, a) +λ3rg(s, a, g ), (5)where r(s, a)comes from Equation (2).。And for thevalue function corresponds to Equation (5), we extendthe multi-head V(st)with a new value head Vg(st, g)asV(st, g) =V(s) +λ3Vg(st, g), where Vg(st, g)approxi-matesPTi=tγi−trg(st, at, g).Avoiding Policy Degeneration.。Letπ⋆denote the well-trained policy from the non-goal reinforcement learningstep.。However, we have observed a performance degenera-tion on basic skill execution when continuing the trainingof the goal-conditioned policy πg,θstarting from π⋆.。Thisis attributed to two aspects: (1) catastrophic forgetting onthe basic skills as the goal-conditioned learning continues;(2) a change in the input of the policy network from sto(s, g), where gintroduces interference in decision-making,as the policy has not encountered goal inputs during non-goal-conditioned training.。To address thes issues, we pro-pose a modification to the goal-conditioned policy learningobjective by introducing a KL-divergence regularizer, andintroduce 20% workers for non-goal policy learning to avoidcatastrophic forgetting.. \n",
      "the piece 548\n",
      " \n",
      " to the goal, offering more nuanced informationthan a binary signal indicating whether it has reached thegoal or not.。In our current implementation, we set p= 1.Thus, the reward function for GCRL is formulated asr(s, a, g ) =r(s, a) +λ3rg(s, a, g ), (5)where r(s, a)comes from Equation (2).。And for thevalue function corresponds to Equation (5), we extendthe multi-head V(st)with a new value head Vg(st, g)asV(st, g) =V(s) +λ3Vg(st, g), where Vg(st, g)approxi-matesPTi=tγi−trg(st, at, g).Avoiding Policy Degeneration.。Letπ⋆denote the well-trained policy from the non-goal reinforcement learningstep.。However, we have observed a performance degenera-tion on basic skill execution when continuing the trainingof the goal-conditioned policy πg,θstarting from π⋆.。Thisis attributed to two aspects: (1) catastrophic forgetting onthe basic skills as the goal-conditioned learning continues;(2) a change in the input of the policy network from sto(s, g), where gintroduces interference in decision-making,as the policy has not encountered goal inputs during non-goal-conditioned training.。To address thes issues, we pro-pose a modification to the goal-conditioned policy learningobjective by introducing a KL-divergence regularizer, andintroduce 20% workers for non-goal policy learning to avoidcatastrophic forgetting.。This regularizer quantifies the dis-tance between π⋆andπg,θwhen πg,θconditioned on g=∅as it is equivalently to non-goal policy:maxθE(s,g)\u0002J(πg,θ)−1g=∅·DKL(π⋆∥πg,θ)\u0003.. \n",
      "the piece 549\n",
      " \n",
      "sr(s, a, g ) =r(s, a) +λ3rg(s, a, g ), (5)where r(s, a)comes from Equation (2).。And for thevalue function corresponds to Equation (5), we extendthe multi-head V(st)with a new value head Vg(st, g)asV(st, g) =V(s) +λ3Vg(st, g), where Vg(st, g)approxi-matesPTi=tγi−trg(st, at, g).Avoiding Policy Degeneration.。Letπ⋆denote the well-trained policy from the non-goal reinforcement learningstep.。However, we have observed a performance degenera-tion on basic skill execution when continuing the trainingof the goal-conditioned policy πg,θstarting from π⋆.。Thisis attributed to two aspects: (1) catastrophic forgetting onthe basic skills as the goal-conditioned learning continues;(2) a change in the input of the policy network from sto(s, g), where gintroduces interference in decision-making,as the policy has not encountered goal inputs during non-goal-conditioned training.。To address thes issues, we pro-pose a modification to the goal-conditioned policy learningobjective by introducing a KL-divergence regularizer, andintroduce 20% workers for non-goal policy learning to avoidcatastrophic forgetting.。This regularizer quantifies the dis-tance between π⋆andπg,θwhen πg,θconditioned on g=∅as it is equivalently to non-goal policy:maxθE(s,g)\u0002J(πg,θ)−1g=∅·DKL(π⋆∥πg,θ)\u0003.。(6)J(πg,θ)the policy loss in PPO, and 1g=∅indicates that theKL-divergence term is only activated when an empty goalinput for πg,θ.. \n",
      "the piece 550\n",
      " \n",
      " value head Vg(st, g)asV(st, g) =V(s) +λ3Vg(st, g), where Vg(st, g)approxi-matesPTi=tγi−trg(st, at, g).Avoiding Policy Degeneration.。Letπ⋆denote the well-trained policy from the non-goal reinforcement learningstep.。However, we have observed a performance degenera-tion on basic skill execution when continuing the trainingof the goal-conditioned policy πg,θstarting from π⋆.。Thisis attributed to two aspects: (1) catastrophic forgetting onthe basic skills as the goal-conditioned learning continues;(2) a change in the input of the policy network from sto(s, g), where gintroduces interference in decision-making,as the policy has not encountered goal inputs during non-goal-conditioned training.。To address thes issues, we pro-pose a modification to the goal-conditioned policy learningobjective by introducing a KL-divergence regularizer, andintroduce 20% workers for non-goal policy learning to avoidcatastrophic forgetting.。This regularizer quantifies the dis-tance between π⋆andπg,θwhen πg,θconditioned on g=∅as it is equivalently to non-goal policy:maxθE(s,g)\u0002J(πg,θ)−1g=∅·DKL(π⋆∥πg,θ)\u0003.。(6)J(πg,θ)the policy loss in PPO, and 1g=∅indicates that theKL-divergence term is only activated when an empty goalinput for πg,θ.。Algorithm 3, Appendix H summarizes thelearning process.. \n",
      "the piece 551\n",
      " \n",
      "ave observed a performance degenera-tion on basic skill execution when continuing the trainingof the goal-conditioned policy πg,θstarting from π⋆.。Thisis attributed to two aspects: (1) catastrophic forgetting onthe basic skills as the goal-conditioned learning continues;(2) a change in the input of the policy network from sto(s, g), where gintroduces interference in decision-making,as the policy has not encountered goal inputs during non-goal-conditioned training.。To address thes issues, we pro-pose a modification to the goal-conditioned policy learningobjective by introducing a KL-divergence regularizer, andintroduce 20% workers for non-goal policy learning to avoidcatastrophic forgetting.。This regularizer quantifies the dis-tance between π⋆andπg,θwhen πg,θconditioned on g=∅as it is equivalently to non-goal policy:maxθE(s,g)\u0002J(πg,θ)−1g=∅·DKL(π⋆∥πg,θ)\u0003.。(6)J(πg,θ)the policy loss in PPO, and 1g=∅indicates that theKL-divergence term is only activated when an empty goalinput for πg,θ.。Algorithm 3, Appendix H summarizes thelearning process.。Furthermore, we observed that occasion-ally sampling experience from π⋆to train πg,θcan alsorelieve the degeneration.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation4.3.. \n",
      "the piece 552\n",
      " \n",
      "he trainingof the goal-conditioned policy πg,θstarting from π⋆.。Thisis attributed to two aspects: (1) catastrophic forgetting onthe basic skills as the goal-conditioned learning continues;(2) a change in the input of the policy network from sto(s, g), where gintroduces interference in decision-making,as the policy has not encountered goal inputs during non-goal-conditioned training.。To address thes issues, we pro-pose a modification to the goal-conditioned policy learningobjective by introducing a KL-divergence regularizer, andintroduce 20% workers for non-goal policy learning to avoidcatastrophic forgetting.。This regularizer quantifies the dis-tance between π⋆andπg,θwhen πg,θconditioned on g=∅as it is equivalently to non-goal policy:maxθE(s,g)\u0002J(πg,θ)−1g=∅·DKL(π⋆∥πg,θ)\u0003.。(6)J(πg,θ)the policy loss in PPO, and 1g=∅indicates that theKL-divergence term is only activated when an empty goalinput for πg,θ.。Algorithm 3, Appendix H summarizes thelearning process.。Furthermore, we observed that occasion-ally sampling experience from π⋆to train πg,θcan alsorelieve the degeneration.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation4.3.。Fine-tuning a LLM-based Goal GeneratorLetIrepresent the set of natural language instructions, andOthe set of abstracted environment states in text.. \n",
      "the piece 553\n",
      " \n",
      "itioned learning continues;(2) a change in the input of the policy network from sto(s, g), where gintroduces interference in decision-making,as the policy has not encountered goal inputs during non-goal-conditioned training.。To address thes issues, we pro-pose a modification to the goal-conditioned policy learningobjective by introducing a KL-divergence regularizer, andintroduce 20% workers for non-goal policy learning to avoidcatastrophic forgetting.。This regularizer quantifies the dis-tance between π⋆andπg,θwhen πg,θconditioned on g=∅as it is equivalently to non-goal policy:maxθE(s,g)\u0002J(πg,θ)−1g=∅·DKL(π⋆∥πg,θ)\u0003.。(6)J(πg,θ)the policy loss in PPO, and 1g=∅indicates that theKL-divergence term is only activated when an empty goalinput for πg,θ.。Algorithm 3, Appendix H summarizes thelearning process.。Furthermore, we observed that occasion-ally sampling experience from π⋆to train πg,θcan alsorelieve the degeneration.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation4.3.。Fine-tuning a LLM-based Goal GeneratorLetIrepresent the set of natural language instructions, andOthe set of abstracted environment states in text.。Our ob-jective is to fine-tune a pre-trained LLM as a goal generator,denoted as Gllm:O×I → G , which means Gllmgeneratesa goal relevant to a given instruction with the considerationof current environment context, i.e., a state abstraction.Dataset Construction.. \n",
      "the piece 554\n",
      " \n",
      "e by introducing a KL-divergence regularizer, andintroduce 20% workers for non-goal policy learning to avoidcatastrophic forgetting.。This regularizer quantifies the dis-tance between π⋆andπg,θwhen πg,θconditioned on g=∅as it is equivalently to non-goal policy:maxθE(s,g)\u0002J(πg,θ)−1g=∅·DKL(π⋆∥πg,θ)\u0003.。(6)J(πg,θ)the policy loss in PPO, and 1g=∅indicates that theKL-divergence term is only activated when an empty goalinput for πg,θ.。Algorithm 3, Appendix H summarizes thelearning process.。Furthermore, we observed that occasion-ally sampling experience from π⋆to train πg,θcan alsorelieve the degeneration.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation4.3.。Fine-tuning a LLM-based Goal GeneratorLetIrepresent the set of natural language instructions, andOthe set of abstracted environment states in text.。Our ob-jective is to fine-tune a pre-trained LLM as a goal generator,denoted as Gllm:O×I → G , which means Gllmgeneratesa goal relevant to a given instruction with the considerationof current environment context, i.e., a state abstraction.Dataset Construction.。To achieve that, we first constructOusing states collected by the πg,θ.. \n",
      "the piece 555\n",
      " \n",
      "valently to non-goal policy:maxθE(s,g)\u0002J(πg,θ)−1g=∅·DKL(π⋆∥πg,θ)\u0003.。(6)J(πg,θ)the policy loss in PPO, and 1g=∅indicates that theKL-divergence term is only activated when an empty goalinput for πg,θ.。Algorithm 3, Appendix H summarizes thelearning process.。Furthermore, we observed that occasion-ally sampling experience from π⋆to train πg,θcan alsorelieve the degeneration.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation4.3.。Fine-tuning a LLM-based Goal GeneratorLetIrepresent the set of natural language instructions, andOthe set of abstracted environment states in text.。Our ob-jective is to fine-tune a pre-trained LLM as a goal generator,denoted as Gllm:O×I → G , which means Gllmgeneratesa goal relevant to a given instruction with the considerationof current environment context, i.e., a state abstraction.Dataset Construction.。To achieve that, we first constructOusing states collected by the πg,θ.。Each abstraction o∈ Oencapsulates essential state features of its correspondingstates, and the extraction rules are outlined in Appendix B.For the creation of I, we leverage various instruction gener-ation to ensure its diversity and scalability, aligning with ouroverarching goal of achieving open-endedness in the instruc-tion space.. \n",
      "the piece 556\n",
      " \n",
      "n an empty goalinput for πg,θ.。Algorithm 3, Appendix H summarizes thelearning process.。Furthermore, we observed that occasion-ally sampling experience from π⋆to train πg,θcan alsorelieve the degeneration.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation4.3.。Fine-tuning a LLM-based Goal GeneratorLetIrepresent the set of natural language instructions, andOthe set of abstracted environment states in text.。Our ob-jective is to fine-tune a pre-trained LLM as a goal generator,denoted as Gllm:O×I → G , which means Gllmgeneratesa goal relevant to a given instruction with the considerationof current environment context, i.e., a state abstraction.Dataset Construction.。To achieve that, we first constructOusing states collected by the πg,θ.。Each abstraction o∈ Oencapsulates essential state features of its correspondingstates, and the extraction rules are outlined in Appendix B.For the creation of I, we leverage various instruction gener-ation to ensure its diversity and scalability, aligning with ouroverarching goal of achieving open-endedness in the instruc-tion space.。Specifically, Iis derived from four types.. \n",
      "the piece 557\n",
      " \n",
      "ampling experience from π⋆to train πg,θcan alsorelieve the degeneration.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation4.3.。Fine-tuning a LLM-based Goal GeneratorLetIrepresent the set of natural language instructions, andOthe set of abstracted environment states in text.。Our ob-jective is to fine-tune a pre-trained LLM as a goal generator,denoted as Gllm:O×I → G , which means Gllmgeneratesa goal relevant to a given instruction with the considerationof current environment context, i.e., a state abstraction.Dataset Construction.。To achieve that, we first constructOusing states collected by the πg,θ.。Each abstraction o∈ Oencapsulates essential state features of its correspondingstates, and the extraction rules are outlined in Appendix B.For the creation of I, we leverage various instruction gener-ation to ensure its diversity and scalability, aligning with ouroverarching goal of achieving open-endedness in the instruc-tion space.。Specifically, Iis derived from four types.。Mostof these are formulated through a tuple of an initial state anda target state/trajectory collected by πg, which aims to alignGllmandπgat environmental comprehension.. \n",
      "the piece 558\n",
      " \n",
      "e degeneration.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation4.3.。Fine-tuning a LLM-based Goal GeneratorLetIrepresent the set of natural language instructions, andOthe set of abstracted environment states in text.。Our ob-jective is to fine-tune a pre-trained LLM as a goal generator,denoted as Gllm:O×I → G , which means Gllmgeneratesa goal relevant to a given instruction with the considerationof current environment context, i.e., a state abstraction.Dataset Construction.。To achieve that, we first constructOusing states collected by the πg,θ.。Each abstraction o∈ Oencapsulates essential state features of its correspondingstates, and the extraction rules are outlined in Appendix B.For the creation of I, we leverage various instruction gener-ation to ensure its diversity and scalability, aligning with ouroverarching goal of achieving open-endedness in the instruc-tion space.。Specifically, Iis derived from four types.。Mostof these are formulated through a tuple of an initial state anda target state/trajectory collected by πg, which aims to alignGllmandπgat environmental comprehension.。Then, weleverage this data and GPT-4 (OpenAI, 2023) to generateappropriate instruction.. \n",
      "the piece 559\n",
      " \n",
      "et of abstracted environment states in text.。Our ob-jective is to fine-tune a pre-trained LLM as a goal generator,denoted as Gllm:O×I → G , which means Gllmgeneratesa goal relevant to a given instruction with the considerationof current environment context, i.e., a state abstraction.Dataset Construction.。To achieve that, we first constructOusing states collected by the πg,θ.。Each abstraction o∈ Oencapsulates essential state features of its correspondingstates, and the extraction rules are outlined in Appendix B.For the creation of I, we leverage various instruction gener-ation to ensure its diversity and scalability, aligning with ouroverarching goal of achieving open-endedness in the instruc-tion space.。Specifically, Iis derived from four types.。Mostof these are formulated through a tuple of an initial state anda target state/trajectory collected by πg, which aims to alignGllmandπgat environmental comprehension.。Then, weleverage this data and GPT-4 (OpenAI, 2023) to generateappropriate instruction.。This instruction aims to direct fromthe specified initial state to the intended target state, andCoT (Wei et al., 2023) is deployed to enhance performance.Specifically, the four types of instruction generation are (1)IH(Human Instructions, HI) : human-annotated instruc-tions; (2) IS(State Instructions, SI) : GPT-4-generatedinstructions by giving a tuple of states (s, s′)where the sthe initial state that sampled from agent trajectories and s′the target state that is manually constructed by modifyingfeatures of the s; (3)IA(Agent Instructions, AI) : GPT-4-generated instructions by giving a pair of (s, τ)where sthe initial state, τthe agent trajectory; and (4) IR(RandomInstructions, RI) : a mixture of the above three instructionsets to form a supplementary dataset.. \n",
      "the piece 560\n",
      " \n",
      "ns Gllmgeneratesa goal relevant to a given instruction with the considerationof current environment context, i.e., a state abstraction.Dataset Construction.。To achieve that, we first constructOusing states collected by the πg,θ.。Each abstraction o∈ Oencapsulates essential state features of its correspondingstates, and the extraction rules are outlined in Appendix B.For the creation of I, we leverage various instruction gener-ation to ensure its diversity and scalability, aligning with ouroverarching goal of achieving open-endedness in the instruc-tion space.。Specifically, Iis derived from four types.。Mostof these are formulated through a tuple of an initial state anda target state/trajectory collected by πg, which aims to alignGllmandπgat environmental comprehension.。Then, weleverage this data and GPT-4 (OpenAI, 2023) to generateappropriate instruction.。This instruction aims to direct fromthe specified initial state to the intended target state, andCoT (Wei et al., 2023) is deployed to enhance performance.Specifically, the four types of instruction generation are (1)IH(Human Instructions, HI) : human-annotated instruc-tions; (2) IS(State Instructions, SI) : GPT-4-generatedinstructions by giving a tuple of states (s, s′)where the sthe initial state that sampled from agent trajectories and s′the target state that is manually constructed by modifyingfeatures of the s; (3)IA(Agent Instructions, AI) : GPT-4-generated instructions by giving a pair of (s, τ)where sthe initial state, τthe agent trajectory; and (4) IR(RandomInstructions, RI) : a mixture of the above three instructionsets to form a supplementary dataset.。By accompanying OwithI, we further construct Dx={(o, ι)|(o, ι)∈ O × I} .Subsequently, we employ GPT-4 to generate appropriategoals ˆGusingDxas labeled data for training Gllm, result-ing in a dataset D={(o, ι, g )|(o, ι, g )∈ O × I × ˆG}.. \n",
      "the piece 561\n",
      " \n",
      " essential state features of its correspondingstates, and the extraction rules are outlined in Appendix B.For the creation of I, we leverage various instruction gener-ation to ensure its diversity and scalability, aligning with ouroverarching goal of achieving open-endedness in the instruc-tion space.。Specifically, Iis derived from four types.。Mostof these are formulated through a tuple of an initial state anda target state/trajectory collected by πg, which aims to alignGllmandπgat environmental comprehension.。Then, weleverage this data and GPT-4 (OpenAI, 2023) to generateappropriate instruction.。This instruction aims to direct fromthe specified initial state to the intended target state, andCoT (Wei et al., 2023) is deployed to enhance performance.Specifically, the four types of instruction generation are (1)IH(Human Instructions, HI) : human-annotated instruc-tions; (2) IS(State Instructions, SI) : GPT-4-generatedinstructions by giving a tuple of states (s, s′)where the sthe initial state that sampled from agent trajectories and s′the target state that is manually constructed by modifyingfeatures of the s; (3)IA(Agent Instructions, AI) : GPT-4-generated instructions by giving a pair of (s, τ)where sthe initial state, τthe agent trajectory; and (4) IR(RandomInstructions, RI) : a mixture of the above three instructionsets to form a supplementary dataset.。By accompanying OwithI, we further construct Dx={(o, ι)|(o, ι)∈ O × I} .Subsequently, we employ GPT-4 to generate appropriategoals ˆGusingDxas labeled data for training Gllm, result-ing in a dataset D={(o, ι, g )|(o, ι, g )∈ O × I × ˆG}.。Toensure that the goals generated by GPT-4 conform to theformat we want, a comprehensive prompt engineering en-deavor was conducted to establish a set of predeterminedrules for GPT-4.. \n",
      "the piece 562\n",
      " \n",
      "rules are outlined in Appendix B.For the creation of I, we leverage various instruction gener-ation to ensure its diversity and scalability, aligning with ouroverarching goal of achieving open-endedness in the instruc-tion space.。Specifically, Iis derived from four types.。Mostof these are formulated through a tuple of an initial state anda target state/trajectory collected by πg, which aims to alignGllmandπgat environmental comprehension.。Then, weleverage this data and GPT-4 (OpenAI, 2023) to generateappropriate instruction.。This instruction aims to direct fromthe specified initial state to the intended target state, andCoT (Wei et al., 2023) is deployed to enhance performance.Specifically, the four types of instruction generation are (1)IH(Human Instructions, HI) : human-annotated instruc-tions; (2) IS(State Instructions, SI) : GPT-4-generatedinstructions by giving a tuple of states (s, s′)where the sthe initial state that sampled from agent trajectories and s′the target state that is manually constructed by modifyingfeatures of the s; (3)IA(Agent Instructions, AI) : GPT-4-generated instructions by giving a pair of (s, τ)where sthe initial state, τthe agent trajectory; and (4) IR(RandomInstructions, RI) : a mixture of the above three instructionsets to form a supplementary dataset.。By accompanying OwithI, we further construct Dx={(o, ι)|(o, ι)∈ O × I} .Subsequently, we employ GPT-4 to generate appropriategoals ˆGusingDxas labeled data for training Gllm, result-ing in a dataset D={(o, ι, g )|(o, ι, g )∈ O × I × ˆG}.。Toensure that the goals generated by GPT-4 conform to theformat we want, a comprehensive prompt engineering en-deavor was conducted to establish a set of predeterminedrules for GPT-4.。The rule-based prompts that guide GPT-4’sresponses are documented in Table 10, with examples ofprompts for generation provided in Table 17.Multi-step Fine-tuning.. \n",
      "the piece 563\n",
      " \n",
      "anda target state/trajectory collected by πg, which aims to alignGllmandπgat environmental comprehension.。Then, weleverage this data and GPT-4 (OpenAI, 2023) to generateappropriate instruction.。This instruction aims to direct fromthe specified initial state to the intended target state, andCoT (Wei et al., 2023) is deployed to enhance performance.Specifically, the four types of instruction generation are (1)IH(Human Instructions, HI) : human-annotated instruc-tions; (2) IS(State Instructions, SI) : GPT-4-generatedinstructions by giving a tuple of states (s, s′)where the sthe initial state that sampled from agent trajectories and s′the target state that is manually constructed by modifyingfeatures of the s; (3)IA(Agent Instructions, AI) : GPT-4-generated instructions by giving a pair of (s, τ)where sthe initial state, τthe agent trajectory; and (4) IR(RandomInstructions, RI) : a mixture of the above three instructionsets to form a supplementary dataset.。By accompanying OwithI, we further construct Dx={(o, ι)|(o, ι)∈ O × I} .Subsequently, we employ GPT-4 to generate appropriategoals ˆGusingDxas labeled data for training Gllm, result-ing in a dataset D={(o, ι, g )|(o, ι, g )∈ O × I × ˆG}.。Toensure that the goals generated by GPT-4 conform to theformat we want, a comprehensive prompt engineering en-deavor was conducted to establish a set of predeterminedrules for GPT-4.。The rule-based prompts that guide GPT-4’sresponses are documented in Table 10, with examples ofprompts for generation provided in Table 17.Multi-step Fine-tuning.。We fine-tune ChatGLM-6B withLoRA (Hu et al., 2021) in three steps, as illustrated inFigure 8.. \n",
      "the piece 564\n",
      " \n",
      ", which aims to alignGllmandπgat environmental comprehension.。Then, weleverage this data and GPT-4 (OpenAI, 2023) to generateappropriate instruction.。This instruction aims to direct fromthe specified initial state to the intended target state, andCoT (Wei et al., 2023) is deployed to enhance performance.Specifically, the four types of instruction generation are (1)IH(Human Instructions, HI) : human-annotated instruc-tions; (2) IS(State Instructions, SI) : GPT-4-generatedinstructions by giving a tuple of states (s, s′)where the sthe initial state that sampled from agent trajectories and s′the target state that is manually constructed by modifyingfeatures of the s; (3)IA(Agent Instructions, AI) : GPT-4-generated instructions by giving a pair of (s, τ)where sthe initial state, τthe agent trajectory; and (4) IR(RandomInstructions, RI) : a mixture of the above three instructionsets to form a supplementary dataset.。By accompanying OwithI, we further construct Dx={(o, ι)|(o, ι)∈ O × I} .Subsequently, we employ GPT-4 to generate appropriategoals ˆGusingDxas labeled data for training Gllm, result-ing in a dataset D={(o, ι, g )|(o, ι, g )∈ O × I × ˆG}.。Toensure that the goals generated by GPT-4 conform to theformat we want, a comprehensive prompt engineering en-deavor was conducted to establish a set of predeterminedrules for GPT-4.。The rule-based prompts that guide GPT-4’sresponses are documented in Table 10, with examples ofprompts for generation provided in Table 17.Multi-step Fine-tuning.。We fine-tune ChatGLM-6B withLoRA (Hu et al., 2021) in three steps, as illustrated inFigure 8.。The steps include (1) CoT-assisted fine-tuning(CoFT) : we split the CoT steps of building Iinto indepen-dent training data, aiming to expand the volume of trainingdata as well as enhance the goal generator’s reasoning andunderstanding to Dx; (2) Supervised Fine-tuning (SFT) :strictly formatting the LLM-generated goals and furtherimproving the accuracy; and (3) Ensemble Fine-tuning(EFT) : multiple checkpoints of Gllmare utilized to gen-erate goal candidates for each (o, ι)∈ Dx, then sub-goalswith highest counts are reconstructed as a ground goal tofine-tune the model to enhance the generation.4.4.. \n",
      "the piece 565\n",
      " \n",
      " to direct fromthe specified initial state to the intended target state, andCoT (Wei et al., 2023) is deployed to enhance performance.Specifically, the four types of instruction generation are (1)IH(Human Instructions, HI) : human-annotated instruc-tions; (2) IS(State Instructions, SI) : GPT-4-generatedinstructions by giving a tuple of states (s, s′)where the sthe initial state that sampled from agent trajectories and s′the target state that is manually constructed by modifyingfeatures of the s; (3)IA(Agent Instructions, AI) : GPT-4-generated instructions by giving a pair of (s, τ)where sthe initial state, τthe agent trajectory; and (4) IR(RandomInstructions, RI) : a mixture of the above three instructionsets to form a supplementary dataset.。By accompanying OwithI, we further construct Dx={(o, ι)|(o, ι)∈ O × I} .Subsequently, we employ GPT-4 to generate appropriategoals ˆGusingDxas labeled data for training Gllm, result-ing in a dataset D={(o, ι, g )|(o, ι, g )∈ O × I × ˆG}.。Toensure that the goals generated by GPT-4 conform to theformat we want, a comprehensive prompt engineering en-deavor was conducted to establish a set of predeterminedrules for GPT-4.。The rule-based prompts that guide GPT-4’sresponses are documented in Table 10, with examples ofprompts for generation provided in Table 17.Multi-step Fine-tuning.。We fine-tune ChatGLM-6B withLoRA (Hu et al., 2021) in three steps, as illustrated inFigure 8.。The steps include (1) CoT-assisted fine-tuning(CoFT) : we split the CoT steps of building Iinto indepen-dent training data, aiming to expand the volume of trainingdata as well as enhance the goal generator’s reasoning andunderstanding to Dx; (2) Supervised Fine-tuning (SFT) :strictly formatting the LLM-generated goals and furtherimproving the accuracy; and (3) Ensemble Fine-tuning(EFT) : multiple checkpoints of Gllmare utilized to gen-erate goal candidates for each (o, ι)∈ Dx, then sub-goalswith highest counts are reconstructed as a ground goal tofine-tune the model to enhance the generation.4.4.。Collaborative TrainingAfter completing the above training steps, we obtained awell-trained goal generator Gllmand goal-conditioned pol-icyπgthat satisfactorily adhere to their respective goal distri-butions.. \n",
      "the piece 566\n",
      " \n",
      "l., 2023) is deployed to enhance performance.Specifically, the four types of instruction generation are (1)IH(Human Instructions, HI) : human-annotated instruc-tions; (2) IS(State Instructions, SI) : GPT-4-generatedinstructions by giving a tuple of states (s, s′)where the sthe initial state that sampled from agent trajectories and s′the target state that is manually constructed by modifyingfeatures of the s; (3)IA(Agent Instructions, AI) : GPT-4-generated instructions by giving a pair of (s, τ)where sthe initial state, τthe agent trajectory; and (4) IR(RandomInstructions, RI) : a mixture of the above three instructionsets to form a supplementary dataset.。By accompanying OwithI, we further construct Dx={(o, ι)|(o, ι)∈ O × I} .Subsequently, we employ GPT-4 to generate appropriategoals ˆGusingDxas labeled data for training Gllm, result-ing in a dataset D={(o, ι, g )|(o, ι, g )∈ O × I × ˆG}.。Toensure that the goals generated by GPT-4 conform to theformat we want, a comprehensive prompt engineering en-deavor was conducted to establish a set of predeterminedrules for GPT-4.。The rule-based prompts that guide GPT-4’sresponses are documented in Table 10, with examples ofprompts for generation provided in Table 17.Multi-step Fine-tuning.。We fine-tune ChatGLM-6B withLoRA (Hu et al., 2021) in three steps, as illustrated inFigure 8.。The steps include (1) CoT-assisted fine-tuning(CoFT) : we split the CoT steps of building Iinto indepen-dent training data, aiming to expand the volume of trainingdata as well as enhance the goal generator’s reasoning andunderstanding to Dx; (2) Supervised Fine-tuning (SFT) :strictly formatting the LLM-generated goals and furtherimproving the accuracy; and (3) Ensemble Fine-tuning(EFT) : multiple checkpoints of Gllmare utilized to gen-erate goal candidates for each (o, ι)∈ Dx, then sub-goalswith highest counts are reconstructed as a ground goal tofine-tune the model to enhance the generation.4.4.。Collaborative TrainingAfter completing the above training steps, we obtained awell-trained goal generator Gllmand goal-conditioned pol-icyπgthat satisfactorily adhere to their respective goal distri-butions.。However, an inconsistency persists between Gllmandπgstemming from their independent training objectives,where Gllmaimed to generate goals that satisfy given in-structions, and πgfocused on exploring goals.. \n",
      "the piece 567\n",
      " \n",
      "te appropriategoals ˆGusingDxas labeled data for training Gllm, result-ing in a dataset D={(o, ι, g )|(o, ι, g )∈ O × I × ˆG}.。Toensure that the goals generated by GPT-4 conform to theformat we want, a comprehensive prompt engineering en-deavor was conducted to establish a set of predeterminedrules for GPT-4.。The rule-based prompts that guide GPT-4’sresponses are documented in Table 10, with examples ofprompts for generation provided in Table 17.Multi-step Fine-tuning.。We fine-tune ChatGLM-6B withLoRA (Hu et al., 2021) in three steps, as illustrated inFigure 8.。The steps include (1) CoT-assisted fine-tuning(CoFT) : we split the CoT steps of building Iinto indepen-dent training data, aiming to expand the volume of trainingdata as well as enhance the goal generator’s reasoning andunderstanding to Dx; (2) Supervised Fine-tuning (SFT) :strictly formatting the LLM-generated goals and furtherimproving the accuracy; and (3) Ensemble Fine-tuning(EFT) : multiple checkpoints of Gllmare utilized to gen-erate goal candidates for each (o, ι)∈ Dx, then sub-goalswith highest counts are reconstructed as a ground goal tofine-tune the model to enhance the generation.4.4.。Collaborative TrainingAfter completing the above training steps, we obtained awell-trained goal generator Gllmand goal-conditioned pol-icyπgthat satisfactorily adhere to their respective goal distri-butions.。However, an inconsistency persists between Gllmandπgstemming from their independent training objectives,where Gllmaimed to generate goals that satisfy given in-structions, and πgfocused on exploring goals.。Therefore,we introduce co-training to address the aforementioned is-sue ensuring that the goals generated by Gllmare not onlylinguistically sound but also aligned with the capabilities ofπg.. \n",
      "the piece 568\n",
      " \n",
      "eavor was conducted to establish a set of predeterminedrules for GPT-4.。The rule-based prompts that guide GPT-4’sresponses are documented in Table 10, with examples ofprompts for generation provided in Table 17.Multi-step Fine-tuning.。We fine-tune ChatGLM-6B withLoRA (Hu et al., 2021) in three steps, as illustrated inFigure 8.。The steps include (1) CoT-assisted fine-tuning(CoFT) : we split the CoT steps of building Iinto indepen-dent training data, aiming to expand the volume of trainingdata as well as enhance the goal generator’s reasoning andunderstanding to Dx; (2) Supervised Fine-tuning (SFT) :strictly formatting the LLM-generated goals and furtherimproving the accuracy; and (3) Ensemble Fine-tuning(EFT) : multiple checkpoints of Gllmare utilized to gen-erate goal candidates for each (o, ι)∈ Dx, then sub-goalswith highest counts are reconstructed as a ground goal tofine-tune the model to enhance the generation.4.4.。Collaborative TrainingAfter completing the above training steps, we obtained awell-trained goal generator Gllmand goal-conditioned pol-icyπgthat satisfactorily adhere to their respective goal distri-butions.。However, an inconsistency persists between Gllmandπgstemming from their independent training objectives,where Gllmaimed to generate goals that satisfy given in-structions, and πgfocused on exploring goals.。Therefore,we introduce co-training to address the aforementioned is-sue ensuring that the goals generated by Gllmare not onlylinguistically sound but also aligned with the capabilities ofπg.。We formulate the co-training as follows:(πg = arg max πgEg∼PG|Gllm\u0002Vπg(s, g)\u0003PG|Gllm = arg max PG|GllmEg∼PG|Gllm[VGllm(s, g)],(7)where PG|Gllmthe goal distribution conditioned by Gllm,V(s, g)denotes an approximate evaluation for πgorGllm,in general, a state value function.. \n",
      "the piece 569\n",
      " \n",
      "tion provided in Table 17.Multi-step Fine-tuning.。We fine-tune ChatGLM-6B withLoRA (Hu et al., 2021) in three steps, as illustrated inFigure 8.。The steps include (1) CoT-assisted fine-tuning(CoFT) : we split the CoT steps of building Iinto indepen-dent training data, aiming to expand the volume of trainingdata as well as enhance the goal generator’s reasoning andunderstanding to Dx; (2) Supervised Fine-tuning (SFT) :strictly formatting the LLM-generated goals and furtherimproving the accuracy; and (3) Ensemble Fine-tuning(EFT) : multiple checkpoints of Gllmare utilized to gen-erate goal candidates for each (o, ι)∈ Dx, then sub-goalswith highest counts are reconstructed as a ground goal tofine-tune the model to enhance the generation.4.4.。Collaborative TrainingAfter completing the above training steps, we obtained awell-trained goal generator Gllmand goal-conditioned pol-icyπgthat satisfactorily adhere to their respective goal distri-butions.。However, an inconsistency persists between Gllmandπgstemming from their independent training objectives,where Gllmaimed to generate goals that satisfy given in-structions, and πgfocused on exploring goals.。Therefore,we introduce co-training to address the aforementioned is-sue ensuring that the goals generated by Gllmare not onlylinguistically sound but also aligned with the capabilities ofπg.。We formulate the co-training as follows:(πg = arg max πgEg∼PG|Gllm\u0002Vπg(s, g)\u0003PG|Gllm = arg max PG|GllmEg∼PG|Gllm[VGllm(s, g)],(7)where PG|Gllmthe goal distribution conditioned by Gllm,V(s, g)denotes an approximate evaluation for πgorGllm,in general, a state value function.。It is noteworthy that ourco-training framework is close to a hierarchical reinforce-ment learning framework (HRL) (Vezhnevets et al., 2017),where the Manager (comparable to Gllm) plans goals for thelearning of the Worker (comparable to πg), with RL beingperformed for each.. \n",
      "the piece 570\n",
      " \n",
      ") CoT-assisted fine-tuning(CoFT) : we split the CoT steps of building Iinto indepen-dent training data, aiming to expand the volume of trainingdata as well as enhance the goal generator’s reasoning andunderstanding to Dx; (2) Supervised Fine-tuning (SFT) :strictly formatting the LLM-generated goals and furtherimproving the accuracy; and (3) Ensemble Fine-tuning(EFT) : multiple checkpoints of Gllmare utilized to gen-erate goal candidates for each (o, ι)∈ Dx, then sub-goalswith highest counts are reconstructed as a ground goal tofine-tune the model to enhance the generation.4.4.。Collaborative TrainingAfter completing the above training steps, we obtained awell-trained goal generator Gllmand goal-conditioned pol-icyπgthat satisfactorily adhere to their respective goal distri-butions.。However, an inconsistency persists between Gllmandπgstemming from their independent training objectives,where Gllmaimed to generate goals that satisfy given in-structions, and πgfocused on exploring goals.。Therefore,we introduce co-training to address the aforementioned is-sue ensuring that the goals generated by Gllmare not onlylinguistically sound but also aligned with the capabilities ofπg.。We formulate the co-training as follows:(πg = arg max πgEg∼PG|Gllm\u0002Vπg(s, g)\u0003PG|Gllm = arg max PG|GllmEg∼PG|Gllm[VGllm(s, g)],(7)where PG|Gllmthe goal distribution conditioned by Gllm,V(s, g)denotes an approximate evaluation for πgorGllm,in general, a state value function.。It is noteworthy that ourco-training framework is close to a hierarchical reinforce-ment learning framework (HRL) (Vezhnevets et al., 2017),where the Manager (comparable to Gllm) plans goals for thelearning of the Worker (comparable to πg), with RL beingperformed for each.。Inspired by HRL, we implement co-training by integrating the goal-conditioned training of πgand Reinforcement Learning with Agent Feedback (RLAF)forGllm.. \n",
      "the piece 571\n",
      " \n",
      "ng data, aiming to expand the volume of trainingdata as well as enhance the goal generator’s reasoning andunderstanding to Dx; (2) Supervised Fine-tuning (SFT) :strictly formatting the LLM-generated goals and furtherimproving the accuracy; and (3) Ensemble Fine-tuning(EFT) : multiple checkpoints of Gllmare utilized to gen-erate goal candidates for each (o, ι)∈ Dx, then sub-goalswith highest counts are reconstructed as a ground goal tofine-tune the model to enhance the generation.4.4.。Collaborative TrainingAfter completing the above training steps, we obtained awell-trained goal generator Gllmand goal-conditioned pol-icyπgthat satisfactorily adhere to their respective goal distri-butions.。However, an inconsistency persists between Gllmandπgstemming from their independent training objectives,where Gllmaimed to generate goals that satisfy given in-structions, and πgfocused on exploring goals.。Therefore,we introduce co-training to address the aforementioned is-sue ensuring that the goals generated by Gllmare not onlylinguistically sound but also aligned with the capabilities ofπg.。We formulate the co-training as follows:(πg = arg max πgEg∼PG|Gllm\u0002Vπg(s, g)\u0003PG|Gllm = arg max PG|GllmEg∼PG|Gllm[VGllm(s, g)],(7)where PG|Gllmthe goal distribution conditioned by Gllm,V(s, g)denotes an approximate evaluation for πgorGllm,in general, a state value function.。It is noteworthy that ourco-training framework is close to a hierarchical reinforce-ment learning framework (HRL) (Vezhnevets et al., 2017),where the Manager (comparable to Gllm) plans goals for thelearning of the Worker (comparable to πg), with RL beingperformed for each.。Inspired by HRL, we implement co-training by integrating the goal-conditioned training of πgand Reinforcement Learning with Agent Feedback (RLAF)forGllm.。RLAF is built upon PPO, with a reward shapingthat considers (1) Rfthe evaluation of goal completion,where a high reward indicates that a goal is completed orthe reachable probability from current state; (2) Retheevaluation of crucial sub-goal completion, which involvesexamining cases by pairing instructions in a batch with a setof essential sub-goals; (3) Rmthe evaluation of outputtingthe proper goal format, with the LLM being penalized basedon edit distance.. \n",
      "the piece 572\n",
      " \n",
      "al-conditioned pol-icyπgthat satisfactorily adhere to their respective goal distri-butions.。However, an inconsistency persists between Gllmandπgstemming from their independent training objectives,where Gllmaimed to generate goals that satisfy given in-structions, and πgfocused on exploring goals.。Therefore,we introduce co-training to address the aforementioned is-sue ensuring that the goals generated by Gllmare not onlylinguistically sound but also aligned with the capabilities ofπg.。We formulate the co-training as follows:(πg = arg max πgEg∼PG|Gllm\u0002Vπg(s, g)\u0003PG|Gllm = arg max PG|GllmEg∼PG|Gllm[VGllm(s, g)],(7)where PG|Gllmthe goal distribution conditioned by Gllm,V(s, g)denotes an approximate evaluation for πgorGllm,in general, a state value function.。It is noteworthy that ourco-training framework is close to a hierarchical reinforce-ment learning framework (HRL) (Vezhnevets et al., 2017),where the Manager (comparable to Gllm) plans goals for thelearning of the Worker (comparable to πg), with RL beingperformed for each.。Inspired by HRL, we implement co-training by integrating the goal-conditioned training of πgand Reinforcement Learning with Agent Feedback (RLAF)forGllm.。RLAF is built upon PPO, with a reward shapingthat considers (1) Rfthe evaluation of goal completion,where a high reward indicates that a goal is completed orthe reachable probability from current state; (2) Retheevaluation of crucial sub-goal completion, which involvesexamining cases by pairing instructions in a batch with a setof essential sub-goals; (3) Rmthe evaluation of outputtingthe proper goal format, with the LLM being penalized basedon edit distance.。Then, we can express the reward func-tion as R(s, ι, g ) =Rf(s, g) +Re(s, g) +Rm(s, g)andAppendix K includes more details.. \n",
      "the piece 573\n",
      " \n",
      "ed to generate goals that satisfy given in-structions, and πgfocused on exploring goals.。Therefore,we introduce co-training to address the aforementioned is-sue ensuring that the goals generated by Gllmare not onlylinguistically sound but also aligned with the capabilities ofπg.。We formulate the co-training as follows:(πg = arg max πgEg∼PG|Gllm\u0002Vπg(s, g)\u0003PG|Gllm = arg max PG|GllmEg∼PG|Gllm[VGllm(s, g)],(7)where PG|Gllmthe goal distribution conditioned by Gllm,V(s, g)denotes an approximate evaluation for πgorGllm,in general, a state value function.。It is noteworthy that ourco-training framework is close to a hierarchical reinforce-ment learning framework (HRL) (Vezhnevets et al., 2017),where the Manager (comparable to Gllm) plans goals for thelearning of the Worker (comparable to πg), with RL beingperformed for each.。Inspired by HRL, we implement co-training by integrating the goal-conditioned training of πgand Reinforcement Learning with Agent Feedback (RLAF)forGllm.。RLAF is built upon PPO, with a reward shapingthat considers (1) Rfthe evaluation of goal completion,where a high reward indicates that a goal is completed orthe reachable probability from current state; (2) Retheevaluation of crucial sub-goal completion, which involvesexamining cases by pairing instructions in a batch with a setof essential sub-goals; (3) Rmthe evaluation of outputtingthe proper goal format, with the LLM being penalized basedon edit distance.。Then, we can express the reward func-tion as R(s, ι, g ) =Rf(s, g) +Re(s, g) +Rm(s, g)andAppendix K includes more details.。We observed the train-ing will lead Gllmandπgcompromise to a local optimal,i.e.,Gllmcomforts a high completion ratio for πgbut ne-glect consistency with instructions, and πgsimultaneouslyrewards Gllmwith a high completion ratio.. \n",
      "the piece 574\n",
      " \n",
      "ot onlylinguistically sound but also aligned with the capabilities ofπg.。We formulate the co-training as follows:(πg = arg max πgEg∼PG|Gllm\u0002Vπg(s, g)\u0003PG|Gllm = arg max PG|GllmEg∼PG|Gllm[VGllm(s, g)],(7)where PG|Gllmthe goal distribution conditioned by Gllm,V(s, g)denotes an approximate evaluation for πgorGllm,in general, a state value function.。It is noteworthy that ourco-training framework is close to a hierarchical reinforce-ment learning framework (HRL) (Vezhnevets et al., 2017),where the Manager (comparable to Gllm) plans goals for thelearning of the Worker (comparable to πg), with RL beingperformed for each.。Inspired by HRL, we implement co-training by integrating the goal-conditioned training of πgand Reinforcement Learning with Agent Feedback (RLAF)forGllm.。RLAF is built upon PPO, with a reward shapingthat considers (1) Rfthe evaluation of goal completion,where a high reward indicates that a goal is completed orthe reachable probability from current state; (2) Retheevaluation of crucial sub-goal completion, which involvesexamining cases by pairing instructions in a batch with a setof essential sub-goals; (3) Rmthe evaluation of outputtingthe proper goal format, with the LLM being penalized basedon edit distance.。Then, we can express the reward func-tion as R(s, ι, g ) =Rf(s, g) +Re(s, g) +Rm(s, g)andAppendix K includes more details.。We observed the train-ing will lead Gllmandπgcompromise to a local optimal,i.e.,Gllmcomforts a high completion ratio for πgbut ne-glect consistency with instructions, and πgsimultaneouslyrewards Gllmwith a high completion ratio.。Furthermore,as the policy training continuing, the evaluation for goalgeneration is out-date.. \n",
      "the piece 575\n",
      " \n",
      "s, g)],(7)where PG|Gllmthe goal distribution conditioned by Gllm,V(s, g)denotes an approximate evaluation for πgorGllm,in general, a state value function.。It is noteworthy that ourco-training framework is close to a hierarchical reinforce-ment learning framework (HRL) (Vezhnevets et al., 2017),where the Manager (comparable to Gllm) plans goals for thelearning of the Worker (comparable to πg), with RL beingperformed for each.。Inspired by HRL, we implement co-training by integrating the goal-conditioned training of πgand Reinforcement Learning with Agent Feedback (RLAF)forGllm.。RLAF is built upon PPO, with a reward shapingthat considers (1) Rfthe evaluation of goal completion,where a high reward indicates that a goal is completed orthe reachable probability from current state; (2) Retheevaluation of crucial sub-goal completion, which involvesexamining cases by pairing instructions in a batch with a setof essential sub-goals; (3) Rmthe evaluation of outputtingthe proper goal format, with the LLM being penalized basedon edit distance.。Then, we can express the reward func-tion as R(s, ι, g ) =Rf(s, g) +Re(s, g) +Rm(s, g)andAppendix K includes more details.。We observed the train-ing will lead Gllmandπgcompromise to a local optimal,i.e.,Gllmcomforts a high completion ratio for πgbut ne-glect consistency with instructions, and πgsimultaneouslyrewards Gllmwith a high completion ratio.。Furthermore,as the policy training continuing, the evaluation for goalgeneration is out-date.。To fix this issue, we propose a pe-riodic reset for the RLAF, i.e., the parameters of the Gllmwill be reset for every set number of steps to avoid beingtrapped in a local convergence, achieving enhanced goalcompletion, and keeping goals consistent with human in-Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationstructions.. \n",
      "the piece 576\n",
      " \n",
      "evets et al., 2017),where the Manager (comparable to Gllm) plans goals for thelearning of the Worker (comparable to πg), with RL beingperformed for each.。Inspired by HRL, we implement co-training by integrating the goal-conditioned training of πgand Reinforcement Learning with Agent Feedback (RLAF)forGllm.。RLAF is built upon PPO, with a reward shapingthat considers (1) Rfthe evaluation of goal completion,where a high reward indicates that a goal is completed orthe reachable probability from current state; (2) Retheevaluation of crucial sub-goal completion, which involvesexamining cases by pairing instructions in a batch with a setof essential sub-goals; (3) Rmthe evaluation of outputtingthe proper goal format, with the LLM being penalized basedon edit distance.。Then, we can express the reward func-tion as R(s, ι, g ) =Rf(s, g) +Re(s, g) +Rm(s, g)andAppendix K includes more details.。We observed the train-ing will lead Gllmandπgcompromise to a local optimal,i.e.,Gllmcomforts a high completion ratio for πgbut ne-glect consistency with instructions, and πgsimultaneouslyrewards Gllmwith a high completion ratio.。Furthermore,as the policy training continuing, the evaluation for goalgeneration is out-date.。To fix this issue, we propose a pe-riodic reset for the RLAF, i.e., the parameters of the Gllmwill be reset for every set number of steps to avoid beingtrapped in a local convergence, achieving enhanced goalcompletion, and keeping goals consistent with human in-Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationstructions.。Considering the training efficiency, we conductLoRA (Hu et al., 2021) to update the model weights forGllm.. \n",
      "the piece 577\n",
      " \n",
      "th Agent Feedback (RLAF)forGllm.。RLAF is built upon PPO, with a reward shapingthat considers (1) Rfthe evaluation of goal completion,where a high reward indicates that a goal is completed orthe reachable probability from current state; (2) Retheevaluation of crucial sub-goal completion, which involvesexamining cases by pairing instructions in a batch with a setof essential sub-goals; (3) Rmthe evaluation of outputtingthe proper goal format, with the LLM being penalized basedon edit distance.。Then, we can express the reward func-tion as R(s, ι, g ) =Rf(s, g) +Re(s, g) +Rm(s, g)andAppendix K includes more details.。We observed the train-ing will lead Gllmandπgcompromise to a local optimal,i.e.,Gllmcomforts a high completion ratio for πgbut ne-glect consistency with instructions, and πgsimultaneouslyrewards Gllmwith a high completion ratio.。Furthermore,as the policy training continuing, the evaluation for goalgeneration is out-date.。To fix this issue, we propose a pe-riodic reset for the RLAF, i.e., the parameters of the Gllmwill be reset for every set number of steps to avoid beingtrapped in a local convergence, achieving enhanced goalcompletion, and keeping goals consistent with human in-Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationstructions.。Considering the training efficiency, we conductLoRA (Hu et al., 2021) to update the model weights forGllm.。Figure 8 illustrates the whole training process, andAlgorithm 1 summarizes the corresponding pseudo-code.5.. \n",
      "the piece 578\n",
      " \n",
      "dicates that a goal is completed orthe reachable probability from current state; (2) Retheevaluation of crucial sub-goal completion, which involvesexamining cases by pairing instructions in a batch with a setof essential sub-goals; (3) Rmthe evaluation of outputtingthe proper goal format, with the LLM being penalized basedon edit distance.。Then, we can express the reward func-tion as R(s, ι, g ) =Rf(s, g) +Re(s, g) +Rm(s, g)andAppendix K includes more details.。We observed the train-ing will lead Gllmandπgcompromise to a local optimal,i.e.,Gllmcomforts a high completion ratio for πgbut ne-glect consistency with instructions, and πgsimultaneouslyrewards Gllmwith a high completion ratio.。Furthermore,as the policy training continuing, the evaluation for goalgeneration is out-date.。To fix this issue, we propose a pe-riodic reset for the RLAF, i.e., the parameters of the Gllmwill be reset for every set number of steps to avoid beingtrapped in a local convergence, achieving enhanced goalcompletion, and keeping goals consistent with human in-Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationstructions.。Considering the training efficiency, we conductLoRA (Hu et al., 2021) to update the model weights forGllm.。Figure 8 illustrates the whole training process, andAlgorithm 1 summarizes the corresponding pseudo-code.5.。ExperimentWe conduct empirical experiments to evaluate the efficacy ofboth stages of our proposed OpenPAL.. \n",
      "the piece 579\n",
      " \n",
      "We observed the train-ing will lead Gllmandπgcompromise to a local optimal,i.e.,Gllmcomforts a high completion ratio for πgbut ne-glect consistency with instructions, and πgsimultaneouslyrewards Gllmwith a high completion ratio.。Furthermore,as the policy training continuing, the evaluation for goalgeneration is out-date.。To fix this issue, we propose a pe-riodic reset for the RLAF, i.e., the parameters of the Gllmwill be reset for every set number of steps to avoid beingtrapped in a local convergence, achieving enhanced goalcompletion, and keeping goals consistent with human in-Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationstructions.。Considering the training efficiency, we conductLoRA (Hu et al., 2021) to update the model weights forGllm.。Figure 8 illustrates the whole training process, andAlgorithm 1 summarizes the corresponding pseudo-code.5.。ExperimentWe conduct empirical experiments to evaluate the efficacy ofboth stages of our proposed OpenPAL.。To make the Contrasatisfy the learning requirements, we give well-designedspaces and reward functions as follows.Observation Space.. \n",
      "the piece 580\n",
      " \n",
      "ut ne-glect consistency with instructions, and πgsimultaneouslyrewards Gllmwith a high completion ratio.。Furthermore,as the policy training continuing, the evaluation for goalgeneration is out-date.。To fix this issue, we propose a pe-riodic reset for the RLAF, i.e., the parameters of the Gllmwill be reset for every set number of steps to avoid beingtrapped in a local convergence, achieving enhanced goalcompletion, and keeping goals consistent with human in-Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationstructions.。Considering the training efficiency, we conductLoRA (Hu et al., 2021) to update the model weights forGllm.。Figure 8 illustrates the whole training process, andAlgorithm 1 summarizes the corresponding pseudo-code.5.。ExperimentWe conduct empirical experiments to evaluate the efficacy ofboth stages of our proposed OpenPAL.。To make the Contrasatisfy the learning requirements, we give well-designedspaces and reward functions as follows.Observation Space.。The observation space encompassesmany factors, such as unit features detailing the agent states,those of other players and environmental features captur-ing interaction events.. \n",
      "the piece 581\n",
      " \n",
      " pe-riodic reset for the RLAF, i.e., the parameters of the Gllmwill be reset for every set number of steps to avoid beingtrapped in a local convergence, achieving enhanced goalcompletion, and keeping goals consistent with human in-Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationstructions.。Considering the training efficiency, we conductLoRA (Hu et al., 2021) to update the model weights forGllm.。Figure 8 illustrates the whole training process, andAlgorithm 1 summarizes the corresponding pseudo-code.5.。ExperimentWe conduct empirical experiments to evaluate the efficacy ofboth stages of our proposed OpenPAL.。To make the Contrasatisfy the learning requirements, we give well-designedspaces and reward functions as follows.Observation Space.。The observation space encompassesmany factors, such as unit features detailing the agent states,those of other players and environmental features captur-ing interaction events.。Additionally, an agent-centric RGBbird’s-eye-view (BEV) of the local environment is consid-ered.. \n",
      "the piece 582\n",
      " \n",
      "er of steps to avoid beingtrapped in a local convergence, achieving enhanced goalcompletion, and keeping goals consistent with human in-Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationstructions.。Considering the training efficiency, we conductLoRA (Hu et al., 2021) to update the model weights forGllm.。Figure 8 illustrates the whole training process, andAlgorithm 1 summarizes the corresponding pseudo-code.5.。ExperimentWe conduct empirical experiments to evaluate the efficacy ofboth stages of our proposed OpenPAL.。To make the Contrasatisfy the learning requirements, we give well-designedspaces and reward functions as follows.Observation Space.。The observation space encompassesmany factors, such as unit features detailing the agent states,those of other players and environmental features captur-ing interaction events.。Additionally, an agent-centric RGBbird’s-eye-view (BEV) of the local environment is consid-ered.。Table 4 includes detailed information.Action Space.. \n",
      "the piece 583\n",
      " \n",
      " the whole training process, andAlgorithm 1 summarizes the corresponding pseudo-code.5.。ExperimentWe conduct empirical experiments to evaluate the efficacy ofboth stages of our proposed OpenPAL.。To make the Contrasatisfy the learning requirements, we give well-designedspaces and reward functions as follows.Observation Space.。The observation space encompassesmany factors, such as unit features detailing the agent states,those of other players and environmental features captur-ing interaction events.。Additionally, an agent-centric RGBbird’s-eye-view (BEV) of the local environment is consid-ered.。Table 4 includes detailed information.Action Space.。The action space is implemented on topof Contra’s micro-operation API, comprising a collectionof multi-grained actions.. \n",
      "the piece 584\n",
      " \n",
      " empirical experiments to evaluate the efficacy ofboth stages of our proposed OpenPAL.。To make the Contrasatisfy the learning requirements, we give well-designedspaces and reward functions as follows.Observation Space.。The observation space encompassesmany factors, such as unit features detailing the agent states,those of other players and environmental features captur-ing interaction events.。Additionally, an agent-centric RGBbird’s-eye-view (BEV) of the local environment is consid-ered.。Table 4 includes detailed information.Action Space.。The action space is implemented on topof Contra’s micro-operation API, comprising a collectionof multi-grained actions.。These actions range from fine-grained movements, such as six-degrees-of-freedom move-ment and weapon usage, to compound actions in coarse-grained categories, such as firing at a target, and each actionis executed over a duration of 200ms, hence the controlfrequency is 5Hz.. \n",
      "the piece 585\n",
      " \n",
      "sfy the learning requirements, we give well-designedspaces and reward functions as follows.Observation Space.。The observation space encompassesmany factors, such as unit features detailing the agent states,those of other players and environmental features captur-ing interaction events.。Additionally, an agent-centric RGBbird’s-eye-view (BEV) of the local environment is consid-ered.。Table 4 includes detailed information.Action Space.。The action space is implemented on topof Contra’s micro-operation API, comprising a collectionof multi-grained actions.。These actions range from fine-grained movements, such as six-degrees-of-freedom move-ment and weapon usage, to compound actions in coarse-grained categories, such as firing at a target, and each actionis executed over a duration of 200ms, hence the controlfrequency is 5Hz.。The total size of the action space is 54.Further details in Table 6.Reward Functions.. \n",
      "the piece 586\n",
      " \n",
      ".。The observation space encompassesmany factors, such as unit features detailing the agent states,those of other players and environmental features captur-ing interaction events.。Additionally, an agent-centric RGBbird’s-eye-view (BEV) of the local environment is consid-ered.。Table 4 includes detailed information.Action Space.。The action space is implemented on topof Contra’s micro-operation API, comprising a collectionof multi-grained actions.。These actions range from fine-grained movements, such as six-degrees-of-freedom move-ment and weapon usage, to compound actions in coarse-grained categories, such as firing at a target, and each actionis executed over a duration of 200ms, hence the controlfrequency is 5Hz.。The total size of the action space is 54.Further details in Table 6.Reward Functions.。A comprehensive representation isemployed for the reward function, considering various fac-tors contributing to goal-conditioned policy learning.. \n",
      "the piece 587\n",
      " \n",
      "ental features captur-ing interaction events.。Additionally, an agent-centric RGBbird’s-eye-view (BEV) of the local environment is consid-ered.。Table 4 includes detailed information.Action Space.。The action space is implemented on topof Contra’s micro-operation API, comprising a collectionof multi-grained actions.。These actions range from fine-grained movements, such as six-degrees-of-freedom move-ment and weapon usage, to compound actions in coarse-grained categories, such as firing at a target, and each actionis executed over a duration of 200ms, hence the controlfrequency is 5Hz.。The total size of the action space is 54.Further details in Table 6.Reward Functions.。A comprehensive representation isemployed for the reward function, considering various fac-tors contributing to goal-conditioned policy learning.。Thesefactors are organized as a linear combination to formulatethe reward function.. \n",
      "the piece 588\n",
      " \n",
      "on.Action Space.。The action space is implemented on topof Contra’s micro-operation API, comprising a collectionof multi-grained actions.。These actions range from fine-grained movements, such as six-degrees-of-freedom move-ment and weapon usage, to compound actions in coarse-grained categories, such as firing at a target, and each actionis executed over a duration of 200ms, hence the controlfrequency is 5Hz.。The total size of the action space is 54.Further details in Table 6.Reward Functions.。A comprehensive representation isemployed for the reward function, considering various fac-tors contributing to goal-conditioned policy learning.。Thesefactors are organized as a linear combination to formulatethe reward function.。Furthermore, we determine the weightsfor the combination with a two-fold principle: (1) assigningweights to reward items based on their scales and empha-sizing important factors; (2) dynamically adjusting weightsin response to learning feedback, such as decreasing or in-creasing the weights of corresponding factors.. \n",
      "the piece 589\n",
      " \n",
      " a collectionof multi-grained actions.。These actions range from fine-grained movements, such as six-degrees-of-freedom move-ment and weapon usage, to compound actions in coarse-grained categories, such as firing at a target, and each actionis executed over a duration of 200ms, hence the controlfrequency is 5Hz.。The total size of the action space is 54.Further details in Table 6.Reward Functions.。A comprehensive representation isemployed for the reward function, considering various fac-tors contributing to goal-conditioned policy learning.。Thesefactors are organized as a linear combination to formulatethe reward function.。Furthermore, we determine the weightsfor the combination with a two-fold principle: (1) assigningweights to reward items based on their scales and empha-sizing important factors; (2) dynamically adjusting weightsin response to learning feedback, such as decreasing or in-creasing the weights of corresponding factors.。Additionalinformation is available in Appendix A.5.1.. \n",
      "the piece 590\n",
      " \n",
      "range from fine-grained movements, such as six-degrees-of-freedom move-ment and weapon usage, to compound actions in coarse-grained categories, such as firing at a target, and each actionis executed over a duration of 200ms, hence the controlfrequency is 5Hz.。The total size of the action space is 54.Further details in Table 6.Reward Functions.。A comprehensive representation isemployed for the reward function, considering various fac-tors contributing to goal-conditioned policy learning.。Thesefactors are organized as a linear combination to formulatethe reward function.。Furthermore, we determine the weightsfor the combination with a two-fold principle: (1) assigningweights to reward items based on their scales and empha-sizing important factors; (2) dynamically adjusting weightsin response to learning feedback, such as decreasing or in-creasing the weights of corresponding factors.。Additionalinformation is available in Appendix A.5.1.。Evaluating Goal-conditioned RLWe evaluate the πgof stage I from three distinct perspectivesto verify the open-endedness achieved on G: (1) the comple-tion ratio, (2) generalization capability concerning unseengoals, and (3) robustness when integrating goal-conditionedlearning atop non-goal learning.. \n",
      "the piece 591\n",
      " \n",
      "se-grained categories, such as firing at a target, and each actionis executed over a duration of 200ms, hence the controlfrequency is 5Hz.。The total size of the action space is 54.Further details in Table 6.Reward Functions.。A comprehensive representation isemployed for the reward function, considering various fac-tors contributing to goal-conditioned policy learning.。Thesefactors are organized as a linear combination to formulatethe reward function.。Furthermore, we determine the weightsfor the combination with a two-fold principle: (1) assigningweights to reward items based on their scales and empha-sizing important factors; (2) dynamically adjusting weightsin response to learning feedback, such as decreasing or in-creasing the weights of corresponding factors.。Additionalinformation is available in Appendix A.5.1.。Evaluating Goal-conditioned RLWe evaluate the πgof stage I from three distinct perspectivesto verify the open-endedness achieved on G: (1) the comple-tion ratio, (2) generalization capability concerning unseengoals, and (3) robustness when integrating goal-conditionedlearning atop non-goal learning.。Given that GCRL in Open-PAL comprises random and hindsight stages, our evaluationinvolves a comparative analysis with a baseline, HER , i.e.,training the RL agent with hindsight goal generation.. \n",
      "the piece 592\n",
      " \n",
      "reward function, considering various fac-tors contributing to goal-conditioned policy learning.。Thesefactors are organized as a linear combination to formulatethe reward function.。Furthermore, we determine the weightsfor the combination with a two-fold principle: (1) assigningweights to reward items based on their scales and empha-sizing important factors; (2) dynamically adjusting weightsin response to learning feedback, such as decreasing or in-creasing the weights of corresponding factors.。Additionalinformation is available in Appendix A.5.1.。Evaluating Goal-conditioned RLWe evaluate the πgof stage I from three distinct perspectivesto verify the open-endedness achieved on G: (1) the comple-tion ratio, (2) generalization capability concerning unseengoals, and (3) robustness when integrating goal-conditionedlearning atop non-goal learning.。Given that GCRL in Open-PAL comprises random and hindsight stages, our evaluationinvolves a comparative analysis with a baseline, HER , i.e.,training the RL agent with hindsight goal generation.。Fig-ure 2(a) presents a comparison of the goal completion ratioacross different methods on a validation dataset where goalsare generated using GrndandGop.. \n",
      "the piece 593\n",
      " \n",
      "earning.。Thesefactors are organized as a linear combination to formulatethe reward function.。Furthermore, we determine the weightsfor the combination with a two-fold principle: (1) assigningweights to reward items based on their scales and empha-sizing important factors; (2) dynamically adjusting weightsin response to learning feedback, such as decreasing or in-creasing the weights of corresponding factors.。Additionalinformation is available in Appendix A.5.1.。Evaluating Goal-conditioned RLWe evaluate the πgof stage I from three distinct perspectivesto verify the open-endedness achieved on G: (1) the comple-tion ratio, (2) generalization capability concerning unseengoals, and (3) robustness when integrating goal-conditionedlearning atop non-goal learning.。Given that GCRL in Open-PAL comprises random and hindsight stages, our evaluationinvolves a comparative analysis with a baseline, HER , i.e.,training the RL agent with hindsight goal generation.。Fig-ure 2(a) presents a comparison of the goal completion ratioacross different methods on a validation dataset where goalsare generated using GrndandGop.。As depicted in Fig-ure 2(a), our method surpasses HER by ≈3.4%.. \n",
      "the piece 594\n",
      " \n",
      "on with a two-fold principle: (1) assigningweights to reward items based on their scales and empha-sizing important factors; (2) dynamically adjusting weightsin response to learning feedback, such as decreasing or in-creasing the weights of corresponding factors.。Additionalinformation is available in Appendix A.5.1.。Evaluating Goal-conditioned RLWe evaluate the πgof stage I from three distinct perspectivesto verify the open-endedness achieved on G: (1) the comple-tion ratio, (2) generalization capability concerning unseengoals, and (3) robustness when integrating goal-conditionedlearning atop non-goal learning.。Given that GCRL in Open-PAL comprises random and hindsight stages, our evaluationinvolves a comparative analysis with a baseline, HER , i.e.,training the RL agent with hindsight goal generation.。Fig-ure 2(a) presents a comparison of the goal completion ratioacross different methods on a validation dataset where goalsare generated using GrndandGop.。As depicted in Fig-ure 2(a), our method surpasses HER by ≈3.4%.。Figure 2(b)evaluates the generalization on unseen goals, addressing thesecond aspect mentioned earlier.. \n",
      "the piece 595\n",
      " \n",
      "les and empha-sizing important factors; (2) dynamically adjusting weightsin response to learning feedback, such as decreasing or in-creasing the weights of corresponding factors.。Additionalinformation is available in Appendix A.5.1.。Evaluating Goal-conditioned RLWe evaluate the πgof stage I from three distinct perspectivesto verify the open-endedness achieved on G: (1) the comple-tion ratio, (2) generalization capability concerning unseengoals, and (3) robustness when integrating goal-conditionedlearning atop non-goal learning.。Given that GCRL in Open-PAL comprises random and hindsight stages, our evaluationinvolves a comparative analysis with a baseline, HER , i.e.,training the RL agent with hindsight goal generation.。Fig-ure 2(a) presents a comparison of the goal completion ratioacross different methods on a validation dataset where goalsare generated using GrndandGop.。As depicted in Fig-ure 2(a), our method surpasses HER by ≈3.4%.。Figure 2(b)evaluates the generalization on unseen goals, addressing thesecond aspect mentioned earlier.。It is noteworthy that theunseen goals are re-combinations of goals obtained withHER and Gllm.. \n",
      "the piece 596\n",
      " \n",
      "tivesto verify the open-endedness achieved on G: (1) the comple-tion ratio, (2) generalization capability concerning unseengoals, and (3) robustness when integrating goal-conditionedlearning atop non-goal learning.。Given that GCRL in Open-PAL comprises random and hindsight stages, our evaluationinvolves a comparative analysis with a baseline, HER , i.e.,training the RL agent with hindsight goal generation.。Fig-ure 2(a) presents a comparison of the goal completion ratioacross different methods on a validation dataset where goalsare generated using GrndandGop.。As depicted in Fig-ure 2(a), our method surpasses HER by ≈3.4%.。Figure 2(b)evaluates the generalization on unseen goals, addressing thesecond aspect mentioned earlier.。It is noteworthy that theunseen goals are re-combinations of goals obtained withHER and Gllm.。As indicated in Figure 2(b), our methodexcels over the baseline in terms of completion ratio.. \n",
      "the piece 597\n",
      " \n",
      "e comple-tion ratio, (2) generalization capability concerning unseengoals, and (3) robustness when integrating goal-conditionedlearning atop non-goal learning.。Given that GCRL in Open-PAL comprises random and hindsight stages, our evaluationinvolves a comparative analysis with a baseline, HER , i.e.,training the RL agent with hindsight goal generation.。Fig-ure 2(a) presents a comparison of the goal completion ratioacross different methods on a validation dataset where goalsare generated using GrndandGop.。As depicted in Fig-ure 2(a), our method surpasses HER by ≈3.4%.。Figure 2(b)evaluates the generalization on unseen goals, addressing thesecond aspect mentioned earlier.。It is noteworthy that theunseen goals are re-combinations of goals obtained withHER and Gllm.。As indicated in Figure 2(b), our methodexcels over the baseline in terms of completion ratio.。Fig-ure 2(c) answers the third point by comparing the use ofKL-divergence regularizer for policy learning, consideringchanges in overall performance and the ability to eliminateenemies.. \n",
      "the piece 598\n",
      " \n",
      "raining the RL agent with hindsight goal generation.。Fig-ure 2(a) presents a comparison of the goal completion ratioacross different methods on a validation dataset where goalsare generated using GrndandGop.。As depicted in Fig-ure 2(a), our method surpasses HER by ≈3.4%.。Figure 2(b)evaluates the generalization on unseen goals, addressing thesecond aspect mentioned earlier.。It is noteworthy that theunseen goals are re-combinations of goals obtained withHER and Gllm.。As indicated in Figure 2(b), our methodexcels over the baseline in terms of completion ratio.。Fig-ure 2(c) answers the third point by comparing the use ofKL-divergence regularizer for policy learning, consideringchanges in overall performance and the ability to eliminateenemies.。Three metrics are designed for evaluation: (1)Mean basic reward per step, which indicates whether thecurrent policy degenerates in performing basic skills perstep against a well-trained non-goal policy, and intentionalto emphasize the agent’s immediate responsiveness overfinal results; (2) #Enemies killed, representing the averagenumber of enemies killed by the agent per episode; and (3)#Enemies knocked down, representing the average numberof enemies knocked down by the agent per episode.5.2.. \n",
      "the piece 599\n",
      " \n",
      "GrndandGop.。As depicted in Fig-ure 2(a), our method surpasses HER by ≈3.4%.。Figure 2(b)evaluates the generalization on unseen goals, addressing thesecond aspect mentioned earlier.。It is noteworthy that theunseen goals are re-combinations of goals obtained withHER and Gllm.。As indicated in Figure 2(b), our methodexcels over the baseline in terms of completion ratio.。Fig-ure 2(c) answers the third point by comparing the use ofKL-divergence regularizer for policy learning, consideringchanges in overall performance and the ability to eliminateenemies.。Three metrics are designed for evaluation: (1)Mean basic reward per step, which indicates whether thecurrent policy degenerates in performing basic skills perstep against a well-trained non-goal policy, and intentionalto emphasize the agent’s immediate responsiveness overfinal results; (2) #Enemies killed, representing the averagenumber of enemies killed by the agent per episode; and (3)#Enemies knocked down, representing the average numberof enemies knocked down by the agent per episode.5.2.。Evaluating LLM-based Goal GenerationWe conducted evaluation of Gllmthrough two compara-tive experiments on GPT-4-generated instruction datasets,aiming to investigate the impact of different instructiondatasets and fine-tuning paradigms.. \n",
      "the piece 600\n",
      " \n",
      "pect mentioned earlier.。It is noteworthy that theunseen goals are re-combinations of goals obtained withHER and Gllm.。As indicated in Figure 2(b), our methodexcels over the baseline in terms of completion ratio.。Fig-ure 2(c) answers the third point by comparing the use ofKL-divergence regularizer for policy learning, consideringchanges in overall performance and the ability to eliminateenemies.。Three metrics are designed for evaluation: (1)Mean basic reward per step, which indicates whether thecurrent policy degenerates in performing basic skills perstep against a well-trained non-goal policy, and intentionalto emphasize the agent’s immediate responsiveness overfinal results; (2) #Enemies killed, representing the averagenumber of enemies killed by the agent per episode; and (3)#Enemies knocked down, representing the average numberof enemies knocked down by the agent per episode.5.2.。Evaluating LLM-based Goal GenerationWe conducted evaluation of Gllmthrough two compara-tive experiments on GPT-4-generated instruction datasets,aiming to investigate the impact of different instructiondatasets and fine-tuning paradigms.。The evaluation metricsemployed encompass precision, recall, and F1 score.. \n",
      "the piece 601\n",
      " \n",
      " re-combinations of goals obtained withHER and Gllm.。As indicated in Figure 2(b), our methodexcels over the baseline in terms of completion ratio.。Fig-ure 2(c) answers the third point by comparing the use ofKL-divergence regularizer for policy learning, consideringchanges in overall performance and the ability to eliminateenemies.。Three metrics are designed for evaluation: (1)Mean basic reward per step, which indicates whether thecurrent policy degenerates in performing basic skills perstep against a well-trained non-goal policy, and intentionalto emphasize the agent’s immediate responsiveness overfinal results; (2) #Enemies killed, representing the averagenumber of enemies killed by the agent per episode; and (3)#Enemies knocked down, representing the average numberof enemies knocked down by the agent per episode.5.2.。Evaluating LLM-based Goal GenerationWe conducted evaluation of Gllmthrough two compara-tive experiments on GPT-4-generated instruction datasets,aiming to investigate the impact of different instructiondatasets and fine-tuning paradigms.。The evaluation metricsemployed encompass precision, recall, and F1 score.。It’sworth noting that a potential issue in determining the pre-cision of generating sub-goals that are close in semantics.For instance, associating the sub-goal “moving speed” val-ues “very fast” versus “fast” may be perceived as a negativeinstance under precision measurement.. \n",
      "the piece 602\n",
      " \n",
      "he baseline in terms of completion ratio.。Fig-ure 2(c) answers the third point by comparing the use ofKL-divergence regularizer for policy learning, consideringchanges in overall performance and the ability to eliminateenemies.。Three metrics are designed for evaluation: (1)Mean basic reward per step, which indicates whether thecurrent policy degenerates in performing basic skills perstep against a well-trained non-goal policy, and intentionalto emphasize the agent’s immediate responsiveness overfinal results; (2) #Enemies killed, representing the averagenumber of enemies killed by the agent per episode; and (3)#Enemies knocked down, representing the average numberof enemies knocked down by the agent per episode.5.2.。Evaluating LLM-based Goal GenerationWe conducted evaluation of Gllmthrough two compara-tive experiments on GPT-4-generated instruction datasets,aiming to investigate the impact of different instructiondatasets and fine-tuning paradigms.。The evaluation metricsemployed encompass precision, recall, and F1 score.。It’sworth noting that a potential issue in determining the pre-cision of generating sub-goals that are close in semantics.For instance, associating the sub-goal “moving speed” val-ues “very fast” versus “fast” may be perceived as a negativeinstance under precision measurement.。Consequently, weargue that the generation of such sub-goals should weighmore in choosing sub-goal than determining values.. \n",
      "the piece 603\n",
      " \n",
      " use ofKL-divergence regularizer for policy learning, consideringchanges in overall performance and the ability to eliminateenemies.。Three metrics are designed for evaluation: (1)Mean basic reward per step, which indicates whether thecurrent policy degenerates in performing basic skills perstep against a well-trained non-goal policy, and intentionalto emphasize the agent’s immediate responsiveness overfinal results; (2) #Enemies killed, representing the averagenumber of enemies killed by the agent per episode; and (3)#Enemies knocked down, representing the average numberof enemies knocked down by the agent per episode.5.2.。Evaluating LLM-based Goal GenerationWe conducted evaluation of Gllmthrough two compara-tive experiments on GPT-4-generated instruction datasets,aiming to investigate the impact of different instructiondatasets and fine-tuning paradigms.。The evaluation metricsemployed encompass precision, recall, and F1 score.。It’sworth noting that a potential issue in determining the pre-cision of generating sub-goals that are close in semantics.For instance, associating the sub-goal “moving speed” val-ues “very fast” versus “fast” may be perceived as a negativeinstance under precision measurement.。Consequently, weargue that the generation of such sub-goals should weighmore in choosing sub-goal than determining values.。Thus,we further propose three choice-based metrics: precision(choice), recall (choice), and F1 (choice).. \n",
      "the piece 604\n",
      " \n",
      " and the ability to eliminateenemies.。Three metrics are designed for evaluation: (1)Mean basic reward per step, which indicates whether thecurrent policy degenerates in performing basic skills perstep against a well-trained non-goal policy, and intentionalto emphasize the agent’s immediate responsiveness overfinal results; (2) #Enemies killed, representing the averagenumber of enemies killed by the agent per episode; and (3)#Enemies knocked down, representing the average numberof enemies knocked down by the agent per episode.5.2.。Evaluating LLM-based Goal GenerationWe conducted evaluation of Gllmthrough two compara-tive experiments on GPT-4-generated instruction datasets,aiming to investigate the impact of different instructiondatasets and fine-tuning paradigms.。The evaluation metricsemployed encompass precision, recall, and F1 score.。It’sworth noting that a potential issue in determining the pre-cision of generating sub-goals that are close in semantics.For instance, associating the sub-goal “moving speed” val-ues “very fast” versus “fast” may be perceived as a negativeinstance under precision measurement.。Consequently, weargue that the generation of such sub-goals should weighmore in choosing sub-goal than determining values.。Thus,we further propose three choice-based metrics: precision(choice), recall (choice), and F1 (choice).。Table 1 providesDataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)HI 0.435 0.611 0.361 0.517 0.395 0.560AI 0.474 0.611 0.419 0.532 0.445 0.569SI 0.444 0.601 0.413 0.539 0.428 0.568RI 0.499 0.633 0.414 0.526 0.453 0.574ALL 0.555 0.685 0.505 0.621 0.529 0.652Table 1: Evaluation on different datasets.. \n",
      "the piece 605\n",
      " \n",
      "kills perstep against a well-trained non-goal policy, and intentionalto emphasize the agent’s immediate responsiveness overfinal results; (2) #Enemies killed, representing the averagenumber of enemies killed by the agent per episode; and (3)#Enemies knocked down, representing the average numberof enemies knocked down by the agent per episode.5.2.。Evaluating LLM-based Goal GenerationWe conducted evaluation of Gllmthrough two compara-tive experiments on GPT-4-generated instruction datasets,aiming to investigate the impact of different instructiondatasets and fine-tuning paradigms.。The evaluation metricsemployed encompass precision, recall, and F1 score.。It’sworth noting that a potential issue in determining the pre-cision of generating sub-goals that are close in semantics.For instance, associating the sub-goal “moving speed” val-ues “very fast” versus “fast” may be perceived as a negativeinstance under precision measurement.。Consequently, weargue that the generation of such sub-goals should weighmore in choosing sub-goal than determining values.。Thus,we further propose three choice-based metrics: precision(choice), recall (choice), and F1 (choice).。Table 1 providesDataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)HI 0.435 0.611 0.361 0.517 0.395 0.560AI 0.474 0.611 0.419 0.532 0.445 0.569SI 0.444 0.601 0.413 0.539 0.428 0.568RI 0.499 0.633 0.414 0.526 0.453 0.574ALL 0.555 0.685 0.505 0.621 0.529 0.652Table 1: Evaluation on different datasets.。“ALL” the propor-tional mixture of the four base datasets.a comparison of five types of instruction datasets used in themulti-step fine-tuning process for Gllm.. \n",
      "the piece 606\n",
      " \n",
      " to investigate the impact of different instructiondatasets and fine-tuning paradigms.。The evaluation metricsemployed encompass precision, recall, and F1 score.。It’sworth noting that a potential issue in determining the pre-cision of generating sub-goals that are close in semantics.For instance, associating the sub-goal “moving speed” val-ues “very fast” versus “fast” may be perceived as a negativeinstance under precision measurement.。Consequently, weargue that the generation of such sub-goals should weighmore in choosing sub-goal than determining values.。Thus,we further propose three choice-based metrics: precision(choice), recall (choice), and F1 (choice).。Table 1 providesDataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)HI 0.435 0.611 0.361 0.517 0.395 0.560AI 0.474 0.611 0.419 0.532 0.445 0.569SI 0.444 0.601 0.413 0.539 0.428 0.568RI 0.499 0.633 0.414 0.526 0.453 0.574ALL 0.555 0.685 0.505 0.621 0.529 0.652Table 1: Evaluation on different datasets.。“ALL” the propor-tional mixture of the four base datasets.a comparison of five types of instruction datasets used in themulti-step fine-tuning process for Gllm.。The comparisonreveals that utilizing a mixture significantly outperformsindividual base datasets, which indicates a mixture aidsGllmin capturing human preferences and understandingthe implications of each abstracted state, thereby enhancinggoal generation.. \n",
      "the piece 607\n",
      " \n",
      "rating sub-goals that are close in semantics.For instance, associating the sub-goal “moving speed” val-ues “very fast” versus “fast” may be perceived as a negativeinstance under precision measurement.。Consequently, weargue that the generation of such sub-goals should weighmore in choosing sub-goal than determining values.。Thus,we further propose three choice-based metrics: precision(choice), recall (choice), and F1 (choice).。Table 1 providesDataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)HI 0.435 0.611 0.361 0.517 0.395 0.560AI 0.474 0.611 0.419 0.532 0.445 0.569SI 0.444 0.601 0.413 0.539 0.428 0.568RI 0.499 0.633 0.414 0.526 0.453 0.574ALL 0.555 0.685 0.505 0.621 0.529 0.652Table 1: Evaluation on different datasets.。“ALL” the propor-tional mixture of the four base datasets.a comparison of five types of instruction datasets used in themulti-step fine-tuning process for Gllm.。The comparisonreveals that utilizing a mixture significantly outperformsindividual base datasets, which indicates a mixture aidsGllmin capturing human preferences and understandingthe implications of each abstracted state, thereby enhancinggoal generation.。Table 2 compares four kinds of fine-tuningwith the proposed multi-step fine-tuning, including (1) SFT:Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 1 COLLABORATIVE TRAINING1:Input: θthe parameters of πg,θ;β={βllm, βLoRA}forGllm,β ,βllmthe pre-trained parameters of Gllm;βLoRA thefine-tuned LoRA parameters of Gllm;I=IH∪ IS∪ IA∪ IRthe instruction set2:Reloading θ,βllm, merging βLoRA intoβllm3:forloop=1, 2, ... do4: Initialize a new βLoRA andβ={βllm, βLoRA}5: foriteration=1, 2, ..., n do6: Agents from a batch of workers send states {sj|j= 1, .. \n",
      "the piece 608\n",
      " \n",
      "sub-goal “moving speed” val-ues “very fast” versus “fast” may be perceived as a negativeinstance under precision measurement.。Consequently, weargue that the generation of such sub-goals should weighmore in choosing sub-goal than determining values.。Thus,we further propose three choice-based metrics: precision(choice), recall (choice), and F1 (choice).。Table 1 providesDataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)HI 0.435 0.611 0.361 0.517 0.395 0.560AI 0.474 0.611 0.419 0.532 0.445 0.569SI 0.444 0.601 0.413 0.539 0.428 0.568RI 0.499 0.633 0.414 0.526 0.453 0.574ALL 0.555 0.685 0.505 0.621 0.529 0.652Table 1: Evaluation on different datasets.。“ALL” the propor-tional mixture of the four base datasets.a comparison of five types of instruction datasets used in themulti-step fine-tuning process for Gllm.。The comparisonreveals that utilizing a mixture significantly outperformsindividual base datasets, which indicates a mixture aidsGllmin capturing human preferences and understandingthe implications of each abstracted state, thereby enhancinggoal generation.。Table 2 compares four kinds of fine-tuningwith the proposed multi-step fine-tuning, including (1) SFT:Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 1 COLLABORATIVE TRAINING1:Input: θthe parameters of πg,θ;β={βllm, βLoRA}forGllm,β ,βllmthe pre-trained parameters of Gllm;βLoRA thefine-tuned LoRA parameters of Gllm;I=IH∪ IS∪ IA∪ IRthe instruction set2:Reloading θ,βllm, merging βLoRA intoβllm3:forloop=1, 2, ... do4: Initialize a new βLoRA andβ={βllm, βLoRA}5: foriteration=1, 2, ..., n do6: Agents from a batch of workers send states {sj|j= 1, .。.. \n",
      "the piece 609\n",
      " \n",
      "choice-based metrics: precision(choice), recall (choice), and F1 (choice).。Table 1 providesDataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)HI 0.435 0.611 0.361 0.517 0.395 0.560AI 0.474 0.611 0.419 0.532 0.445 0.569SI 0.444 0.601 0.413 0.539 0.428 0.568RI 0.499 0.633 0.414 0.526 0.453 0.574ALL 0.555 0.685 0.505 0.621 0.529 0.652Table 1: Evaluation on different datasets.。“ALL” the propor-tional mixture of the four base datasets.a comparison of five types of instruction datasets used in themulti-step fine-tuning process for Gllm.。The comparisonreveals that utilizing a mixture significantly outperformsindividual base datasets, which indicates a mixture aidsGllmin capturing human preferences and understandingthe implications of each abstracted state, thereby enhancinggoal generation.。Table 2 compares four kinds of fine-tuningwith the proposed multi-step fine-tuning, including (1) SFT:Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 1 COLLABORATIVE TRAINING1:Input: θthe parameters of πg,θ;β={βllm, βLoRA}forGllm,β ,βllmthe pre-trained parameters of Gllm;βLoRA thefine-tuned LoRA parameters of Gllm;I=IH∪ IS∪ IA∪ IRthe instruction set2:Reloading θ,βllm, merging βLoRA intoβllm3:forloop=1, 2, ... do4: Initialize a new βLoRA andβ={βllm, βLoRA}5: foriteration=1, 2, ..., n do6: Agents from a batch of workers send states {sj|j= 1, .。.。.. \n",
      "the piece 610\n",
      " \n",
      ")RecallRecall(Choice)F1F1(Choice)HI 0.435 0.611 0.361 0.517 0.395 0.560AI 0.474 0.611 0.419 0.532 0.445 0.569SI 0.444 0.601 0.413 0.539 0.428 0.568RI 0.499 0.633 0.414 0.526 0.453 0.574ALL 0.555 0.685 0.505 0.621 0.529 0.652Table 1: Evaluation on different datasets.。“ALL” the propor-tional mixture of the four base datasets.a comparison of five types of instruction datasets used in themulti-step fine-tuning process for Gllm.。The comparisonreveals that utilizing a mixture significantly outperformsindividual base datasets, which indicates a mixture aidsGllmin capturing human preferences and understandingthe implications of each abstracted state, thereby enhancinggoal generation.。Table 2 compares four kinds of fine-tuningwith the proposed multi-step fine-tuning, including (1) SFT:Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 1 COLLABORATIVE TRAINING1:Input: θthe parameters of πg,θ;β={βllm, βLoRA}forGllm,β ,βllmthe pre-trained parameters of Gllm;βLoRA thefine-tuned LoRA parameters of Gllm;I=IH∪ IS∪ IA∪ IRthe instruction set2:Reloading θ,βllm, merging βLoRA intoβllm3:forloop=1, 2, ... do4: Initialize a new βLoRA andβ={βllm, βLoRA}5: foriteration=1, 2, ..., n do6: Agents from a batch of workers send states {sj|j= 1, .。.。.。, m }toGllm7: Random sample a batch of instructions: Itrain ={ιj|j= 1, .. \n",
      "the piece 611\n",
      " \n",
      "569SI 0.444 0.601 0.413 0.539 0.428 0.568RI 0.499 0.633 0.414 0.526 0.453 0.574ALL 0.555 0.685 0.505 0.621 0.529 0.652Table 1: Evaluation on different datasets.。“ALL” the propor-tional mixture of the four base datasets.a comparison of five types of instruction datasets used in themulti-step fine-tuning process for Gllm.。The comparisonreveals that utilizing a mixture significantly outperformsindividual base datasets, which indicates a mixture aidsGllmin capturing human preferences and understandingthe implications of each abstracted state, thereby enhancinggoal generation.。Table 2 compares four kinds of fine-tuningwith the proposed multi-step fine-tuning, including (1) SFT:Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 1 COLLABORATIVE TRAINING1:Input: θthe parameters of πg,θ;β={βllm, βLoRA}forGllm,β ,βllmthe pre-trained parameters of Gllm;βLoRA thefine-tuned LoRA parameters of Gllm;I=IH∪ IS∪ IA∪ IRthe instruction set2:Reloading θ,βllm, merging βLoRA intoβllm3:forloop=1, 2, ... do4: Initialize a new βLoRA andβ={βllm, βLoRA}5: foriteration=1, 2, ..., n do6: Agents from a batch of workers send states {sj|j= 1, .。.。.。, m }toGllm7: Random sample a batch of instructions: Itrain ={ιj|j= 1, .。.. \n",
      "the piece 612\n",
      " \n",
      "llm.。The comparisonreveals that utilizing a mixture significantly outperformsindividual base datasets, which indicates a mixture aidsGllmin capturing human preferences and understandingthe implications of each abstracted state, thereby enhancinggoal generation.。Table 2 compares four kinds of fine-tuningwith the proposed multi-step fine-tuning, including (1) SFT:Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 1 COLLABORATIVE TRAINING1:Input: θthe parameters of πg,θ;β={βllm, βLoRA}forGllm,β ,βllmthe pre-trained parameters of Gllm;βLoRA thefine-tuned LoRA parameters of Gllm;I=IH∪ IS∪ IA∪ IRthe instruction set2:Reloading θ,βllm, merging βLoRA intoβllm3:forloop=1, 2, ... do4: Initialize a new βLoRA andβ={βllm, βLoRA}5: foriteration=1, 2, ..., n do6: Agents from a batch of workers send states {sj|j= 1, .。.。.。, m }toGllm7: Random sample a batch of instructions: Itrain ={ιj|j= 1, .。.。.. \n",
      "the piece 613\n",
      " \n",
      "ences and understandingthe implications of each abstracted state, thereby enhancinggoal generation.。Table 2 compares four kinds of fine-tuningwith the proposed multi-step fine-tuning, including (1) SFT:Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 1 COLLABORATIVE TRAINING1:Input: θthe parameters of πg,θ;β={βllm, βLoRA}forGllm,β ,βllmthe pre-trained parameters of Gllm;βLoRA thefine-tuned LoRA parameters of Gllm;I=IH∪ IS∪ IA∪ IRthe instruction set2:Reloading θ,βllm, merging βLoRA intoβllm3:forloop=1, 2, ... do4: Initialize a new βLoRA andβ={βllm, βLoRA}5: foriteration=1, 2, ..., n do6: Agents from a batch of workers send states {sj|j= 1, .。.。.。, m }toGllm7: Random sample a batch of instructions: Itrain ={ιj|j= 1, .。.。.。, m } ⊂ I8: Generate goals in string with LLM: Gs={gj∼Gllm,β(sj, ιj)|j= 1, .. \n",
      "the piece 614\n",
      " \n",
      "idirectional AdaptationAlgorithm 1 COLLABORATIVE TRAINING1:Input: θthe parameters of πg,θ;β={βllm, βLoRA}forGllm,β ,βllmthe pre-trained parameters of Gllm;βLoRA thefine-tuned LoRA parameters of Gllm;I=IH∪ IS∪ IA∪ IRthe instruction set2:Reloading θ,βllm, merging βLoRA intoβllm3:forloop=1, 2, ... do4: Initialize a new βLoRA andβ={βllm, βLoRA}5: foriteration=1, 2, ..., n do6: Agents from a batch of workers send states {sj|j= 1, .。.。.。, m }toGllm7: Random sample a batch of instructions: Itrain ={ιj|j= 1, .。.。.。, m } ⊂ I8: Generate goals in string with LLM: Gs={gj∼Gllm,β(sj, ιj)|j= 1, .。.. \n",
      "the piece 615\n",
      " \n",
      ".。.。, m }and parse Gsto formatted goals: G9: Distribute Gto agents πg,θ, then collect trajectories {τj}and returns {Rj}to form(τ, R) ={(τj, Rj)|j= 1, .。.。.。, m },where τj={gj, s1, a1, r1, .。.。.。, s Tj, aTj, rTj}, Rj=TjXt=1rt10: Update θwith Equation (6) and (τ, R)11: Filter completed goals GcfromG, extract Rthe set from (τ, R)as agent feedback rewards Rf12: Compute examination reward by evaluating crucial sub-goal completion as Re=REWARD e(Gc,IS∩ Itrain,G),and formatting reward: Rm=REWARD m(gs, g), refer to Appendix K13: Update βLoRA with PPO, and merge updated βLoRA intoβllm14: end for15:end forTuning PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)SFT 0.547 0.663 0.490 0.602 0.517 0.632CoTF 0.533 0.652 0.487 0.599 0.509 0.624CoTF→SFT 0.555 0.685 0.505 0.621 0.529 0.652CoTF→SFT→EFT 0.590 0.694 0.501 0.593 0.516 0.629Table 2: Evaluation on different tuning methods.only use the target prompt without CoT data to supervisedfine-tuning, which can be regarded as a baseline for a naiveSFT; (2) CoTF: only CoT-assisted fine-tuning; (3) CoTF→SFT: further SFT target prompt after CoTF; (4) CoTF→SFT→EFT: further ensemble fine-tuning target promptafter CoTF.. \n",
      "the piece 616\n",
      " \n",
      "。, m }and parse Gsto formatted goals: G9: Distribute Gto agents πg,θ, then collect trajectories {τj}and returns {Rj}to form(τ, R) ={(τj, Rj)|j= 1, .。.。.。, m },where τj={gj, s1, a1, r1, .。.。.。, s Tj, aTj, rTj}, Rj=TjXt=1rt10: Update θwith Equation (6) and (τ, R)11: Filter completed goals GcfromG, extract Rthe set from (τ, R)as agent feedback rewards Rf12: Compute examination reward by evaluating crucial sub-goal completion as Re=REWARD e(Gc,IS∩ Itrain,G),and formatting reward: Rm=REWARD m(gs, g), refer to Appendix K13: Update βLoRA with PPO, and merge updated βLoRA intoβllm14: end for15:end forTuning PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)SFT 0.547 0.663 0.490 0.602 0.517 0.632CoTF 0.533 0.652 0.487 0.599 0.509 0.624CoTF→SFT 0.555 0.685 0.505 0.621 0.529 0.652CoTF→SFT→EFT 0.590 0.694 0.501 0.593 0.516 0.629Table 2: Evaluation on different tuning methods.only use the target prompt without CoT data to supervisedfine-tuning, which can be regarded as a baseline for a naiveSFT; (2) CoTF: only CoT-assisted fine-tuning; (3) CoTF→SFT: further SFT target prompt after CoTF; (4) CoTF→SFT→EFT: further ensemble fine-tuning target promptafter CoTF.。With the comparison, we conclude that CoTFand SFT can improve each other and achieve better per-formance.. \n",
      "the piece 617\n",
      " \n",
      "m }and parse Gsto formatted goals: G9: Distribute Gto agents πg,θ, then collect trajectories {τj}and returns {Rj}to form(τ, R) ={(τj, Rj)|j= 1, .。.。.。, m },where τj={gj, s1, a1, r1, .。.。.。, s Tj, aTj, rTj}, Rj=TjXt=1rt10: Update θwith Equation (6) and (τ, R)11: Filter completed goals GcfromG, extract Rthe set from (τ, R)as agent feedback rewards Rf12: Compute examination reward by evaluating crucial sub-goal completion as Re=REWARD e(Gc,IS∩ Itrain,G),and formatting reward: Rm=REWARD m(gs, g), refer to Appendix K13: Update βLoRA with PPO, and merge updated βLoRA intoβllm14: end for15:end forTuning PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)SFT 0.547 0.663 0.490 0.602 0.517 0.632CoTF 0.533 0.652 0.487 0.599 0.509 0.624CoTF→SFT 0.555 0.685 0.505 0.621 0.529 0.652CoTF→SFT→EFT 0.590 0.694 0.501 0.593 0.516 0.629Table 2: Evaluation on different tuning methods.only use the target prompt without CoT data to supervisedfine-tuning, which can be regarded as a baseline for a naiveSFT; (2) CoTF: only CoT-assisted fine-tuning; (3) CoTF→SFT: further SFT target prompt after CoTF; (4) CoTF→SFT→EFT: further ensemble fine-tuning target promptafter CoTF.。With the comparison, we conclude that CoTFand SFT can improve each other and achieve better per-formance.。Furthermore, ensemble fine-tuning significantlyenhances precision while marginally decreasing recall, mak-ing it more suitable for generating accurate concise goals.5.3.. \n",
      "the piece 618\n",
      " \n",
      "llect trajectories {τj}and returns {Rj}to form(τ, R) ={(τj, Rj)|j= 1, .。.。.。, m },where τj={gj, s1, a1, r1, .。.。.。, s Tj, aTj, rTj}, Rj=TjXt=1rt10: Update θwith Equation (6) and (τ, R)11: Filter completed goals GcfromG, extract Rthe set from (τ, R)as agent feedback rewards Rf12: Compute examination reward by evaluating crucial sub-goal completion as Re=REWARD e(Gc,IS∩ Itrain,G),and formatting reward: Rm=REWARD m(gs, g), refer to Appendix K13: Update βLoRA with PPO, and merge updated βLoRA intoβllm14: end for15:end forTuning PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)SFT 0.547 0.663 0.490 0.602 0.517 0.632CoTF 0.533 0.652 0.487 0.599 0.509 0.624CoTF→SFT 0.555 0.685 0.505 0.621 0.529 0.652CoTF→SFT→EFT 0.590 0.694 0.501 0.593 0.516 0.629Table 2: Evaluation on different tuning methods.only use the target prompt without CoT data to supervisedfine-tuning, which can be regarded as a baseline for a naiveSFT; (2) CoTF: only CoT-assisted fine-tuning; (3) CoTF→SFT: further SFT target prompt after CoTF; (4) CoTF→SFT→EFT: further ensemble fine-tuning target promptafter CoTF.。With the comparison, we conclude that CoTFand SFT can improve each other and achieve better per-formance.。Furthermore, ensemble fine-tuning significantlyenhances precision while marginally decreasing recall, mak-ing it more suitable for generating accurate concise goals.5.3.。Evaluating Co-trainingWe conduct an analysis of the completion ratio correspond-ing to the number of valid sub-goals during the co-trainingprocess.. \n",
      "the piece 619\n",
      " \n",
      "ct trajectories {τj}and returns {Rj}to form(τ, R) ={(τj, Rj)|j= 1, .。.。.。, m },where τj={gj, s1, a1, r1, .。.。.。, s Tj, aTj, rTj}, Rj=TjXt=1rt10: Update θwith Equation (6) and (τ, R)11: Filter completed goals GcfromG, extract Rthe set from (τ, R)as agent feedback rewards Rf12: Compute examination reward by evaluating crucial sub-goal completion as Re=REWARD e(Gc,IS∩ Itrain,G),and formatting reward: Rm=REWARD m(gs, g), refer to Appendix K13: Update βLoRA with PPO, and merge updated βLoRA intoβllm14: end for15:end forTuning PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)SFT 0.547 0.663 0.490 0.602 0.517 0.632CoTF 0.533 0.652 0.487 0.599 0.509 0.624CoTF→SFT 0.555 0.685 0.505 0.621 0.529 0.652CoTF→SFT→EFT 0.590 0.694 0.501 0.593 0.516 0.629Table 2: Evaluation on different tuning methods.only use the target prompt without CoT data to supervisedfine-tuning, which can be regarded as a baseline for a naiveSFT; (2) CoTF: only CoT-assisted fine-tuning; (3) CoTF→SFT: further SFT target prompt after CoTF; (4) CoTF→SFT→EFT: further ensemble fine-tuning target promptafter CoTF.。With the comparison, we conclude that CoTFand SFT can improve each other and achieve better per-formance.。Furthermore, ensemble fine-tuning significantlyenhances precision while marginally decreasing recall, mak-ing it more suitable for generating accurate concise goals.5.3.。Evaluating Co-trainingWe conduct an analysis of the completion ratio correspond-ing to the number of valid sub-goals during the co-trainingprocess.。Though the dimension size of goal space achieves68, the number of sub-goals for valid goals predominantlyfalls within the range of 1 to 7.. \n",
      "the piece 620\n",
      " \n",
      "trajectories {τj}and returns {Rj}to form(τ, R) ={(τj, Rj)|j= 1, .。.。.。, m },where τj={gj, s1, a1, r1, .。.。.。, s Tj, aTj, rTj}, Rj=TjXt=1rt10: Update θwith Equation (6) and (τ, R)11: Filter completed goals GcfromG, extract Rthe set from (τ, R)as agent feedback rewards Rf12: Compute examination reward by evaluating crucial sub-goal completion as Re=REWARD e(Gc,IS∩ Itrain,G),and formatting reward: Rm=REWARD m(gs, g), refer to Appendix K13: Update βLoRA with PPO, and merge updated βLoRA intoβllm14: end for15:end forTuning PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)SFT 0.547 0.663 0.490 0.602 0.517 0.632CoTF 0.533 0.652 0.487 0.599 0.509 0.624CoTF→SFT 0.555 0.685 0.505 0.621 0.529 0.652CoTF→SFT→EFT 0.590 0.694 0.501 0.593 0.516 0.629Table 2: Evaluation on different tuning methods.only use the target prompt without CoT data to supervisedfine-tuning, which can be regarded as a baseline for a naiveSFT; (2) CoTF: only CoT-assisted fine-tuning; (3) CoTF→SFT: further SFT target prompt after CoTF; (4) CoTF→SFT→EFT: further ensemble fine-tuning target promptafter CoTF.。With the comparison, we conclude that CoTFand SFT can improve each other and achieve better per-formance.。Furthermore, ensemble fine-tuning significantlyenhances precision while marginally decreasing recall, mak-ing it more suitable for generating accurate concise goals.5.3.。Evaluating Co-trainingWe conduct an analysis of the completion ratio correspond-ing to the number of valid sub-goals during the co-trainingprocess.。Though the dimension size of goal space achieves68, the number of sub-goals for valid goals predominantlyfalls within the range of 1 to 7.。This is rational as completinga goal with an excessive number of sub-goals is exceedinglychallenging for a policy, even impossibility for human toachieve.. \n",
      "the piece 621\n",
      " \n",
      "ere τj={gj, s1, a1, r1, .。.。.。, s Tj, aTj, rTj}, Rj=TjXt=1rt10: Update θwith Equation (6) and (τ, R)11: Filter completed goals GcfromG, extract Rthe set from (τ, R)as agent feedback rewards Rf12: Compute examination reward by evaluating crucial sub-goal completion as Re=REWARD e(Gc,IS∩ Itrain,G),and formatting reward: Rm=REWARD m(gs, g), refer to Appendix K13: Update βLoRA with PPO, and merge updated βLoRA intoβllm14: end for15:end forTuning PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)SFT 0.547 0.663 0.490 0.602 0.517 0.632CoTF 0.533 0.652 0.487 0.599 0.509 0.624CoTF→SFT 0.555 0.685 0.505 0.621 0.529 0.652CoTF→SFT→EFT 0.590 0.694 0.501 0.593 0.516 0.629Table 2: Evaluation on different tuning methods.only use the target prompt without CoT data to supervisedfine-tuning, which can be regarded as a baseline for a naiveSFT; (2) CoTF: only CoT-assisted fine-tuning; (3) CoTF→SFT: further SFT target prompt after CoTF; (4) CoTF→SFT→EFT: further ensemble fine-tuning target promptafter CoTF.。With the comparison, we conclude that CoTFand SFT can improve each other and achieve better per-formance.。Furthermore, ensemble fine-tuning significantlyenhances precision while marginally decreasing recall, mak-ing it more suitable for generating accurate concise goals.5.3.。Evaluating Co-trainingWe conduct an analysis of the completion ratio correspond-ing to the number of valid sub-goals during the co-trainingprocess.。Though the dimension size of goal space achieves68, the number of sub-goals for valid goals predominantlyfalls within the range of 1 to 7.。This is rational as completinga goal with an excessive number of sub-goals is exceedinglychallenging for a policy, even impossibility for human toachieve.。Furthermore, Figure 3(a) shows that the improve-Instruction Goal(Before co-training) Goal(After co-training)Stop!Whether prone position: TrueAverage velocity: StaticAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementGet down,stay hidden.Whether prone position: TrueAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementAverage velocity: StaticWhether seen by enemy: FalseEnemy!Rush and fire.Whether hold a gun: TrueWhether have bullets: TrueHorizontal direction of view: SouthwestWhether seen enemy: TrueAverage velocity: FastLength of distance moved: longAverage velocity: FastWhether hold a gun: TrueHorizontal direction of movement: SouthwestWhether seen enemy: TrueDamage to enemy: HighHorizontal direction of view: SouthwestEnemies nearby,move to defendand avoid damage.Whether prone position: TrueAverage velocity: FastWhether hold a gun: TrueHealth level: FullWhether to restore health: TrueLength of distance moved: longAverage velocity: FastWhether prone position: FalseHorizontal direction of movement: North(Position of enemy: South)Whether hold a gun: TrueTable 3: Comparison of goal-generation.. \n",
      "the piece 622\n",
      " \n",
      " τj={gj, s1, a1, r1, .。.。.。, s Tj, aTj, rTj}, Rj=TjXt=1rt10: Update θwith Equation (6) and (τ, R)11: Filter completed goals GcfromG, extract Rthe set from (τ, R)as agent feedback rewards Rf12: Compute examination reward by evaluating crucial sub-goal completion as Re=REWARD e(Gc,IS∩ Itrain,G),and formatting reward: Rm=REWARD m(gs, g), refer to Appendix K13: Update βLoRA with PPO, and merge updated βLoRA intoβllm14: end for15:end forTuning PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)SFT 0.547 0.663 0.490 0.602 0.517 0.632CoTF 0.533 0.652 0.487 0.599 0.509 0.624CoTF→SFT 0.555 0.685 0.505 0.621 0.529 0.652CoTF→SFT→EFT 0.590 0.694 0.501 0.593 0.516 0.629Table 2: Evaluation on different tuning methods.only use the target prompt without CoT data to supervisedfine-tuning, which can be regarded as a baseline for a naiveSFT; (2) CoTF: only CoT-assisted fine-tuning; (3) CoTF→SFT: further SFT target prompt after CoTF; (4) CoTF→SFT→EFT: further ensemble fine-tuning target promptafter CoTF.。With the comparison, we conclude that CoTFand SFT can improve each other and achieve better per-formance.。Furthermore, ensemble fine-tuning significantlyenhances precision while marginally decreasing recall, mak-ing it more suitable for generating accurate concise goals.5.3.。Evaluating Co-trainingWe conduct an analysis of the completion ratio correspond-ing to the number of valid sub-goals during the co-trainingprocess.。Though the dimension size of goal space achieves68, the number of sub-goals for valid goals predominantlyfalls within the range of 1 to 7.。This is rational as completinga goal with an excessive number of sub-goals is exceedinglychallenging for a policy, even impossibility for human toachieve.。Furthermore, Figure 3(a) shows that the improve-Instruction Goal(Before co-training) Goal(After co-training)Stop!Whether prone position: TrueAverage velocity: StaticAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementGet down,stay hidden.Whether prone position: TrueAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementAverage velocity: StaticWhether seen by enemy: FalseEnemy!Rush and fire.Whether hold a gun: TrueWhether have bullets: TrueHorizontal direction of view: SouthwestWhether seen enemy: TrueAverage velocity: FastLength of distance moved: longAverage velocity: FastWhether hold a gun: TrueHorizontal direction of movement: SouthwestWhether seen enemy: TrueDamage to enemy: HighHorizontal direction of view: SouthwestEnemies nearby,move to defendand avoid damage.Whether prone position: TrueAverage velocity: FastWhether hold a gun: TrueHealth level: FullWhether to restore health: TrueLength of distance moved: longAverage velocity: FastWhether prone position: FalseHorizontal direction of movement: North(Position of enemy: South)Whether hold a gun: TrueTable 3: Comparison of goal-generation.。Cyan the helpful,pink the conflicting, and orange the critical sub-goals.. \n",
      "the piece 623\n",
      " \n",
      "={gj, s1, a1, r1, .。.。.。, s Tj, aTj, rTj}, Rj=TjXt=1rt10: Update θwith Equation (6) and (τ, R)11: Filter completed goals GcfromG, extract Rthe set from (τ, R)as agent feedback rewards Rf12: Compute examination reward by evaluating crucial sub-goal completion as Re=REWARD e(Gc,IS∩ Itrain,G),and formatting reward: Rm=REWARD m(gs, g), refer to Appendix K13: Update βLoRA with PPO, and merge updated βLoRA intoβllm14: end for15:end forTuning PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)SFT 0.547 0.663 0.490 0.602 0.517 0.632CoTF 0.533 0.652 0.487 0.599 0.509 0.624CoTF→SFT 0.555 0.685 0.505 0.621 0.529 0.652CoTF→SFT→EFT 0.590 0.694 0.501 0.593 0.516 0.629Table 2: Evaluation on different tuning methods.only use the target prompt without CoT data to supervisedfine-tuning, which can be regarded as a baseline for a naiveSFT; (2) CoTF: only CoT-assisted fine-tuning; (3) CoTF→SFT: further SFT target prompt after CoTF; (4) CoTF→SFT→EFT: further ensemble fine-tuning target promptafter CoTF.。With the comparison, we conclude that CoTFand SFT can improve each other and achieve better per-formance.。Furthermore, ensemble fine-tuning significantlyenhances precision while marginally decreasing recall, mak-ing it more suitable for generating accurate concise goals.5.3.。Evaluating Co-trainingWe conduct an analysis of the completion ratio correspond-ing to the number of valid sub-goals during the co-trainingprocess.。Though the dimension size of goal space achieves68, the number of sub-goals for valid goals predominantlyfalls within the range of 1 to 7.。This is rational as completinga goal with an excessive number of sub-goals is exceedinglychallenging for a policy, even impossibility for human toachieve.。Furthermore, Figure 3(a) shows that the improve-Instruction Goal(Before co-training) Goal(After co-training)Stop!Whether prone position: TrueAverage velocity: StaticAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementGet down,stay hidden.Whether prone position: TrueAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementAverage velocity: StaticWhether seen by enemy: FalseEnemy!Rush and fire.Whether hold a gun: TrueWhether have bullets: TrueHorizontal direction of view: SouthwestWhether seen enemy: TrueAverage velocity: FastLength of distance moved: longAverage velocity: FastWhether hold a gun: TrueHorizontal direction of movement: SouthwestWhether seen enemy: TrueDamage to enemy: HighHorizontal direction of view: SouthwestEnemies nearby,move to defendand avoid damage.Whether prone position: TrueAverage velocity: FastWhether hold a gun: TrueHealth level: FullWhether to restore health: TrueLength of distance moved: longAverage velocity: FastWhether prone position: FalseHorizontal direction of movement: North(Position of enemy: South)Whether hold a gun: TrueTable 3: Comparison of goal-generation.。Cyan the helpful,pink the conflicting, and orange the critical sub-goals.。Itis evident that co-training enables goal-generation to avoidconflicts of sub-goals and improves reasonability by includ-ing helpful and critical sub-goals.ments mainly lies in 2≤ |g| ≤4, because |g|= 1 is tooeasy while |g| ≥5is too hard to complete.. \n",
      "the piece 624\n",
      " \n",
      "om (τ, R)as agent feedback rewards Rf12: Compute examination reward by evaluating crucial sub-goal completion as Re=REWARD e(Gc,IS∩ Itrain,G),and formatting reward: Rm=REWARD m(gs, g), refer to Appendix K13: Update βLoRA with PPO, and merge updated βLoRA intoβllm14: end for15:end forTuning PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)SFT 0.547 0.663 0.490 0.602 0.517 0.632CoTF 0.533 0.652 0.487 0.599 0.509 0.624CoTF→SFT 0.555 0.685 0.505 0.621 0.529 0.652CoTF→SFT→EFT 0.590 0.694 0.501 0.593 0.516 0.629Table 2: Evaluation on different tuning methods.only use the target prompt without CoT data to supervisedfine-tuning, which can be regarded as a baseline for a naiveSFT; (2) CoTF: only CoT-assisted fine-tuning; (3) CoTF→SFT: further SFT target prompt after CoTF; (4) CoTF→SFT→EFT: further ensemble fine-tuning target promptafter CoTF.。With the comparison, we conclude that CoTFand SFT can improve each other and achieve better per-formance.。Furthermore, ensemble fine-tuning significantlyenhances precision while marginally decreasing recall, mak-ing it more suitable for generating accurate concise goals.5.3.。Evaluating Co-trainingWe conduct an analysis of the completion ratio correspond-ing to the number of valid sub-goals during the co-trainingprocess.。Though the dimension size of goal space achieves68, the number of sub-goals for valid goals predominantlyfalls within the range of 1 to 7.。This is rational as completinga goal with an excessive number of sub-goals is exceedinglychallenging for a policy, even impossibility for human toachieve.。Furthermore, Figure 3(a) shows that the improve-Instruction Goal(Before co-training) Goal(After co-training)Stop!Whether prone position: TrueAverage velocity: StaticAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementGet down,stay hidden.Whether prone position: TrueAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementAverage velocity: StaticWhether seen by enemy: FalseEnemy!Rush and fire.Whether hold a gun: TrueWhether have bullets: TrueHorizontal direction of view: SouthwestWhether seen enemy: TrueAverage velocity: FastLength of distance moved: longAverage velocity: FastWhether hold a gun: TrueHorizontal direction of movement: SouthwestWhether seen enemy: TrueDamage to enemy: HighHorizontal direction of view: SouthwestEnemies nearby,move to defendand avoid damage.Whether prone position: TrueAverage velocity: FastWhether hold a gun: TrueHealth level: FullWhether to restore health: TrueLength of distance moved: longAverage velocity: FastWhether prone position: FalseHorizontal direction of movement: North(Position of enemy: South)Whether hold a gun: TrueTable 3: Comparison of goal-generation.。Cyan the helpful,pink the conflicting, and orange the critical sub-goals.。Itis evident that co-training enables goal-generation to avoidconflicts of sub-goals and improves reasonability by includ-ing helpful and critical sub-goals.ments mainly lies in 2≤ |g| ≤4, because |g|= 1 is tooeasy while |g| ≥5is too hard to complete.。Figure 3(b)shows a case of |g|= 3 that co-training indeed improvesthe completion ratio as the green curve.. \n",
      "the piece 625\n",
      " \n",
      "(τ, R)as agent feedback rewards Rf12: Compute examination reward by evaluating crucial sub-goal completion as Re=REWARD e(Gc,IS∩ Itrain,G),and formatting reward: Rm=REWARD m(gs, g), refer to Appendix K13: Update βLoRA with PPO, and merge updated βLoRA intoβllm14: end for15:end forTuning PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)SFT 0.547 0.663 0.490 0.602 0.517 0.632CoTF 0.533 0.652 0.487 0.599 0.509 0.624CoTF→SFT 0.555 0.685 0.505 0.621 0.529 0.652CoTF→SFT→EFT 0.590 0.694 0.501 0.593 0.516 0.629Table 2: Evaluation on different tuning methods.only use the target prompt without CoT data to supervisedfine-tuning, which can be regarded as a baseline for a naiveSFT; (2) CoTF: only CoT-assisted fine-tuning; (3) CoTF→SFT: further SFT target prompt after CoTF; (4) CoTF→SFT→EFT: further ensemble fine-tuning target promptafter CoTF.。With the comparison, we conclude that CoTFand SFT can improve each other and achieve better per-formance.。Furthermore, ensemble fine-tuning significantlyenhances precision while marginally decreasing recall, mak-ing it more suitable for generating accurate concise goals.5.3.。Evaluating Co-trainingWe conduct an analysis of the completion ratio correspond-ing to the number of valid sub-goals during the co-trainingprocess.。Though the dimension size of goal space achieves68, the number of sub-goals for valid goals predominantlyfalls within the range of 1 to 7.。This is rational as completinga goal with an excessive number of sub-goals is exceedinglychallenging for a policy, even impossibility for human toachieve.。Furthermore, Figure 3(a) shows that the improve-Instruction Goal(Before co-training) Goal(After co-training)Stop!Whether prone position: TrueAverage velocity: StaticAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementGet down,stay hidden.Whether prone position: TrueAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementAverage velocity: StaticWhether seen by enemy: FalseEnemy!Rush and fire.Whether hold a gun: TrueWhether have bullets: TrueHorizontal direction of view: SouthwestWhether seen enemy: TrueAverage velocity: FastLength of distance moved: longAverage velocity: FastWhether hold a gun: TrueHorizontal direction of movement: SouthwestWhether seen enemy: TrueDamage to enemy: HighHorizontal direction of view: SouthwestEnemies nearby,move to defendand avoid damage.Whether prone position: TrueAverage velocity: FastWhether hold a gun: TrueHealth level: FullWhether to restore health: TrueLength of distance moved: longAverage velocity: FastWhether prone position: FalseHorizontal direction of movement: North(Position of enemy: South)Whether hold a gun: TrueTable 3: Comparison of goal-generation.。Cyan the helpful,pink the conflicting, and orange the critical sub-goals.。Itis evident that co-training enables goal-generation to avoidconflicts of sub-goals and improves reasonability by includ-ing helpful and critical sub-goals.ments mainly lies in 2≤ |g| ≤4, because |g|= 1 is tooeasy while |g| ≥5is too hard to complete.。Figure 3(b)shows a case of |g|= 3 that co-training indeed improvesthe completion ratio as the green curve.。It is noteworthythat the performance suddenly downgrades at each reset.This phenomenon is attributed to the reset of Gllmbreaksthe adaptation with πg, avoiding being trapped in local opti-mal.. \n",
      "the piece 626\n",
      " \n",
      " R)as agent feedback rewards Rf12: Compute examination reward by evaluating crucial sub-goal completion as Re=REWARD e(Gc,IS∩ Itrain,G),and formatting reward: Rm=REWARD m(gs, g), refer to Appendix K13: Update βLoRA with PPO, and merge updated βLoRA intoβllm14: end for15:end forTuning PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)SFT 0.547 0.663 0.490 0.602 0.517 0.632CoTF 0.533 0.652 0.487 0.599 0.509 0.624CoTF→SFT 0.555 0.685 0.505 0.621 0.529 0.652CoTF→SFT→EFT 0.590 0.694 0.501 0.593 0.516 0.629Table 2: Evaluation on different tuning methods.only use the target prompt without CoT data to supervisedfine-tuning, which can be regarded as a baseline for a naiveSFT; (2) CoTF: only CoT-assisted fine-tuning; (3) CoTF→SFT: further SFT target prompt after CoTF; (4) CoTF→SFT→EFT: further ensemble fine-tuning target promptafter CoTF.。With the comparison, we conclude that CoTFand SFT can improve each other and achieve better per-formance.。Furthermore, ensemble fine-tuning significantlyenhances precision while marginally decreasing recall, mak-ing it more suitable for generating accurate concise goals.5.3.。Evaluating Co-trainingWe conduct an analysis of the completion ratio correspond-ing to the number of valid sub-goals during the co-trainingprocess.。Though the dimension size of goal space achieves68, the number of sub-goals for valid goals predominantlyfalls within the range of 1 to 7.。This is rational as completinga goal with an excessive number of sub-goals is exceedinglychallenging for a policy, even impossibility for human toachieve.。Furthermore, Figure 3(a) shows that the improve-Instruction Goal(Before co-training) Goal(After co-training)Stop!Whether prone position: TrueAverage velocity: StaticAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementGet down,stay hidden.Whether prone position: TrueAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementAverage velocity: StaticWhether seen by enemy: FalseEnemy!Rush and fire.Whether hold a gun: TrueWhether have bullets: TrueHorizontal direction of view: SouthwestWhether seen enemy: TrueAverage velocity: FastLength of distance moved: longAverage velocity: FastWhether hold a gun: TrueHorizontal direction of movement: SouthwestWhether seen enemy: TrueDamage to enemy: HighHorizontal direction of view: SouthwestEnemies nearby,move to defendand avoid damage.Whether prone position: TrueAverage velocity: FastWhether hold a gun: TrueHealth level: FullWhether to restore health: TrueLength of distance moved: longAverage velocity: FastWhether prone position: FalseHorizontal direction of movement: North(Position of enemy: South)Whether hold a gun: TrueTable 3: Comparison of goal-generation.。Cyan the helpful,pink the conflicting, and orange the critical sub-goals.。Itis evident that co-training enables goal-generation to avoidconflicts of sub-goals and improves reasonability by includ-ing helpful and critical sub-goals.ments mainly lies in 2≤ |g| ≤4, because |g|= 1 is tooeasy while |g| ≥5is too hard to complete.。Figure 3(b)shows a case of |g|= 3 that co-training indeed improvesthe completion ratio as the green curve.。It is noteworthythat the performance suddenly downgrades at each reset.This phenomenon is attributed to the reset of Gllmbreaksthe adaptation with πg, avoiding being trapped in local opti-mal.。Meanwhile, the performance tends to converge, whichindicates the successor loops produce a better adaptationBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps24252627282930Goal Completion Ratio (%)OursHER(a)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps15.017.520.022.525.027.530.032.5Goal Completion Ratio (%)OursHER (b)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps0.100.080.060.040.020.00Reward0.30.40.50.60.70.8Game StatisticsMean basic reward per stepMean basic reward per step (No KL)#Enemies killed#Enemies knocked down#Enemies killed (No KL)#Enemies knocked down (No KL) (c)Figure 2: (a) The goal completion rate on training dataset; (b) The goal completion rate on unseen goals, i.e., the test dataset;(c) The evaluation of policy learning in cases of w/ and w/o KL-divergence regularizer.. \n",
      "the piece 627\n",
      " \n",
      "Compute examination reward by evaluating crucial sub-goal completion as Re=REWARD e(Gc,IS∩ Itrain,G),and formatting reward: Rm=REWARD m(gs, g), refer to Appendix K13: Update βLoRA with PPO, and merge updated βLoRA intoβllm14: end for15:end forTuning PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)SFT 0.547 0.663 0.490 0.602 0.517 0.632CoTF 0.533 0.652 0.487 0.599 0.509 0.624CoTF→SFT 0.555 0.685 0.505 0.621 0.529 0.652CoTF→SFT→EFT 0.590 0.694 0.501 0.593 0.516 0.629Table 2: Evaluation on different tuning methods.only use the target prompt without CoT data to supervisedfine-tuning, which can be regarded as a baseline for a naiveSFT; (2) CoTF: only CoT-assisted fine-tuning; (3) CoTF→SFT: further SFT target prompt after CoTF; (4) CoTF→SFT→EFT: further ensemble fine-tuning target promptafter CoTF.。With the comparison, we conclude that CoTFand SFT can improve each other and achieve better per-formance.。Furthermore, ensemble fine-tuning significantlyenhances precision while marginally decreasing recall, mak-ing it more suitable for generating accurate concise goals.5.3.。Evaluating Co-trainingWe conduct an analysis of the completion ratio correspond-ing to the number of valid sub-goals during the co-trainingprocess.。Though the dimension size of goal space achieves68, the number of sub-goals for valid goals predominantlyfalls within the range of 1 to 7.。This is rational as completinga goal with an excessive number of sub-goals is exceedinglychallenging for a policy, even impossibility for human toachieve.。Furthermore, Figure 3(a) shows that the improve-Instruction Goal(Before co-training) Goal(After co-training)Stop!Whether prone position: TrueAverage velocity: StaticAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementGet down,stay hidden.Whether prone position: TrueAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementAverage velocity: StaticWhether seen by enemy: FalseEnemy!Rush and fire.Whether hold a gun: TrueWhether have bullets: TrueHorizontal direction of view: SouthwestWhether seen enemy: TrueAverage velocity: FastLength of distance moved: longAverage velocity: FastWhether hold a gun: TrueHorizontal direction of movement: SouthwestWhether seen enemy: TrueDamage to enemy: HighHorizontal direction of view: SouthwestEnemies nearby,move to defendand avoid damage.Whether prone position: TrueAverage velocity: FastWhether hold a gun: TrueHealth level: FullWhether to restore health: TrueLength of distance moved: longAverage velocity: FastWhether prone position: FalseHorizontal direction of movement: North(Position of enemy: South)Whether hold a gun: TrueTable 3: Comparison of goal-generation.。Cyan the helpful,pink the conflicting, and orange the critical sub-goals.。Itis evident that co-training enables goal-generation to avoidconflicts of sub-goals and improves reasonability by includ-ing helpful and critical sub-goals.ments mainly lies in 2≤ |g| ≤4, because |g|= 1 is tooeasy while |g| ≥5is too hard to complete.。Figure 3(b)shows a case of |g|= 3 that co-training indeed improvesthe completion ratio as the green curve.。It is noteworthythat the performance suddenly downgrades at each reset.This phenomenon is attributed to the reset of Gllmbreaksthe adaptation with πg, avoiding being trapped in local opti-mal.。Meanwhile, the performance tends to converge, whichindicates the successor loops produce a better adaptationBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps24252627282930Goal Completion Ratio (%)OursHER(a)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps15.017.520.022.525.027.530.032.5Goal Completion Ratio (%)OursHER (b)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps0.100.080.060.040.020.00Reward0.30.40.50.60.70.8Game StatisticsMean basic reward per stepMean basic reward per step (No KL)#Enemies killed#Enemies knocked down#Enemies killed (No KL)#Enemies knocked down (No KL) (c)Figure 2: (a) The goal completion rate on training dataset; (b) The goal completion rate on unseen goals, i.e., the test dataset;(c) The evaluation of policy learning in cases of w/ and w/o KL-divergence regularizer.。(a) Goal completion rate ( 1≤ |g| ≤7)Trend of  p o l i c y  a b i l i t ySmoothe d  C o m p l e t i o n  R a t eOriginal C o m p l e t i o n  R a t eReset Lo o p sGoal Com p l e t i o n  R a t i o  ( % )(b) Goal completion rate ( |g|= 3)Sub goal        g4   g8   g12   g16   g20 Time in one loop0.0250.0500.0750.1000.1250.1500.1750.200Possibility (c) Sub-goals distributionFigure 3: (a) The completion ratio of goals with dimension size ranges from 1 to 7; (b) The goal completion ratio of goalsthat|g|= 3, the trend curve reflects the improving completion ratio; (c) The sub-goals distribution changes along thetraining in one loop of co-training, where the description of each giis included in Table 14.between LLM and policy than before.. \n",
      "the piece 628\n",
      " \n",
      "pute examination reward by evaluating crucial sub-goal completion as Re=REWARD e(Gc,IS∩ Itrain,G),and formatting reward: Rm=REWARD m(gs, g), refer to Appendix K13: Update βLoRA with PPO, and merge updated βLoRA intoβllm14: end for15:end forTuning PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)SFT 0.547 0.663 0.490 0.602 0.517 0.632CoTF 0.533 0.652 0.487 0.599 0.509 0.624CoTF→SFT 0.555 0.685 0.505 0.621 0.529 0.652CoTF→SFT→EFT 0.590 0.694 0.501 0.593 0.516 0.629Table 2: Evaluation on different tuning methods.only use the target prompt without CoT data to supervisedfine-tuning, which can be regarded as a baseline for a naiveSFT; (2) CoTF: only CoT-assisted fine-tuning; (3) CoTF→SFT: further SFT target prompt after CoTF; (4) CoTF→SFT→EFT: further ensemble fine-tuning target promptafter CoTF.。With the comparison, we conclude that CoTFand SFT can improve each other and achieve better per-formance.。Furthermore, ensemble fine-tuning significantlyenhances precision while marginally decreasing recall, mak-ing it more suitable for generating accurate concise goals.5.3.。Evaluating Co-trainingWe conduct an analysis of the completion ratio correspond-ing to the number of valid sub-goals during the co-trainingprocess.。Though the dimension size of goal space achieves68, the number of sub-goals for valid goals predominantlyfalls within the range of 1 to 7.。This is rational as completinga goal with an excessive number of sub-goals is exceedinglychallenging for a policy, even impossibility for human toachieve.。Furthermore, Figure 3(a) shows that the improve-Instruction Goal(Before co-training) Goal(After co-training)Stop!Whether prone position: TrueAverage velocity: StaticAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementGet down,stay hidden.Whether prone position: TrueAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementAverage velocity: StaticWhether seen by enemy: FalseEnemy!Rush and fire.Whether hold a gun: TrueWhether have bullets: TrueHorizontal direction of view: SouthwestWhether seen enemy: TrueAverage velocity: FastLength of distance moved: longAverage velocity: FastWhether hold a gun: TrueHorizontal direction of movement: SouthwestWhether seen enemy: TrueDamage to enemy: HighHorizontal direction of view: SouthwestEnemies nearby,move to defendand avoid damage.Whether prone position: TrueAverage velocity: FastWhether hold a gun: TrueHealth level: FullWhether to restore health: TrueLength of distance moved: longAverage velocity: FastWhether prone position: FalseHorizontal direction of movement: North(Position of enemy: South)Whether hold a gun: TrueTable 3: Comparison of goal-generation.。Cyan the helpful,pink the conflicting, and orange the critical sub-goals.。Itis evident that co-training enables goal-generation to avoidconflicts of sub-goals and improves reasonability by includ-ing helpful and critical sub-goals.ments mainly lies in 2≤ |g| ≤4, because |g|= 1 is tooeasy while |g| ≥5is too hard to complete.。Figure 3(b)shows a case of |g|= 3 that co-training indeed improvesthe completion ratio as the green curve.。It is noteworthythat the performance suddenly downgrades at each reset.This phenomenon is attributed to the reset of Gllmbreaksthe adaptation with πg, avoiding being trapped in local opti-mal.。Meanwhile, the performance tends to converge, whichindicates the successor loops produce a better adaptationBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps24252627282930Goal Completion Ratio (%)OursHER(a)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps15.017.520.022.525.027.530.032.5Goal Completion Ratio (%)OursHER (b)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps0.100.080.060.040.020.00Reward0.30.40.50.60.70.8Game StatisticsMean basic reward per stepMean basic reward per step (No KL)#Enemies killed#Enemies knocked down#Enemies killed (No KL)#Enemies knocked down (No KL) (c)Figure 2: (a) The goal completion rate on training dataset; (b) The goal completion rate on unseen goals, i.e., the test dataset;(c) The evaluation of policy learning in cases of w/ and w/o KL-divergence regularizer.。(a) Goal completion rate ( 1≤ |g| ≤7)Trend of  p o l i c y  a b i l i t ySmoothe d  C o m p l e t i o n  R a t eOriginal C o m p l e t i o n  R a t eReset Lo o p sGoal Com p l e t i o n  R a t i o  ( % )(b) Goal completion rate ( |g|= 3)Sub goal        g4   g8   g12   g16   g20 Time in one loop0.0250.0500.0750.1000.1250.1500.1750.200Possibility (c) Sub-goals distributionFigure 3: (a) The completion ratio of goals with dimension size ranges from 1 to 7; (b) The goal completion ratio of goalsthat|g|= 3, the trend curve reflects the improving completion ratio; (c) The sub-goals distribution changes along thetraining in one loop of co-training, where the description of each giis included in Table 14.between LLM and policy than before.。Additionally, weinvestigated the change in the generation probability of sub-goals (Table 14) during co-training.. \n",
      "the piece 629\n",
      " \n",
      "e examination reward by evaluating crucial sub-goal completion as Re=REWARD e(Gc,IS∩ Itrain,G),and formatting reward: Rm=REWARD m(gs, g), refer to Appendix K13: Update βLoRA with PPO, and merge updated βLoRA intoβllm14: end for15:end forTuning PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)SFT 0.547 0.663 0.490 0.602 0.517 0.632CoTF 0.533 0.652 0.487 0.599 0.509 0.624CoTF→SFT 0.555 0.685 0.505 0.621 0.529 0.652CoTF→SFT→EFT 0.590 0.694 0.501 0.593 0.516 0.629Table 2: Evaluation on different tuning methods.only use the target prompt without CoT data to supervisedfine-tuning, which can be regarded as a baseline for a naiveSFT; (2) CoTF: only CoT-assisted fine-tuning; (3) CoTF→SFT: further SFT target prompt after CoTF; (4) CoTF→SFT→EFT: further ensemble fine-tuning target promptafter CoTF.。With the comparison, we conclude that CoTFand SFT can improve each other and achieve better per-formance.。Furthermore, ensemble fine-tuning significantlyenhances precision while marginally decreasing recall, mak-ing it more suitable for generating accurate concise goals.5.3.。Evaluating Co-trainingWe conduct an analysis of the completion ratio correspond-ing to the number of valid sub-goals during the co-trainingprocess.。Though the dimension size of goal space achieves68, the number of sub-goals for valid goals predominantlyfalls within the range of 1 to 7.。This is rational as completinga goal with an excessive number of sub-goals is exceedinglychallenging for a policy, even impossibility for human toachieve.。Furthermore, Figure 3(a) shows that the improve-Instruction Goal(Before co-training) Goal(After co-training)Stop!Whether prone position: TrueAverage velocity: StaticAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementGet down,stay hidden.Whether prone position: TrueAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementAverage velocity: StaticWhether seen by enemy: FalseEnemy!Rush and fire.Whether hold a gun: TrueWhether have bullets: TrueHorizontal direction of view: SouthwestWhether seen enemy: TrueAverage velocity: FastLength of distance moved: longAverage velocity: FastWhether hold a gun: TrueHorizontal direction of movement: SouthwestWhether seen enemy: TrueDamage to enemy: HighHorizontal direction of view: SouthwestEnemies nearby,move to defendand avoid damage.Whether prone position: TrueAverage velocity: FastWhether hold a gun: TrueHealth level: FullWhether to restore health: TrueLength of distance moved: longAverage velocity: FastWhether prone position: FalseHorizontal direction of movement: North(Position of enemy: South)Whether hold a gun: TrueTable 3: Comparison of goal-generation.。Cyan the helpful,pink the conflicting, and orange the critical sub-goals.。Itis evident that co-training enables goal-generation to avoidconflicts of sub-goals and improves reasonability by includ-ing helpful and critical sub-goals.ments mainly lies in 2≤ |g| ≤4, because |g|= 1 is tooeasy while |g| ≥5is too hard to complete.。Figure 3(b)shows a case of |g|= 3 that co-training indeed improvesthe completion ratio as the green curve.。It is noteworthythat the performance suddenly downgrades at each reset.This phenomenon is attributed to the reset of Gllmbreaksthe adaptation with πg, avoiding being trapped in local opti-mal.。Meanwhile, the performance tends to converge, whichindicates the successor loops produce a better adaptationBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps24252627282930Goal Completion Ratio (%)OursHER(a)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps15.017.520.022.525.027.530.032.5Goal Completion Ratio (%)OursHER (b)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps0.100.080.060.040.020.00Reward0.30.40.50.60.70.8Game StatisticsMean basic reward per stepMean basic reward per step (No KL)#Enemies killed#Enemies knocked down#Enemies killed (No KL)#Enemies knocked down (No KL) (c)Figure 2: (a) The goal completion rate on training dataset; (b) The goal completion rate on unseen goals, i.e., the test dataset;(c) The evaluation of policy learning in cases of w/ and w/o KL-divergence regularizer.。(a) Goal completion rate ( 1≤ |g| ≤7)Trend of  p o l i c y  a b i l i t ySmoothe d  C o m p l e t i o n  R a t eOriginal C o m p l e t i o n  R a t eReset Lo o p sGoal Com p l e t i o n  R a t i o  ( % )(b) Goal completion rate ( |g|= 3)Sub goal        g4   g8   g12   g16   g20 Time in one loop0.0250.0500.0750.1000.1250.1500.1750.200Possibility (c) Sub-goals distributionFigure 3: (a) The completion ratio of goals with dimension size ranges from 1 to 7; (b) The goal completion ratio of goalsthat|g|= 3, the trend curve reflects the improving completion ratio; (c) The sub-goals distribution changes along thetraining in one loop of co-training, where the description of each giis included in Table 14.between LLM and policy than before.。Additionally, weinvestigated the change in the generation probability of sub-goals (Table 14) during co-training.。Specifically, Figure 3(c)illustrates changes within a training loop, while Figure 11(a)indicates changes across loops.. \n",
      "the piece 630\n",
      " \n",
      "hile marginally decreasing recall, mak-ing it more suitable for generating accurate concise goals.5.3.。Evaluating Co-trainingWe conduct an analysis of the completion ratio correspond-ing to the number of valid sub-goals during the co-trainingprocess.。Though the dimension size of goal space achieves68, the number of sub-goals for valid goals predominantlyfalls within the range of 1 to 7.。This is rational as completinga goal with an excessive number of sub-goals is exceedinglychallenging for a policy, even impossibility for human toachieve.。Furthermore, Figure 3(a) shows that the improve-Instruction Goal(Before co-training) Goal(After co-training)Stop!Whether prone position: TrueAverage velocity: StaticAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementGet down,stay hidden.Whether prone position: TrueAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementAverage velocity: StaticWhether seen by enemy: FalseEnemy!Rush and fire.Whether hold a gun: TrueWhether have bullets: TrueHorizontal direction of view: SouthwestWhether seen enemy: TrueAverage velocity: FastLength of distance moved: longAverage velocity: FastWhether hold a gun: TrueHorizontal direction of movement: SouthwestWhether seen enemy: TrueDamage to enemy: HighHorizontal direction of view: SouthwestEnemies nearby,move to defendand avoid damage.Whether prone position: TrueAverage velocity: FastWhether hold a gun: TrueHealth level: FullWhether to restore health: TrueLength of distance moved: longAverage velocity: FastWhether prone position: FalseHorizontal direction of movement: North(Position of enemy: South)Whether hold a gun: TrueTable 3: Comparison of goal-generation.。Cyan the helpful,pink the conflicting, and orange the critical sub-goals.。Itis evident that co-training enables goal-generation to avoidconflicts of sub-goals and improves reasonability by includ-ing helpful and critical sub-goals.ments mainly lies in 2≤ |g| ≤4, because |g|= 1 is tooeasy while |g| ≥5is too hard to complete.。Figure 3(b)shows a case of |g|= 3 that co-training indeed improvesthe completion ratio as the green curve.。It is noteworthythat the performance suddenly downgrades at each reset.This phenomenon is attributed to the reset of Gllmbreaksthe adaptation with πg, avoiding being trapped in local opti-mal.。Meanwhile, the performance tends to converge, whichindicates the successor loops produce a better adaptationBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps24252627282930Goal Completion Ratio (%)OursHER(a)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps15.017.520.022.525.027.530.032.5Goal Completion Ratio (%)OursHER (b)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps0.100.080.060.040.020.00Reward0.30.40.50.60.70.8Game StatisticsMean basic reward per stepMean basic reward per step (No KL)#Enemies killed#Enemies knocked down#Enemies killed (No KL)#Enemies knocked down (No KL) (c)Figure 2: (a) The goal completion rate on training dataset; (b) The goal completion rate on unseen goals, i.e., the test dataset;(c) The evaluation of policy learning in cases of w/ and w/o KL-divergence regularizer.。(a) Goal completion rate ( 1≤ |g| ≤7)Trend of  p o l i c y  a b i l i t ySmoothe d  C o m p l e t i o n  R a t eOriginal C o m p l e t i o n  R a t eReset Lo o p sGoal Com p l e t i o n  R a t i o  ( % )(b) Goal completion rate ( |g|= 3)Sub goal        g4   g8   g12   g16   g20 Time in one loop0.0250.0500.0750.1000.1250.1500.1750.200Possibility (c) Sub-goals distributionFigure 3: (a) The completion ratio of goals with dimension size ranges from 1 to 7; (b) The goal completion ratio of goalsthat|g|= 3, the trend curve reflects the improving completion ratio; (c) The sub-goals distribution changes along thetraining in one loop of co-training, where the description of each giis included in Table 14.between LLM and policy than before.。Additionally, weinvestigated the change in the generation probability of sub-goals (Table 14) during co-training.。Specifically, Figure 3(c)illustrates changes within a training loop, while Figure 11(a)indicates changes across loops.。As training progresses, theprobabilities associated with each giundergo gradual modi-fications.. \n",
      "the piece 631\n",
      " \n",
      "uating Co-trainingWe conduct an analysis of the completion ratio correspond-ing to the number of valid sub-goals during the co-trainingprocess.。Though the dimension size of goal space achieves68, the number of sub-goals for valid goals predominantlyfalls within the range of 1 to 7.。This is rational as completinga goal with an excessive number of sub-goals is exceedinglychallenging for a policy, even impossibility for human toachieve.。Furthermore, Figure 3(a) shows that the improve-Instruction Goal(Before co-training) Goal(After co-training)Stop!Whether prone position: TrueAverage velocity: StaticAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementGet down,stay hidden.Whether prone position: TrueAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementAverage velocity: StaticWhether seen by enemy: FalseEnemy!Rush and fire.Whether hold a gun: TrueWhether have bullets: TrueHorizontal direction of view: SouthwestWhether seen enemy: TrueAverage velocity: FastLength of distance moved: longAverage velocity: FastWhether hold a gun: TrueHorizontal direction of movement: SouthwestWhether seen enemy: TrueDamage to enemy: HighHorizontal direction of view: SouthwestEnemies nearby,move to defendand avoid damage.Whether prone position: TrueAverage velocity: FastWhether hold a gun: TrueHealth level: FullWhether to restore health: TrueLength of distance moved: longAverage velocity: FastWhether prone position: FalseHorizontal direction of movement: North(Position of enemy: South)Whether hold a gun: TrueTable 3: Comparison of goal-generation.。Cyan the helpful,pink the conflicting, and orange the critical sub-goals.。Itis evident that co-training enables goal-generation to avoidconflicts of sub-goals and improves reasonability by includ-ing helpful and critical sub-goals.ments mainly lies in 2≤ |g| ≤4, because |g|= 1 is tooeasy while |g| ≥5is too hard to complete.。Figure 3(b)shows a case of |g|= 3 that co-training indeed improvesthe completion ratio as the green curve.。It is noteworthythat the performance suddenly downgrades at each reset.This phenomenon is attributed to the reset of Gllmbreaksthe adaptation with πg, avoiding being trapped in local opti-mal.。Meanwhile, the performance tends to converge, whichindicates the successor loops produce a better adaptationBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps24252627282930Goal Completion Ratio (%)OursHER(a)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps15.017.520.022.525.027.530.032.5Goal Completion Ratio (%)OursHER (b)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps0.100.080.060.040.020.00Reward0.30.40.50.60.70.8Game StatisticsMean basic reward per stepMean basic reward per step (No KL)#Enemies killed#Enemies knocked down#Enemies killed (No KL)#Enemies knocked down (No KL) (c)Figure 2: (a) The goal completion rate on training dataset; (b) The goal completion rate on unseen goals, i.e., the test dataset;(c) The evaluation of policy learning in cases of w/ and w/o KL-divergence regularizer.。(a) Goal completion rate ( 1≤ |g| ≤7)Trend of  p o l i c y  a b i l i t ySmoothe d  C o m p l e t i o n  R a t eOriginal C o m p l e t i o n  R a t eReset Lo o p sGoal Com p l e t i o n  R a t i o  ( % )(b) Goal completion rate ( |g|= 3)Sub goal        g4   g8   g12   g16   g20 Time in one loop0.0250.0500.0750.1000.1250.1500.1750.200Possibility (c) Sub-goals distributionFigure 3: (a) The completion ratio of goals with dimension size ranges from 1 to 7; (b) The goal completion ratio of goalsthat|g|= 3, the trend curve reflects the improving completion ratio; (c) The sub-goals distribution changes along thetraining in one loop of co-training, where the description of each giis included in Table 14.between LLM and policy than before.。Additionally, weinvestigated the change in the generation probability of sub-goals (Table 14) during co-training.。Specifically, Figure 3(c)illustrates changes within a training loop, while Figure 11(a)indicates changes across loops.。As training progresses, theprobabilities associated with each giundergo gradual modi-fications.。For instance, sub-goals with growing probabilitiesare central to the agent private states due to their relativelyattainable nature and influence in agent interaction.. \n",
      "the piece 632\n",
      " \n",
      "f goal space achieves68, the number of sub-goals for valid goals predominantlyfalls within the range of 1 to 7.。This is rational as completinga goal with an excessive number of sub-goals is exceedinglychallenging for a policy, even impossibility for human toachieve.。Furthermore, Figure 3(a) shows that the improve-Instruction Goal(Before co-training) Goal(After co-training)Stop!Whether prone position: TrueAverage velocity: StaticAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementGet down,stay hidden.Whether prone position: TrueAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementAverage velocity: StaticWhether seen by enemy: FalseEnemy!Rush and fire.Whether hold a gun: TrueWhether have bullets: TrueHorizontal direction of view: SouthwestWhether seen enemy: TrueAverage velocity: FastLength of distance moved: longAverage velocity: FastWhether hold a gun: TrueHorizontal direction of movement: SouthwestWhether seen enemy: TrueDamage to enemy: HighHorizontal direction of view: SouthwestEnemies nearby,move to defendand avoid damage.Whether prone position: TrueAverage velocity: FastWhether hold a gun: TrueHealth level: FullWhether to restore health: TrueLength of distance moved: longAverage velocity: FastWhether prone position: FalseHorizontal direction of movement: North(Position of enemy: South)Whether hold a gun: TrueTable 3: Comparison of goal-generation.。Cyan the helpful,pink the conflicting, and orange the critical sub-goals.。Itis evident that co-training enables goal-generation to avoidconflicts of sub-goals and improves reasonability by includ-ing helpful and critical sub-goals.ments mainly lies in 2≤ |g| ≤4, because |g|= 1 is tooeasy while |g| ≥5is too hard to complete.。Figure 3(b)shows a case of |g|= 3 that co-training indeed improvesthe completion ratio as the green curve.。It is noteworthythat the performance suddenly downgrades at each reset.This phenomenon is attributed to the reset of Gllmbreaksthe adaptation with πg, avoiding being trapped in local opti-mal.。Meanwhile, the performance tends to converge, whichindicates the successor loops produce a better adaptationBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps24252627282930Goal Completion Ratio (%)OursHER(a)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps15.017.520.022.525.027.530.032.5Goal Completion Ratio (%)OursHER (b)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps0.100.080.060.040.020.00Reward0.30.40.50.60.70.8Game StatisticsMean basic reward per stepMean basic reward per step (No KL)#Enemies killed#Enemies knocked down#Enemies killed (No KL)#Enemies knocked down (No KL) (c)Figure 2: (a) The goal completion rate on training dataset; (b) The goal completion rate on unseen goals, i.e., the test dataset;(c) The evaluation of policy learning in cases of w/ and w/o KL-divergence regularizer.。(a) Goal completion rate ( 1≤ |g| ≤7)Trend of  p o l i c y  a b i l i t ySmoothe d  C o m p l e t i o n  R a t eOriginal C o m p l e t i o n  R a t eReset Lo o p sGoal Com p l e t i o n  R a t i o  ( % )(b) Goal completion rate ( |g|= 3)Sub goal        g4   g8   g12   g16   g20 Time in one loop0.0250.0500.0750.1000.1250.1500.1750.200Possibility (c) Sub-goals distributionFigure 3: (a) The completion ratio of goals with dimension size ranges from 1 to 7; (b) The goal completion ratio of goalsthat|g|= 3, the trend curve reflects the improving completion ratio; (c) The sub-goals distribution changes along thetraining in one loop of co-training, where the description of each giis included in Table 14.between LLM and policy than before.。Additionally, weinvestigated the change in the generation probability of sub-goals (Table 14) during co-training.。Specifically, Figure 3(c)illustrates changes within a training loop, while Figure 11(a)indicates changes across loops.。As training progresses, theprobabilities associated with each giundergo gradual modi-fications.。For instance, sub-goals with growing probabilitiesare central to the agent private states due to their relativelyattainable nature and influence in agent interaction.。Con-versely, sub-goals with falling probabilities are central toother players’ states, as they are not directly changed byagent actions, and Gllmtends to generate outputs for thesesub-goals only when absolutely necessary.. \n",
      "the piece 633\n",
      " \n",
      "with an excessive number of sub-goals is exceedinglychallenging for a policy, even impossibility for human toachieve.。Furthermore, Figure 3(a) shows that the improve-Instruction Goal(Before co-training) Goal(After co-training)Stop!Whether prone position: TrueAverage velocity: StaticAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementGet down,stay hidden.Whether prone position: TrueAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementAverage velocity: StaticWhether seen by enemy: FalseEnemy!Rush and fire.Whether hold a gun: TrueWhether have bullets: TrueHorizontal direction of view: SouthwestWhether seen enemy: TrueAverage velocity: FastLength of distance moved: longAverage velocity: FastWhether hold a gun: TrueHorizontal direction of movement: SouthwestWhether seen enemy: TrueDamage to enemy: HighHorizontal direction of view: SouthwestEnemies nearby,move to defendand avoid damage.Whether prone position: TrueAverage velocity: FastWhether hold a gun: TrueHealth level: FullWhether to restore health: TrueLength of distance moved: longAverage velocity: FastWhether prone position: FalseHorizontal direction of movement: North(Position of enemy: South)Whether hold a gun: TrueTable 3: Comparison of goal-generation.。Cyan the helpful,pink the conflicting, and orange the critical sub-goals.。Itis evident that co-training enables goal-generation to avoidconflicts of sub-goals and improves reasonability by includ-ing helpful and critical sub-goals.ments mainly lies in 2≤ |g| ≤4, because |g|= 1 is tooeasy while |g| ≥5is too hard to complete.。Figure 3(b)shows a case of |g|= 3 that co-training indeed improvesthe completion ratio as the green curve.。It is noteworthythat the performance suddenly downgrades at each reset.This phenomenon is attributed to the reset of Gllmbreaksthe adaptation with πg, avoiding being trapped in local opti-mal.。Meanwhile, the performance tends to converge, whichindicates the successor loops produce a better adaptationBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps24252627282930Goal Completion Ratio (%)OursHER(a)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps15.017.520.022.525.027.530.032.5Goal Completion Ratio (%)OursHER (b)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps0.100.080.060.040.020.00Reward0.30.40.50.60.70.8Game StatisticsMean basic reward per stepMean basic reward per step (No KL)#Enemies killed#Enemies knocked down#Enemies killed (No KL)#Enemies knocked down (No KL) (c)Figure 2: (a) The goal completion rate on training dataset; (b) The goal completion rate on unseen goals, i.e., the test dataset;(c) The evaluation of policy learning in cases of w/ and w/o KL-divergence regularizer.。(a) Goal completion rate ( 1≤ |g| ≤7)Trend of  p o l i c y  a b i l i t ySmoothe d  C o m p l e t i o n  R a t eOriginal C o m p l e t i o n  R a t eReset Lo o p sGoal Com p l e t i o n  R a t i o  ( % )(b) Goal completion rate ( |g|= 3)Sub goal        g4   g8   g12   g16   g20 Time in one loop0.0250.0500.0750.1000.1250.1500.1750.200Possibility (c) Sub-goals distributionFigure 3: (a) The completion ratio of goals with dimension size ranges from 1 to 7; (b) The goal completion ratio of goalsthat|g|= 3, the trend curve reflects the improving completion ratio; (c) The sub-goals distribution changes along thetraining in one loop of co-training, where the description of each giis included in Table 14.between LLM and policy than before.。Additionally, weinvestigated the change in the generation probability of sub-goals (Table 14) during co-training.。Specifically, Figure 3(c)illustrates changes within a training loop, while Figure 11(a)indicates changes across loops.。As training progresses, theprobabilities associated with each giundergo gradual modi-fications.。For instance, sub-goals with growing probabilitiesare central to the agent private states due to their relativelyattainable nature and influence in agent interaction.。Con-versely, sub-goals with falling probabilities are central toother players’ states, as they are not directly changed byagent actions, and Gllmtends to generate outputs for thesesub-goals only when absolutely necessary.。To investigatethe impact of co-training to Gllm, we have also identifiedthe changes of goal-generation for an instruction, as shownin Table 3.. \n",
      "the piece 634\n",
      " \n",
      "a) shows that the improve-Instruction Goal(Before co-training) Goal(After co-training)Stop!Whether prone position: TrueAverage velocity: StaticAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementGet down,stay hidden.Whether prone position: TrueAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementAverage velocity: StaticWhether seen by enemy: FalseEnemy!Rush and fire.Whether hold a gun: TrueWhether have bullets: TrueHorizontal direction of view: SouthwestWhether seen enemy: TrueAverage velocity: FastLength of distance moved: longAverage velocity: FastWhether hold a gun: TrueHorizontal direction of movement: SouthwestWhether seen enemy: TrueDamage to enemy: HighHorizontal direction of view: SouthwestEnemies nearby,move to defendand avoid damage.Whether prone position: TrueAverage velocity: FastWhether hold a gun: TrueHealth level: FullWhether to restore health: TrueLength of distance moved: longAverage velocity: FastWhether prone position: FalseHorizontal direction of movement: North(Position of enemy: South)Whether hold a gun: TrueTable 3: Comparison of goal-generation.。Cyan the helpful,pink the conflicting, and orange the critical sub-goals.。Itis evident that co-training enables goal-generation to avoidconflicts of sub-goals and improves reasonability by includ-ing helpful and critical sub-goals.ments mainly lies in 2≤ |g| ≤4, because |g|= 1 is tooeasy while |g| ≥5is too hard to complete.。Figure 3(b)shows a case of |g|= 3 that co-training indeed improvesthe completion ratio as the green curve.。It is noteworthythat the performance suddenly downgrades at each reset.This phenomenon is attributed to the reset of Gllmbreaksthe adaptation with πg, avoiding being trapped in local opti-mal.。Meanwhile, the performance tends to converge, whichindicates the successor loops produce a better adaptationBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps24252627282930Goal Completion Ratio (%)OursHER(a)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps15.017.520.022.525.027.530.032.5Goal Completion Ratio (%)OursHER (b)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps0.100.080.060.040.020.00Reward0.30.40.50.60.70.8Game StatisticsMean basic reward per stepMean basic reward per step (No KL)#Enemies killed#Enemies knocked down#Enemies killed (No KL)#Enemies knocked down (No KL) (c)Figure 2: (a) The goal completion rate on training dataset; (b) The goal completion rate on unseen goals, i.e., the test dataset;(c) The evaluation of policy learning in cases of w/ and w/o KL-divergence regularizer.。(a) Goal completion rate ( 1≤ |g| ≤7)Trend of  p o l i c y  a b i l i t ySmoothe d  C o m p l e t i o n  R a t eOriginal C o m p l e t i o n  R a t eReset Lo o p sGoal Com p l e t i o n  R a t i o  ( % )(b) Goal completion rate ( |g|= 3)Sub goal        g4   g8   g12   g16   g20 Time in one loop0.0250.0500.0750.1000.1250.1500.1750.200Possibility (c) Sub-goals distributionFigure 3: (a) The completion ratio of goals with dimension size ranges from 1 to 7; (b) The goal completion ratio of goalsthat|g|= 3, the trend curve reflects the improving completion ratio; (c) The sub-goals distribution changes along thetraining in one loop of co-training, where the description of each giis included in Table 14.between LLM and policy than before.。Additionally, weinvestigated the change in the generation probability of sub-goals (Table 14) during co-training.。Specifically, Figure 3(c)illustrates changes within a training loop, while Figure 11(a)indicates changes across loops.。As training progresses, theprobabilities associated with each giundergo gradual modi-fications.。For instance, sub-goals with growing probabilitiesare central to the agent private states due to their relativelyattainable nature and influence in agent interaction.。Con-versely, sub-goals with falling probabilities are central toother players’ states, as they are not directly changed byagent actions, and Gllmtends to generate outputs for thesesub-goals only when absolutely necessary.。To investigatethe impact of co-training to Gllm, we have also identifiedthe changes of goal-generation for an instruction, as shownin Table 3.。Evidently, after co-training, Gllmdemonstratesits capacity to eliminate contradictory and irrational ele-ments within the initial objectives and exhibits the ability tointroduce new sub-goals, thereby rendering the overall goalmore attainable, all while retaining its exceptional semanticcomprehension capabilities.6.. \n",
      "the piece 635\n",
      " \n",
      "ity: StaticWhether prone position: TrueLength of distance moved: No MovementGet down,stay hidden.Whether prone position: TrueAverage velocity: StaticWhether prone position: TrueLength of distance moved: No MovementAverage velocity: StaticWhether seen by enemy: FalseEnemy!Rush and fire.Whether hold a gun: TrueWhether have bullets: TrueHorizontal direction of view: SouthwestWhether seen enemy: TrueAverage velocity: FastLength of distance moved: longAverage velocity: FastWhether hold a gun: TrueHorizontal direction of movement: SouthwestWhether seen enemy: TrueDamage to enemy: HighHorizontal direction of view: SouthwestEnemies nearby,move to defendand avoid damage.Whether prone position: TrueAverage velocity: FastWhether hold a gun: TrueHealth level: FullWhether to restore health: TrueLength of distance moved: longAverage velocity: FastWhether prone position: FalseHorizontal direction of movement: North(Position of enemy: South)Whether hold a gun: TrueTable 3: Comparison of goal-generation.。Cyan the helpful,pink the conflicting, and orange the critical sub-goals.。Itis evident that co-training enables goal-generation to avoidconflicts of sub-goals and improves reasonability by includ-ing helpful and critical sub-goals.ments mainly lies in 2≤ |g| ≤4, because |g|= 1 is tooeasy while |g| ≥5is too hard to complete.。Figure 3(b)shows a case of |g|= 3 that co-training indeed improvesthe completion ratio as the green curve.。It is noteworthythat the performance suddenly downgrades at each reset.This phenomenon is attributed to the reset of Gllmbreaksthe adaptation with πg, avoiding being trapped in local opti-mal.。Meanwhile, the performance tends to converge, whichindicates the successor loops produce a better adaptationBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps24252627282930Goal Completion Ratio (%)OursHER(a)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps15.017.520.022.525.027.530.032.5Goal Completion Ratio (%)OursHER (b)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps0.100.080.060.040.020.00Reward0.30.40.50.60.70.8Game StatisticsMean basic reward per stepMean basic reward per step (No KL)#Enemies killed#Enemies knocked down#Enemies killed (No KL)#Enemies knocked down (No KL) (c)Figure 2: (a) The goal completion rate on training dataset; (b) The goal completion rate on unseen goals, i.e., the test dataset;(c) The evaluation of policy learning in cases of w/ and w/o KL-divergence regularizer.。(a) Goal completion rate ( 1≤ |g| ≤7)Trend of  p o l i c y  a b i l i t ySmoothe d  C o m p l e t i o n  R a t eOriginal C o m p l e t i o n  R a t eReset Lo o p sGoal Com p l e t i o n  R a t i o  ( % )(b) Goal completion rate ( |g|= 3)Sub goal        g4   g8   g12   g16   g20 Time in one loop0.0250.0500.0750.1000.1250.1500.1750.200Possibility (c) Sub-goals distributionFigure 3: (a) The completion ratio of goals with dimension size ranges from 1 to 7; (b) The goal completion ratio of goalsthat|g|= 3, the trend curve reflects the improving completion ratio; (c) The sub-goals distribution changes along thetraining in one loop of co-training, where the description of each giis included in Table 14.between LLM and policy than before.。Additionally, weinvestigated the change in the generation probability of sub-goals (Table 14) during co-training.。Specifically, Figure 3(c)illustrates changes within a training loop, while Figure 11(a)indicates changes across loops.。As training progresses, theprobabilities associated with each giundergo gradual modi-fications.。For instance, sub-goals with growing probabilitiesare central to the agent private states due to their relativelyattainable nature and influence in agent interaction.。Con-versely, sub-goals with falling probabilities are central toother players’ states, as they are not directly changed byagent actions, and Gllmtends to generate outputs for thesesub-goals only when absolutely necessary.。To investigatethe impact of co-training to Gllm, we have also identifiedthe changes of goal-generation for an instruction, as shownin Table 3.。Evidently, after co-training, Gllmdemonstratesits capacity to eliminate contradictory and irrational ele-ments within the initial objectives and exhibits the ability tointroduce new sub-goals, thereby rendering the overall goalmore attainable, all while retaining its exceptional semanticcomprehension capabilities.6.。ConclusionIn this paper, we propose OpenPAL experts on learningopen-ended embodied agents for human-AI interaction, ex-celling in achieving instruction open-endedness through atwo-stage learning process.. \n",
      "the piece 636\n",
      " \n",
      "bility by includ-ing helpful and critical sub-goals.ments mainly lies in 2≤ |g| ≤4, because |g|= 1 is tooeasy while |g| ≥5is too hard to complete.。Figure 3(b)shows a case of |g|= 3 that co-training indeed improvesthe completion ratio as the green curve.。It is noteworthythat the performance suddenly downgrades at each reset.This phenomenon is attributed to the reset of Gllmbreaksthe adaptation with πg, avoiding being trapped in local opti-mal.。Meanwhile, the performance tends to converge, whichindicates the successor loops produce a better adaptationBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps24252627282930Goal Completion Ratio (%)OursHER(a)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps15.017.520.022.525.027.530.032.5Goal Completion Ratio (%)OursHER (b)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps0.100.080.060.040.020.00Reward0.30.40.50.60.70.8Game StatisticsMean basic reward per stepMean basic reward per step (No KL)#Enemies killed#Enemies knocked down#Enemies killed (No KL)#Enemies knocked down (No KL) (c)Figure 2: (a) The goal completion rate on training dataset; (b) The goal completion rate on unseen goals, i.e., the test dataset;(c) The evaluation of policy learning in cases of w/ and w/o KL-divergence regularizer.。(a) Goal completion rate ( 1≤ |g| ≤7)Trend of  p o l i c y  a b i l i t ySmoothe d  C o m p l e t i o n  R a t eOriginal C o m p l e t i o n  R a t eReset Lo o p sGoal Com p l e t i o n  R a t i o  ( % )(b) Goal completion rate ( |g|= 3)Sub goal        g4   g8   g12   g16   g20 Time in one loop0.0250.0500.0750.1000.1250.1500.1750.200Possibility (c) Sub-goals distributionFigure 3: (a) The completion ratio of goals with dimension size ranges from 1 to 7; (b) The goal completion ratio of goalsthat|g|= 3, the trend curve reflects the improving completion ratio; (c) The sub-goals distribution changes along thetraining in one loop of co-training, where the description of each giis included in Table 14.between LLM and policy than before.。Additionally, weinvestigated the change in the generation probability of sub-goals (Table 14) during co-training.。Specifically, Figure 3(c)illustrates changes within a training loop, while Figure 11(a)indicates changes across loops.。As training progresses, theprobabilities associated with each giundergo gradual modi-fications.。For instance, sub-goals with growing probabilitiesare central to the agent private states due to their relativelyattainable nature and influence in agent interaction.。Con-versely, sub-goals with falling probabilities are central toother players’ states, as they are not directly changed byagent actions, and Gllmtends to generate outputs for thesesub-goals only when absolutely necessary.。To investigatethe impact of co-training to Gllm, we have also identifiedthe changes of goal-generation for an instruction, as shownin Table 3.。Evidently, after co-training, Gllmdemonstratesits capacity to eliminate contradictory and irrational ele-ments within the initial objectives and exhibits the ability tointroduce new sub-goals, thereby rendering the overall goalmore attainable, all while retaining its exceptional semanticcomprehension capabilities.6.。ConclusionIn this paper, we propose OpenPAL experts on learningopen-ended embodied agents for human-AI interaction, ex-celling in achieving instruction open-endedness through atwo-stage learning process.。The empirical results on Contrarepresent that OpenPAL shows the potential as a practicalsolution for human-AI interaction in complex situations.Despite the positive results, we admit there are still somelimitations to our work that would be expected to be re-searched in the future—for instance, a truly open-endedgoal description instead of the handcrafted goal space in thecurrent version; supporting multi-modality input/output tofree from expensive feature engineering.Author Contribution StatementThe authors confirm their contribution as follows:Shaopeng Zhai : team leadership, open-ended learning,LLM/RLAF training, agent analysis, architecture designJie Wang : infrastructure/framework engineering, non-goalBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationagent training, open-ended learning, ablation studies, fea-ture engineeringTianyi Zhang : non-goal agent training, open-ended learn-ing, feature engineeringFuxian Huang : non-goal agent training, paper writing,open-ended learningQi Zhang : LLM training, RLAF training, paper writing,ablation studiesMing Zhou : co-training framework, curriculum research,paper writingJing Hou : LLM training, paper writingReferencesAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O.,and Zaremba, W. Hindsight experience replay.. \n",
      "the piece 637\n",
      " \n",
      " |g| ≤4, because |g|= 1 is tooeasy while |g| ≥5is too hard to complete.。Figure 3(b)shows a case of |g|= 3 that co-training indeed improvesthe completion ratio as the green curve.。It is noteworthythat the performance suddenly downgrades at each reset.This phenomenon is attributed to the reset of Gllmbreaksthe adaptation with πg, avoiding being trapped in local opti-mal.。Meanwhile, the performance tends to converge, whichindicates the successor loops produce a better adaptationBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps24252627282930Goal Completion Ratio (%)OursHER(a)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps15.017.520.022.525.027.530.032.5Goal Completion Ratio (%)OursHER (b)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps0.100.080.060.040.020.00Reward0.30.40.50.60.70.8Game StatisticsMean basic reward per stepMean basic reward per step (No KL)#Enemies killed#Enemies knocked down#Enemies killed (No KL)#Enemies knocked down (No KL) (c)Figure 2: (a) The goal completion rate on training dataset; (b) The goal completion rate on unseen goals, i.e., the test dataset;(c) The evaluation of policy learning in cases of w/ and w/o KL-divergence regularizer.。(a) Goal completion rate ( 1≤ |g| ≤7)Trend of  p o l i c y  a b i l i t ySmoothe d  C o m p l e t i o n  R a t eOriginal C o m p l e t i o n  R a t eReset Lo o p sGoal Com p l e t i o n  R a t i o  ( % )(b) Goal completion rate ( |g|= 3)Sub goal        g4   g8   g12   g16   g20 Time in one loop0.0250.0500.0750.1000.1250.1500.1750.200Possibility (c) Sub-goals distributionFigure 3: (a) The completion ratio of goals with dimension size ranges from 1 to 7; (b) The goal completion ratio of goalsthat|g|= 3, the trend curve reflects the improving completion ratio; (c) The sub-goals distribution changes along thetraining in one loop of co-training, where the description of each giis included in Table 14.between LLM and policy than before.。Additionally, weinvestigated the change in the generation probability of sub-goals (Table 14) during co-training.。Specifically, Figure 3(c)illustrates changes within a training loop, while Figure 11(a)indicates changes across loops.。As training progresses, theprobabilities associated with each giundergo gradual modi-fications.。For instance, sub-goals with growing probabilitiesare central to the agent private states due to their relativelyattainable nature and influence in agent interaction.。Con-versely, sub-goals with falling probabilities are central toother players’ states, as they are not directly changed byagent actions, and Gllmtends to generate outputs for thesesub-goals only when absolutely necessary.。To investigatethe impact of co-training to Gllm, we have also identifiedthe changes of goal-generation for an instruction, as shownin Table 3.。Evidently, after co-training, Gllmdemonstratesits capacity to eliminate contradictory and irrational ele-ments within the initial objectives and exhibits the ability tointroduce new sub-goals, thereby rendering the overall goalmore attainable, all while retaining its exceptional semanticcomprehension capabilities.6.。ConclusionIn this paper, we propose OpenPAL experts on learningopen-ended embodied agents for human-AI interaction, ex-celling in achieving instruction open-endedness through atwo-stage learning process.。The empirical results on Contrarepresent that OpenPAL shows the potential as a practicalsolution for human-AI interaction in complex situations.Despite the positive results, we admit there are still somelimitations to our work that would be expected to be re-searched in the future—for instance, a truly open-endedgoal description instead of the handcrafted goal space in thecurrent version; supporting multi-modality input/output tofree from expensive feature engineering.Author Contribution StatementThe authors confirm their contribution as follows:Shaopeng Zhai : team leadership, open-ended learning,LLM/RLAF training, agent analysis, architecture designJie Wang : infrastructure/framework engineering, non-goalBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationagent training, open-ended learning, ablation studies, fea-ture engineeringTianyi Zhang : non-goal agent training, open-ended learn-ing, feature engineeringFuxian Huang : non-goal agent training, paper writing,open-ended learningQi Zhang : LLM training, RLAF training, paper writing,ablation studiesMing Zhou : co-training framework, curriculum research,paper writingJing Hou : LLM training, paper writingReferencesAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O.,and Zaremba, W. Hindsight experience replay.。Advancesin neural information processing systems , 30, 2017.Bai, C., Liu, P., Zhao, W., and Tang, X.. \n",
      "the piece 638\n",
      " \n",
      "s phenomenon is attributed to the reset of Gllmbreaksthe adaptation with πg, avoiding being trapped in local opti-mal.。Meanwhile, the performance tends to converge, whichindicates the successor loops produce a better adaptationBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps24252627282930Goal Completion Ratio (%)OursHER(a)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps15.017.520.022.525.027.530.032.5Goal Completion Ratio (%)OursHER (b)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps0.100.080.060.040.020.00Reward0.30.40.50.60.70.8Game StatisticsMean basic reward per stepMean basic reward per step (No KL)#Enemies killed#Enemies knocked down#Enemies killed (No KL)#Enemies knocked down (No KL) (c)Figure 2: (a) The goal completion rate on training dataset; (b) The goal completion rate on unseen goals, i.e., the test dataset;(c) The evaluation of policy learning in cases of w/ and w/o KL-divergence regularizer.。(a) Goal completion rate ( 1≤ |g| ≤7)Trend of  p o l i c y  a b i l i t ySmoothe d  C o m p l e t i o n  R a t eOriginal C o m p l e t i o n  R a t eReset Lo o p sGoal Com p l e t i o n  R a t i o  ( % )(b) Goal completion rate ( |g|= 3)Sub goal        g4   g8   g12   g16   g20 Time in one loop0.0250.0500.0750.1000.1250.1500.1750.200Possibility (c) Sub-goals distributionFigure 3: (a) The completion ratio of goals with dimension size ranges from 1 to 7; (b) The goal completion ratio of goalsthat|g|= 3, the trend curve reflects the improving completion ratio; (c) The sub-goals distribution changes along thetraining in one loop of co-training, where the description of each giis included in Table 14.between LLM and policy than before.。Additionally, weinvestigated the change in the generation probability of sub-goals (Table 14) during co-training.。Specifically, Figure 3(c)illustrates changes within a training loop, while Figure 11(a)indicates changes across loops.。As training progresses, theprobabilities associated with each giundergo gradual modi-fications.。For instance, sub-goals with growing probabilitiesare central to the agent private states due to their relativelyattainable nature and influence in agent interaction.。Con-versely, sub-goals with falling probabilities are central toother players’ states, as they are not directly changed byagent actions, and Gllmtends to generate outputs for thesesub-goals only when absolutely necessary.。To investigatethe impact of co-training to Gllm, we have also identifiedthe changes of goal-generation for an instruction, as shownin Table 3.。Evidently, after co-training, Gllmdemonstratesits capacity to eliminate contradictory and irrational ele-ments within the initial objectives and exhibits the ability tointroduce new sub-goals, thereby rendering the overall goalmore attainable, all while retaining its exceptional semanticcomprehension capabilities.6.。ConclusionIn this paper, we propose OpenPAL experts on learningopen-ended embodied agents for human-AI interaction, ex-celling in achieving instruction open-endedness through atwo-stage learning process.。The empirical results on Contrarepresent that OpenPAL shows the potential as a practicalsolution for human-AI interaction in complex situations.Despite the positive results, we admit there are still somelimitations to our work that would be expected to be re-searched in the future—for instance, a truly open-endedgoal description instead of the handcrafted goal space in thecurrent version; supporting multi-modality input/output tofree from expensive feature engineering.Author Contribution StatementThe authors confirm their contribution as follows:Shaopeng Zhai : team leadership, open-ended learning,LLM/RLAF training, agent analysis, architecture designJie Wang : infrastructure/framework engineering, non-goalBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationagent training, open-ended learning, ablation studies, fea-ture engineeringTianyi Zhang : non-goal agent training, open-ended learn-ing, feature engineeringFuxian Huang : non-goal agent training, paper writing,open-ended learningQi Zhang : LLM training, RLAF training, paper writing,ablation studiesMing Zhou : co-training framework, curriculum research,paper writingJing Hou : LLM training, paper writingReferencesAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O.,and Zaremba, W. Hindsight experience replay.。Advancesin neural information processing systems , 30, 2017.Bai, C., Liu, P., Zhao, W., and Tang, X.。Guided goal gen-eration for hindsight multi-goal reinforcement learning.Neurocomputing , 359:353–367, 2019.Balduzzi, D., Garnelo, M., Bachrach, Y ., Czarnecki, W.,Perolat, J., Jaderberg, M., and Graepel, T. Open-endedlearning in symmetric zero-sum games.. \n",
      "the piece 639\n",
      " \n",
      " opti-mal.。Meanwhile, the performance tends to converge, whichindicates the successor loops produce a better adaptationBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps24252627282930Goal Completion Ratio (%)OursHER(a)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps15.017.520.022.525.027.530.032.5Goal Completion Ratio (%)OursHER (b)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps0.100.080.060.040.020.00Reward0.30.40.50.60.70.8Game StatisticsMean basic reward per stepMean basic reward per step (No KL)#Enemies killed#Enemies knocked down#Enemies killed (No KL)#Enemies knocked down (No KL) (c)Figure 2: (a) The goal completion rate on training dataset; (b) The goal completion rate on unseen goals, i.e., the test dataset;(c) The evaluation of policy learning in cases of w/ and w/o KL-divergence regularizer.。(a) Goal completion rate ( 1≤ |g| ≤7)Trend of  p o l i c y  a b i l i t ySmoothe d  C o m p l e t i o n  R a t eOriginal C o m p l e t i o n  R a t eReset Lo o p sGoal Com p l e t i o n  R a t i o  ( % )(b) Goal completion rate ( |g|= 3)Sub goal        g4   g8   g12   g16   g20 Time in one loop0.0250.0500.0750.1000.1250.1500.1750.200Possibility (c) Sub-goals distributionFigure 3: (a) The completion ratio of goals with dimension size ranges from 1 to 7; (b) The goal completion ratio of goalsthat|g|= 3, the trend curve reflects the improving completion ratio; (c) The sub-goals distribution changes along thetraining in one loop of co-training, where the description of each giis included in Table 14.between LLM and policy than before.。Additionally, weinvestigated the change in the generation probability of sub-goals (Table 14) during co-training.。Specifically, Figure 3(c)illustrates changes within a training loop, while Figure 11(a)indicates changes across loops.。As training progresses, theprobabilities associated with each giundergo gradual modi-fications.。For instance, sub-goals with growing probabilitiesare central to the agent private states due to their relativelyattainable nature and influence in agent interaction.。Con-versely, sub-goals with falling probabilities are central toother players’ states, as they are not directly changed byagent actions, and Gllmtends to generate outputs for thesesub-goals only when absolutely necessary.。To investigatethe impact of co-training to Gllm, we have also identifiedthe changes of goal-generation for an instruction, as shownin Table 3.。Evidently, after co-training, Gllmdemonstratesits capacity to eliminate contradictory and irrational ele-ments within the initial objectives and exhibits the ability tointroduce new sub-goals, thereby rendering the overall goalmore attainable, all while retaining its exceptional semanticcomprehension capabilities.6.。ConclusionIn this paper, we propose OpenPAL experts on learningopen-ended embodied agents for human-AI interaction, ex-celling in achieving instruction open-endedness through atwo-stage learning process.。The empirical results on Contrarepresent that OpenPAL shows the potential as a practicalsolution for human-AI interaction in complex situations.Despite the positive results, we admit there are still somelimitations to our work that would be expected to be re-searched in the future—for instance, a truly open-endedgoal description instead of the handcrafted goal space in thecurrent version; supporting multi-modality input/output tofree from expensive feature engineering.Author Contribution StatementThe authors confirm their contribution as follows:Shaopeng Zhai : team leadership, open-ended learning,LLM/RLAF training, agent analysis, architecture designJie Wang : infrastructure/framework engineering, non-goalBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationagent training, open-ended learning, ablation studies, fea-ture engineeringTianyi Zhang : non-goal agent training, open-ended learn-ing, feature engineeringFuxian Huang : non-goal agent training, paper writing,open-ended learningQi Zhang : LLM training, RLAF training, paper writing,ablation studiesMing Zhou : co-training framework, curriculum research,paper writingJing Hou : LLM training, paper writingReferencesAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O.,and Zaremba, W. Hindsight experience replay.。Advancesin neural information processing systems , 30, 2017.Bai, C., Liu, P., Zhao, W., and Tang, X.。Guided goal gen-eration for hindsight multi-goal reinforcement learning.Neurocomputing , 359:353–367, 2019.Balduzzi, D., Garnelo, M., Bachrach, Y ., Czarnecki, W.,Perolat, J., Jaderberg, M., and Graepel, T. Open-endedlearning in symmetric zero-sum games.。In InternationalConference on Machine Learning , pp.. \n",
      "the piece 640\n",
      " \n",
      "tion00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps24252627282930Goal Completion Ratio (%)OursHER(a)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps15.017.520.022.525.027.530.032.5Goal Completion Ratio (%)OursHER (b)00.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0MTraining Steps0.100.080.060.040.020.00Reward0.30.40.50.60.70.8Game StatisticsMean basic reward per stepMean basic reward per step (No KL)#Enemies killed#Enemies knocked down#Enemies killed (No KL)#Enemies knocked down (No KL) (c)Figure 2: (a) The goal completion rate on training dataset; (b) The goal completion rate on unseen goals, i.e., the test dataset;(c) The evaluation of policy learning in cases of w/ and w/o KL-divergence regularizer.。(a) Goal completion rate ( 1≤ |g| ≤7)Trend of  p o l i c y  a b i l i t ySmoothe d  C o m p l e t i o n  R a t eOriginal C o m p l e t i o n  R a t eReset Lo o p sGoal Com p l e t i o n  R a t i o  ( % )(b) Goal completion rate ( |g|= 3)Sub goal        g4   g8   g12   g16   g20 Time in one loop0.0250.0500.0750.1000.1250.1500.1750.200Possibility (c) Sub-goals distributionFigure 3: (a) The completion ratio of goals with dimension size ranges from 1 to 7; (b) The goal completion ratio of goalsthat|g|= 3, the trend curve reflects the improving completion ratio; (c) The sub-goals distribution changes along thetraining in one loop of co-training, where the description of each giis included in Table 14.between LLM and policy than before.。Additionally, weinvestigated the change in the generation probability of sub-goals (Table 14) during co-training.。Specifically, Figure 3(c)illustrates changes within a training loop, while Figure 11(a)indicates changes across loops.。As training progresses, theprobabilities associated with each giundergo gradual modi-fications.。For instance, sub-goals with growing probabilitiesare central to the agent private states due to their relativelyattainable nature and influence in agent interaction.。Con-versely, sub-goals with falling probabilities are central toother players’ states, as they are not directly changed byagent actions, and Gllmtends to generate outputs for thesesub-goals only when absolutely necessary.。To investigatethe impact of co-training to Gllm, we have also identifiedthe changes of goal-generation for an instruction, as shownin Table 3.。Evidently, after co-training, Gllmdemonstratesits capacity to eliminate contradictory and irrational ele-ments within the initial objectives and exhibits the ability tointroduce new sub-goals, thereby rendering the overall goalmore attainable, all while retaining its exceptional semanticcomprehension capabilities.6.。ConclusionIn this paper, we propose OpenPAL experts on learningopen-ended embodied agents for human-AI interaction, ex-celling in achieving instruction open-endedness through atwo-stage learning process.。The empirical results on Contrarepresent that OpenPAL shows the potential as a practicalsolution for human-AI interaction in complex situations.Despite the positive results, we admit there are still somelimitations to our work that would be expected to be re-searched in the future—for instance, a truly open-endedgoal description instead of the handcrafted goal space in thecurrent version; supporting multi-modality input/output tofree from expensive feature engineering.Author Contribution StatementThe authors confirm their contribution as follows:Shaopeng Zhai : team leadership, open-ended learning,LLM/RLAF training, agent analysis, architecture designJie Wang : infrastructure/framework engineering, non-goalBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationagent training, open-ended learning, ablation studies, fea-ture engineeringTianyi Zhang : non-goal agent training, open-ended learn-ing, feature engineeringFuxian Huang : non-goal agent training, paper writing,open-ended learningQi Zhang : LLM training, RLAF training, paper writing,ablation studiesMing Zhou : co-training framework, curriculum research,paper writingJing Hou : LLM training, paper writingReferencesAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O.,and Zaremba, W. Hindsight experience replay.。Advancesin neural information processing systems , 30, 2017.Bai, C., Liu, P., Zhao, W., and Tang, X.。Guided goal gen-eration for hindsight multi-goal reinforcement learning.Neurocomputing , 359:353–367, 2019.Balduzzi, D., Garnelo, M., Bachrach, Y ., Czarnecki, W.,Perolat, J., Jaderberg, M., and Graepel, T. Open-endedlearning in symmetric zero-sum games.。In InternationalConference on Machine Learning , pp.。434–443.. \n",
      "the piece 641\n",
      " \n",
      "n  R a t i o  ( % )(b) Goal completion rate ( |g|= 3)Sub goal        g4   g8   g12   g16   g20 Time in one loop0.0250.0500.0750.1000.1250.1500.1750.200Possibility (c) Sub-goals distributionFigure 3: (a) The completion ratio of goals with dimension size ranges from 1 to 7; (b) The goal completion ratio of goalsthat|g|= 3, the trend curve reflects the improving completion ratio; (c) The sub-goals distribution changes along thetraining in one loop of co-training, where the description of each giis included in Table 14.between LLM and policy than before.。Additionally, weinvestigated the change in the generation probability of sub-goals (Table 14) during co-training.。Specifically, Figure 3(c)illustrates changes within a training loop, while Figure 11(a)indicates changes across loops.。As training progresses, theprobabilities associated with each giundergo gradual modi-fications.。For instance, sub-goals with growing probabilitiesare central to the agent private states due to their relativelyattainable nature and influence in agent interaction.。Con-versely, sub-goals with falling probabilities are central toother players’ states, as they are not directly changed byagent actions, and Gllmtends to generate outputs for thesesub-goals only when absolutely necessary.。To investigatethe impact of co-training to Gllm, we have also identifiedthe changes of goal-generation for an instruction, as shownin Table 3.。Evidently, after co-training, Gllmdemonstratesits capacity to eliminate contradictory and irrational ele-ments within the initial objectives and exhibits the ability tointroduce new sub-goals, thereby rendering the overall goalmore attainable, all while retaining its exceptional semanticcomprehension capabilities.6.。ConclusionIn this paper, we propose OpenPAL experts on learningopen-ended embodied agents for human-AI interaction, ex-celling in achieving instruction open-endedness through atwo-stage learning process.。The empirical results on Contrarepresent that OpenPAL shows the potential as a practicalsolution for human-AI interaction in complex situations.Despite the positive results, we admit there are still somelimitations to our work that would be expected to be re-searched in the future—for instance, a truly open-endedgoal description instead of the handcrafted goal space in thecurrent version; supporting multi-modality input/output tofree from expensive feature engineering.Author Contribution StatementThe authors confirm their contribution as follows:Shaopeng Zhai : team leadership, open-ended learning,LLM/RLAF training, agent analysis, architecture designJie Wang : infrastructure/framework engineering, non-goalBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationagent training, open-ended learning, ablation studies, fea-ture engineeringTianyi Zhang : non-goal agent training, open-ended learn-ing, feature engineeringFuxian Huang : non-goal agent training, paper writing,open-ended learningQi Zhang : LLM training, RLAF training, paper writing,ablation studiesMing Zhou : co-training framework, curriculum research,paper writingJing Hou : LLM training, paper writingReferencesAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O.,and Zaremba, W. Hindsight experience replay.。Advancesin neural information processing systems , 30, 2017.Bai, C., Liu, P., Zhao, W., and Tang, X.。Guided goal gen-eration for hindsight multi-goal reinforcement learning.Neurocomputing , 359:353–367, 2019.Balduzzi, D., Garnelo, M., Bachrach, Y ., Czarnecki, W.,Perolat, J., Jaderberg, M., and Graepel, T. Open-endedlearning in symmetric zero-sum games.。In InternationalConference on Machine Learning , pp.。434–443.。PMLR,2019.Berner, C., Brockman, G., Chan, B., Cheung, V ., D˛ ebiak, P.,Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse,C., et al.. \n",
      "the piece 642\n",
      " \n",
      "ile Figure 11(a)indicates changes across loops.。As training progresses, theprobabilities associated with each giundergo gradual modi-fications.。For instance, sub-goals with growing probabilitiesare central to the agent private states due to their relativelyattainable nature and influence in agent interaction.。Con-versely, sub-goals with falling probabilities are central toother players’ states, as they are not directly changed byagent actions, and Gllmtends to generate outputs for thesesub-goals only when absolutely necessary.。To investigatethe impact of co-training to Gllm, we have also identifiedthe changes of goal-generation for an instruction, as shownin Table 3.。Evidently, after co-training, Gllmdemonstratesits capacity to eliminate contradictory and irrational ele-ments within the initial objectives and exhibits the ability tointroduce new sub-goals, thereby rendering the overall goalmore attainable, all while retaining its exceptional semanticcomprehension capabilities.6.。ConclusionIn this paper, we propose OpenPAL experts on learningopen-ended embodied agents for human-AI interaction, ex-celling in achieving instruction open-endedness through atwo-stage learning process.。The empirical results on Contrarepresent that OpenPAL shows the potential as a practicalsolution for human-AI interaction in complex situations.Despite the positive results, we admit there are still somelimitations to our work that would be expected to be re-searched in the future—for instance, a truly open-endedgoal description instead of the handcrafted goal space in thecurrent version; supporting multi-modality input/output tofree from expensive feature engineering.Author Contribution StatementThe authors confirm their contribution as follows:Shaopeng Zhai : team leadership, open-ended learning,LLM/RLAF training, agent analysis, architecture designJie Wang : infrastructure/framework engineering, non-goalBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationagent training, open-ended learning, ablation studies, fea-ture engineeringTianyi Zhang : non-goal agent training, open-ended learn-ing, feature engineeringFuxian Huang : non-goal agent training, paper writing,open-ended learningQi Zhang : LLM training, RLAF training, paper writing,ablation studiesMing Zhou : co-training framework, curriculum research,paper writingJing Hou : LLM training, paper writingReferencesAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O.,and Zaremba, W. Hindsight experience replay.。Advancesin neural information processing systems , 30, 2017.Bai, C., Liu, P., Zhao, W., and Tang, X.。Guided goal gen-eration for hindsight multi-goal reinforcement learning.Neurocomputing , 359:353–367, 2019.Balduzzi, D., Garnelo, M., Bachrach, Y ., Czarnecki, W.,Perolat, J., Jaderberg, M., and Graepel, T. Open-endedlearning in symmetric zero-sum games.。In InternationalConference on Machine Learning , pp.。434–443.。PMLR,2019.Berner, C., Brockman, G., Chan, B., Cheung, V ., D˛ ebiak, P.,Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse,C., et al.。Dota 2 with large scale deep reinforcementlearning, 2019.Brohan, A., Brown, N., Carbajal, J., Chebotar, Y ., Dabis, J.,Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A.,Hsu, J., et al.. \n",
      "the piece 643\n",
      " \n",
      "ergo gradual modi-fications.。For instance, sub-goals with growing probabilitiesare central to the agent private states due to their relativelyattainable nature and influence in agent interaction.。Con-versely, sub-goals with falling probabilities are central toother players’ states, as they are not directly changed byagent actions, and Gllmtends to generate outputs for thesesub-goals only when absolutely necessary.。To investigatethe impact of co-training to Gllm, we have also identifiedthe changes of goal-generation for an instruction, as shownin Table 3.。Evidently, after co-training, Gllmdemonstratesits capacity to eliminate contradictory and irrational ele-ments within the initial objectives and exhibits the ability tointroduce new sub-goals, thereby rendering the overall goalmore attainable, all while retaining its exceptional semanticcomprehension capabilities.6.。ConclusionIn this paper, we propose OpenPAL experts on learningopen-ended embodied agents for human-AI interaction, ex-celling in achieving instruction open-endedness through atwo-stage learning process.。The empirical results on Contrarepresent that OpenPAL shows the potential as a practicalsolution for human-AI interaction in complex situations.Despite the positive results, we admit there are still somelimitations to our work that would be expected to be re-searched in the future—for instance, a truly open-endedgoal description instead of the handcrafted goal space in thecurrent version; supporting multi-modality input/output tofree from expensive feature engineering.Author Contribution StatementThe authors confirm their contribution as follows:Shaopeng Zhai : team leadership, open-ended learning,LLM/RLAF training, agent analysis, architecture designJie Wang : infrastructure/framework engineering, non-goalBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationagent training, open-ended learning, ablation studies, fea-ture engineeringTianyi Zhang : non-goal agent training, open-ended learn-ing, feature engineeringFuxian Huang : non-goal agent training, paper writing,open-ended learningQi Zhang : LLM training, RLAF training, paper writing,ablation studiesMing Zhou : co-training framework, curriculum research,paper writingJing Hou : LLM training, paper writingReferencesAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O.,and Zaremba, W. Hindsight experience replay.。Advancesin neural information processing systems , 30, 2017.Bai, C., Liu, P., Zhao, W., and Tang, X.。Guided goal gen-eration for hindsight multi-goal reinforcement learning.Neurocomputing , 359:353–367, 2019.Balduzzi, D., Garnelo, M., Bachrach, Y ., Czarnecki, W.,Perolat, J., Jaderberg, M., and Graepel, T. Open-endedlearning in symmetric zero-sum games.。In InternationalConference on Machine Learning , pp.。434–443.。PMLR,2019.Berner, C., Brockman, G., Chan, B., Cheung, V ., D˛ ebiak, P.,Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse,C., et al.。Dota 2 with large scale deep reinforcementlearning, 2019.Brohan, A., Brown, N., Carbajal, J., Chebotar, Y ., Dabis, J.,Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A.,Hsu, J., et al.。Rt-1: Robotics transformer for real-worldcontrol at scale.. \n",
      "the piece 644\n",
      " \n",
      "ue to their relativelyattainable nature and influence in agent interaction.。Con-versely, sub-goals with falling probabilities are central toother players’ states, as they are not directly changed byagent actions, and Gllmtends to generate outputs for thesesub-goals only when absolutely necessary.。To investigatethe impact of co-training to Gllm, we have also identifiedthe changes of goal-generation for an instruction, as shownin Table 3.。Evidently, after co-training, Gllmdemonstratesits capacity to eliminate contradictory and irrational ele-ments within the initial objectives and exhibits the ability tointroduce new sub-goals, thereby rendering the overall goalmore attainable, all while retaining its exceptional semanticcomprehension capabilities.6.。ConclusionIn this paper, we propose OpenPAL experts on learningopen-ended embodied agents for human-AI interaction, ex-celling in achieving instruction open-endedness through atwo-stage learning process.。The empirical results on Contrarepresent that OpenPAL shows the potential as a practicalsolution for human-AI interaction in complex situations.Despite the positive results, we admit there are still somelimitations to our work that would be expected to be re-searched in the future—for instance, a truly open-endedgoal description instead of the handcrafted goal space in thecurrent version; supporting multi-modality input/output tofree from expensive feature engineering.Author Contribution StatementThe authors confirm their contribution as follows:Shaopeng Zhai : team leadership, open-ended learning,LLM/RLAF training, agent analysis, architecture designJie Wang : infrastructure/framework engineering, non-goalBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationagent training, open-ended learning, ablation studies, fea-ture engineeringTianyi Zhang : non-goal agent training, open-ended learn-ing, feature engineeringFuxian Huang : non-goal agent training, paper writing,open-ended learningQi Zhang : LLM training, RLAF training, paper writing,ablation studiesMing Zhou : co-training framework, curriculum research,paper writingJing Hou : LLM training, paper writingReferencesAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O.,and Zaremba, W. Hindsight experience replay.。Advancesin neural information processing systems , 30, 2017.Bai, C., Liu, P., Zhao, W., and Tang, X.。Guided goal gen-eration for hindsight multi-goal reinforcement learning.Neurocomputing , 359:353–367, 2019.Balduzzi, D., Garnelo, M., Bachrach, Y ., Czarnecki, W.,Perolat, J., Jaderberg, M., and Graepel, T. Open-endedlearning in symmetric zero-sum games.。In InternationalConference on Machine Learning , pp.。434–443.。PMLR,2019.Berner, C., Brockman, G., Chan, B., Cheung, V ., D˛ ebiak, P.,Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse,C., et al.。Dota 2 with large scale deep reinforcementlearning, 2019.Brohan, A., Brown, N., Carbajal, J., Chebotar, Y ., Dabis, J.,Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A.,Hsu, J., et al.。Rt-1: Robotics transformer for real-worldcontrol at scale.。arXiv preprint arXiv:2212.06817 , 2022.Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,S., Radford, A., Sutskever, I., and Amodei, D. Languagemodels are few-shot learners, 2020.Burda, Y ., Edwards, H., Storkey, A., and Klimov, O. Explo-ration by random network distillation.. \n",
      "the piece 645\n",
      " \n",
      "s with falling probabilities are central toother players’ states, as they are not directly changed byagent actions, and Gllmtends to generate outputs for thesesub-goals only when absolutely necessary.。To investigatethe impact of co-training to Gllm, we have also identifiedthe changes of goal-generation for an instruction, as shownin Table 3.。Evidently, after co-training, Gllmdemonstratesits capacity to eliminate contradictory and irrational ele-ments within the initial objectives and exhibits the ability tointroduce new sub-goals, thereby rendering the overall goalmore attainable, all while retaining its exceptional semanticcomprehension capabilities.6.。ConclusionIn this paper, we propose OpenPAL experts on learningopen-ended embodied agents for human-AI interaction, ex-celling in achieving instruction open-endedness through atwo-stage learning process.。The empirical results on Contrarepresent that OpenPAL shows the potential as a practicalsolution for human-AI interaction in complex situations.Despite the positive results, we admit there are still somelimitations to our work that would be expected to be re-searched in the future—for instance, a truly open-endedgoal description instead of the handcrafted goal space in thecurrent version; supporting multi-modality input/output tofree from expensive feature engineering.Author Contribution StatementThe authors confirm their contribution as follows:Shaopeng Zhai : team leadership, open-ended learning,LLM/RLAF training, agent analysis, architecture designJie Wang : infrastructure/framework engineering, non-goalBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationagent training, open-ended learning, ablation studies, fea-ture engineeringTianyi Zhang : non-goal agent training, open-ended learn-ing, feature engineeringFuxian Huang : non-goal agent training, paper writing,open-ended learningQi Zhang : LLM training, RLAF training, paper writing,ablation studiesMing Zhou : co-training framework, curriculum research,paper writingJing Hou : LLM training, paper writingReferencesAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O.,and Zaremba, W. Hindsight experience replay.。Advancesin neural information processing systems , 30, 2017.Bai, C., Liu, P., Zhao, W., and Tang, X.。Guided goal gen-eration for hindsight multi-goal reinforcement learning.Neurocomputing , 359:353–367, 2019.Balduzzi, D., Garnelo, M., Bachrach, Y ., Czarnecki, W.,Perolat, J., Jaderberg, M., and Graepel, T. Open-endedlearning in symmetric zero-sum games.。In InternationalConference on Machine Learning , pp.。434–443.。PMLR,2019.Berner, C., Brockman, G., Chan, B., Cheung, V ., D˛ ebiak, P.,Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse,C., et al.。Dota 2 with large scale deep reinforcementlearning, 2019.Brohan, A., Brown, N., Carbajal, J., Chebotar, Y ., Dabis, J.,Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A.,Hsu, J., et al.。Rt-1: Robotics transformer for real-worldcontrol at scale.。arXiv preprint arXiv:2212.06817 , 2022.Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,S., Radford, A., Sutskever, I., and Amodei, D. Languagemodels are few-shot learners, 2020.Burda, Y ., Edwards, H., Storkey, A., and Klimov, O. Explo-ration by random network distillation.。In InternationalConference on Learning Representations , 2018.Ding, H., Tang, Y ., Wu, Q., Wang, B., Chen, C., andWang, Z.. \n",
      "the piece 646\n",
      " \n",
      " only when absolutely necessary.。To investigatethe impact of co-training to Gllm, we have also identifiedthe changes of goal-generation for an instruction, as shownin Table 3.。Evidently, after co-training, Gllmdemonstratesits capacity to eliminate contradictory and irrational ele-ments within the initial objectives and exhibits the ability tointroduce new sub-goals, thereby rendering the overall goalmore attainable, all while retaining its exceptional semanticcomprehension capabilities.6.。ConclusionIn this paper, we propose OpenPAL experts on learningopen-ended embodied agents for human-AI interaction, ex-celling in achieving instruction open-endedness through atwo-stage learning process.。The empirical results on Contrarepresent that OpenPAL shows the potential as a practicalsolution for human-AI interaction in complex situations.Despite the positive results, we admit there are still somelimitations to our work that would be expected to be re-searched in the future—for instance, a truly open-endedgoal description instead of the handcrafted goal space in thecurrent version; supporting multi-modality input/output tofree from expensive feature engineering.Author Contribution StatementThe authors confirm their contribution as follows:Shaopeng Zhai : team leadership, open-ended learning,LLM/RLAF training, agent analysis, architecture designJie Wang : infrastructure/framework engineering, non-goalBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationagent training, open-ended learning, ablation studies, fea-ture engineeringTianyi Zhang : non-goal agent training, open-ended learn-ing, feature engineeringFuxian Huang : non-goal agent training, paper writing,open-ended learningQi Zhang : LLM training, RLAF training, paper writing,ablation studiesMing Zhou : co-training framework, curriculum research,paper writingJing Hou : LLM training, paper writingReferencesAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O.,and Zaremba, W. Hindsight experience replay.。Advancesin neural information processing systems , 30, 2017.Bai, C., Liu, P., Zhao, W., and Tang, X.。Guided goal gen-eration for hindsight multi-goal reinforcement learning.Neurocomputing , 359:353–367, 2019.Balduzzi, D., Garnelo, M., Bachrach, Y ., Czarnecki, W.,Perolat, J., Jaderberg, M., and Graepel, T. Open-endedlearning in symmetric zero-sum games.。In InternationalConference on Machine Learning , pp.。434–443.。PMLR,2019.Berner, C., Brockman, G., Chan, B., Cheung, V ., D˛ ebiak, P.,Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse,C., et al.。Dota 2 with large scale deep reinforcementlearning, 2019.Brohan, A., Brown, N., Carbajal, J., Chebotar, Y ., Dabis, J.,Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A.,Hsu, J., et al.。Rt-1: Robotics transformer for real-worldcontrol at scale.。arXiv preprint arXiv:2212.06817 , 2022.Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,S., Radford, A., Sutskever, I., and Amodei, D. Languagemodels are few-shot learners, 2020.Burda, Y ., Edwards, H., Storkey, A., and Klimov, O. Explo-ration by random network distillation.。In InternationalConference on Learning Representations , 2018.Ding, H., Tang, Y ., Wu, Q., Wang, B., Chen, C., andWang, Z.。Magnetic field-based reward shaping for goal-conditioned reinforcement learning.. \n",
      "the piece 647\n",
      " \n",
      "ts capacity to eliminate contradictory and irrational ele-ments within the initial objectives and exhibits the ability tointroduce new sub-goals, thereby rendering the overall goalmore attainable, all while retaining its exceptional semanticcomprehension capabilities.6.。ConclusionIn this paper, we propose OpenPAL experts on learningopen-ended embodied agents for human-AI interaction, ex-celling in achieving instruction open-endedness through atwo-stage learning process.。The empirical results on Contrarepresent that OpenPAL shows the potential as a practicalsolution for human-AI interaction in complex situations.Despite the positive results, we admit there are still somelimitations to our work that would be expected to be re-searched in the future—for instance, a truly open-endedgoal description instead of the handcrafted goal space in thecurrent version; supporting multi-modality input/output tofree from expensive feature engineering.Author Contribution StatementThe authors confirm their contribution as follows:Shaopeng Zhai : team leadership, open-ended learning,LLM/RLAF training, agent analysis, architecture designJie Wang : infrastructure/framework engineering, non-goalBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationagent training, open-ended learning, ablation studies, fea-ture engineeringTianyi Zhang : non-goal agent training, open-ended learn-ing, feature engineeringFuxian Huang : non-goal agent training, paper writing,open-ended learningQi Zhang : LLM training, RLAF training, paper writing,ablation studiesMing Zhou : co-training framework, curriculum research,paper writingJing Hou : LLM training, paper writingReferencesAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O.,and Zaremba, W. Hindsight experience replay.。Advancesin neural information processing systems , 30, 2017.Bai, C., Liu, P., Zhao, W., and Tang, X.。Guided goal gen-eration for hindsight multi-goal reinforcement learning.Neurocomputing , 359:353–367, 2019.Balduzzi, D., Garnelo, M., Bachrach, Y ., Czarnecki, W.,Perolat, J., Jaderberg, M., and Graepel, T. Open-endedlearning in symmetric zero-sum games.。In InternationalConference on Machine Learning , pp.。434–443.。PMLR,2019.Berner, C., Brockman, G., Chan, B., Cheung, V ., D˛ ebiak, P.,Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse,C., et al.。Dota 2 with large scale deep reinforcementlearning, 2019.Brohan, A., Brown, N., Carbajal, J., Chebotar, Y ., Dabis, J.,Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A.,Hsu, J., et al.。Rt-1: Robotics transformer for real-worldcontrol at scale.。arXiv preprint arXiv:2212.06817 , 2022.Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,S., Radford, A., Sutskever, I., and Amodei, D. Languagemodels are few-shot learners, 2020.Burda, Y ., Edwards, H., Storkey, A., and Klimov, O. Explo-ration by random network distillation.。In InternationalConference on Learning Representations , 2018.Ding, H., Tang, Y ., Wu, Q., Wang, B., Chen, C., andWang, Z.。Magnetic field-based reward shaping for goal-conditioned reinforcement learning.。IEEE/CAA Journalof Automatica Sinica , 10(12):1–15, 2023.Du, Y ., Watkins, O., Wang, Z., Colas, C., Darrell, T., Abbeel,P., Gupta, A., and Andreas, J.. \n",
      "the piece 648\n",
      " \n",
      ", thereby rendering the overall goalmore attainable, all while retaining its exceptional semanticcomprehension capabilities.6.。ConclusionIn this paper, we propose OpenPAL experts on learningopen-ended embodied agents for human-AI interaction, ex-celling in achieving instruction open-endedness through atwo-stage learning process.。The empirical results on Contrarepresent that OpenPAL shows the potential as a practicalsolution for human-AI interaction in complex situations.Despite the positive results, we admit there are still somelimitations to our work that would be expected to be re-searched in the future—for instance, a truly open-endedgoal description instead of the handcrafted goal space in thecurrent version; supporting multi-modality input/output tofree from expensive feature engineering.Author Contribution StatementThe authors confirm their contribution as follows:Shaopeng Zhai : team leadership, open-ended learning,LLM/RLAF training, agent analysis, architecture designJie Wang : infrastructure/framework engineering, non-goalBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationagent training, open-ended learning, ablation studies, fea-ture engineeringTianyi Zhang : non-goal agent training, open-ended learn-ing, feature engineeringFuxian Huang : non-goal agent training, paper writing,open-ended learningQi Zhang : LLM training, RLAF training, paper writing,ablation studiesMing Zhou : co-training framework, curriculum research,paper writingJing Hou : LLM training, paper writingReferencesAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O.,and Zaremba, W. Hindsight experience replay.。Advancesin neural information processing systems , 30, 2017.Bai, C., Liu, P., Zhao, W., and Tang, X.。Guided goal gen-eration for hindsight multi-goal reinforcement learning.Neurocomputing , 359:353–367, 2019.Balduzzi, D., Garnelo, M., Bachrach, Y ., Czarnecki, W.,Perolat, J., Jaderberg, M., and Graepel, T. Open-endedlearning in symmetric zero-sum games.。In InternationalConference on Machine Learning , pp.。434–443.。PMLR,2019.Berner, C., Brockman, G., Chan, B., Cheung, V ., D˛ ebiak, P.,Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse,C., et al.。Dota 2 with large scale deep reinforcementlearning, 2019.Brohan, A., Brown, N., Carbajal, J., Chebotar, Y ., Dabis, J.,Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A.,Hsu, J., et al.。Rt-1: Robotics transformer for real-worldcontrol at scale.。arXiv preprint arXiv:2212.06817 , 2022.Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,S., Radford, A., Sutskever, I., and Amodei, D. Languagemodels are few-shot learners, 2020.Burda, Y ., Edwards, H., Storkey, A., and Klimov, O. Explo-ration by random network distillation.。In InternationalConference on Learning Representations , 2018.Ding, H., Tang, Y ., Wu, Q., Wang, B., Chen, C., andWang, Z.。Magnetic field-based reward shaping for goal-conditioned reinforcement learning.。IEEE/CAA Journalof Automatica Sinica , 10(12):1–15, 2023.Du, Y ., Watkins, O., Wang, Z., Colas, C., Darrell, T., Abbeel,P., Gupta, A., and Andreas, J.。Guiding pretraining inreinforcement learning with large language models.. \n",
      "the piece 649\n",
      " \n",
      "ng process.。The empirical results on Contrarepresent that OpenPAL shows the potential as a practicalsolution for human-AI interaction in complex situations.Despite the positive results, we admit there are still somelimitations to our work that would be expected to be re-searched in the future—for instance, a truly open-endedgoal description instead of the handcrafted goal space in thecurrent version; supporting multi-modality input/output tofree from expensive feature engineering.Author Contribution StatementThe authors confirm their contribution as follows:Shaopeng Zhai : team leadership, open-ended learning,LLM/RLAF training, agent analysis, architecture designJie Wang : infrastructure/framework engineering, non-goalBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationagent training, open-ended learning, ablation studies, fea-ture engineeringTianyi Zhang : non-goal agent training, open-ended learn-ing, feature engineeringFuxian Huang : non-goal agent training, paper writing,open-ended learningQi Zhang : LLM training, RLAF training, paper writing,ablation studiesMing Zhou : co-training framework, curriculum research,paper writingJing Hou : LLM training, paper writingReferencesAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O.,and Zaremba, W. Hindsight experience replay.。Advancesin neural information processing systems , 30, 2017.Bai, C., Liu, P., Zhao, W., and Tang, X.。Guided goal gen-eration for hindsight multi-goal reinforcement learning.Neurocomputing , 359:353–367, 2019.Balduzzi, D., Garnelo, M., Bachrach, Y ., Czarnecki, W.,Perolat, J., Jaderberg, M., and Graepel, T. Open-endedlearning in symmetric zero-sum games.。In InternationalConference on Machine Learning , pp.。434–443.。PMLR,2019.Berner, C., Brockman, G., Chan, B., Cheung, V ., D˛ ebiak, P.,Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse,C., et al.。Dota 2 with large scale deep reinforcementlearning, 2019.Brohan, A., Brown, N., Carbajal, J., Chebotar, Y ., Dabis, J.,Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A.,Hsu, J., et al.。Rt-1: Robotics transformer for real-worldcontrol at scale.。arXiv preprint arXiv:2212.06817 , 2022.Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,S., Radford, A., Sutskever, I., and Amodei, D. Languagemodels are few-shot learners, 2020.Burda, Y ., Edwards, H., Storkey, A., and Klimov, O. Explo-ration by random network distillation.。In InternationalConference on Learning Representations , 2018.Ding, H., Tang, Y ., Wu, Q., Wang, B., Chen, C., andWang, Z.。Magnetic field-based reward shaping for goal-conditioned reinforcement learning.。IEEE/CAA Journalof Automatica Sinica , 10(12):1–15, 2023.Du, Y ., Watkins, O., Wang, Z., Colas, C., Darrell, T., Abbeel,P., Gupta, A., and Andreas, J.。Guiding pretraining inreinforcement learning with large language models.。arXivpreprint arXiv:2302.06692 , 2023.Du, Z., Qian, Y ., Liu, X., Ding, M., Qiu, J., Yang, Z., andTang, J. Glm: General language model pretraining withautoregressive blank infilling.. \n",
      "the piece 650\n",
      " \n",
      "still somelimitations to our work that would be expected to be re-searched in the future—for instance, a truly open-endedgoal description instead of the handcrafted goal space in thecurrent version; supporting multi-modality input/output tofree from expensive feature engineering.Author Contribution StatementThe authors confirm their contribution as follows:Shaopeng Zhai : team leadership, open-ended learning,LLM/RLAF training, agent analysis, architecture designJie Wang : infrastructure/framework engineering, non-goalBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationagent training, open-ended learning, ablation studies, fea-ture engineeringTianyi Zhang : non-goal agent training, open-ended learn-ing, feature engineeringFuxian Huang : non-goal agent training, paper writing,open-ended learningQi Zhang : LLM training, RLAF training, paper writing,ablation studiesMing Zhou : co-training framework, curriculum research,paper writingJing Hou : LLM training, paper writingReferencesAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O.,and Zaremba, W. Hindsight experience replay.。Advancesin neural information processing systems , 30, 2017.Bai, C., Liu, P., Zhao, W., and Tang, X.。Guided goal gen-eration for hindsight multi-goal reinforcement learning.Neurocomputing , 359:353–367, 2019.Balduzzi, D., Garnelo, M., Bachrach, Y ., Czarnecki, W.,Perolat, J., Jaderberg, M., and Graepel, T. Open-endedlearning in symmetric zero-sum games.。In InternationalConference on Machine Learning , pp.。434–443.。PMLR,2019.Berner, C., Brockman, G., Chan, B., Cheung, V ., D˛ ebiak, P.,Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse,C., et al.。Dota 2 with large scale deep reinforcementlearning, 2019.Brohan, A., Brown, N., Carbajal, J., Chebotar, Y ., Dabis, J.,Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A.,Hsu, J., et al.。Rt-1: Robotics transformer for real-worldcontrol at scale.。arXiv preprint arXiv:2212.06817 , 2022.Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,S., Radford, A., Sutskever, I., and Amodei, D. Languagemodels are few-shot learners, 2020.Burda, Y ., Edwards, H., Storkey, A., and Klimov, O. Explo-ration by random network distillation.。In InternationalConference on Learning Representations , 2018.Ding, H., Tang, Y ., Wu, Q., Wang, B., Chen, C., andWang, Z.。Magnetic field-based reward shaping for goal-conditioned reinforcement learning.。IEEE/CAA Journalof Automatica Sinica , 10(12):1–15, 2023.Du, Y ., Watkins, O., Wang, Z., Colas, C., Darrell, T., Abbeel,P., Gupta, A., and Andreas, J.。Guiding pretraining inreinforcement learning with large language models.。arXivpreprint arXiv:2302.06692 , 2023.Du, Z., Qian, Y ., Liu, X., Ding, M., Qiu, J., Yang, Z., andTang, J. Glm: General language model pretraining withautoregressive blank infilling.。In Proceedings of the 60thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers) , pp.. \n",
      "the piece 651\n",
      " \n",
      "353–367, 2019.Balduzzi, D., Garnelo, M., Bachrach, Y ., Czarnecki, W.,Perolat, J., Jaderberg, M., and Graepel, T. Open-endedlearning in symmetric zero-sum games.。In InternationalConference on Machine Learning , pp.。434–443.。PMLR,2019.Berner, C., Brockman, G., Chan, B., Cheung, V ., D˛ ebiak, P.,Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse,C., et al.。Dota 2 with large scale deep reinforcementlearning, 2019.Brohan, A., Brown, N., Carbajal, J., Chebotar, Y ., Dabis, J.,Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A.,Hsu, J., et al.。Rt-1: Robotics transformer for real-worldcontrol at scale.。arXiv preprint arXiv:2212.06817 , 2022.Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,S., Radford, A., Sutskever, I., and Amodei, D. Languagemodels are few-shot learners, 2020.Burda, Y ., Edwards, H., Storkey, A., and Klimov, O. Explo-ration by random network distillation.。In InternationalConference on Learning Representations , 2018.Ding, H., Tang, Y ., Wu, Q., Wang, B., Chen, C., andWang, Z.。Magnetic field-based reward shaping for goal-conditioned reinforcement learning.。IEEE/CAA Journalof Automatica Sinica , 10(12):1–15, 2023.Du, Y ., Watkins, O., Wang, Z., Colas, C., Darrell, T., Abbeel,P., Gupta, A., and Andreas, J.。Guiding pretraining inreinforcement learning with large language models.。arXivpreprint arXiv:2302.06692 , 2023.Du, Z., Qian, Y ., Liu, X., Ding, M., Qiu, J., Yang, Z., andTang, J. Glm: General language model pretraining withautoregressive blank infilling.。In Proceedings of the 60thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers) , pp.。320–335, 2022.Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., andClune, J.. \n",
      "the piece 652\n",
      " \n",
      "Graepel, T. Open-endedlearning in symmetric zero-sum games.。In InternationalConference on Machine Learning , pp.。434–443.。PMLR,2019.Berner, C., Brockman, G., Chan, B., Cheung, V ., D˛ ebiak, P.,Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse,C., et al.。Dota 2 with large scale deep reinforcementlearning, 2019.Brohan, A., Brown, N., Carbajal, J., Chebotar, Y ., Dabis, J.,Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A.,Hsu, J., et al.。Rt-1: Robotics transformer for real-worldcontrol at scale.。arXiv preprint arXiv:2212.06817 , 2022.Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,S., Radford, A., Sutskever, I., and Amodei, D. Languagemodels are few-shot learners, 2020.Burda, Y ., Edwards, H., Storkey, A., and Klimov, O. Explo-ration by random network distillation.。In InternationalConference on Learning Representations , 2018.Ding, H., Tang, Y ., Wu, Q., Wang, B., Chen, C., andWang, Z.。Magnetic field-based reward shaping for goal-conditioned reinforcement learning.。IEEE/CAA Journalof Automatica Sinica , 10(12):1–15, 2023.Du, Y ., Watkins, O., Wang, Z., Colas, C., Darrell, T., Abbeel,P., Gupta, A., and Andreas, J.。Guiding pretraining inreinforcement learning with large language models.。arXivpreprint arXiv:2302.06692 , 2023.Du, Z., Qian, Y ., Liu, X., Ding, M., Qiu, J., Yang, Z., andTang, J. Glm: General language model pretraining withautoregressive blank infilling.。In Proceedings of the 60thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers) , pp.。320–335, 2022.Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., andClune, J.。First return, then explore.. \n",
      "the piece 653\n",
      " \n",
      " al.。Dota 2 with large scale deep reinforcementlearning, 2019.Brohan, A., Brown, N., Carbajal, J., Chebotar, Y ., Dabis, J.,Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A.,Hsu, J., et al.。Rt-1: Robotics transformer for real-worldcontrol at scale.。arXiv preprint arXiv:2212.06817 , 2022.Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,S., Radford, A., Sutskever, I., and Amodei, D. Languagemodels are few-shot learners, 2020.Burda, Y ., Edwards, H., Storkey, A., and Klimov, O. Explo-ration by random network distillation.。In InternationalConference on Learning Representations , 2018.Ding, H., Tang, Y ., Wu, Q., Wang, B., Chen, C., andWang, Z.。Magnetic field-based reward shaping for goal-conditioned reinforcement learning.。IEEE/CAA Journalof Automatica Sinica , 10(12):1–15, 2023.Du, Y ., Watkins, O., Wang, Z., Colas, C., Darrell, T., Abbeel,P., Gupta, A., and Andreas, J.。Guiding pretraining inreinforcement learning with large language models.。arXivpreprint arXiv:2302.06692 , 2023.Du, Z., Qian, Y ., Liu, X., Ding, M., Qiu, J., Yang, Z., andTang, J. Glm: General language model pretraining withautoregressive blank infilling.。In Proceedings of the 60thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers) , pp.。320–335, 2022.Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., andClune, J.。First return, then explore.。Nature , 590(7847):580–586, 2021.Fang, M., Zhou, T., Du, Y ., Han, L., and Zhang, Z.Curriculum-guided hindsight experience replay.. \n",
      "the piece 654\n",
      " \n",
      "g, 2019.Brohan, A., Brown, N., Carbajal, J., Chebotar, Y ., Dabis, J.,Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A.,Hsu, J., et al.。Rt-1: Robotics transformer for real-worldcontrol at scale.。arXiv preprint arXiv:2212.06817 , 2022.Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,S., Radford, A., Sutskever, I., and Amodei, D. Languagemodels are few-shot learners, 2020.Burda, Y ., Edwards, H., Storkey, A., and Klimov, O. Explo-ration by random network distillation.。In InternationalConference on Learning Representations , 2018.Ding, H., Tang, Y ., Wu, Q., Wang, B., Chen, C., andWang, Z.。Magnetic field-based reward shaping for goal-conditioned reinforcement learning.。IEEE/CAA Journalof Automatica Sinica , 10(12):1–15, 2023.Du, Y ., Watkins, O., Wang, Z., Colas, C., Darrell, T., Abbeel,P., Gupta, A., and Andreas, J.。Guiding pretraining inreinforcement learning with large language models.。arXivpreprint arXiv:2302.06692 , 2023.Du, Z., Qian, Y ., Liu, X., Ding, M., Qiu, J., Yang, Z., andTang, J. Glm: General language model pretraining withautoregressive blank infilling.。In Proceedings of the 60thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers) , pp.。320–335, 2022.Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., andClune, J.。First return, then explore.。Nature , 590(7847):580–586, 2021.Fang, M., Zhou, T., Du, Y ., Han, L., and Zhang, Z.Curriculum-guided hindsight experience replay.。Ad-vances in neural information processing systems , 32,2019.Gautam, A., Jain, H., Senger, A., and Dhand, G. Battleroyale: First-person shooter game.. \n",
      "the piece 655\n",
      " \n",
      "ohan, A., Brown, N., Carbajal, J., Chebotar, Y ., Dabis, J.,Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A.,Hsu, J., et al.。Rt-1: Robotics transformer for real-worldcontrol at scale.。arXiv preprint arXiv:2212.06817 , 2022.Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,S., Radford, A., Sutskever, I., and Amodei, D. Languagemodels are few-shot learners, 2020.Burda, Y ., Edwards, H., Storkey, A., and Klimov, O. Explo-ration by random network distillation.。In InternationalConference on Learning Representations , 2018.Ding, H., Tang, Y ., Wu, Q., Wang, B., Chen, C., andWang, Z.。Magnetic field-based reward shaping for goal-conditioned reinforcement learning.。IEEE/CAA Journalof Automatica Sinica , 10(12):1–15, 2023.Du, Y ., Watkins, O., Wang, Z., Colas, C., Darrell, T., Abbeel,P., Gupta, A., and Andreas, J.。Guiding pretraining inreinforcement learning with large language models.。arXivpreprint arXiv:2302.06692 , 2023.Du, Z., Qian, Y ., Liu, X., Ding, M., Qiu, J., Yang, Z., andTang, J. Glm: General language model pretraining withautoregressive blank infilling.。In Proceedings of the 60thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers) , pp.。320–335, 2022.Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., andClune, J.。First return, then explore.。Nature , 590(7847):580–586, 2021.Fang, M., Zhou, T., Du, Y ., Han, L., and Zhang, Z.Curriculum-guided hindsight experience replay.。Ad-vances in neural information processing systems , 32,2019.Gautam, A., Jain, H., Senger, A., and Dhand, G. Battleroyale: First-person shooter game.。In Proceedings ofthe International Conference on Innovative Computing &Communication (ICICC) , 2021.Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R.,Soyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg,M., Teplyashin, D., et al.. \n",
      "the piece 656\n",
      " \n",
      "tics transformer for real-worldcontrol at scale.。arXiv preprint arXiv:2212.06817 , 2022.Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,S., Radford, A., Sutskever, I., and Amodei, D. Languagemodels are few-shot learners, 2020.Burda, Y ., Edwards, H., Storkey, A., and Klimov, O. Explo-ration by random network distillation.。In InternationalConference on Learning Representations , 2018.Ding, H., Tang, Y ., Wu, Q., Wang, B., Chen, C., andWang, Z.。Magnetic field-based reward shaping for goal-conditioned reinforcement learning.。IEEE/CAA Journalof Automatica Sinica , 10(12):1–15, 2023.Du, Y ., Watkins, O., Wang, Z., Colas, C., Darrell, T., Abbeel,P., Gupta, A., and Andreas, J.。Guiding pretraining inreinforcement learning with large language models.。arXivpreprint arXiv:2302.06692 , 2023.Du, Z., Qian, Y ., Liu, X., Ding, M., Qiu, J., Yang, Z., andTang, J. Glm: General language model pretraining withautoregressive blank infilling.。In Proceedings of the 60thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers) , pp.。320–335, 2022.Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., andClune, J.。First return, then explore.。Nature , 590(7847):580–586, 2021.Fang, M., Zhou, T., Du, Y ., Han, L., and Zhang, Z.Curriculum-guided hindsight experience replay.。Ad-vances in neural information processing systems , 32,2019.Gautam, A., Jain, H., Senger, A., and Dhand, G. Battleroyale: First-person shooter game.。In Proceedings ofthe International Conference on Innovative Computing &Communication (ICICC) , 2021.Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R.,Soyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg,M., Teplyashin, D., et al.。Grounded language learning ina simulated 3d world.. \n",
      "the piece 657\n",
      " \n",
      "astry, G.,Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,S., Radford, A., Sutskever, I., and Amodei, D. Languagemodels are few-shot learners, 2020.Burda, Y ., Edwards, H., Storkey, A., and Klimov, O. Explo-ration by random network distillation.。In InternationalConference on Learning Representations , 2018.Ding, H., Tang, Y ., Wu, Q., Wang, B., Chen, C., andWang, Z.。Magnetic field-based reward shaping for goal-conditioned reinforcement learning.。IEEE/CAA Journalof Automatica Sinica , 10(12):1–15, 2023.Du, Y ., Watkins, O., Wang, Z., Colas, C., Darrell, T., Abbeel,P., Gupta, A., and Andreas, J.。Guiding pretraining inreinforcement learning with large language models.。arXivpreprint arXiv:2302.06692 , 2023.Du, Z., Qian, Y ., Liu, X., Ding, M., Qiu, J., Yang, Z., andTang, J. Glm: General language model pretraining withautoregressive blank infilling.。In Proceedings of the 60thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers) , pp.。320–335, 2022.Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., andClune, J.。First return, then explore.。Nature , 590(7847):580–586, 2021.Fang, M., Zhou, T., Du, Y ., Han, L., and Zhang, Z.Curriculum-guided hindsight experience replay.。Ad-vances in neural information processing systems , 32,2019.Gautam, A., Jain, H., Senger, A., and Dhand, G. Battleroyale: First-person shooter game.。In Proceedings ofthe International Conference on Innovative Computing &Communication (ICICC) , 2021.Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R.,Soyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg,M., Teplyashin, D., et al.。Grounded language learning ina simulated 3d world.。arXiv preprint arXiv:1706.06551 ,2017.Hu, B., Zhao, C., Zhang, P., Zhou, Z., Yang, Y ., Xu, Z., andLiu, B.. \n",
      "the piece 658\n",
      " \n",
      "r, G.,Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,S., Radford, A., Sutskever, I., and Amodei, D. Languagemodels are few-shot learners, 2020.Burda, Y ., Edwards, H., Storkey, A., and Klimov, O. Explo-ration by random network distillation.。In InternationalConference on Learning Representations , 2018.Ding, H., Tang, Y ., Wu, Q., Wang, B., Chen, C., andWang, Z.。Magnetic field-based reward shaping for goal-conditioned reinforcement learning.。IEEE/CAA Journalof Automatica Sinica , 10(12):1–15, 2023.Du, Y ., Watkins, O., Wang, Z., Colas, C., Darrell, T., Abbeel,P., Gupta, A., and Andreas, J.。Guiding pretraining inreinforcement learning with large language models.。arXivpreprint arXiv:2302.06692 , 2023.Du, Z., Qian, Y ., Liu, X., Ding, M., Qiu, J., Yang, Z., andTang, J. Glm: General language model pretraining withautoregressive blank infilling.。In Proceedings of the 60thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers) , pp.。320–335, 2022.Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., andClune, J.。First return, then explore.。Nature , 590(7847):580–586, 2021.Fang, M., Zhou, T., Du, Y ., Han, L., and Zhang, Z.Curriculum-guided hindsight experience replay.。Ad-vances in neural information processing systems , 32,2019.Gautam, A., Jain, H., Senger, A., and Dhand, G. Battleroyale: First-person shooter game.。In Proceedings ofthe International Conference on Innovative Computing &Communication (ICICC) , 2021.Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R.,Soyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg,M., Teplyashin, D., et al.。Grounded language learning ina simulated 3d world.。arXiv preprint arXiv:1706.06551 ,2017.Hu, B., Zhao, C., Zhang, P., Zhou, Z., Yang, Y ., Xu, Z., andLiu, B.。Enabling intelligent interactions between an agentand an llm: A reinforcement learning approach, 2023.Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,S., Wang, L., and Chen, W. Lora: Low-rank adaptationof large language models, 2021.Huang, W., Wang, C., Zhang, R., Li, Y ., Wu, J., and Fei-Fei,L.. \n",
      "the piece 659\n",
      " \n",
      ".。IEEE/CAA Journalof Automatica Sinica , 10(12):1–15, 2023.Du, Y ., Watkins, O., Wang, Z., Colas, C., Darrell, T., Abbeel,P., Gupta, A., and Andreas, J.。Guiding pretraining inreinforcement learning with large language models.。arXivpreprint arXiv:2302.06692 , 2023.Du, Z., Qian, Y ., Liu, X., Ding, M., Qiu, J., Yang, Z., andTang, J. Glm: General language model pretraining withautoregressive blank infilling.。In Proceedings of the 60thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers) , pp.。320–335, 2022.Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., andClune, J.。First return, then explore.。Nature , 590(7847):580–586, 2021.Fang, M., Zhou, T., Du, Y ., Han, L., and Zhang, Z.Curriculum-guided hindsight experience replay.。Ad-vances in neural information processing systems , 32,2019.Gautam, A., Jain, H., Senger, A., and Dhand, G. Battleroyale: First-person shooter game.。In Proceedings ofthe International Conference on Innovative Computing &Communication (ICICC) , 2021.Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R.,Soyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg,M., Teplyashin, D., et al.。Grounded language learning ina simulated 3d world.。arXiv preprint arXiv:1706.06551 ,2017.Hu, B., Zhao, C., Zhang, P., Zhou, Z., Yang, Y ., Xu, Z., andLiu, B.。Enabling intelligent interactions between an agentand an llm: A reinforcement learning approach, 2023.Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,S., Wang, L., and Chen, W. Lora: Low-rank adaptationof large language models, 2021.Huang, W., Wang, C., Zhang, R., Li, Y ., Wu, J., and Fei-Fei,L.。V oxposer: Composable 3d value maps for roboticmanipulation with language models.. \n",
      "the piece 660\n",
      " \n",
      ", Gupta, A., and Andreas, J.。Guiding pretraining inreinforcement learning with large language models.。arXivpreprint arXiv:2302.06692 , 2023.Du, Z., Qian, Y ., Liu, X., Ding, M., Qiu, J., Yang, Z., andTang, J. Glm: General language model pretraining withautoregressive blank infilling.。In Proceedings of the 60thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers) , pp.。320–335, 2022.Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., andClune, J.。First return, then explore.。Nature , 590(7847):580–586, 2021.Fang, M., Zhou, T., Du, Y ., Han, L., and Zhang, Z.Curriculum-guided hindsight experience replay.。Ad-vances in neural information processing systems , 32,2019.Gautam, A., Jain, H., Senger, A., and Dhand, G. Battleroyale: First-person shooter game.。In Proceedings ofthe International Conference on Innovative Computing &Communication (ICICC) , 2021.Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R.,Soyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg,M., Teplyashin, D., et al.。Grounded language learning ina simulated 3d world.。arXiv preprint arXiv:1706.06551 ,2017.Hu, B., Zhao, C., Zhang, P., Zhou, Z., Yang, Y ., Xu, Z., andLiu, B.。Enabling intelligent interactions between an agentand an llm: A reinforcement learning approach, 2023.Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,S., Wang, L., and Chen, W. Lora: Low-rank adaptationof large language models, 2021.Huang, W., Wang, C., Zhang, R., Li, Y ., Wu, J., and Fei-Fei,L.。V oxposer: Composable 3d value maps for roboticmanipulation with language models.。In Conference onRobot Learning , pp.. \n",
      "the piece 661\n",
      " \n",
      "ge language models.。arXivpreprint arXiv:2302.06692 , 2023.Du, Z., Qian, Y ., Liu, X., Ding, M., Qiu, J., Yang, Z., andTang, J. Glm: General language model pretraining withautoregressive blank infilling.。In Proceedings of the 60thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers) , pp.。320–335, 2022.Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., andClune, J.。First return, then explore.。Nature , 590(7847):580–586, 2021.Fang, M., Zhou, T., Du, Y ., Han, L., and Zhang, Z.Curriculum-guided hindsight experience replay.。Ad-vances in neural information processing systems , 32,2019.Gautam, A., Jain, H., Senger, A., and Dhand, G. Battleroyale: First-person shooter game.。In Proceedings ofthe International Conference on Innovative Computing &Communication (ICICC) , 2021.Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R.,Soyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg,M., Teplyashin, D., et al.。Grounded language learning ina simulated 3d world.。arXiv preprint arXiv:1706.06551 ,2017.Hu, B., Zhao, C., Zhang, P., Zhou, Z., Yang, Y ., Xu, Z., andLiu, B.。Enabling intelligent interactions between an agentand an llm: A reinforcement learning approach, 2023.Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,S., Wang, L., and Chen, W. Lora: Low-rank adaptationof large language models, 2021.Huang, W., Wang, C., Zhang, R., Li, Y ., Wu, J., and Fei-Fei,L.。V oxposer: Composable 3d value maps for roboticmanipulation with language models.。In Conference onRobot Learning , pp.。540–562.. \n",
      "the piece 662\n",
      " \n",
      "el pretraining withautoregressive blank infilling.。In Proceedings of the 60thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers) , pp.。320–335, 2022.Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., andClune, J.。First return, then explore.。Nature , 590(7847):580–586, 2021.Fang, M., Zhou, T., Du, Y ., Han, L., and Zhang, Z.Curriculum-guided hindsight experience replay.。Ad-vances in neural information processing systems , 32,2019.Gautam, A., Jain, H., Senger, A., and Dhand, G. Battleroyale: First-person shooter game.。In Proceedings ofthe International Conference on Innovative Computing &Communication (ICICC) , 2021.Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R.,Soyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg,M., Teplyashin, D., et al.。Grounded language learning ina simulated 3d world.。arXiv preprint arXiv:1706.06551 ,2017.Hu, B., Zhao, C., Zhang, P., Zhou, Z., Yang, Y ., Xu, Z., andLiu, B.。Enabling intelligent interactions between an agentand an llm: A reinforcement learning approach, 2023.Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,S., Wang, L., and Chen, W. Lora: Low-rank adaptationof large language models, 2021.Huang, W., Wang, C., Zhang, R., Li, Y ., Wu, J., and Fei-Fei,L.。V oxposer: Composable 3d value maps for roboticmanipulation with language models.。In Conference onRobot Learning , pp.。540–562.。PMLR, 2023.Jiang, Y ., Gupta, A., Zhang, Z., Wang, G., Dou, Y ., Chen, Y .,Fei-Fei, L., Anandkumar, A., Zhu, Y ., and Fan, L. Vima:Robot manipulation with multimodal prompts.. \n",
      "the piece 663\n",
      " \n",
      "0thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers) , pp.。320–335, 2022.Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., andClune, J.。First return, then explore.。Nature , 590(7847):580–586, 2021.Fang, M., Zhou, T., Du, Y ., Han, L., and Zhang, Z.Curriculum-guided hindsight experience replay.。Ad-vances in neural information processing systems , 32,2019.Gautam, A., Jain, H., Senger, A., and Dhand, G. Battleroyale: First-person shooter game.。In Proceedings ofthe International Conference on Innovative Computing &Communication (ICICC) , 2021.Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R.,Soyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg,M., Teplyashin, D., et al.。Grounded language learning ina simulated 3d world.。arXiv preprint arXiv:1706.06551 ,2017.Hu, B., Zhao, C., Zhang, P., Zhou, Z., Yang, Y ., Xu, Z., andLiu, B.。Enabling intelligent interactions between an agentand an llm: A reinforcement learning approach, 2023.Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,S., Wang, L., and Chen, W. Lora: Low-rank adaptationof large language models, 2021.Huang, W., Wang, C., Zhang, R., Li, Y ., Wu, J., and Fei-Fei,L.。V oxposer: Composable 3d value maps for roboticmanipulation with language models.。In Conference onRobot Learning , pp.。540–562.。PMLR, 2023.Jiang, Y ., Gupta, A., Zhang, Z., Wang, G., Dou, Y ., Chen, Y .,Fei-Fei, L., Anandkumar, A., Zhu, Y ., and Fan, L. Vima:Robot manipulation with multimodal prompts.。2023.Khandelwal, A., Weihs, L., Mottaghi, R., and Kembhavi,A.. \n",
      "the piece 664\n",
      " \n",
      "return, then explore.。Nature , 590(7847):580–586, 2021.Fang, M., Zhou, T., Du, Y ., Han, L., and Zhang, Z.Curriculum-guided hindsight experience replay.。Ad-vances in neural information processing systems , 32,2019.Gautam, A., Jain, H., Senger, A., and Dhand, G. Battleroyale: First-person shooter game.。In Proceedings ofthe International Conference on Innovative Computing &Communication (ICICC) , 2021.Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R.,Soyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg,M., Teplyashin, D., et al.。Grounded language learning ina simulated 3d world.。arXiv preprint arXiv:1706.06551 ,2017.Hu, B., Zhao, C., Zhang, P., Zhou, Z., Yang, Y ., Xu, Z., andLiu, B.。Enabling intelligent interactions between an agentand an llm: A reinforcement learning approach, 2023.Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,S., Wang, L., and Chen, W. Lora: Low-rank adaptationof large language models, 2021.Huang, W., Wang, C., Zhang, R., Li, Y ., Wu, J., and Fei-Fei,L.。V oxposer: Composable 3d value maps for roboticmanipulation with language models.。In Conference onRobot Learning , pp.。540–562.。PMLR, 2023.Jiang, Y ., Gupta, A., Zhang, Z., Wang, G., Dou, Y ., Chen, Y .,Fei-Fei, L., Anandkumar, A., Zhu, Y ., and Fan, L. Vima:Robot manipulation with multimodal prompts.。2023.Khandelwal, A., Weihs, L., Mottaghi, R., and Kembhavi,A.。Simple but effective: Clip embeddings for embodiedai.. \n",
      "the piece 665\n",
      " \n",
      "ded hindsight experience replay.。Ad-vances in neural information processing systems , 32,2019.Gautam, A., Jain, H., Senger, A., and Dhand, G. Battleroyale: First-person shooter game.。In Proceedings ofthe International Conference on Innovative Computing &Communication (ICICC) , 2021.Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R.,Soyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg,M., Teplyashin, D., et al.。Grounded language learning ina simulated 3d world.。arXiv preprint arXiv:1706.06551 ,2017.Hu, B., Zhao, C., Zhang, P., Zhou, Z., Yang, Y ., Xu, Z., andLiu, B.。Enabling intelligent interactions between an agentand an llm: A reinforcement learning approach, 2023.Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,S., Wang, L., and Chen, W. Lora: Low-rank adaptationof large language models, 2021.Huang, W., Wang, C., Zhang, R., Li, Y ., Wu, J., and Fei-Fei,L.。V oxposer: Composable 3d value maps for roboticmanipulation with language models.。In Conference onRobot Learning , pp.。540–562.。PMLR, 2023.Jiang, Y ., Gupta, A., Zhang, Z., Wang, G., Dou, Y ., Chen, Y .,Fei-Fei, L., Anandkumar, A., Zhu, Y ., and Fan, L. Vima:Robot manipulation with multimodal prompts.。2023.Khandelwal, A., Weihs, L., Mottaghi, R., and Kembhavi,A.。Simple but effective: Clip embeddings for embodiedai.。In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition , pp.. \n",
      "the piece 666\n",
      " \n",
      " , 32,2019.Gautam, A., Jain, H., Senger, A., and Dhand, G. Battleroyale: First-person shooter game.。In Proceedings ofthe International Conference on Innovative Computing &Communication (ICICC) , 2021.Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R.,Soyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg,M., Teplyashin, D., et al.。Grounded language learning ina simulated 3d world.。arXiv preprint arXiv:1706.06551 ,2017.Hu, B., Zhao, C., Zhang, P., Zhou, Z., Yang, Y ., Xu, Z., andLiu, B.。Enabling intelligent interactions between an agentand an llm: A reinforcement learning approach, 2023.Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,S., Wang, L., and Chen, W. Lora: Low-rank adaptationof large language models, 2021.Huang, W., Wang, C., Zhang, R., Li, Y ., Wu, J., and Fei-Fei,L.。V oxposer: Composable 3d value maps for roboticmanipulation with language models.。In Conference onRobot Learning , pp.。540–562.。PMLR, 2023.Jiang, Y ., Gupta, A., Zhang, Z., Wang, G., Dou, Y ., Chen, Y .,Fei-Fei, L., Anandkumar, A., Zhu, Y ., and Fan, L. Vima:Robot manipulation with multimodal prompts.。2023.Khandelwal, A., Weihs, L., Mottaghi, R., and Kembhavi,A.。Simple but effective: Clip embeddings for embodiedai.。In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition , pp.。14829–14838,2022.Liu, M., Zhu, M., and Zhang, W. Goal-conditioned re-inforcement learning: Problems and solutions.. \n",
      "the piece 667\n",
      " \n",
      "H., Senger, A., and Dhand, G. Battleroyale: First-person shooter game.。In Proceedings ofthe International Conference on Innovative Computing &Communication (ICICC) , 2021.Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R.,Soyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg,M., Teplyashin, D., et al.。Grounded language learning ina simulated 3d world.。arXiv preprint arXiv:1706.06551 ,2017.Hu, B., Zhao, C., Zhang, P., Zhou, Z., Yang, Y ., Xu, Z., andLiu, B.。Enabling intelligent interactions between an agentand an llm: A reinforcement learning approach, 2023.Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,S., Wang, L., and Chen, W. Lora: Low-rank adaptationof large language models, 2021.Huang, W., Wang, C., Zhang, R., Li, Y ., Wu, J., and Fei-Fei,L.。V oxposer: Composable 3d value maps for roboticmanipulation with language models.。In Conference onRobot Learning , pp.。540–562.。PMLR, 2023.Jiang, Y ., Gupta, A., Zhang, Z., Wang, G., Dou, Y ., Chen, Y .,Fei-Fei, L., Anandkumar, A., Zhu, Y ., and Fan, L. Vima:Robot manipulation with multimodal prompts.。2023.Khandelwal, A., Weihs, L., Mottaghi, R., and Kembhavi,A.。Simple but effective: Clip embeddings for embodiedai.。In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition , pp.。14829–14838,2022.Liu, M., Zhu, M., and Zhang, W. Goal-conditioned re-inforcement learning: Problems and solutions.。arXivpreprint arXiv:2201.08299 , 2022.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,C., Hesse, C., Jain, S., Kosaraju, V ., Saunders, W., et al.Webgpt: Browser-assisted question-answering with hu-man feedback.. \n",
      "the piece 668\n",
      " \n",
      "omputing &Communication (ICICC) , 2021.Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R.,Soyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg,M., Teplyashin, D., et al.。Grounded language learning ina simulated 3d world.。arXiv preprint arXiv:1706.06551 ,2017.Hu, B., Zhao, C., Zhang, P., Zhou, Z., Yang, Y ., Xu, Z., andLiu, B.。Enabling intelligent interactions between an agentand an llm: A reinforcement learning approach, 2023.Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,S., Wang, L., and Chen, W. Lora: Low-rank adaptationof large language models, 2021.Huang, W., Wang, C., Zhang, R., Li, Y ., Wu, J., and Fei-Fei,L.。V oxposer: Composable 3d value maps for roboticmanipulation with language models.。In Conference onRobot Learning , pp.。540–562.。PMLR, 2023.Jiang, Y ., Gupta, A., Zhang, Z., Wang, G., Dou, Y ., Chen, Y .,Fei-Fei, L., Anandkumar, A., Zhu, Y ., and Fan, L. Vima:Robot manipulation with multimodal prompts.。2023.Khandelwal, A., Weihs, L., Mottaghi, R., and Kembhavi,A.。Simple but effective: Clip embeddings for embodiedai.。In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition , pp.。14829–14838,2022.Liu, M., Zhu, M., and Zhang, W. Goal-conditioned re-inforcement learning: Problems and solutions.。arXivpreprint arXiv:2201.08299 , 2022.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,C., Hesse, C., Jain, S., Kosaraju, V ., Saunders, W., et al.Webgpt: Browser-assisted question-answering with hu-man feedback.。arXiv preprint arXiv:2112.09332 , 2021.Ng, A. Y ., Harada, D., and Russell, S. Policy invarianceunder reward transformations: Theory and applicationto reward shaping.. \n",
      "the piece 669\n",
      " \n",
      "rg,M., Teplyashin, D., et al.。Grounded language learning ina simulated 3d world.。arXiv preprint arXiv:1706.06551 ,2017.Hu, B., Zhao, C., Zhang, P., Zhou, Z., Yang, Y ., Xu, Z., andLiu, B.。Enabling intelligent interactions between an agentand an llm: A reinforcement learning approach, 2023.Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,S., Wang, L., and Chen, W. Lora: Low-rank adaptationof large language models, 2021.Huang, W., Wang, C., Zhang, R., Li, Y ., Wu, J., and Fei-Fei,L.。V oxposer: Composable 3d value maps for roboticmanipulation with language models.。In Conference onRobot Learning , pp.。540–562.。PMLR, 2023.Jiang, Y ., Gupta, A., Zhang, Z., Wang, G., Dou, Y ., Chen, Y .,Fei-Fei, L., Anandkumar, A., Zhu, Y ., and Fan, L. Vima:Robot manipulation with multimodal prompts.。2023.Khandelwal, A., Weihs, L., Mottaghi, R., and Kembhavi,A.。Simple but effective: Clip embeddings for embodiedai.。In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition , pp.。14829–14838,2022.Liu, M., Zhu, M., and Zhang, W. Goal-conditioned re-inforcement learning: Problems and solutions.。arXivpreprint arXiv:2201.08299 , 2022.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,C., Hesse, C., Jain, S., Kosaraju, V ., Saunders, W., et al.Webgpt: Browser-assisted question-answering with hu-man feedback.。arXiv preprint arXiv:2112.09332 , 2021.Ng, A. Y ., Harada, D., and Russell, S. Policy invarianceunder reward transformations: Theory and applicationto reward shaping.。In Icml, volume 99, pp.. \n",
      "the piece 670\n",
      " \n",
      "n llm: A reinforcement learning approach, 2023.Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,S., Wang, L., and Chen, W. Lora: Low-rank adaptationof large language models, 2021.Huang, W., Wang, C., Zhang, R., Li, Y ., Wu, J., and Fei-Fei,L.。V oxposer: Composable 3d value maps for roboticmanipulation with language models.。In Conference onRobot Learning , pp.。540–562.。PMLR, 2023.Jiang, Y ., Gupta, A., Zhang, Z., Wang, G., Dou, Y ., Chen, Y .,Fei-Fei, L., Anandkumar, A., Zhu, Y ., and Fan, L. Vima:Robot manipulation with multimodal prompts.。2023.Khandelwal, A., Weihs, L., Mottaghi, R., and Kembhavi,A.。Simple but effective: Clip embeddings for embodiedai.。In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition , pp.。14829–14838,2022.Liu, M., Zhu, M., and Zhang, W. Goal-conditioned re-inforcement learning: Problems and solutions.。arXivpreprint arXiv:2201.08299 , 2022.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,C., Hesse, C., Jain, S., Kosaraju, V ., Saunders, W., et al.Webgpt: Browser-assisted question-answering with hu-man feedback.。arXiv preprint arXiv:2112.09332 , 2021.Ng, A. Y ., Harada, D., and Russell, S. Policy invarianceunder reward transformations: Theory and applicationto reward shaping.。In Icml, volume 99, pp.。278–287.Citeseer, 1999.Nikulin, A., Kurenkov, V ., Tarasov, D., and Kolesnikov, S.Anti-exploration by random network distillation.. \n",
      "the piece 671\n",
      " \n",
      ". J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,S., Wang, L., and Chen, W. Lora: Low-rank adaptationof large language models, 2021.Huang, W., Wang, C., Zhang, R., Li, Y ., Wu, J., and Fei-Fei,L.。V oxposer: Composable 3d value maps for roboticmanipulation with language models.。In Conference onRobot Learning , pp.。540–562.。PMLR, 2023.Jiang, Y ., Gupta, A., Zhang, Z., Wang, G., Dou, Y ., Chen, Y .,Fei-Fei, L., Anandkumar, A., Zhu, Y ., and Fan, L. Vima:Robot manipulation with multimodal prompts.。2023.Khandelwal, A., Weihs, L., Mottaghi, R., and Kembhavi,A.。Simple but effective: Clip embeddings for embodiedai.。In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition , pp.。14829–14838,2022.Liu, M., Zhu, M., and Zhang, W. Goal-conditioned re-inforcement learning: Problems and solutions.。arXivpreprint arXiv:2201.08299 , 2022.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,C., Hesse, C., Jain, S., Kosaraju, V ., Saunders, W., et al.Webgpt: Browser-assisted question-answering with hu-man feedback.。arXiv preprint arXiv:2112.09332 , 2021.Ng, A. Y ., Harada, D., and Russell, S. Policy invarianceunder reward transformations: Theory and applicationto reward shaping.。In Icml, volume 99, pp.。278–287.Citeseer, 1999.Nikulin, A., Kurenkov, V ., Tarasov, D., and Kolesnikov, S.Anti-exploration by random network distillation.。arXivpreprint arXiv:2301.13616 , 2023.OpenAI.. \n",
      "the piece 672\n",
      " \n",
      "onof large language models, 2021.Huang, W., Wang, C., Zhang, R., Li, Y ., Wu, J., and Fei-Fei,L.。V oxposer: Composable 3d value maps for roboticmanipulation with language models.。In Conference onRobot Learning , pp.。540–562.。PMLR, 2023.Jiang, Y ., Gupta, A., Zhang, Z., Wang, G., Dou, Y ., Chen, Y .,Fei-Fei, L., Anandkumar, A., Zhu, Y ., and Fan, L. Vima:Robot manipulation with multimodal prompts.。2023.Khandelwal, A., Weihs, L., Mottaghi, R., and Kembhavi,A.。Simple but effective: Clip embeddings for embodiedai.。In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition , pp.。14829–14838,2022.Liu, M., Zhu, M., and Zhang, W. Goal-conditioned re-inforcement learning: Problems and solutions.。arXivpreprint arXiv:2201.08299 , 2022.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,C., Hesse, C., Jain, S., Kosaraju, V ., Saunders, W., et al.Webgpt: Browser-assisted question-answering with hu-man feedback.。arXiv preprint arXiv:2112.09332 , 2021.Ng, A. Y ., Harada, D., and Russell, S. Policy invarianceunder reward transformations: Theory and applicationto reward shaping.。In Icml, volume 99, pp.。278–287.Citeseer, 1999.Nikulin, A., Kurenkov, V ., Tarasov, D., and Kolesnikov, S.Anti-exploration by random network distillation.。arXivpreprint arXiv:2301.13616 , 2023.OpenAI.。Gpt-4 technical report, 2023.Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,et al.. \n",
      "the piece 673\n",
      " \n",
      "Anandkumar, A., Zhu, Y ., and Fan, L. Vima:Robot manipulation with multimodal prompts.。2023.Khandelwal, A., Weihs, L., Mottaghi, R., and Kembhavi,A.。Simple but effective: Clip embeddings for embodiedai.。In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition , pp.。14829–14838,2022.Liu, M., Zhu, M., and Zhang, W. Goal-conditioned re-inforcement learning: Problems and solutions.。arXivpreprint arXiv:2201.08299 , 2022.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,C., Hesse, C., Jain, S., Kosaraju, V ., Saunders, W., et al.Webgpt: Browser-assisted question-answering with hu-man feedback.。arXiv preprint arXiv:2112.09332 , 2021.Ng, A. Y ., Harada, D., and Russell, S. Policy invarianceunder reward transformations: Theory and applicationto reward shaping.。In Icml, volume 99, pp.。278–287.Citeseer, 1999.Nikulin, A., Kurenkov, V ., Tarasov, D., and Kolesnikov, S.Anti-exploration by random network distillation.。arXivpreprint arXiv:2301.13616 , 2023.OpenAI.。Gpt-4 technical report, 2023.Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,et al.。Training language models to follow instructionswith human feedback.. \n",
      "the piece 674\n",
      " \n",
      "ts.。2023.Khandelwal, A., Weihs, L., Mottaghi, R., and Kembhavi,A.。Simple but effective: Clip embeddings for embodiedai.。In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition , pp.。14829–14838,2022.Liu, M., Zhu, M., and Zhang, W. Goal-conditioned re-inforcement learning: Problems and solutions.。arXivpreprint arXiv:2201.08299 , 2022.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,C., Hesse, C., Jain, S., Kosaraju, V ., Saunders, W., et al.Webgpt: Browser-assisted question-answering with hu-man feedback.。arXiv preprint arXiv:2112.09332 , 2021.Ng, A. Y ., Harada, D., and Russell, S. Policy invarianceunder reward transformations: Theory and applicationto reward shaping.。In Icml, volume 99, pp.。278–287.Citeseer, 1999.Nikulin, A., Kurenkov, V ., Tarasov, D., and Kolesnikov, S.Anti-exploration by random network distillation.。arXivpreprint arXiv:2301.13616 , 2023.OpenAI.。Gpt-4 technical report, 2023.Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,et al.。Training language models to follow instructionswith human feedback.。Advances in Neural InformationProcessing Systems , 35:27730–27744, 2022.Ren, Z., Dong, K., Zhou, Y ., Liu, Q., and Peng, J. Explo-ration via hindsight goal generation.. \n",
      "the piece 675\n",
      " \n",
      "ttaghi, R., and Kembhavi,A.。Simple but effective: Clip embeddings for embodiedai.。In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition , pp.。14829–14838,2022.Liu, M., Zhu, M., and Zhang, W. Goal-conditioned re-inforcement learning: Problems and solutions.。arXivpreprint arXiv:2201.08299 , 2022.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,C., Hesse, C., Jain, S., Kosaraju, V ., Saunders, W., et al.Webgpt: Browser-assisted question-answering with hu-man feedback.。arXiv preprint arXiv:2112.09332 , 2021.Ng, A. Y ., Harada, D., and Russell, S. Policy invarianceunder reward transformations: Theory and applicationto reward shaping.。In Icml, volume 99, pp.。278–287.Citeseer, 1999.Nikulin, A., Kurenkov, V ., Tarasov, D., and Kolesnikov, S.Anti-exploration by random network distillation.。arXivpreprint arXiv:2301.13616 , 2023.OpenAI.。Gpt-4 technical report, 2023.Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,et al.。Training language models to follow instructionswith human feedback.。Advances in Neural InformationProcessing Systems , 35:27730–27744, 2022.Ren, Z., Dong, K., Zhou, Y ., Liu, Q., and Peng, J. Explo-ration via hindsight goal generation.。Advances in NeuralInformation Processing Systems , 32, 2019.Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-tized experience replay.. \n",
      "the piece 676\n",
      " \n",
      ", and Kembhavi,A.。Simple but effective: Clip embeddings for embodiedai.。In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition , pp.。14829–14838,2022.Liu, M., Zhu, M., and Zhang, W. Goal-conditioned re-inforcement learning: Problems and solutions.。arXivpreprint arXiv:2201.08299 , 2022.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,C., Hesse, C., Jain, S., Kosaraju, V ., Saunders, W., et al.Webgpt: Browser-assisted question-answering with hu-man feedback.。arXiv preprint arXiv:2112.09332 , 2021.Ng, A. Y ., Harada, D., and Russell, S. Policy invarianceunder reward transformations: Theory and applicationto reward shaping.。In Icml, volume 99, pp.。278–287.Citeseer, 1999.Nikulin, A., Kurenkov, V ., Tarasov, D., and Kolesnikov, S.Anti-exploration by random network distillation.。arXivpreprint arXiv:2301.13616 , 2023.OpenAI.。Gpt-4 technical report, 2023.Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,et al.。Training language models to follow instructionswith human feedback.。Advances in Neural InformationProcessing Systems , 35:27730–27744, 2022.Ren, Z., Dong, K., Zhou, Y ., Liu, Q., and Peng, J. Explo-ration via hindsight goal generation.。Advances in NeuralInformation Processing Systems , 32, 2019.Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-tized experience replay.。arXiv preprint arXiv:1511.05952 ,2015.Schulman, J., Wolski, F., Dhariwal, P., Radford, A., andKlimov, O. Proximal policy optimization algorithms,2017.Team, O. E. L., Stooke, A., Mahajan, A., Barros, C., Deck,C., Bauer, J., Sygnowski, J., Trebacz, M., Jaderberg, M.,Mathieu, M., et al.. \n",
      "the piece 677\n",
      " \n",
      "2022.Liu, M., Zhu, M., and Zhang, W. Goal-conditioned re-inforcement learning: Problems and solutions.。arXivpreprint arXiv:2201.08299 , 2022.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,C., Hesse, C., Jain, S., Kosaraju, V ., Saunders, W., et al.Webgpt: Browser-assisted question-answering with hu-man feedback.。arXiv preprint arXiv:2112.09332 , 2021.Ng, A. Y ., Harada, D., and Russell, S. Policy invarianceunder reward transformations: Theory and applicationto reward shaping.。In Icml, volume 99, pp.。278–287.Citeseer, 1999.Nikulin, A., Kurenkov, V ., Tarasov, D., and Kolesnikov, S.Anti-exploration by random network distillation.。arXivpreprint arXiv:2301.13616 , 2023.OpenAI.。Gpt-4 technical report, 2023.Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,et al.。Training language models to follow instructionswith human feedback.。Advances in Neural InformationProcessing Systems , 35:27730–27744, 2022.Ren, Z., Dong, K., Zhou, Y ., Liu, Q., and Peng, J. Explo-ration via hindsight goal generation.。Advances in NeuralInformation Processing Systems , 32, 2019.Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-tized experience replay.。arXiv preprint arXiv:1511.05952 ,2015.Schulman, J., Wolski, F., Dhariwal, P., Radford, A., andKlimov, O. Proximal policy optimization algorithms,2017.Team, O. E. L., Stooke, A., Mahajan, A., Barros, C., Deck,C., Bauer, J., Sygnowski, J., Trebacz, M., Jaderberg, M.,Mathieu, M., et al.。Open-ended learning leads to gener-ally capable agents.. \n",
      "the piece 678\n",
      " \n",
      "ement learning: Problems and solutions.。arXivpreprint arXiv:2201.08299 , 2022.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,C., Hesse, C., Jain, S., Kosaraju, V ., Saunders, W., et al.Webgpt: Browser-assisted question-answering with hu-man feedback.。arXiv preprint arXiv:2112.09332 , 2021.Ng, A. Y ., Harada, D., and Russell, S. Policy invarianceunder reward transformations: Theory and applicationto reward shaping.。In Icml, volume 99, pp.。278–287.Citeseer, 1999.Nikulin, A., Kurenkov, V ., Tarasov, D., and Kolesnikov, S.Anti-exploration by random network distillation.。arXivpreprint arXiv:2301.13616 , 2023.OpenAI.。Gpt-4 technical report, 2023.Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,et al.。Training language models to follow instructionswith human feedback.。Advances in Neural InformationProcessing Systems , 35:27730–27744, 2022.Ren, Z., Dong, K., Zhou, Y ., Liu, Q., and Peng, J. Explo-ration via hindsight goal generation.。Advances in NeuralInformation Processing Systems , 32, 2019.Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-tized experience replay.。arXiv preprint arXiv:1511.05952 ,2015.Schulman, J., Wolski, F., Dhariwal, P., Radford, A., andKlimov, O. Proximal policy optimization algorithms,2017.Team, O. E. L., Stooke, A., Mahajan, A., Barros, C., Deck,C., Bauer, J., Sygnowski, J., Trebacz, M., Jaderberg, M.,Mathieu, M., et al.。Open-ended learning leads to gener-ally capable agents.。arXiv preprint arXiv:2107.12808 ,2021.Trott, A., Zheng, S., Xiong, C., and Socher, R. Keepingyour distance: Solving sparse reward tasks using self-balancing shaped rewards, 2019.Van der Maaten, L. and Hinton, G. Visualizing data usingt-sne.. \n",
      "the piece 679\n",
      " \n",
      "rXiv:2201.08299 , 2022.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,C., Hesse, C., Jain, S., Kosaraju, V ., Saunders, W., et al.Webgpt: Browser-assisted question-answering with hu-man feedback.。arXiv preprint arXiv:2112.09332 , 2021.Ng, A. Y ., Harada, D., and Russell, S. Policy invarianceunder reward transformations: Theory and applicationto reward shaping.。In Icml, volume 99, pp.。278–287.Citeseer, 1999.Nikulin, A., Kurenkov, V ., Tarasov, D., and Kolesnikov, S.Anti-exploration by random network distillation.。arXivpreprint arXiv:2301.13616 , 2023.OpenAI.。Gpt-4 technical report, 2023.Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,et al.。Training language models to follow instructionswith human feedback.。Advances in Neural InformationProcessing Systems , 35:27730–27744, 2022.Ren, Z., Dong, K., Zhou, Y ., Liu, Q., and Peng, J. Explo-ration via hindsight goal generation.。Advances in NeuralInformation Processing Systems , 32, 2019.Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-tized experience replay.。arXiv preprint arXiv:1511.05952 ,2015.Schulman, J., Wolski, F., Dhariwal, P., Radford, A., andKlimov, O. Proximal policy optimization algorithms,2017.Team, O. E. L., Stooke, A., Mahajan, A., Barros, C., Deck,C., Bauer, J., Sygnowski, J., Trebacz, M., Jaderberg, M.,Mathieu, M., et al.。Open-ended learning leads to gener-ally capable agents.。arXiv preprint arXiv:2107.12808 ,2021.Trott, A., Zheng, S., Xiong, C., and Socher, R. Keepingyour distance: Solving sparse reward tasks using self-balancing shaped rewards, 2019.Van der Maaten, L. and Hinton, G. Visualizing data usingt-sne.。Journal of machine learning research , 9(11), 2008.Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jader-berg, M., Silver, D., and Kavukcuoglu, K. Feudal net-works for hierarchical reinforcement learning.. \n",
      "the piece 680\n",
      " \n",
      "daptationNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,C., Hesse, C., Jain, S., Kosaraju, V ., Saunders, W., et al.Webgpt: Browser-assisted question-answering with hu-man feedback.。arXiv preprint arXiv:2112.09332 , 2021.Ng, A. Y ., Harada, D., and Russell, S. Policy invarianceunder reward transformations: Theory and applicationto reward shaping.。In Icml, volume 99, pp.。278–287.Citeseer, 1999.Nikulin, A., Kurenkov, V ., Tarasov, D., and Kolesnikov, S.Anti-exploration by random network distillation.。arXivpreprint arXiv:2301.13616 , 2023.OpenAI.。Gpt-4 technical report, 2023.Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,et al.。Training language models to follow instructionswith human feedback.。Advances in Neural InformationProcessing Systems , 35:27730–27744, 2022.Ren, Z., Dong, K., Zhou, Y ., Liu, Q., and Peng, J. Explo-ration via hindsight goal generation.。Advances in NeuralInformation Processing Systems , 32, 2019.Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-tized experience replay.。arXiv preprint arXiv:1511.05952 ,2015.Schulman, J., Wolski, F., Dhariwal, P., Radford, A., andKlimov, O. Proximal policy optimization algorithms,2017.Team, O. E. L., Stooke, A., Mahajan, A., Barros, C., Deck,C., Bauer, J., Sygnowski, J., Trebacz, M., Jaderberg, M.,Mathieu, M., et al.。Open-ended learning leads to gener-ally capable agents.。arXiv preprint arXiv:2107.12808 ,2021.Trott, A., Zheng, S., Xiong, C., and Socher, R. Keepingyour distance: Solving sparse reward tasks using self-balancing shaped rewards, 2019.Van der Maaten, L. and Hinton, G. Visualizing data usingt-sne.。Journal of machine learning research , 9(11), 2008.Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jader-berg, M., Silver, D., and Kavukcuoglu, K. Feudal net-works for hierarchical reinforcement learning.。In Interna-tional Conference on Machine Learning , pp.. \n",
      "the piece 681\n",
      " \n",
      "s, W., et al.Webgpt: Browser-assisted question-answering with hu-man feedback.。arXiv preprint arXiv:2112.09332 , 2021.Ng, A. Y ., Harada, D., and Russell, S. Policy invarianceunder reward transformations: Theory and applicationto reward shaping.。In Icml, volume 99, pp.。278–287.Citeseer, 1999.Nikulin, A., Kurenkov, V ., Tarasov, D., and Kolesnikov, S.Anti-exploration by random network distillation.。arXivpreprint arXiv:2301.13616 , 2023.OpenAI.。Gpt-4 technical report, 2023.Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,et al.。Training language models to follow instructionswith human feedback.。Advances in Neural InformationProcessing Systems , 35:27730–27744, 2022.Ren, Z., Dong, K., Zhou, Y ., Liu, Q., and Peng, J. Explo-ration via hindsight goal generation.。Advances in NeuralInformation Processing Systems , 32, 2019.Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-tized experience replay.。arXiv preprint arXiv:1511.05952 ,2015.Schulman, J., Wolski, F., Dhariwal, P., Radford, A., andKlimov, O. Proximal policy optimization algorithms,2017.Team, O. E. L., Stooke, A., Mahajan, A., Barros, C., Deck,C., Bauer, J., Sygnowski, J., Trebacz, M., Jaderberg, M.,Mathieu, M., et al.。Open-ended learning leads to gener-ally capable agents.。arXiv preprint arXiv:2107.12808 ,2021.Trott, A., Zheng, S., Xiong, C., and Socher, R. Keepingyour distance: Solving sparse reward tasks using self-balancing shaped rewards, 2019.Van der Maaten, L. and Hinton, G. Visualizing data usingt-sne.。Journal of machine learning research , 9(11), 2008.Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jader-berg, M., Silver, D., and Kavukcuoglu, K. Feudal net-works for hierarchical reinforcement learning.。In Interna-tional Conference on Machine Learning , pp.。3540–3549.PMLR, 2017.Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds,T., Georgiev, P., et al.. \n",
      "the piece 682\n",
      " \n",
      ", Kurenkov, V ., Tarasov, D., and Kolesnikov, S.Anti-exploration by random network distillation.。arXivpreprint arXiv:2301.13616 , 2023.OpenAI.。Gpt-4 technical report, 2023.Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,et al.。Training language models to follow instructionswith human feedback.。Advances in Neural InformationProcessing Systems , 35:27730–27744, 2022.Ren, Z., Dong, K., Zhou, Y ., Liu, Q., and Peng, J. Explo-ration via hindsight goal generation.。Advances in NeuralInformation Processing Systems , 32, 2019.Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-tized experience replay.。arXiv preprint arXiv:1511.05952 ,2015.Schulman, J., Wolski, F., Dhariwal, P., Radford, A., andKlimov, O. Proximal policy optimization algorithms,2017.Team, O. E. L., Stooke, A., Mahajan, A., Barros, C., Deck,C., Bauer, J., Sygnowski, J., Trebacz, M., Jaderberg, M.,Mathieu, M., et al.。Open-ended learning leads to gener-ally capable agents.。arXiv preprint arXiv:2107.12808 ,2021.Trott, A., Zheng, S., Xiong, C., and Socher, R. Keepingyour distance: Solving sparse reward tasks using self-balancing shaped rewards, 2019.Van der Maaten, L. and Hinton, G. Visualizing data usingt-sne.。Journal of machine learning research , 9(11), 2008.Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jader-berg, M., Silver, D., and Kavukcuoglu, K. Feudal net-works for hierarchical reinforcement learning.。In Interna-tional Conference on Machine Learning , pp.。3540–3549.PMLR, 2017.Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds,T., Georgiev, P., et al.。Grandmaster level in starcraft iiusing multi-agent reinforcement learning.. \n",
      "the piece 683\n",
      " \n",
      "023.Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,et al.。Training language models to follow instructionswith human feedback.。Advances in Neural InformationProcessing Systems , 35:27730–27744, 2022.Ren, Z., Dong, K., Zhou, Y ., Liu, Q., and Peng, J. Explo-ration via hindsight goal generation.。Advances in NeuralInformation Processing Systems , 32, 2019.Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-tized experience replay.。arXiv preprint arXiv:1511.05952 ,2015.Schulman, J., Wolski, F., Dhariwal, P., Radford, A., andKlimov, O. Proximal policy optimization algorithms,2017.Team, O. E. L., Stooke, A., Mahajan, A., Barros, C., Deck,C., Bauer, J., Sygnowski, J., Trebacz, M., Jaderberg, M.,Mathieu, M., et al.。Open-ended learning leads to gener-ally capable agents.。arXiv preprint arXiv:2107.12808 ,2021.Trott, A., Zheng, S., Xiong, C., and Socher, R. Keepingyour distance: Solving sparse reward tasks using self-balancing shaped rewards, 2019.Van der Maaten, L. and Hinton, G. Visualizing data usingt-sne.。Journal of machine learning research , 9(11), 2008.Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jader-berg, M., Silver, D., and Kavukcuoglu, K. Feudal net-works for hierarchical reinforcement learning.。In Interna-tional Conference on Machine Learning , pp.。3540–3549.PMLR, 2017.Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds,T., Georgiev, P., et al.。Grandmaster level in starcraft iiusing multi-agent reinforcement learning.。Nature , 575(7782):350–354, 2019.Wagner, R. A. and Fischer, M. J.. \n",
      "the piece 684\n",
      " \n",
      "iang, X., Almeida, D., Wainwright, C.,Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,et al.。Training language models to follow instructionswith human feedback.。Advances in Neural InformationProcessing Systems , 35:27730–27744, 2022.Ren, Z., Dong, K., Zhou, Y ., Liu, Q., and Peng, J. Explo-ration via hindsight goal generation.。Advances in NeuralInformation Processing Systems , 32, 2019.Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-tized experience replay.。arXiv preprint arXiv:1511.05952 ,2015.Schulman, J., Wolski, F., Dhariwal, P., Radford, A., andKlimov, O. Proximal policy optimization algorithms,2017.Team, O. E. L., Stooke, A., Mahajan, A., Barros, C., Deck,C., Bauer, J., Sygnowski, J., Trebacz, M., Jaderberg, M.,Mathieu, M., et al.。Open-ended learning leads to gener-ally capable agents.。arXiv preprint arXiv:2107.12808 ,2021.Trott, A., Zheng, S., Xiong, C., and Socher, R. Keepingyour distance: Solving sparse reward tasks using self-balancing shaped rewards, 2019.Van der Maaten, L. and Hinton, G. Visualizing data usingt-sne.。Journal of machine learning research , 9(11), 2008.Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jader-berg, M., Silver, D., and Kavukcuoglu, K. Feudal net-works for hierarchical reinforcement learning.。In Interna-tional Conference on Machine Learning , pp.。3540–3549.PMLR, 2017.Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds,T., Georgiev, P., et al.。Grandmaster level in starcraft iiusing multi-agent reinforcement learning.。Nature , 575(7782):350–354, 2019.Wagner, R. A. and Fischer, M. J.。The string-to-string cor-rection problem.. \n",
      "the piece 685\n",
      " \n",
      "low instructionswith human feedback.。Advances in Neural InformationProcessing Systems , 35:27730–27744, 2022.Ren, Z., Dong, K., Zhou, Y ., Liu, Q., and Peng, J. Explo-ration via hindsight goal generation.。Advances in NeuralInformation Processing Systems , 32, 2019.Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-tized experience replay.。arXiv preprint arXiv:1511.05952 ,2015.Schulman, J., Wolski, F., Dhariwal, P., Radford, A., andKlimov, O. Proximal policy optimization algorithms,2017.Team, O. E. L., Stooke, A., Mahajan, A., Barros, C., Deck,C., Bauer, J., Sygnowski, J., Trebacz, M., Jaderberg, M.,Mathieu, M., et al.。Open-ended learning leads to gener-ally capable agents.。arXiv preprint arXiv:2107.12808 ,2021.Trott, A., Zheng, S., Xiong, C., and Socher, R. Keepingyour distance: Solving sparse reward tasks using self-balancing shaped rewards, 2019.Van der Maaten, L. and Hinton, G. Visualizing data usingt-sne.。Journal of machine learning research , 9(11), 2008.Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jader-berg, M., Silver, D., and Kavukcuoglu, K. Feudal net-works for hierarchical reinforcement learning.。In Interna-tional Conference on Machine Learning , pp.。3540–3549.PMLR, 2017.Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds,T., Georgiev, P., et al.。Grandmaster level in starcraft iiusing multi-agent reinforcement learning.。Nature , 575(7782):350–354, 2019.Wagner, R. A. and Fischer, M. J.。The string-to-string cor-rection problem.。Journal of the ACM (JACM) , 21(1):168–173, 1974.Wang, G., Xie, Y ., Jiang, Y ., Mandlekar, A., Xiao, C., Zhu,Y ., Fan, L., and Anandkumar, A. V oyager: An open-ended embodied agent with large language models.. \n",
      "the piece 686\n",
      " \n",
      "n Neural InformationProcessing Systems , 35:27730–27744, 2022.Ren, Z., Dong, K., Zhou, Y ., Liu, Q., and Peng, J. Explo-ration via hindsight goal generation.。Advances in NeuralInformation Processing Systems , 32, 2019.Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-tized experience replay.。arXiv preprint arXiv:1511.05952 ,2015.Schulman, J., Wolski, F., Dhariwal, P., Radford, A., andKlimov, O. Proximal policy optimization algorithms,2017.Team, O. E. L., Stooke, A., Mahajan, A., Barros, C., Deck,C., Bauer, J., Sygnowski, J., Trebacz, M., Jaderberg, M.,Mathieu, M., et al.。Open-ended learning leads to gener-ally capable agents.。arXiv preprint arXiv:2107.12808 ,2021.Trott, A., Zheng, S., Xiong, C., and Socher, R. Keepingyour distance: Solving sparse reward tasks using self-balancing shaped rewards, 2019.Van der Maaten, L. and Hinton, G. Visualizing data usingt-sne.。Journal of machine learning research , 9(11), 2008.Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jader-berg, M., Silver, D., and Kavukcuoglu, K. Feudal net-works for hierarchical reinforcement learning.。In Interna-tional Conference on Machine Learning , pp.。3540–3549.PMLR, 2017.Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds,T., Georgiev, P., et al.。Grandmaster level in starcraft iiusing multi-agent reinforcement learning.。Nature , 575(7782):350–354, 2019.Wagner, R. A. and Fischer, M. J.。The string-to-string cor-rection problem.。Journal of the ACM (JACM) , 21(1):168–173, 1974.Wang, G., Xie, Y ., Jiang, Y ., Mandlekar, A., Xiao, C., Zhu,Y ., Fan, L., and Anandkumar, A. V oyager: An open-ended embodied agent with large language models.。arXivpreprint arXiv:2305.16291 , 2023a.Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y .. \n",
      "the piece 687\n",
      " \n",
      "tion.。Advances in NeuralInformation Processing Systems , 32, 2019.Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-tized experience replay.。arXiv preprint arXiv:1511.05952 ,2015.Schulman, J., Wolski, F., Dhariwal, P., Radford, A., andKlimov, O. Proximal policy optimization algorithms,2017.Team, O. E. L., Stooke, A., Mahajan, A., Barros, C., Deck,C., Bauer, J., Sygnowski, J., Trebacz, M., Jaderberg, M.,Mathieu, M., et al.。Open-ended learning leads to gener-ally capable agents.。arXiv preprint arXiv:2107.12808 ,2021.Trott, A., Zheng, S., Xiong, C., and Socher, R. Keepingyour distance: Solving sparse reward tasks using self-balancing shaped rewards, 2019.Van der Maaten, L. and Hinton, G. Visualizing data usingt-sne.。Journal of machine learning research , 9(11), 2008.Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jader-berg, M., Silver, D., and Kavukcuoglu, K. Feudal net-works for hierarchical reinforcement learning.。In Interna-tional Conference on Machine Learning , pp.。3540–3549.PMLR, 2017.Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds,T., Georgiev, P., et al.。Grandmaster level in starcraft iiusing multi-agent reinforcement learning.。Nature , 575(7782):350–354, 2019.Wagner, R. A. and Fischer, M. J.。The string-to-string cor-rection problem.。Journal of the ACM (JACM) , 21(1):168–173, 1974.Wang, G., Xie, Y ., Jiang, Y ., Mandlekar, A., Xiao, C., Zhu,Y ., Fan, L., and Anandkumar, A. V oyager: An open-ended embodied agent with large language models.。arXivpreprint arXiv:2305.16291 , 2023a.Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y .。Describe,explain, plan and select: Interactive planning with largelanguage models enables open-world multi-task agents.arXiv preprint arXiv:2302.01560 , 2023b.Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thoughtprompting elicits reasoning in large language models,2023.Weng, L. Curriculum for reinforcement learn-ing.. \n",
      "the piece 688\n",
      " \n",
      "aul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-tized experience replay.。arXiv preprint arXiv:1511.05952 ,2015.Schulman, J., Wolski, F., Dhariwal, P., Radford, A., andKlimov, O. Proximal policy optimization algorithms,2017.Team, O. E. L., Stooke, A., Mahajan, A., Barros, C., Deck,C., Bauer, J., Sygnowski, J., Trebacz, M., Jaderberg, M.,Mathieu, M., et al.。Open-ended learning leads to gener-ally capable agents.。arXiv preprint arXiv:2107.12808 ,2021.Trott, A., Zheng, S., Xiong, C., and Socher, R. Keepingyour distance: Solving sparse reward tasks using self-balancing shaped rewards, 2019.Van der Maaten, L. and Hinton, G. Visualizing data usingt-sne.。Journal of machine learning research , 9(11), 2008.Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jader-berg, M., Silver, D., and Kavukcuoglu, K. Feudal net-works for hierarchical reinforcement learning.。In Interna-tional Conference on Machine Learning , pp.。3540–3549.PMLR, 2017.Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds,T., Georgiev, P., et al.。Grandmaster level in starcraft iiusing multi-agent reinforcement learning.。Nature , 575(7782):350–354, 2019.Wagner, R. A. and Fischer, M. J.。The string-to-string cor-rection problem.。Journal of the ACM (JACM) , 21(1):168–173, 1974.Wang, G., Xie, Y ., Jiang, Y ., Mandlekar, A., Xiao, C., Zhu,Y ., Fan, L., and Anandkumar, A. V oyager: An open-ended embodied agent with large language models.。arXivpreprint arXiv:2305.16291 , 2023a.Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y .。Describe,explain, plan and select: Interactive planning with largelanguage models enables open-world multi-task agents.arXiv preprint arXiv:2302.01560 , 2023b.Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thoughtprompting elicits reasoning in large language models,2023.Weng, L. Curriculum for reinforcement learn-ing.。lilianweng.github.io , Jan 2020.. \n",
      "the piece 689\n",
      " \n",
      "A., andKlimov, O. Proximal policy optimization algorithms,2017.Team, O. E. L., Stooke, A., Mahajan, A., Barros, C., Deck,C., Bauer, J., Sygnowski, J., Trebacz, M., Jaderberg, M.,Mathieu, M., et al.。Open-ended learning leads to gener-ally capable agents.。arXiv preprint arXiv:2107.12808 ,2021.Trott, A., Zheng, S., Xiong, C., and Socher, R. Keepingyour distance: Solving sparse reward tasks using self-balancing shaped rewards, 2019.Van der Maaten, L. and Hinton, G. Visualizing data usingt-sne.。Journal of machine learning research , 9(11), 2008.Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jader-berg, M., Silver, D., and Kavukcuoglu, K. Feudal net-works for hierarchical reinforcement learning.。In Interna-tional Conference on Machine Learning , pp.。3540–3549.PMLR, 2017.Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds,T., Georgiev, P., et al.。Grandmaster level in starcraft iiusing multi-agent reinforcement learning.。Nature , 575(7782):350–354, 2019.Wagner, R. A. and Fischer, M. J.。The string-to-string cor-rection problem.。Journal of the ACM (JACM) , 21(1):168–173, 1974.Wang, G., Xie, Y ., Jiang, Y ., Mandlekar, A., Xiao, C., Zhu,Y ., Fan, L., and Anandkumar, A. V oyager: An open-ended embodied agent with large language models.。arXivpreprint arXiv:2305.16291 , 2023a.Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y .。Describe,explain, plan and select: Interactive planning with largelanguage models enables open-world multi-task agents.arXiv preprint arXiv:2302.01560 , 2023b.Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thoughtprompting elicits reasoning in large language models,2023.Weng, L. Curriculum for reinforcement learn-ing.。lilianweng.github.io , Jan 2020.。URLhttps://lilianweng.github.io/posts/2020-01-29-curriculum-rl/ .Zhang, T., Xu, H., Wang, X., Wu, Y ., Keutzer, K., Gonza-lez, J. E., and Tian, Y .. \n",
      "the piece 690\n",
      " \n",
      " J., Trebacz, M., Jaderberg, M.,Mathieu, M., et al.。Open-ended learning leads to gener-ally capable agents.。arXiv preprint arXiv:2107.12808 ,2021.Trott, A., Zheng, S., Xiong, C., and Socher, R. Keepingyour distance: Solving sparse reward tasks using self-balancing shaped rewards, 2019.Van der Maaten, L. and Hinton, G. Visualizing data usingt-sne.。Journal of machine learning research , 9(11), 2008.Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jader-berg, M., Silver, D., and Kavukcuoglu, K. Feudal net-works for hierarchical reinforcement learning.。In Interna-tional Conference on Machine Learning , pp.。3540–3549.PMLR, 2017.Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds,T., Georgiev, P., et al.。Grandmaster level in starcraft iiusing multi-agent reinforcement learning.。Nature , 575(7782):350–354, 2019.Wagner, R. A. and Fischer, M. J.。The string-to-string cor-rection problem.。Journal of the ACM (JACM) , 21(1):168–173, 1974.Wang, G., Xie, Y ., Jiang, Y ., Mandlekar, A., Xiao, C., Zhu,Y ., Fan, L., and Anandkumar, A. V oyager: An open-ended embodied agent with large language models.。arXivpreprint arXiv:2305.16291 , 2023a.Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y .。Describe,explain, plan and select: Interactive planning with largelanguage models enables open-world multi-task agents.arXiv preprint arXiv:2302.01560 , 2023b.Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thoughtprompting elicits reasoning in large language models,2023.Weng, L. Curriculum for reinforcement learn-ing.。lilianweng.github.io , Jan 2020.。URLhttps://lilianweng.github.io/posts/2020-01-29-curriculum-rl/ .Zhang, T., Xu, H., Wang, X., Wu, Y ., Keutzer, K., Gonza-lez, J. E., and Tian, Y .。Noveld: A simple yet effectiveexploration criterion.. \n",
      "the piece 691\n",
      " \n",
      "Van der Maaten, L. and Hinton, G. Visualizing data usingt-sne.。Journal of machine learning research , 9(11), 2008.Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jader-berg, M., Silver, D., and Kavukcuoglu, K. Feudal net-works for hierarchical reinforcement learning.。In Interna-tional Conference on Machine Learning , pp.。3540–3549.PMLR, 2017.Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds,T., Georgiev, P., et al.。Grandmaster level in starcraft iiusing multi-agent reinforcement learning.。Nature , 575(7782):350–354, 2019.Wagner, R. A. and Fischer, M. J.。The string-to-string cor-rection problem.。Journal of the ACM (JACM) , 21(1):168–173, 1974.Wang, G., Xie, Y ., Jiang, Y ., Mandlekar, A., Xiao, C., Zhu,Y ., Fan, L., and Anandkumar, A. V oyager: An open-ended embodied agent with large language models.。arXivpreprint arXiv:2305.16291 , 2023a.Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y .。Describe,explain, plan and select: Interactive planning with largelanguage models enables open-world multi-task agents.arXiv preprint arXiv:2302.01560 , 2023b.Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thoughtprompting elicits reasoning in large language models,2023.Weng, L. Curriculum for reinforcement learn-ing.。lilianweng.github.io , Jan 2020.。URLhttps://lilianweng.github.io/posts/2020-01-29-curriculum-rl/ .Zhang, T., Xu, H., Wang, X., Wu, Y ., Keutzer, K., Gonza-lez, J. E., and Tian, Y .。Noveld: A simple yet effectiveexploration criterion.。Advances in Neural InformationProcessing Systems , 34:25217–25230, 2021.Zheng, Q., Zhang, A., and Grover, A. Online decision trans-former.. \n",
      "the piece 692\n",
      " \n",
      "-sne.。Journal of machine learning research , 9(11), 2008.Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jader-berg, M., Silver, D., and Kavukcuoglu, K. Feudal net-works for hierarchical reinforcement learning.。In Interna-tional Conference on Machine Learning , pp.。3540–3549.PMLR, 2017.Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds,T., Georgiev, P., et al.。Grandmaster level in starcraft iiusing multi-agent reinforcement learning.。Nature , 575(7782):350–354, 2019.Wagner, R. A. and Fischer, M. J.。The string-to-string cor-rection problem.。Journal of the ACM (JACM) , 21(1):168–173, 1974.Wang, G., Xie, Y ., Jiang, Y ., Mandlekar, A., Xiao, C., Zhu,Y ., Fan, L., and Anandkumar, A. V oyager: An open-ended embodied agent with large language models.。arXivpreprint arXiv:2305.16291 , 2023a.Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y .。Describe,explain, plan and select: Interactive planning with largelanguage models enables open-world multi-task agents.arXiv preprint arXiv:2302.01560 , 2023b.Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thoughtprompting elicits reasoning in large language models,2023.Weng, L. Curriculum for reinforcement learn-ing.。lilianweng.github.io , Jan 2020.。URLhttps://lilianweng.github.io/posts/2020-01-29-curriculum-rl/ .Zhang, T., Xu, H., Wang, X., Wu, Y ., Keutzer, K., Gonza-lez, J. E., and Tian, Y .。Noveld: A simple yet effectiveexploration criterion.。Advances in Neural InformationProcessing Systems , 34:25217–25230, 2021.Zheng, Q., Zhang, A., and Grover, A. Online decision trans-former.。In international conference on machine learning ,pp.. \n",
      "the piece 693\n",
      " \n",
      "rence on Machine Learning , pp.。3540–3549.PMLR, 2017.Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds,T., Georgiev, P., et al.。Grandmaster level in starcraft iiusing multi-agent reinforcement learning.。Nature , 575(7782):350–354, 2019.Wagner, R. A. and Fischer, M. J.。The string-to-string cor-rection problem.。Journal of the ACM (JACM) , 21(1):168–173, 1974.Wang, G., Xie, Y ., Jiang, Y ., Mandlekar, A., Xiao, C., Zhu,Y ., Fan, L., and Anandkumar, A. V oyager: An open-ended embodied agent with large language models.。arXivpreprint arXiv:2305.16291 , 2023a.Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y .。Describe,explain, plan and select: Interactive planning with largelanguage models enables open-world multi-task agents.arXiv preprint arXiv:2302.01560 , 2023b.Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thoughtprompting elicits reasoning in large language models,2023.Weng, L. Curriculum for reinforcement learn-ing.。lilianweng.github.io , Jan 2020.。URLhttps://lilianweng.github.io/posts/2020-01-29-curriculum-rl/ .Zhang, T., Xu, H., Wang, X., Wu, Y ., Keutzer, K., Gonza-lez, J. E., and Tian, Y .。Noveld: A simple yet effectiveexploration criterion.。Advances in Neural InformationProcessing Systems , 34:25217–25230, 2021.Zheng, Q., Zhang, A., and Grover, A. Online decision trans-former.。In international conference on machine learning ,pp.。27042–27059.. \n",
      "the piece 694\n",
      " \n",
      "tarcraft iiusing multi-agent reinforcement learning.。Nature , 575(7782):350–354, 2019.Wagner, R. A. and Fischer, M. J.。The string-to-string cor-rection problem.。Journal of the ACM (JACM) , 21(1):168–173, 1974.Wang, G., Xie, Y ., Jiang, Y ., Mandlekar, A., Xiao, C., Zhu,Y ., Fan, L., and Anandkumar, A. V oyager: An open-ended embodied agent with large language models.。arXivpreprint arXiv:2305.16291 , 2023a.Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y .。Describe,explain, plan and select: Interactive planning with largelanguage models enables open-world multi-task agents.arXiv preprint arXiv:2302.01560 , 2023b.Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thoughtprompting elicits reasoning in large language models,2023.Weng, L. Curriculum for reinforcement learn-ing.。lilianweng.github.io , Jan 2020.。URLhttps://lilianweng.github.io/posts/2020-01-29-curriculum-rl/ .Zhang, T., Xu, H., Wang, X., Wu, Y ., Keutzer, K., Gonza-lez, J. E., and Tian, Y .。Noveld: A simple yet effectiveexploration criterion.。Advances in Neural InformationProcessing Systems , 34:25217–25230, 2021.Zheng, Q., Zhang, A., and Grover, A. Online decision trans-former.。In international conference on machine learning ,pp.。27042–27059.。PMLR, 2022.Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F.,Wu, J., Wohlhart, P., Welker, S., Wahid, A., et al.. \n",
      "the piece 695\n",
      " \n",
      "ure , 575(7782):350–354, 2019.Wagner, R. A. and Fischer, M. J.。The string-to-string cor-rection problem.。Journal of the ACM (JACM) , 21(1):168–173, 1974.Wang, G., Xie, Y ., Jiang, Y ., Mandlekar, A., Xiao, C., Zhu,Y ., Fan, L., and Anandkumar, A. V oyager: An open-ended embodied agent with large language models.。arXivpreprint arXiv:2305.16291 , 2023a.Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y .。Describe,explain, plan and select: Interactive planning with largelanguage models enables open-world multi-task agents.arXiv preprint arXiv:2302.01560 , 2023b.Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thoughtprompting elicits reasoning in large language models,2023.Weng, L. Curriculum for reinforcement learn-ing.。lilianweng.github.io , Jan 2020.。URLhttps://lilianweng.github.io/posts/2020-01-29-curriculum-rl/ .Zhang, T., Xu, H., Wang, X., Wu, Y ., Keutzer, K., Gonza-lez, J. E., and Tian, Y .。Noveld: A simple yet effectiveexploration criterion.。Advances in Neural InformationProcessing Systems , 34:25217–25230, 2021.Zheng, Q., Zhang, A., and Grover, A. Online decision trans-former.。In international conference on machine learning ,pp.。27042–27059.。PMLR, 2022.Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F.,Wu, J., Wohlhart, P., Welker, S., Wahid, A., et al.。Rt-2:Vision-language-action models transfer web knowledgeto robotic control.. \n",
      "the piece 696\n",
      " \n",
      ", Xie, Y ., Jiang, Y ., Mandlekar, A., Xiao, C., Zhu,Y ., Fan, L., and Anandkumar, A. V oyager: An open-ended embodied agent with large language models.。arXivpreprint arXiv:2305.16291 , 2023a.Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y .。Describe,explain, plan and select: Interactive planning with largelanguage models enables open-world multi-task agents.arXiv preprint arXiv:2302.01560 , 2023b.Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thoughtprompting elicits reasoning in large language models,2023.Weng, L. Curriculum for reinforcement learn-ing.。lilianweng.github.io , Jan 2020.。URLhttps://lilianweng.github.io/posts/2020-01-29-curriculum-rl/ .Zhang, T., Xu, H., Wang, X., Wu, Y ., Keutzer, K., Gonza-lez, J. E., and Tian, Y .。Noveld: A simple yet effectiveexploration criterion.。Advances in Neural InformationProcessing Systems , 34:25217–25230, 2021.Zheng, Q., Zhang, A., and Grover, A. Online decision trans-former.。In international conference on machine learning ,pp.。27042–27059.。PMLR, 2022.Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F.,Wu, J., Wohlhart, P., Welker, S., Wahid, A., et al.。Rt-2:Vision-language-action models transfer web knowledgeto robotic control.。In 7th Annual Conference on RobotLearning , 2023.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationA.. \n",
      "the piece 697\n",
      " \n",
      "kumar, A. V oyager: An open-ended embodied agent with large language models.。arXivpreprint arXiv:2305.16291 , 2023a.Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y .。Describe,explain, plan and select: Interactive planning with largelanguage models enables open-world multi-task agents.arXiv preprint arXiv:2302.01560 , 2023b.Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thoughtprompting elicits reasoning in large language models,2023.Weng, L. Curriculum for reinforcement learn-ing.。lilianweng.github.io , Jan 2020.。URLhttps://lilianweng.github.io/posts/2020-01-29-curriculum-rl/ .Zhang, T., Xu, H., Wang, X., Wu, Y ., Keutzer, K., Gonza-lez, J. E., and Tian, Y .。Noveld: A simple yet effectiveexploration criterion.。Advances in Neural InformationProcessing Systems , 34:25217–25230, 2021.Zheng, Q., Zhang, A., and Grover, A. Online decision trans-former.。In international conference on machine learning ,pp.。27042–27059.。PMLR, 2022.Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F.,Wu, J., Wohlhart, P., Welker, S., Wahid, A., et al.。Rt-2:Vision-language-action models transfer web knowledgeto robotic control.。In 7th Annual Conference on RobotLearning , 2023.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationA.。Contra: The EnvironmentContra seamlessly merges the last-man-standing gameplay dynamics with the survival, exploration, and scavenging elementsinherent in first-person shooting games (Gautam et al., 2021).. \n",
      "the piece 698\n",
      " \n",
      "e models.。arXivpreprint arXiv:2305.16291 , 2023a.Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y .。Describe,explain, plan and select: Interactive planning with largelanguage models enables open-world multi-task agents.arXiv preprint arXiv:2302.01560 , 2023b.Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thoughtprompting elicits reasoning in large language models,2023.Weng, L. Curriculum for reinforcement learn-ing.。lilianweng.github.io , Jan 2020.。URLhttps://lilianweng.github.io/posts/2020-01-29-curriculum-rl/ .Zhang, T., Xu, H., Wang, X., Wu, Y ., Keutzer, K., Gonza-lez, J. E., and Tian, Y .。Noveld: A simple yet effectiveexploration criterion.。Advances in Neural InformationProcessing Systems , 34:25217–25230, 2021.Zheng, Q., Zhang, A., and Grover, A. Online decision trans-former.。In international conference on machine learning ,pp.。27042–27059.。PMLR, 2022.Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F.,Wu, J., Wohlhart, P., Welker, S., Wahid, A., et al.。Rt-2:Vision-language-action models transfer web knowledgeto robotic control.。In 7th Annual Conference on RobotLearning , 2023.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationA.。Contra: The EnvironmentContra seamlessly merges the last-man-standing gameplay dynamics with the survival, exploration, and scavenging elementsinherent in first-person shooting games (Gautam et al., 2021).。The game unfolds with multiple hostile teams, necessitatingplayers to collaborate with teammates, withstand adversaries, and strive to outlast others in the ever-changing arena.. \n",
      "the piece 699\n",
      " \n",
      "2023a.Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y .。Describe,explain, plan and select: Interactive planning with largelanguage models enables open-world multi-task agents.arXiv preprint arXiv:2302.01560 , 2023b.Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thoughtprompting elicits reasoning in large language models,2023.Weng, L. Curriculum for reinforcement learn-ing.。lilianweng.github.io , Jan 2020.。URLhttps://lilianweng.github.io/posts/2020-01-29-curriculum-rl/ .Zhang, T., Xu, H., Wang, X., Wu, Y ., Keutzer, K., Gonza-lez, J. E., and Tian, Y .。Noveld: A simple yet effectiveexploration criterion.。Advances in Neural InformationProcessing Systems , 34:25217–25230, 2021.Zheng, Q., Zhang, A., and Grover, A. Online decision trans-former.。In international conference on machine learning ,pp.。27042–27059.。PMLR, 2022.Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F.,Wu, J., Wohlhart, P., Welker, S., Wahid, A., et al.。Rt-2:Vision-language-action models transfer web knowledgeto robotic control.。In 7th Annual Conference on RobotLearning , 2023.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationA.。Contra: The EnvironmentContra seamlessly merges the last-man-standing gameplay dynamics with the survival, exploration, and scavenging elementsinherent in first-person shooting games (Gautam et al., 2021).。The game unfolds with multiple hostile teams, necessitatingplayers to collaborate with teammates, withstand adversaries, and strive to outlast others in the ever-changing arena.。Theagent’s objectives encompass individual survival and the elimination of encountered enemies.. \n",
      "the piece 700\n",
      " \n",
      "2023b.Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thoughtprompting elicits reasoning in large language models,2023.Weng, L. Curriculum for reinforcement learn-ing.。lilianweng.github.io , Jan 2020.。URLhttps://lilianweng.github.io/posts/2020-01-29-curriculum-rl/ .Zhang, T., Xu, H., Wang, X., Wu, Y ., Keutzer, K., Gonza-lez, J. E., and Tian, Y .。Noveld: A simple yet effectiveexploration criterion.。Advances in Neural InformationProcessing Systems , 34:25217–25230, 2021.Zheng, Q., Zhang, A., and Grover, A. Online decision trans-former.。In international conference on machine learning ,pp.。27042–27059.。PMLR, 2022.Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F.,Wu, J., Wohlhart, P., Welker, S., Wahid, A., et al.。Rt-2:Vision-language-action models transfer web knowledgeto robotic control.。In 7th Annual Conference on RobotLearning , 2023.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationA.。Contra: The EnvironmentContra seamlessly merges the last-man-standing gameplay dynamics with the survival, exploration, and scavenging elementsinherent in first-person shooting games (Gautam et al., 2021).。The game unfolds with multiple hostile teams, necessitatingplayers to collaborate with teammates, withstand adversaries, and strive to outlast others in the ever-changing arena.。Theagent’s objectives encompass individual survival and the elimination of encountered enemies.。In Contra, the agent’s actioninterface is designed to mirror human capabilities, encompassing basic movements and actions like moving and shooting,with action intervals around 200ms, similar to the frequency of human operations.. \n",
      "the piece 701\n",
      " \n",
      "d Zhou, D. Chain-of-thoughtprompting elicits reasoning in large language models,2023.Weng, L. Curriculum for reinforcement learn-ing.。lilianweng.github.io , Jan 2020.。URLhttps://lilianweng.github.io/posts/2020-01-29-curriculum-rl/ .Zhang, T., Xu, H., Wang, X., Wu, Y ., Keutzer, K., Gonza-lez, J. E., and Tian, Y .。Noveld: A simple yet effectiveexploration criterion.。Advances in Neural InformationProcessing Systems , 34:25217–25230, 2021.Zheng, Q., Zhang, A., and Grover, A. Online decision trans-former.。In international conference on machine learning ,pp.。27042–27059.。PMLR, 2022.Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F.,Wu, J., Wohlhart, P., Welker, S., Wahid, A., et al.。Rt-2:Vision-language-action models transfer web knowledgeto robotic control.。In 7th Annual Conference on RobotLearning , 2023.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationA.。Contra: The EnvironmentContra seamlessly merges the last-man-standing gameplay dynamics with the survival, exploration, and scavenging elementsinherent in first-person shooting games (Gautam et al., 2021).。The game unfolds with multiple hostile teams, necessitatingplayers to collaborate with teammates, withstand adversaries, and strive to outlast others in the ever-changing arena.。Theagent’s objectives encompass individual survival and the elimination of encountered enemies.。In Contra, the agent’s actioninterface is designed to mirror human capabilities, encompassing basic movements and actions like moving and shooting,with action intervals around 200ms, similar to the frequency of human operations.。Through these actions, an agent in Contramandates a sequential acquisition of skills, starting from fundamental abilities like walking, jumping, running, and itemcollection.. \n",
      "the piece 702\n",
      " \n",
      " Neural InformationProcessing Systems , 34:25217–25230, 2021.Zheng, Q., Zhang, A., and Grover, A. Online decision trans-former.。In international conference on machine learning ,pp.。27042–27059.。PMLR, 2022.Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F.,Wu, J., Wohlhart, P., Welker, S., Wahid, A., et al.。Rt-2:Vision-language-action models transfer web knowledgeto robotic control.。In 7th Annual Conference on RobotLearning , 2023.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationA.。Contra: The EnvironmentContra seamlessly merges the last-man-standing gameplay dynamics with the survival, exploration, and scavenging elementsinherent in first-person shooting games (Gautam et al., 2021).。The game unfolds with multiple hostile teams, necessitatingplayers to collaborate with teammates, withstand adversaries, and strive to outlast others in the ever-changing arena.。Theagent’s objectives encompass individual survival and the elimination of encountered enemies.。In Contra, the agent’s actioninterface is designed to mirror human capabilities, encompassing basic movements and actions like moving and shooting,with action intervals around 200ms, similar to the frequency of human operations.。Through these actions, an agent in Contramandates a sequential acquisition of skills, starting from fundamental abilities like walking, jumping, running, and itemcollection.。As the learning proceeds, an agent must master more intricate skills such as evading enemy projectiles andcoordinating tactics with teammates.. \n",
      "the piece 703\n",
      " \n",
      "ems , 34:25217–25230, 2021.Zheng, Q., Zhang, A., and Grover, A. Online decision trans-former.。In international conference on machine learning ,pp.。27042–27059.。PMLR, 2022.Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F.,Wu, J., Wohlhart, P., Welker, S., Wahid, A., et al.。Rt-2:Vision-language-action models transfer web knowledgeto robotic control.。In 7th Annual Conference on RobotLearning , 2023.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationA.。Contra: The EnvironmentContra seamlessly merges the last-man-standing gameplay dynamics with the survival, exploration, and scavenging elementsinherent in first-person shooting games (Gautam et al., 2021).。The game unfolds with multiple hostile teams, necessitatingplayers to collaborate with teammates, withstand adversaries, and strive to outlast others in the ever-changing arena.。Theagent’s objectives encompass individual survival and the elimination of encountered enemies.。In Contra, the agent’s actioninterface is designed to mirror human capabilities, encompassing basic movements and actions like moving and shooting,with action intervals around 200ms, similar to the frequency of human operations.。Through these actions, an agent in Contramandates a sequential acquisition of skills, starting from fundamental abilities like walking, jumping, running, and itemcollection.。As the learning proceeds, an agent must master more intricate skills such as evading enemy projectiles andcoordinating tactics with teammates.。This characteristic defines an open-ended learning process where the agent continuallyexplores the game environment to refine mastered skills and acquire new ones.Observation Space.. \n",
      "the piece 704\n",
      " \n",
      "042–27059.。PMLR, 2022.Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F.,Wu, J., Wohlhart, P., Welker, S., Wahid, A., et al.。Rt-2:Vision-language-action models transfer web knowledgeto robotic control.。In 7th Annual Conference on RobotLearning , 2023.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationA.。Contra: The EnvironmentContra seamlessly merges the last-man-standing gameplay dynamics with the survival, exploration, and scavenging elementsinherent in first-person shooting games (Gautam et al., 2021).。The game unfolds with multiple hostile teams, necessitatingplayers to collaborate with teammates, withstand adversaries, and strive to outlast others in the ever-changing arena.。Theagent’s objectives encompass individual survival and the elimination of encountered enemies.。In Contra, the agent’s actioninterface is designed to mirror human capabilities, encompassing basic movements and actions like moving and shooting,with action intervals around 200ms, similar to the frequency of human operations.。Through these actions, an agent in Contramandates a sequential acquisition of skills, starting from fundamental abilities like walking, jumping, running, and itemcollection.。As the learning proceeds, an agent must master more intricate skills such as evading enemy projectiles andcoordinating tactics with teammates.。This characteristic defines an open-ended learning process where the agent continuallyexplores the game environment to refine mastered skills and acquire new ones.Observation Space.。The observation space encompasses various factors, comprising unit features delineating the agent’sstatus, as well as that of other players.. \n",
      "the piece 705\n",
      " \n",
      "u, P., Xiao, T., Xia, F.,Wu, J., Wohlhart, P., Welker, S., Wahid, A., et al.。Rt-2:Vision-language-action models transfer web knowledgeto robotic control.。In 7th Annual Conference on RobotLearning , 2023.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationA.。Contra: The EnvironmentContra seamlessly merges the last-man-standing gameplay dynamics with the survival, exploration, and scavenging elementsinherent in first-person shooting games (Gautam et al., 2021).。The game unfolds with multiple hostile teams, necessitatingplayers to collaborate with teammates, withstand adversaries, and strive to outlast others in the ever-changing arena.。Theagent’s objectives encompass individual survival and the elimination of encountered enemies.。In Contra, the agent’s actioninterface is designed to mirror human capabilities, encompassing basic movements and actions like moving and shooting,with action intervals around 200ms, similar to the frequency of human operations.。Through these actions, an agent in Contramandates a sequential acquisition of skills, starting from fundamental abilities like walking, jumping, running, and itemcollection.。As the learning proceeds, an agent must master more intricate skills such as evading enemy projectiles andcoordinating tactics with teammates.。This characteristic defines an open-ended learning process where the agent continuallyexplores the game environment to refine mastered skills and acquire new ones.Observation Space.。The observation space encompasses various factors, comprising unit features delineating the agent’sstatus, as well as that of other players.。Additionally, it includes environmental features characterizing interaction eventsand an agent-centric RGB bird’s-eye-view of the local observation.. \n",
      "the piece 706\n",
      " \n",
      "otic control.。In 7th Annual Conference on RobotLearning , 2023.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationA.。Contra: The EnvironmentContra seamlessly merges the last-man-standing gameplay dynamics with the survival, exploration, and scavenging elementsinherent in first-person shooting games (Gautam et al., 2021).。The game unfolds with multiple hostile teams, necessitatingplayers to collaborate with teammates, withstand adversaries, and strive to outlast others in the ever-changing arena.。Theagent’s objectives encompass individual survival and the elimination of encountered enemies.。In Contra, the agent’s actioninterface is designed to mirror human capabilities, encompassing basic movements and actions like moving and shooting,with action intervals around 200ms, similar to the frequency of human operations.。Through these actions, an agent in Contramandates a sequential acquisition of skills, starting from fundamental abilities like walking, jumping, running, and itemcollection.。As the learning proceeds, an agent must master more intricate skills such as evading enemy projectiles andcoordinating tactics with teammates.。This characteristic defines an open-ended learning process where the agent continuallyexplores the game environment to refine mastered skills and acquire new ones.Observation Space.。The observation space encompasses various factors, comprising unit features delineating the agent’sstatus, as well as that of other players.。Additionally, it includes environmental features characterizing interaction eventsand an agent-centric RGB bird’s-eye-view of the local observation.。For the details, we include them in Table 4.. \n",
      "the piece 707\n",
      " \n",
      "g , 2023.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationA.。Contra: The EnvironmentContra seamlessly merges the last-man-standing gameplay dynamics with the survival, exploration, and scavenging elementsinherent in first-person shooting games (Gautam et al., 2021).。The game unfolds with multiple hostile teams, necessitatingplayers to collaborate with teammates, withstand adversaries, and strive to outlast others in the ever-changing arena.。Theagent’s objectives encompass individual survival and the elimination of encountered enemies.。In Contra, the agent’s actioninterface is designed to mirror human capabilities, encompassing basic movements and actions like moving and shooting,with action intervals around 200ms, similar to the frequency of human operations.。Through these actions, an agent in Contramandates a sequential acquisition of skills, starting from fundamental abilities like walking, jumping, running, and itemcollection.。As the learning proceeds, an agent must master more intricate skills such as evading enemy projectiles andcoordinating tactics with teammates.。This characteristic defines an open-ended learning process where the agent continuallyexplores the game environment to refine mastered skills and acquire new ones.Observation Space.。The observation space encompasses various factors, comprising unit features delineating the agent’sstatus, as well as that of other players.。Additionally, it includes environmental features characterizing interaction eventsand an agent-centric RGB bird’s-eye-view of the local observation.。For the details, we include them in Table 4.。Given theheterogeneity in the shapes and data types of these features, we adopt independent feature processing for each of them,subsequently concatenating them to serve as input for the policy and value networks.. \n",
      "the piece 708\n",
      " \n",
      "ing Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationA.。Contra: The EnvironmentContra seamlessly merges the last-man-standing gameplay dynamics with the survival, exploration, and scavenging elementsinherent in first-person shooting games (Gautam et al., 2021).。The game unfolds with multiple hostile teams, necessitatingplayers to collaborate with teammates, withstand adversaries, and strive to outlast others in the ever-changing arena.。Theagent’s objectives encompass individual survival and the elimination of encountered enemies.。In Contra, the agent’s actioninterface is designed to mirror human capabilities, encompassing basic movements and actions like moving and shooting,with action intervals around 200ms, similar to the frequency of human operations.。Through these actions, an agent in Contramandates a sequential acquisition of skills, starting from fundamental abilities like walking, jumping, running, and itemcollection.。As the learning proceeds, an agent must master more intricate skills such as evading enemy projectiles andcoordinating tactics with teammates.。This characteristic defines an open-ended learning process where the agent continuallyexplores the game environment to refine mastered skills and acquire new ones.Observation Space.。The observation space encompasses various factors, comprising unit features delineating the agent’sstatus, as well as that of other players.。Additionally, it includes environmental features characterizing interaction eventsand an agent-centric RGB bird’s-eye-view of the local observation.。For the details, we include them in Table 4.。Given theheterogeneity in the shapes and data types of these features, we adopt independent feature processing for each of them,subsequently concatenating them to serve as input for the policy and value networks.。In Figure 4, we present the foundationalUnit Features([j�iiiu-,-----------------'The Agent RBG Bird's-eye-viewOther players -----------------'Environmental featuresFigure 4: Preprocessing for an observation with four types of features.process for handling an observation.. \n",
      "the piece 709\n",
      " \n",
      "ges the last-man-standing gameplay dynamics with the survival, exploration, and scavenging elementsinherent in first-person shooting games (Gautam et al., 2021).。The game unfolds with multiple hostile teams, necessitatingplayers to collaborate with teammates, withstand adversaries, and strive to outlast others in the ever-changing arena.。Theagent’s objectives encompass individual survival and the elimination of encountered enemies.。In Contra, the agent’s actioninterface is designed to mirror human capabilities, encompassing basic movements and actions like moving and shooting,with action intervals around 200ms, similar to the frequency of human operations.。Through these actions, an agent in Contramandates a sequential acquisition of skills, starting from fundamental abilities like walking, jumping, running, and itemcollection.。As the learning proceeds, an agent must master more intricate skills such as evading enemy projectiles andcoordinating tactics with teammates.。This characteristic defines an open-ended learning process where the agent continuallyexplores the game environment to refine mastered skills and acquire new ones.Observation Space.。The observation space encompasses various factors, comprising unit features delineating the agent’sstatus, as well as that of other players.。Additionally, it includes environmental features characterizing interaction eventsand an agent-centric RGB bird’s-eye-view of the local observation.。For the details, we include them in Table 4.。Given theheterogeneity in the shapes and data types of these features, we adopt independent feature processing for each of them,subsequently concatenating them to serve as input for the policy and value networks.。In Figure 4, we present the foundationalUnit Features([j�iiiu-,-----------------'The Agent RBG Bird's-eye-viewOther players -----------------'Environmental featuresFigure 4: Preprocessing for an observation with four types of features.process for handling an observation.。Each observation consists of four types of feature, and each type is associated with anindependent network dedicated to processing and outputting the corresponding embedding.Action Space.. \n",
      "the piece 710\n",
      " \n",
      "d scavenging elementsinherent in first-person shooting games (Gautam et al., 2021).。The game unfolds with multiple hostile teams, necessitatingplayers to collaborate with teammates, withstand adversaries, and strive to outlast others in the ever-changing arena.。Theagent’s objectives encompass individual survival and the elimination of encountered enemies.。In Contra, the agent’s actioninterface is designed to mirror human capabilities, encompassing basic movements and actions like moving and shooting,with action intervals around 200ms, similar to the frequency of human operations.。Through these actions, an agent in Contramandates a sequential acquisition of skills, starting from fundamental abilities like walking, jumping, running, and itemcollection.。As the learning proceeds, an agent must master more intricate skills such as evading enemy projectiles andcoordinating tactics with teammates.。This characteristic defines an open-ended learning process where the agent continuallyexplores the game environment to refine mastered skills and acquire new ones.Observation Space.。The observation space encompasses various factors, comprising unit features delineating the agent’sstatus, as well as that of other players.。Additionally, it includes environmental features characterizing interaction eventsand an agent-centric RGB bird’s-eye-view of the local observation.。For the details, we include them in Table 4.。Given theheterogeneity in the shapes and data types of these features, we adopt independent feature processing for each of them,subsequently concatenating them to serve as input for the policy and value networks.。In Figure 4, we present the foundationalUnit Features([j�iiiu-,-----------------'The Agent RBG Bird's-eye-viewOther players -----------------'Environmental featuresFigure 4: Preprocessing for an observation with four types of features.process for handling an observation.。Each observation consists of four types of feature, and each type is associated with anindependent network dedicated to processing and outputting the corresponding embedding.Action Space.。As introduced in Table 6, the instantiation of action space is achieved through the utilization of the micro-operation API within Contra.. \n",
      "the piece 711\n",
      " \n",
      "cessitatingplayers to collaborate with teammates, withstand adversaries, and strive to outlast others in the ever-changing arena.。Theagent’s objectives encompass individual survival and the elimination of encountered enemies.。In Contra, the agent’s actioninterface is designed to mirror human capabilities, encompassing basic movements and actions like moving and shooting,with action intervals around 200ms, similar to the frequency of human operations.。Through these actions, an agent in Contramandates a sequential acquisition of skills, starting from fundamental abilities like walking, jumping, running, and itemcollection.。As the learning proceeds, an agent must master more intricate skills such as evading enemy projectiles andcoordinating tactics with teammates.。This characteristic defines an open-ended learning process where the agent continuallyexplores the game environment to refine mastered skills and acquire new ones.Observation Space.。The observation space encompasses various factors, comprising unit features delineating the agent’sstatus, as well as that of other players.。Additionally, it includes environmental features characterizing interaction eventsand an agent-centric RGB bird’s-eye-view of the local observation.。For the details, we include them in Table 4.。Given theheterogeneity in the shapes and data types of these features, we adopt independent feature processing for each of them,subsequently concatenating them to serve as input for the policy and value networks.。In Figure 4, we present the foundationalUnit Features([j�iiiu-,-----------------'The Agent RBG Bird's-eye-viewOther players -----------------'Environmental featuresFigure 4: Preprocessing for an observation with four types of features.process for handling an observation.。Each observation consists of four types of feature, and each type is associated with anindependent network dedicated to processing and outputting the corresponding embedding.Action Space.。As introduced in Table 6, the instantiation of action space is achieved through the utilization of the micro-operation API within Contra.。This process gives rise to a compilation of actions characterized by diverse levels of granularity.In a detailed breakdown, the action space comprises several distinct types, namely (1) movement direction action space , itprovides 16 discrete choices, each evenly distributed across a 360◦spectrum, (2) camera yaw direction action space , itoffers 16 choices with an equitable division of 360◦, (3) camera pitch direction action space , it encompasses three distinctvalues: −45◦,0◦,45◦, (4) body action space , it incorporates nine values: slide, stop, crouch, run, jump, ground, openor close door, rescue, and no-op and (5) attacking action space , it comprises: fire, reloading, treat, pick up supply, dropsupply, stop and fire, stop adjust and fire, and (6) weapon switch , it manifests three values: use weapon 0, use weapon 1,and no-weapon.. \n",
      "the piece 712\n",
      " \n",
      "countered enemies.。In Contra, the agent’s actioninterface is designed to mirror human capabilities, encompassing basic movements and actions like moving and shooting,with action intervals around 200ms, similar to the frequency of human operations.。Through these actions, an agent in Contramandates a sequential acquisition of skills, starting from fundamental abilities like walking, jumping, running, and itemcollection.。As the learning proceeds, an agent must master more intricate skills such as evading enemy projectiles andcoordinating tactics with teammates.。This characteristic defines an open-ended learning process where the agent continuallyexplores the game environment to refine mastered skills and acquire new ones.Observation Space.。The observation space encompasses various factors, comprising unit features delineating the agent’sstatus, as well as that of other players.。Additionally, it includes environmental features characterizing interaction eventsand an agent-centric RGB bird’s-eye-view of the local observation.。For the details, we include them in Table 4.。Given theheterogeneity in the shapes and data types of these features, we adopt independent feature processing for each of them,subsequently concatenating them to serve as input for the policy and value networks.。In Figure 4, we present the foundationalUnit Features([j�iiiu-,-----------------'The Agent RBG Bird's-eye-viewOther players -----------------'Environmental featuresFigure 4: Preprocessing for an observation with four types of features.process for handling an observation.。Each observation consists of four types of feature, and each type is associated with anindependent network dedicated to processing and outputting the corresponding embedding.Action Space.。As introduced in Table 6, the instantiation of action space is achieved through the utilization of the micro-operation API within Contra.。This process gives rise to a compilation of actions characterized by diverse levels of granularity.In a detailed breakdown, the action space comprises several distinct types, namely (1) movement direction action space , itprovides 16 discrete choices, each evenly distributed across a 360◦spectrum, (2) camera yaw direction action space , itoffers 16 choices with an equitable division of 360◦, (3) camera pitch direction action space , it encompasses three distinctvalues: −45◦,0◦,45◦, (4) body action space , it incorporates nine values: slide, stop, crouch, run, jump, ground, openor close door, rescue, and no-op and (5) attacking action space , it comprises: fire, reloading, treat, pick up supply, dropsupply, stop and fire, stop adjust and fire, and (6) weapon switch , it manifests three values: use weapon 0, use weapon 1,and no-weapon.。The aggregated dimensionality of the action space is quantified at 54 in total.Reward Engineering.. \n",
      "the piece 713\n",
      " \n",
      "ntervals around 200ms, similar to the frequency of human operations.。Through these actions, an agent in Contramandates a sequential acquisition of skills, starting from fundamental abilities like walking, jumping, running, and itemcollection.。As the learning proceeds, an agent must master more intricate skills such as evading enemy projectiles andcoordinating tactics with teammates.。This characteristic defines an open-ended learning process where the agent continuallyexplores the game environment to refine mastered skills and acquire new ones.Observation Space.。The observation space encompasses various factors, comprising unit features delineating the agent’sstatus, as well as that of other players.。Additionally, it includes environmental features characterizing interaction eventsand an agent-centric RGB bird’s-eye-view of the local observation.。For the details, we include them in Table 4.。Given theheterogeneity in the shapes and data types of these features, we adopt independent feature processing for each of them,subsequently concatenating them to serve as input for the policy and value networks.。In Figure 4, we present the foundationalUnit Features([j�iiiu-,-----------------'The Agent RBG Bird's-eye-viewOther players -----------------'Environmental featuresFigure 4: Preprocessing for an observation with four types of features.process for handling an observation.。Each observation consists of four types of feature, and each type is associated with anindependent network dedicated to processing and outputting the corresponding embedding.Action Space.。As introduced in Table 6, the instantiation of action space is achieved through the utilization of the micro-operation API within Contra.。This process gives rise to a compilation of actions characterized by diverse levels of granularity.In a detailed breakdown, the action space comprises several distinct types, namely (1) movement direction action space , itprovides 16 discrete choices, each evenly distributed across a 360◦spectrum, (2) camera yaw direction action space , itoffers 16 choices with an equitable division of 360◦, (3) camera pitch direction action space , it encompasses three distinctvalues: −45◦,0◦,45◦, (4) body action space , it incorporates nine values: slide, stop, crouch, run, jump, ground, openor close door, rescue, and no-op and (5) attacking action space , it comprises: fire, reloading, treat, pick up supply, dropsupply, stop and fire, stop adjust and fire, and (6) weapon switch , it manifests three values: use weapon 0, use weapon 1,and no-weapon.。The aggregated dimensionality of the action space is quantified at 54 in total.Reward Engineering.。The primary objective of our training regimen is to equip the agent with the ability to playwith other players in Contra while concurrently optimizing its strength in eliminating opponents.. \n",
      "the piece 714\n",
      " \n",
      "ent in Contramandates a sequential acquisition of skills, starting from fundamental abilities like walking, jumping, running, and itemcollection.。As the learning proceeds, an agent must master more intricate skills such as evading enemy projectiles andcoordinating tactics with teammates.。This characteristic defines an open-ended learning process where the agent continuallyexplores the game environment to refine mastered skills and acquire new ones.Observation Space.。The observation space encompasses various factors, comprising unit features delineating the agent’sstatus, as well as that of other players.。Additionally, it includes environmental features characterizing interaction eventsand an agent-centric RGB bird’s-eye-view of the local observation.。For the details, we include them in Table 4.。Given theheterogeneity in the shapes and data types of these features, we adopt independent feature processing for each of them,subsequently concatenating them to serve as input for the policy and value networks.。In Figure 4, we present the foundationalUnit Features([j�iiiu-,-----------------'The Agent RBG Bird's-eye-viewOther players -----------------'Environmental featuresFigure 4: Preprocessing for an observation with four types of features.process for handling an observation.。Each observation consists of four types of feature, and each type is associated with anindependent network dedicated to processing and outputting the corresponding embedding.Action Space.。As introduced in Table 6, the instantiation of action space is achieved through the utilization of the micro-operation API within Contra.。This process gives rise to a compilation of actions characterized by diverse levels of granularity.In a detailed breakdown, the action space comprises several distinct types, namely (1) movement direction action space , itprovides 16 discrete choices, each evenly distributed across a 360◦spectrum, (2) camera yaw direction action space , itoffers 16 choices with an equitable division of 360◦, (3) camera pitch direction action space , it encompasses three distinctvalues: −45◦,0◦,45◦, (4) body action space , it incorporates nine values: slide, stop, crouch, run, jump, ground, openor close door, rescue, and no-op and (5) attacking action space , it comprises: fire, reloading, treat, pick up supply, dropsupply, stop and fire, stop adjust and fire, and (6) weapon switch , it manifests three values: use weapon 0, use weapon 1,and no-weapon.。The aggregated dimensionality of the action space is quantified at 54 in total.Reward Engineering.。The primary objective of our training regimen is to equip the agent with the ability to playwith other players in Contra while concurrently optimizing its strength in eliminating opponents.。To achieve this ob-jective, we have formulated a diverse array of rewards designed to guide policy learning.. \n",
      "the piece 715\n",
      " \n",
      " enemy projectiles andcoordinating tactics with teammates.。This characteristic defines an open-ended learning process where the agent continuallyexplores the game environment to refine mastered skills and acquire new ones.Observation Space.。The observation space encompasses various factors, comprising unit features delineating the agent’sstatus, as well as that of other players.。Additionally, it includes environmental features characterizing interaction eventsand an agent-centric RGB bird’s-eye-view of the local observation.。For the details, we include them in Table 4.。Given theheterogeneity in the shapes and data types of these features, we adopt independent feature processing for each of them,subsequently concatenating them to serve as input for the policy and value networks.。In Figure 4, we present the foundationalUnit Features([j�iiiu-,-----------------'The Agent RBG Bird's-eye-viewOther players -----------------'Environmental featuresFigure 4: Preprocessing for an observation with four types of features.process for handling an observation.。Each observation consists of four types of feature, and each type is associated with anindependent network dedicated to processing and outputting the corresponding embedding.Action Space.。As introduced in Table 6, the instantiation of action space is achieved through the utilization of the micro-operation API within Contra.。This process gives rise to a compilation of actions characterized by diverse levels of granularity.In a detailed breakdown, the action space comprises several distinct types, namely (1) movement direction action space , itprovides 16 discrete choices, each evenly distributed across a 360◦spectrum, (2) camera yaw direction action space , itoffers 16 choices with an equitable division of 360◦, (3) camera pitch direction action space , it encompasses three distinctvalues: −45◦,0◦,45◦, (4) body action space , it incorporates nine values: slide, stop, crouch, run, jump, ground, openor close door, rescue, and no-op and (5) attacking action space , it comprises: fire, reloading, treat, pick up supply, dropsupply, stop and fire, stop adjust and fire, and (6) weapon switch , it manifests three values: use weapon 0, use weapon 1,and no-weapon.。The aggregated dimensionality of the action space is quantified at 54 in total.Reward Engineering.。The primary objective of our training regimen is to equip the agent with the ability to playwith other players in Contra while concurrently optimizing its strength in eliminating opponents.。To achieve this ob-jective, we have formulated a diverse array of rewards designed to guide policy learning.。However, the complexityinvolved in designing and fine-tuning these rewards is evident.. \n",
      "the piece 716\n",
      " \n",
      "to refine mastered skills and acquire new ones.Observation Space.。The observation space encompasses various factors, comprising unit features delineating the agent’sstatus, as well as that of other players.。Additionally, it includes environmental features characterizing interaction eventsand an agent-centric RGB bird’s-eye-view of the local observation.。For the details, we include them in Table 4.。Given theheterogeneity in the shapes and data types of these features, we adopt independent feature processing for each of them,subsequently concatenating them to serve as input for the policy and value networks.。In Figure 4, we present the foundationalUnit Features([j�iiiu-,-----------------'The Agent RBG Bird's-eye-viewOther players -----------------'Environmental featuresFigure 4: Preprocessing for an observation with four types of features.process for handling an observation.。Each observation consists of four types of feature, and each type is associated with anindependent network dedicated to processing and outputting the corresponding embedding.Action Space.。As introduced in Table 6, the instantiation of action space is achieved through the utilization of the micro-operation API within Contra.。This process gives rise to a compilation of actions characterized by diverse levels of granularity.In a detailed breakdown, the action space comprises several distinct types, namely (1) movement direction action space , itprovides 16 discrete choices, each evenly distributed across a 360◦spectrum, (2) camera yaw direction action space , itoffers 16 choices with an equitable division of 360◦, (3) camera pitch direction action space , it encompasses three distinctvalues: −45◦,0◦,45◦, (4) body action space , it incorporates nine values: slide, stop, crouch, run, jump, ground, openor close door, rescue, and no-op and (5) attacking action space , it comprises: fire, reloading, treat, pick up supply, dropsupply, stop and fire, stop adjust and fire, and (6) weapon switch , it manifests three values: use weapon 0, use weapon 1,and no-weapon.。The aggregated dimensionality of the action space is quantified at 54 in total.Reward Engineering.。The primary objective of our training regimen is to equip the agent with the ability to playwith other players in Contra while concurrently optimizing its strength in eliminating opponents.。To achieve this ob-jective, we have formulated a diverse array of rewards designed to guide policy learning.。However, the complexityinvolved in designing and fine-tuning these rewards is evident.。To simplify the engineering, our design is character-ized allocating weights based on the expected value of each reward, ensuring a proportionate influence on the learningprocess.. \n",
      "the piece 717\n",
      " \n",
      "lineating the agent’sstatus, as well as that of other players.。Additionally, it includes environmental features characterizing interaction eventsand an agent-centric RGB bird’s-eye-view of the local observation.。For the details, we include them in Table 4.。Given theheterogeneity in the shapes and data types of these features, we adopt independent feature processing for each of them,subsequently concatenating them to serve as input for the policy and value networks.。In Figure 4, we present the foundationalUnit Features([j�iiiu-,-----------------'The Agent RBG Bird's-eye-viewOther players -----------------'Environmental featuresFigure 4: Preprocessing for an observation with four types of features.process for handling an observation.。Each observation consists of four types of feature, and each type is associated with anindependent network dedicated to processing and outputting the corresponding embedding.Action Space.。As introduced in Table 6, the instantiation of action space is achieved through the utilization of the micro-operation API within Contra.。This process gives rise to a compilation of actions characterized by diverse levels of granularity.In a detailed breakdown, the action space comprises several distinct types, namely (1) movement direction action space , itprovides 16 discrete choices, each evenly distributed across a 360◦spectrum, (2) camera yaw direction action space , itoffers 16 choices with an equitable division of 360◦, (3) camera pitch direction action space , it encompasses three distinctvalues: −45◦,0◦,45◦, (4) body action space , it incorporates nine values: slide, stop, crouch, run, jump, ground, openor close door, rescue, and no-op and (5) attacking action space , it comprises: fire, reloading, treat, pick up supply, dropsupply, stop and fire, stop adjust and fire, and (6) weapon switch , it manifests three values: use weapon 0, use weapon 1,and no-weapon.。The aggregated dimensionality of the action space is quantified at 54 in total.Reward Engineering.。The primary objective of our training regimen is to equip the agent with the ability to playwith other players in Contra while concurrently optimizing its strength in eliminating opponents.。To achieve this ob-jective, we have formulated a diverse array of rewards designed to guide policy learning.。However, the complexityinvolved in designing and fine-tuning these rewards is evident.。To simplify the engineering, our design is character-ized allocating weights based on the expected value of each reward, ensuring a proportionate influence on the learningprocess.。In accordance with the principle, we assume a referenced maximum return of 20, with different rewards as-signed proportions based on their relative importance.. \n",
      "the piece 718\n",
      " \n",
      "ew of the local observation.。For the details, we include them in Table 4.。Given theheterogeneity in the shapes and data types of these features, we adopt independent feature processing for each of them,subsequently concatenating them to serve as input for the policy and value networks.。In Figure 4, we present the foundationalUnit Features([j�iiiu-,-----------------'The Agent RBG Bird's-eye-viewOther players -----------------'Environmental featuresFigure 4: Preprocessing for an observation with four types of features.process for handling an observation.。Each observation consists of four types of feature, and each type is associated with anindependent network dedicated to processing and outputting the corresponding embedding.Action Space.。As introduced in Table 6, the instantiation of action space is achieved through the utilization of the micro-operation API within Contra.。This process gives rise to a compilation of actions characterized by diverse levels of granularity.In a detailed breakdown, the action space comprises several distinct types, namely (1) movement direction action space , itprovides 16 discrete choices, each evenly distributed across a 360◦spectrum, (2) camera yaw direction action space , itoffers 16 choices with an equitable division of 360◦, (3) camera pitch direction action space , it encompasses three distinctvalues: −45◦,0◦,45◦, (4) body action space , it incorporates nine values: slide, stop, crouch, run, jump, ground, openor close door, rescue, and no-op and (5) attacking action space , it comprises: fire, reloading, treat, pick up supply, dropsupply, stop and fire, stop adjust and fire, and (6) weapon switch , it manifests three values: use weapon 0, use weapon 1,and no-weapon.。The aggregated dimensionality of the action space is quantified at 54 in total.Reward Engineering.。The primary objective of our training regimen is to equip the agent with the ability to playwith other players in Contra while concurrently optimizing its strength in eliminating opponents.。To achieve this ob-jective, we have formulated a diverse array of rewards designed to guide policy learning.。However, the complexityinvolved in designing and fine-tuning these rewards is evident.。To simplify the engineering, our design is character-ized allocating weights based on the expected value of each reward, ensuring a proportionate influence on the learningprocess.。In accordance with the principle, we assume a referenced maximum return of 20, with different rewards as-signed proportions based on their relative importance.。Specifically, for critical actions such as knocking down or killingan enemy, their values are set to approximately 4 (20% out of 20).. \n",
      "the piece 719\n",
      " \n",
      "s, we adopt independent feature processing for each of them,subsequently concatenating them to serve as input for the policy and value networks.。In Figure 4, we present the foundationalUnit Features([j�iiiu-,-----------------'The Agent RBG Bird's-eye-viewOther players -----------------'Environmental featuresFigure 4: Preprocessing for an observation with four types of features.process for handling an observation.。Each observation consists of four types of feature, and each type is associated with anindependent network dedicated to processing and outputting the corresponding embedding.Action Space.。As introduced in Table 6, the instantiation of action space is achieved through the utilization of the micro-operation API within Contra.。This process gives rise to a compilation of actions characterized by diverse levels of granularity.In a detailed breakdown, the action space comprises several distinct types, namely (1) movement direction action space , itprovides 16 discrete choices, each evenly distributed across a 360◦spectrum, (2) camera yaw direction action space , itoffers 16 choices with an equitable division of 360◦, (3) camera pitch direction action space , it encompasses three distinctvalues: −45◦,0◦,45◦, (4) body action space , it incorporates nine values: slide, stop, crouch, run, jump, ground, openor close door, rescue, and no-op and (5) attacking action space , it comprises: fire, reloading, treat, pick up supply, dropsupply, stop and fire, stop adjust and fire, and (6) weapon switch , it manifests three values: use weapon 0, use weapon 1,and no-weapon.。The aggregated dimensionality of the action space is quantified at 54 in total.Reward Engineering.。The primary objective of our training regimen is to equip the agent with the ability to playwith other players in Contra while concurrently optimizing its strength in eliminating opponents.。To achieve this ob-jective, we have formulated a diverse array of rewards designed to guide policy learning.。However, the complexityinvolved in designing and fine-tuning these rewards is evident.。To simplify the engineering, our design is character-ized allocating weights based on the expected value of each reward, ensuring a proportionate influence on the learningprocess.。In accordance with the principle, we assume a referenced maximum return of 20, with different rewards as-signed proportions based on their relative importance.。Specifically, for critical actions such as knocking down or killingan enemy, their values are set to approximately 4 (20% out of 20).。Conversely, for less critical actions like scoutingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationor incurring time penalties, their values are set to less than 1 (5% out of 20).. \n",
      "the piece 720\n",
      " \n",
      "gure 4, we present the foundationalUnit Features([j�iiiu-,-----------------'The Agent RBG Bird's-eye-viewOther players -----------------'Environmental featuresFigure 4: Preprocessing for an observation with four types of features.process for handling an observation.。Each observation consists of four types of feature, and each type is associated with anindependent network dedicated to processing and outputting the corresponding embedding.Action Space.。As introduced in Table 6, the instantiation of action space is achieved through the utilization of the micro-operation API within Contra.。This process gives rise to a compilation of actions characterized by diverse levels of granularity.In a detailed breakdown, the action space comprises several distinct types, namely (1) movement direction action space , itprovides 16 discrete choices, each evenly distributed across a 360◦spectrum, (2) camera yaw direction action space , itoffers 16 choices with an equitable division of 360◦, (3) camera pitch direction action space , it encompasses three distinctvalues: −45◦,0◦,45◦, (4) body action space , it incorporates nine values: slide, stop, crouch, run, jump, ground, openor close door, rescue, and no-op and (5) attacking action space , it comprises: fire, reloading, treat, pick up supply, dropsupply, stop and fire, stop adjust and fire, and (6) weapon switch , it manifests three values: use weapon 0, use weapon 1,and no-weapon.。The aggregated dimensionality of the action space is quantified at 54 in total.Reward Engineering.。The primary objective of our training regimen is to equip the agent with the ability to playwith other players in Contra while concurrently optimizing its strength in eliminating opponents.。To achieve this ob-jective, we have formulated a diverse array of rewards designed to guide policy learning.。However, the complexityinvolved in designing and fine-tuning these rewards is evident.。To simplify the engineering, our design is character-ized allocating weights based on the expected value of each reward, ensuring a proportionate influence on the learningprocess.。In accordance with the principle, we assume a referenced maximum return of 20, with different rewards as-signed proportions based on their relative importance.。Specifically, for critical actions such as knocking down or killingan enemy, their values are set to approximately 4 (20% out of 20).。Conversely, for less critical actions like scoutingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationor incurring time penalties, their values are set to less than 1 (5% out of 20).。Detailed specifications are outlined inTable 5.. \n",
      "the piece 721\n",
      " \n",
      "es([j�iiiu-,-----------------'The Agent RBG Bird's-eye-viewOther players -----------------'Environmental featuresFigure 4: Preprocessing for an observation with four types of features.process for handling an observation.。Each observation consists of four types of feature, and each type is associated with anindependent network dedicated to processing and outputting the corresponding embedding.Action Space.。As introduced in Table 6, the instantiation of action space is achieved through the utilization of the micro-operation API within Contra.。This process gives rise to a compilation of actions characterized by diverse levels of granularity.In a detailed breakdown, the action space comprises several distinct types, namely (1) movement direction action space , itprovides 16 discrete choices, each evenly distributed across a 360◦spectrum, (2) camera yaw direction action space , itoffers 16 choices with an equitable division of 360◦, (3) camera pitch direction action space , it encompasses three distinctvalues: −45◦,0◦,45◦, (4) body action space , it incorporates nine values: slide, stop, crouch, run, jump, ground, openor close door, rescue, and no-op and (5) attacking action space , it comprises: fire, reloading, treat, pick up supply, dropsupply, stop and fire, stop adjust and fire, and (6) weapon switch , it manifests three values: use weapon 0, use weapon 1,and no-weapon.。The aggregated dimensionality of the action space is quantified at 54 in total.Reward Engineering.。The primary objective of our training regimen is to equip the agent with the ability to playwith other players in Contra while concurrently optimizing its strength in eliminating opponents.。To achieve this ob-jective, we have formulated a diverse array of rewards designed to guide policy learning.。However, the complexityinvolved in designing and fine-tuning these rewards is evident.。To simplify the engineering, our design is character-ized allocating weights based on the expected value of each reward, ensuring a proportionate influence on the learningprocess.。In accordance with the principle, we assume a referenced maximum return of 20, with different rewards as-signed proportions based on their relative importance.。Specifically, for critical actions such as knocking down or killingan enemy, their values are set to approximately 4 (20% out of 20).。Conversely, for less critical actions like scoutingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationor incurring time penalties, their values are set to less than 1 (5% out of 20).。Detailed specifications are outlined inTable 5.。In accordance with the aforementioned principles, we can now construct the reward function r(s, a)by lin-early composing these factors, facilitating their collaborative influence on guiding policy learning.. \n",
      "the piece 722\n",
      " \n",
      "ation.。Each observation consists of four types of feature, and each type is associated with anindependent network dedicated to processing and outputting the corresponding embedding.Action Space.。As introduced in Table 6, the instantiation of action space is achieved through the utilization of the micro-operation API within Contra.。This process gives rise to a compilation of actions characterized by diverse levels of granularity.In a detailed breakdown, the action space comprises several distinct types, namely (1) movement direction action space , itprovides 16 discrete choices, each evenly distributed across a 360◦spectrum, (2) camera yaw direction action space , itoffers 16 choices with an equitable division of 360◦, (3) camera pitch direction action space , it encompasses three distinctvalues: −45◦,0◦,45◦, (4) body action space , it incorporates nine values: slide, stop, crouch, run, jump, ground, openor close door, rescue, and no-op and (5) attacking action space , it comprises: fire, reloading, treat, pick up supply, dropsupply, stop and fire, stop adjust and fire, and (6) weapon switch , it manifests three values: use weapon 0, use weapon 1,and no-weapon.。The aggregated dimensionality of the action space is quantified at 54 in total.Reward Engineering.。The primary objective of our training regimen is to equip the agent with the ability to playwith other players in Contra while concurrently optimizing its strength in eliminating opponents.。To achieve this ob-jective, we have formulated a diverse array of rewards designed to guide policy learning.。However, the complexityinvolved in designing and fine-tuning these rewards is evident.。To simplify the engineering, our design is character-ized allocating weights based on the expected value of each reward, ensuring a proportionate influence on the learningprocess.。In accordance with the principle, we assume a referenced maximum return of 20, with different rewards as-signed proportions based on their relative importance.。Specifically, for critical actions such as knocking down or killingan enemy, their values are set to approximately 4 (20% out of 20).。Conversely, for less critical actions like scoutingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationor incurring time penalties, their values are set to less than 1 (5% out of 20).。Detailed specifications are outlined inTable 5.。In accordance with the aforementioned principles, we can now construct the reward function r(s, a)by lin-early composing these factors, facilitating their collaborative influence on guiding policy learning.。As delineated inAlgorithm 3, these factors are broadly classified into three categories: basic rewards rb, obstacle avoidance rewardsroa, and goal achievement reward rg.. \n",
      "the piece 723\n",
      " \n",
      "h the utilization of the micro-operation API within Contra.。This process gives rise to a compilation of actions characterized by diverse levels of granularity.In a detailed breakdown, the action space comprises several distinct types, namely (1) movement direction action space , itprovides 16 discrete choices, each evenly distributed across a 360◦spectrum, (2) camera yaw direction action space , itoffers 16 choices with an equitable division of 360◦, (3) camera pitch direction action space , it encompasses three distinctvalues: −45◦,0◦,45◦, (4) body action space , it incorporates nine values: slide, stop, crouch, run, jump, ground, openor close door, rescue, and no-op and (5) attacking action space , it comprises: fire, reloading, treat, pick up supply, dropsupply, stop and fire, stop adjust and fire, and (6) weapon switch , it manifests three values: use weapon 0, use weapon 1,and no-weapon.。The aggregated dimensionality of the action space is quantified at 54 in total.Reward Engineering.。The primary objective of our training regimen is to equip the agent with the ability to playwith other players in Contra while concurrently optimizing its strength in eliminating opponents.。To achieve this ob-jective, we have formulated a diverse array of rewards designed to guide policy learning.。However, the complexityinvolved in designing and fine-tuning these rewards is evident.。To simplify the engineering, our design is character-ized allocating weights based on the expected value of each reward, ensuring a proportionate influence on the learningprocess.。In accordance with the principle, we assume a referenced maximum return of 20, with different rewards as-signed proportions based on their relative importance.。Specifically, for critical actions such as knocking down or killingan enemy, their values are set to approximately 4 (20% out of 20).。Conversely, for less critical actions like scoutingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationor incurring time penalties, their values are set to less than 1 (5% out of 20).。Detailed specifications are outlined inTable 5.。In accordance with the aforementioned principles, we can now construct the reward function r(s, a)by lin-early composing these factors, facilitating their collaborative influence on guiding policy learning.。As delineated inAlgorithm 3, these factors are broadly classified into three categories: basic rewards rb, obstacle avoidance rewardsroa, and goal achievement reward rg.。The basic rewards are primarily directed at steering the agent towards enhancedgame-playing performance, encompassing collaborative engagement with teammates and eliminating adversaries, etc.Feature Class Field Description Dimension1.Unit feature Scalar Includes heroes, teammates, enemies, monster 527Attribute Character ID,team ID,size,skills 28Status HP, oxygen,speed, peek type, alive state, body state,etc.. \n",
      "the piece 724\n",
      " \n",
      "ction space comprises several distinct types, namely (1) movement direction action space , itprovides 16 discrete choices, each evenly distributed across a 360◦spectrum, (2) camera yaw direction action space , itoffers 16 choices with an equitable division of 360◦, (3) camera pitch direction action space , it encompasses three distinctvalues: −45◦,0◦,45◦, (4) body action space , it incorporates nine values: slide, stop, crouch, run, jump, ground, openor close door, rescue, and no-op and (5) attacking action space , it comprises: fire, reloading, treat, pick up supply, dropsupply, stop and fire, stop adjust and fire, and (6) weapon switch , it manifests three values: use weapon 0, use weapon 1,and no-weapon.。The aggregated dimensionality of the action space is quantified at 54 in total.Reward Engineering.。The primary objective of our training regimen is to equip the agent with the ability to playwith other players in Contra while concurrently optimizing its strength in eliminating opponents.。To achieve this ob-jective, we have formulated a diverse array of rewards designed to guide policy learning.。However, the complexityinvolved in designing and fine-tuning these rewards is evident.。To simplify the engineering, our design is character-ized allocating weights based on the expected value of each reward, ensuring a proportionate influence on the learningprocess.。In accordance with the principle, we assume a referenced maximum return of 20, with different rewards as-signed proportions based on their relative importance.。Specifically, for critical actions such as knocking down or killingan enemy, their values are set to approximately 4 (20% out of 20).。Conversely, for less critical actions like scoutingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationor incurring time penalties, their values are set to less than 1 (5% out of 20).。Detailed specifications are outlined inTable 5.。In accordance with the aforementioned principles, we can now construct the reward function r(s, a)by lin-early composing these factors, facilitating their collaborative influence on guiding policy learning.。As delineated inAlgorithm 3, these factors are broadly classified into three categories: basic rewards rb, obstacle avoidance rewardsroa, and goal achievement reward rg.。The basic rewards are primarily directed at steering the agent towards enhancedgame-playing performance, encompassing collaborative engagement with teammates and eliminating adversaries, etc.Feature Class Field Description Dimension1.Unit feature Scalar Includes heroes, teammates, enemies, monster 527Attribute Character ID,team ID,size,skills 28Status HP, oxygen,speed, peek type, alive state, body state,etc.。44Heroes Pose Position, rotation, camera position, camera rotation, etc.. \n",
      "the piece 725\n",
      " \n",
      "ributed across a 360◦spectrum, (2) camera yaw direction action space , itoffers 16 choices with an equitable division of 360◦, (3) camera pitch direction action space , it encompasses three distinctvalues: −45◦,0◦,45◦, (4) body action space , it incorporates nine values: slide, stop, crouch, run, jump, ground, openor close door, rescue, and no-op and (5) attacking action space , it comprises: fire, reloading, treat, pick up supply, dropsupply, stop and fire, stop adjust and fire, and (6) weapon switch , it manifests three values: use weapon 0, use weapon 1,and no-weapon.。The aggregated dimensionality of the action space is quantified at 54 in total.Reward Engineering.。The primary objective of our training regimen is to equip the agent with the ability to playwith other players in Contra while concurrently optimizing its strength in eliminating opponents.。To achieve this ob-jective, we have formulated a diverse array of rewards designed to guide policy learning.。However, the complexityinvolved in designing and fine-tuning these rewards is evident.。To simplify the engineering, our design is character-ized allocating weights based on the expected value of each reward, ensuring a proportionate influence on the learningprocess.。In accordance with the principle, we assume a referenced maximum return of 20, with different rewards as-signed proportions based on their relative importance.。Specifically, for critical actions such as knocking down or killingan enemy, their values are set to approximately 4 (20% out of 20).。Conversely, for less critical actions like scoutingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationor incurring time penalties, their values are set to less than 1 (5% out of 20).。Detailed specifications are outlined inTable 5.。In accordance with the aforementioned principles, we can now construct the reward function r(s, a)by lin-early composing these factors, facilitating their collaborative influence on guiding policy learning.。As delineated inAlgorithm 3, these factors are broadly classified into three categories: basic rewards rb, obstacle avoidance rewardsroa, and goal achievement reward rg.。The basic rewards are primarily directed at steering the agent towards enhancedgame-playing performance, encompassing collaborative engagement with teammates and eliminating adversaries, etc.Feature Class Field Description Dimension1.Unit feature Scalar Includes heroes, teammates, enemies, monster 527Attribute Character ID,team ID,size,skills 28Status HP, oxygen,speed, peek type, alive state, body state,etc.。44Heroes Pose Position, rotation, camera position, camera rotation, etc.。76Item Backpack,weapon 144Attribute Character ID,team ID,size,skills 28Teammates Status HP, oxygen,speed, peek type, alive state, body state,etc.. \n",
      "the piece 726\n",
      " \n",
      "minating opponents.。To achieve this ob-jective, we have formulated a diverse array of rewards designed to guide policy learning.。However, the complexityinvolved in designing and fine-tuning these rewards is evident.。To simplify the engineering, our design is character-ized allocating weights based on the expected value of each reward, ensuring a proportionate influence on the learningprocess.。In accordance with the principle, we assume a referenced maximum return of 20, with different rewards as-signed proportions based on their relative importance.。Specifically, for critical actions such as knocking down or killingan enemy, their values are set to approximately 4 (20% out of 20).。Conversely, for less critical actions like scoutingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationor incurring time penalties, their values are set to less than 1 (5% out of 20).。Detailed specifications are outlined inTable 5.。In accordance with the aforementioned principles, we can now construct the reward function r(s, a)by lin-early composing these factors, facilitating their collaborative influence on guiding policy learning.。As delineated inAlgorithm 3, these factors are broadly classified into three categories: basic rewards rb, obstacle avoidance rewardsroa, and goal achievement reward rg.。The basic rewards are primarily directed at steering the agent towards enhancedgame-playing performance, encompassing collaborative engagement with teammates and eliminating adversaries, etc.Feature Class Field Description Dimension1.Unit feature Scalar Includes heroes, teammates, enemies, monster 527Attribute Character ID,team ID,size,skills 28Status HP, oxygen,speed, peek type, alive state, body state,etc.。44Heroes Pose Position, rotation, camera position, camera rotation, etc.。76Item Backpack,weapon 144Attribute Character ID,team ID,size,skills 28Teammates Status HP, oxygen,speed, peek type, alive state, body state,etc.。30Pose Position, rotation, camera position, camera rotation, etc.. \n",
      "the piece 727\n",
      " \n",
      "ed to guide policy learning.。However, the complexityinvolved in designing and fine-tuning these rewards is evident.。To simplify the engineering, our design is character-ized allocating weights based on the expected value of each reward, ensuring a proportionate influence on the learningprocess.。In accordance with the principle, we assume a referenced maximum return of 20, with different rewards as-signed proportions based on their relative importance.。Specifically, for critical actions such as knocking down or killingan enemy, their values are set to approximately 4 (20% out of 20).。Conversely, for less critical actions like scoutingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationor incurring time penalties, their values are set to less than 1 (5% out of 20).。Detailed specifications are outlined inTable 5.。In accordance with the aforementioned principles, we can now construct the reward function r(s, a)by lin-early composing these factors, facilitating their collaborative influence on guiding policy learning.。As delineated inAlgorithm 3, these factors are broadly classified into three categories: basic rewards rb, obstacle avoidance rewardsroa, and goal achievement reward rg.。The basic rewards are primarily directed at steering the agent towards enhancedgame-playing performance, encompassing collaborative engagement with teammates and eliminating adversaries, etc.Feature Class Field Description Dimension1.Unit feature Scalar Includes heroes, teammates, enemies, monster 527Attribute Character ID,team ID,size,skills 28Status HP, oxygen,speed, peek type, alive state, body state,etc.。44Heroes Pose Position, rotation, camera position, camera rotation, etc.。76Item Backpack,weapon 144Attribute Character ID,team ID,size,skills 28Teammates Status HP, oxygen,speed, peek type, alive state, body state,etc.。30Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Character ID,team ID,size,skills 28Enemies Status HP, oxygen,speed, peek type, alive state, body state,etc.. \n",
      "the piece 728\n",
      " \n",
      "s based on the expected value of each reward, ensuring a proportionate influence on the learningprocess.。In accordance with the principle, we assume a referenced maximum return of 20, with different rewards as-signed proportions based on their relative importance.。Specifically, for critical actions such as knocking down or killingan enemy, their values are set to approximately 4 (20% out of 20).。Conversely, for less critical actions like scoutingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationor incurring time penalties, their values are set to less than 1 (5% out of 20).。Detailed specifications are outlined inTable 5.。In accordance with the aforementioned principles, we can now construct the reward function r(s, a)by lin-early composing these factors, facilitating their collaborative influence on guiding policy learning.。As delineated inAlgorithm 3, these factors are broadly classified into three categories: basic rewards rb, obstacle avoidance rewardsroa, and goal achievement reward rg.。The basic rewards are primarily directed at steering the agent towards enhancedgame-playing performance, encompassing collaborative engagement with teammates and eliminating adversaries, etc.Feature Class Field Description Dimension1.Unit feature Scalar Includes heroes, teammates, enemies, monster 527Attribute Character ID,team ID,size,skills 28Status HP, oxygen,speed, peek type, alive state, body state,etc.。44Heroes Pose Position, rotation, camera position, camera rotation, etc.。76Item Backpack,weapon 144Attribute Character ID,team ID,size,skills 28Teammates Status HP, oxygen,speed, peek type, alive state, body state,etc.。30Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Character ID,team ID,size,skills 28Enemies Status HP, oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.. \n",
      "the piece 729\n",
      " \n",
      "cordance with the principle, we assume a referenced maximum return of 20, with different rewards as-signed proportions based on their relative importance.。Specifically, for critical actions such as knocking down or killingan enemy, their values are set to approximately 4 (20% out of 20).。Conversely, for less critical actions like scoutingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationor incurring time penalties, their values are set to less than 1 (5% out of 20).。Detailed specifications are outlined inTable 5.。In accordance with the aforementioned principles, we can now construct the reward function r(s, a)by lin-early composing these factors, facilitating their collaborative influence on guiding policy learning.。As delineated inAlgorithm 3, these factors are broadly classified into three categories: basic rewards rb, obstacle avoidance rewardsroa, and goal achievement reward rg.。The basic rewards are primarily directed at steering the agent towards enhancedgame-playing performance, encompassing collaborative engagement with teammates and eliminating adversaries, etc.Feature Class Field Description Dimension1.Unit feature Scalar Includes heroes, teammates, enemies, monster 527Attribute Character ID,team ID,size,skills 28Status HP, oxygen,speed, peek type, alive state, body state,etc.。44Heroes Pose Position, rotation, camera position, camera rotation, etc.。76Item Backpack,weapon 144Attribute Character ID,team ID,size,skills 28Teammates Status HP, oxygen,speed, peek type, alive state, body state,etc.。30Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Character ID,team ID,size,skills 28Enemies Status HP, oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Monster type,size 8Monsters Status HP, max HP, HP percent, target type 6Pose Position, rotation, relative position, distance 162.Global feature Scalar Includes circle, event, door and supply 65Status State,pain,radius 4Circle Position Blue circle, white circle 6Time existence time, rest time, total time, delay time, appear time 5Attribute Type,damage,elapsed time 8Event Position Occurred position 3Attribute Type,state 5Door Pose Position,relative position,rotation 8Status Type,size,quantity,etc.. \n",
      "the piece 730\n",
      " \n",
      " rewards as-signed proportions based on their relative importance.。Specifically, for critical actions such as knocking down or killingan enemy, their values are set to approximately 4 (20% out of 20).。Conversely, for less critical actions like scoutingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationor incurring time penalties, their values are set to less than 1 (5% out of 20).。Detailed specifications are outlined inTable 5.。In accordance with the aforementioned principles, we can now construct the reward function r(s, a)by lin-early composing these factors, facilitating their collaborative influence on guiding policy learning.。As delineated inAlgorithm 3, these factors are broadly classified into three categories: basic rewards rb, obstacle avoidance rewardsroa, and goal achievement reward rg.。The basic rewards are primarily directed at steering the agent towards enhancedgame-playing performance, encompassing collaborative engagement with teammates and eliminating adversaries, etc.Feature Class Field Description Dimension1.Unit feature Scalar Includes heroes, teammates, enemies, monster 527Attribute Character ID,team ID,size,skills 28Status HP, oxygen,speed, peek type, alive state, body state,etc.。44Heroes Pose Position, rotation, camera position, camera rotation, etc.。76Item Backpack,weapon 144Attribute Character ID,team ID,size,skills 28Teammates Status HP, oxygen,speed, peek type, alive state, body state,etc.。30Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Character ID,team ID,size,skills 28Enemies Status HP, oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Monster type,size 8Monsters Status HP, max HP, HP percent, target type 6Pose Position, rotation, relative position, distance 162.Global feature Scalar Includes circle, event, door and supply 65Status State,pain,radius 4Circle Position Blue circle, white circle 6Time existence time, rest time, total time, delay time, appear time 5Attribute Type,damage,elapsed time 8Event Position Occurred position 3Attribute Type,state 5Door Pose Position,relative position,rotation 8Status Type,size,quantity,etc.。19Supply Position Position,relative position,distance 73.Invisible enemy feature Scalar Invisible (nearby) enemy feature only for value estimation 104Attribute Character ID,team ID,size,skills 28Invisible enemies Status HP,oxygen,speed, peek type, alive state, body state,etc.. \n",
      "the piece 731\n",
      " \n",
      " 4 (20% out of 20).。Conversely, for less critical actions like scoutingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationor incurring time penalties, their values are set to less than 1 (5% out of 20).。Detailed specifications are outlined inTable 5.。In accordance with the aforementioned principles, we can now construct the reward function r(s, a)by lin-early composing these factors, facilitating their collaborative influence on guiding policy learning.。As delineated inAlgorithm 3, these factors are broadly classified into three categories: basic rewards rb, obstacle avoidance rewardsroa, and goal achievement reward rg.。The basic rewards are primarily directed at steering the agent towards enhancedgame-playing performance, encompassing collaborative engagement with teammates and eliminating adversaries, etc.Feature Class Field Description Dimension1.Unit feature Scalar Includes heroes, teammates, enemies, monster 527Attribute Character ID,team ID,size,skills 28Status HP, oxygen,speed, peek type, alive state, body state,etc.。44Heroes Pose Position, rotation, camera position, camera rotation, etc.。76Item Backpack,weapon 144Attribute Character ID,team ID,size,skills 28Teammates Status HP, oxygen,speed, peek type, alive state, body state,etc.。30Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Character ID,team ID,size,skills 28Enemies Status HP, oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Monster type,size 8Monsters Status HP, max HP, HP percent, target type 6Pose Position, rotation, relative position, distance 162.Global feature Scalar Includes circle, event, door and supply 65Status State,pain,radius 4Circle Position Blue circle, white circle 6Time existence time, rest time, total time, delay time, appear time 5Attribute Type,damage,elapsed time 8Event Position Occurred position 3Attribute Type,state 5Door Pose Position,relative position,rotation 8Status Type,size,quantity,etc.。19Supply Position Position,relative position,distance 73.Invisible enemy feature Scalar Invisible (nearby) enemy feature only for value estimation 104Attribute Character ID,team ID,size,skills 28Invisible enemies Status HP,oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.. \n",
      "the piece 732\n",
      " \n",
      "g time penalties, their values are set to less than 1 (5% out of 20).。Detailed specifications are outlined inTable 5.。In accordance with the aforementioned principles, we can now construct the reward function r(s, a)by lin-early composing these factors, facilitating their collaborative influence on guiding policy learning.。As delineated inAlgorithm 3, these factors are broadly classified into three categories: basic rewards rb, obstacle avoidance rewardsroa, and goal achievement reward rg.。The basic rewards are primarily directed at steering the agent towards enhancedgame-playing performance, encompassing collaborative engagement with teammates and eliminating adversaries, etc.Feature Class Field Description Dimension1.Unit feature Scalar Includes heroes, teammates, enemies, monster 527Attribute Character ID,team ID,size,skills 28Status HP, oxygen,speed, peek type, alive state, body state,etc.。44Heroes Pose Position, rotation, camera position, camera rotation, etc.。76Item Backpack,weapon 144Attribute Character ID,team ID,size,skills 28Teammates Status HP, oxygen,speed, peek type, alive state, body state,etc.。30Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Character ID,team ID,size,skills 28Enemies Status HP, oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Monster type,size 8Monsters Status HP, max HP, HP percent, target type 6Pose Position, rotation, relative position, distance 162.Global feature Scalar Includes circle, event, door and supply 65Status State,pain,radius 4Circle Position Blue circle, white circle 6Time existence time, rest time, total time, delay time, appear time 5Attribute Type,damage,elapsed time 8Event Position Occurred position 3Attribute Type,state 5Door Pose Position,relative position,rotation 8Status Type,size,quantity,etc.。19Supply Position Position,relative position,distance 73.Invisible enemy feature Scalar Invisible (nearby) enemy feature only for value estimation 104Attribute Character ID,team ID,size,skills 28Invisible enemies Status HP,oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。434.Spatial feature Scalar BEV 12288BEV Region Altitude map and aerial view map 3x64x64Table 4: The details of features in the observation space.In the case of roa, the objec-tive is to promote natural naviga-tion and forestall the agent fromencountering obstacles, such asstones and trees.. \n",
      "the piece 733\n",
      " \n",
      "h the aforementioned principles, we can now construct the reward function r(s, a)by lin-early composing these factors, facilitating their collaborative influence on guiding policy learning.。As delineated inAlgorithm 3, these factors are broadly classified into three categories: basic rewards rb, obstacle avoidance rewardsroa, and goal achievement reward rg.。The basic rewards are primarily directed at steering the agent towards enhancedgame-playing performance, encompassing collaborative engagement with teammates and eliminating adversaries, etc.Feature Class Field Description Dimension1.Unit feature Scalar Includes heroes, teammates, enemies, monster 527Attribute Character ID,team ID,size,skills 28Status HP, oxygen,speed, peek type, alive state, body state,etc.。44Heroes Pose Position, rotation, camera position, camera rotation, etc.。76Item Backpack,weapon 144Attribute Character ID,team ID,size,skills 28Teammates Status HP, oxygen,speed, peek type, alive state, body state,etc.。30Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Character ID,team ID,size,skills 28Enemies Status HP, oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Monster type,size 8Monsters Status HP, max HP, HP percent, target type 6Pose Position, rotation, relative position, distance 162.Global feature Scalar Includes circle, event, door and supply 65Status State,pain,radius 4Circle Position Blue circle, white circle 6Time existence time, rest time, total time, delay time, appear time 5Attribute Type,damage,elapsed time 8Event Position Occurred position 3Attribute Type,state 5Door Pose Position,relative position,rotation 8Status Type,size,quantity,etc.。19Supply Position Position,relative position,distance 73.Invisible enemy feature Scalar Invisible (nearby) enemy feature only for value estimation 104Attribute Character ID,team ID,size,skills 28Invisible enemies Status HP,oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。434.Spatial feature Scalar BEV 12288BEV Region Altitude map and aerial view map 3x64x64Table 4: The details of features in the observation space.In the case of roa, the objec-tive is to promote natural naviga-tion and forestall the agent fromencountering obstacles, such asstones and trees.。Regarding theimplementation, penalties areimposed on the agent for devia-tions from the optimal path.. \n",
      "the piece 734\n",
      " \n",
      "thm 3, these factors are broadly classified into three categories: basic rewards rb, obstacle avoidance rewardsroa, and goal achievement reward rg.。The basic rewards are primarily directed at steering the agent towards enhancedgame-playing performance, encompassing collaborative engagement with teammates and eliminating adversaries, etc.Feature Class Field Description Dimension1.Unit feature Scalar Includes heroes, teammates, enemies, monster 527Attribute Character ID,team ID,size,skills 28Status HP, oxygen,speed, peek type, alive state, body state,etc.。44Heroes Pose Position, rotation, camera position, camera rotation, etc.。76Item Backpack,weapon 144Attribute Character ID,team ID,size,skills 28Teammates Status HP, oxygen,speed, peek type, alive state, body state,etc.。30Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Character ID,team ID,size,skills 28Enemies Status HP, oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Monster type,size 8Monsters Status HP, max HP, HP percent, target type 6Pose Position, rotation, relative position, distance 162.Global feature Scalar Includes circle, event, door and supply 65Status State,pain,radius 4Circle Position Blue circle, white circle 6Time existence time, rest time, total time, delay time, appear time 5Attribute Type,damage,elapsed time 8Event Position Occurred position 3Attribute Type,state 5Door Pose Position,relative position,rotation 8Status Type,size,quantity,etc.。19Supply Position Position,relative position,distance 73.Invisible enemy feature Scalar Invisible (nearby) enemy feature only for value estimation 104Attribute Character ID,team ID,size,skills 28Invisible enemies Status HP,oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。434.Spatial feature Scalar BEV 12288BEV Region Altitude map and aerial view map 3x64x64Table 4: The details of features in the observation space.In the case of roa, the objec-tive is to promote natural naviga-tion and forestall the agent fromencountering obstacles, such asstones and trees.。Regarding theimplementation, penalties areimposed on the agent for devia-tions from the optimal path.。Thisoptimal trajectory is determinedby assessing the cosine similar-ity between the agent’s currentmovement direction, a 2D unitvector, provided as an environ-mental signal, and the expectedobstacle-free trajectory derivedfrom the action sequence in thetrajectory:roat=denvt·d⋆t∥denvt∥2∗ ∥d⋆t∥2−1,(8)where denvt the actual move-ment direction of the agent thatcan be directly accessed fromthe environment feedback, d⋆tthe expected movement direc-tion, which is derived by com-bining the expected movementdirection from the previous mo-ment with the movement actiontaken at the current moment.. \n",
      "the piece 735\n",
      " \n",
      "three categories: basic rewards rb, obstacle avoidance rewardsroa, and goal achievement reward rg.。The basic rewards are primarily directed at steering the agent towards enhancedgame-playing performance, encompassing collaborative engagement with teammates and eliminating adversaries, etc.Feature Class Field Description Dimension1.Unit feature Scalar Includes heroes, teammates, enemies, monster 527Attribute Character ID,team ID,size,skills 28Status HP, oxygen,speed, peek type, alive state, body state,etc.。44Heroes Pose Position, rotation, camera position, camera rotation, etc.。76Item Backpack,weapon 144Attribute Character ID,team ID,size,skills 28Teammates Status HP, oxygen,speed, peek type, alive state, body state,etc.。30Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Character ID,team ID,size,skills 28Enemies Status HP, oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Monster type,size 8Monsters Status HP, max HP, HP percent, target type 6Pose Position, rotation, relative position, distance 162.Global feature Scalar Includes circle, event, door and supply 65Status State,pain,radius 4Circle Position Blue circle, white circle 6Time existence time, rest time, total time, delay time, appear time 5Attribute Type,damage,elapsed time 8Event Position Occurred position 3Attribute Type,state 5Door Pose Position,relative position,rotation 8Status Type,size,quantity,etc.。19Supply Position Position,relative position,distance 73.Invisible enemy feature Scalar Invisible (nearby) enemy feature only for value estimation 104Attribute Character ID,team ID,size,skills 28Invisible enemies Status HP,oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。434.Spatial feature Scalar BEV 12288BEV Region Altitude map and aerial view map 3x64x64Table 4: The details of features in the observation space.In the case of roa, the objec-tive is to promote natural naviga-tion and forestall the agent fromencountering obstacles, such asstones and trees.。Regarding theimplementation, penalties areimposed on the agent for devia-tions from the optimal path.。Thisoptimal trajectory is determinedby assessing the cosine similar-ity between the agent’s currentmovement direction, a 2D unitvector, provided as an environ-mental signal, and the expectedobstacle-free trajectory derivedfrom the action sequence in thetrajectory:roat=denvt·d⋆t∥denvt∥2∗ ∥d⋆t∥2−1,(8)where denvt the actual move-ment direction of the agent thatcan be directly accessed fromthe environment feedback, d⋆tthe expected movement direc-tion, which is derived by com-bining the expected movementdirection from the previous mo-ment with the movement actiontaken at the current moment.。Forthe convenience, we summarizethe corresponding pseudo-code in Algorithm 2.. \n",
      "the piece 736\n",
      " \n",
      "mpassing collaborative engagement with teammates and eliminating adversaries, etc.Feature Class Field Description Dimension1.Unit feature Scalar Includes heroes, teammates, enemies, monster 527Attribute Character ID,team ID,size,skills 28Status HP, oxygen,speed, peek type, alive state, body state,etc.。44Heroes Pose Position, rotation, camera position, camera rotation, etc.。76Item Backpack,weapon 144Attribute Character ID,team ID,size,skills 28Teammates Status HP, oxygen,speed, peek type, alive state, body state,etc.。30Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Character ID,team ID,size,skills 28Enemies Status HP, oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Monster type,size 8Monsters Status HP, max HP, HP percent, target type 6Pose Position, rotation, relative position, distance 162.Global feature Scalar Includes circle, event, door and supply 65Status State,pain,radius 4Circle Position Blue circle, white circle 6Time existence time, rest time, total time, delay time, appear time 5Attribute Type,damage,elapsed time 8Event Position Occurred position 3Attribute Type,state 5Door Pose Position,relative position,rotation 8Status Type,size,quantity,etc.。19Supply Position Position,relative position,distance 73.Invisible enemy feature Scalar Invisible (nearby) enemy feature only for value estimation 104Attribute Character ID,team ID,size,skills 28Invisible enemies Status HP,oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。434.Spatial feature Scalar BEV 12288BEV Region Altitude map and aerial view map 3x64x64Table 4: The details of features in the observation space.In the case of roa, the objec-tive is to promote natural naviga-tion and forestall the agent fromencountering obstacles, such asstones and trees.。Regarding theimplementation, penalties areimposed on the agent for devia-tions from the optimal path.。Thisoptimal trajectory is determinedby assessing the cosine similar-ity between the agent’s currentmovement direction, a 2D unitvector, provided as an environ-mental signal, and the expectedobstacle-free trajectory derivedfrom the action sequence in thetrajectory:roat=denvt·d⋆t∥denvt∥2∗ ∥d⋆t∥2−1,(8)where denvt the actual move-ment direction of the agent thatcan be directly accessed fromthe environment feedback, d⋆tthe expected movement direc-tion, which is derived by com-bining the expected movementdirection from the previous mo-ment with the movement actiontaken at the current moment.。Forthe convenience, we summarizethe corresponding pseudo-code in Algorithm 2.。For the line 5 in Algorithm 2, we use ∆tto denote the shifting from agentmovement direction action atto the camera vision degree ω.. \n",
      "the piece 737\n",
      " \n",
      ", enemies, monster 527Attribute Character ID,team ID,size,skills 28Status HP, oxygen,speed, peek type, alive state, body state,etc.。44Heroes Pose Position, rotation, camera position, camera rotation, etc.。76Item Backpack,weapon 144Attribute Character ID,team ID,size,skills 28Teammates Status HP, oxygen,speed, peek type, alive state, body state,etc.。30Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Character ID,team ID,size,skills 28Enemies Status HP, oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Monster type,size 8Monsters Status HP, max HP, HP percent, target type 6Pose Position, rotation, relative position, distance 162.Global feature Scalar Includes circle, event, door and supply 65Status State,pain,radius 4Circle Position Blue circle, white circle 6Time existence time, rest time, total time, delay time, appear time 5Attribute Type,damage,elapsed time 8Event Position Occurred position 3Attribute Type,state 5Door Pose Position,relative position,rotation 8Status Type,size,quantity,etc.。19Supply Position Position,relative position,distance 73.Invisible enemy feature Scalar Invisible (nearby) enemy feature only for value estimation 104Attribute Character ID,team ID,size,skills 28Invisible enemies Status HP,oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。434.Spatial feature Scalar BEV 12288BEV Region Altitude map and aerial view map 3x64x64Table 4: The details of features in the observation space.In the case of roa, the objec-tive is to promote natural naviga-tion and forestall the agent fromencountering obstacles, such asstones and trees.。Regarding theimplementation, penalties areimposed on the agent for devia-tions from the optimal path.。Thisoptimal trajectory is determinedby assessing the cosine similar-ity between the agent’s currentmovement direction, a 2D unitvector, provided as an environ-mental signal, and the expectedobstacle-free trajectory derivedfrom the action sequence in thetrajectory:roat=denvt·d⋆t∥denvt∥2∗ ∥d⋆t∥2−1,(8)where denvt the actual move-ment direction of the agent thatcan be directly accessed fromthe environment feedback, d⋆tthe expected movement direc-tion, which is derived by com-bining the expected movementdirection from the previous mo-ment with the movement actiontaken at the current moment.。Forthe convenience, we summarizethe corresponding pseudo-code in Algorithm 2.。For the line 5 in Algorithm 2, we use ∆tto denote the shifting from agentmovement direction action atto the camera vision degree ω.。To address the issue of the agent getting stuck on obstacles dueto short-term action sequences, we employ a smaller γfor the corresponding value head.. \n",
      "the piece 738\n",
      " \n",
      "tc.。43Attribute Character ID,team ID,size,skills 28Enemies Status HP, oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Monster type,size 8Monsters Status HP, max HP, HP percent, target type 6Pose Position, rotation, relative position, distance 162.Global feature Scalar Includes circle, event, door and supply 65Status State,pain,radius 4Circle Position Blue circle, white circle 6Time existence time, rest time, total time, delay time, appear time 5Attribute Type,damage,elapsed time 8Event Position Occurred position 3Attribute Type,state 5Door Pose Position,relative position,rotation 8Status Type,size,quantity,etc.。19Supply Position Position,relative position,distance 73.Invisible enemy feature Scalar Invisible (nearby) enemy feature only for value estimation 104Attribute Character ID,team ID,size,skills 28Invisible enemies Status HP,oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。434.Spatial feature Scalar BEV 12288BEV Region Altitude map and aerial view map 3x64x64Table 4: The details of features in the observation space.In the case of roa, the objec-tive is to promote natural naviga-tion and forestall the agent fromencountering obstacles, such asstones and trees.。Regarding theimplementation, penalties areimposed on the agent for devia-tions from the optimal path.。Thisoptimal trajectory is determinedby assessing the cosine similar-ity between the agent’s currentmovement direction, a 2D unitvector, provided as an environ-mental signal, and the expectedobstacle-free trajectory derivedfrom the action sequence in thetrajectory:roat=denvt·d⋆t∥denvt∥2∗ ∥d⋆t∥2−1,(8)where denvt the actual move-ment direction of the agent thatcan be directly accessed fromthe environment feedback, d⋆tthe expected movement direc-tion, which is derived by com-bining the expected movementdirection from the previous mo-ment with the movement actiontaken at the current moment.。Forthe convenience, we summarizethe corresponding pseudo-code in Algorithm 2.。For the line 5 in Algorithm 2, we use ∆tto denote the shifting from agentmovement direction action atto the camera vision degree ω.。To address the issue of the agent getting stuck on obstacles dueto short-term action sequences, we employ a smaller γfor the corresponding value head.。Specifically, we set γto 0.92.. \n",
      "the piece 739\n",
      " \n",
      "en,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。43Attribute Monster type,size 8Monsters Status HP, max HP, HP percent, target type 6Pose Position, rotation, relative position, distance 162.Global feature Scalar Includes circle, event, door and supply 65Status State,pain,radius 4Circle Position Blue circle, white circle 6Time existence time, rest time, total time, delay time, appear time 5Attribute Type,damage,elapsed time 8Event Position Occurred position 3Attribute Type,state 5Door Pose Position,relative position,rotation 8Status Type,size,quantity,etc.。19Supply Position Position,relative position,distance 73.Invisible enemy feature Scalar Invisible (nearby) enemy feature only for value estimation 104Attribute Character ID,team ID,size,skills 28Invisible enemies Status HP,oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。434.Spatial feature Scalar BEV 12288BEV Region Altitude map and aerial view map 3x64x64Table 4: The details of features in the observation space.In the case of roa, the objec-tive is to promote natural naviga-tion and forestall the agent fromencountering obstacles, such asstones and trees.。Regarding theimplementation, penalties areimposed on the agent for devia-tions from the optimal path.。Thisoptimal trajectory is determinedby assessing the cosine similar-ity between the agent’s currentmovement direction, a 2D unitvector, provided as an environ-mental signal, and the expectedobstacle-free trajectory derivedfrom the action sequence in thetrajectory:roat=denvt·d⋆t∥denvt∥2∗ ∥d⋆t∥2−1,(8)where denvt the actual move-ment direction of the agent thatcan be directly accessed fromthe environment feedback, d⋆tthe expected movement direc-tion, which is derived by com-bining the expected movementdirection from the previous mo-ment with the movement actiontaken at the current moment.。Forthe convenience, we summarizethe corresponding pseudo-code in Algorithm 2.。For the line 5 in Algorithm 2, we use ∆tto denote the shifting from agentmovement direction action atto the camera vision degree ω.。To address the issue of the agent getting stuck on obstacles dueto short-term action sequences, we employ a smaller γfor the corresponding value head.。Specifically, we set γto 0.92.。Thisadjustment helps mitigate the impact of the obstacle avoidance reward on long-term credit assignment, allowing for a morebalanced consideration of immediate and future rewards in the agent’s decision-making process.. \n",
      "the piece 740\n",
      " \n",
      "Monsters Status HP, max HP, HP percent, target type 6Pose Position, rotation, relative position, distance 162.Global feature Scalar Includes circle, event, door and supply 65Status State,pain,radius 4Circle Position Blue circle, white circle 6Time existence time, rest time, total time, delay time, appear time 5Attribute Type,damage,elapsed time 8Event Position Occurred position 3Attribute Type,state 5Door Pose Position,relative position,rotation 8Status Type,size,quantity,etc.。19Supply Position Position,relative position,distance 73.Invisible enemy feature Scalar Invisible (nearby) enemy feature only for value estimation 104Attribute Character ID,team ID,size,skills 28Invisible enemies Status HP,oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。434.Spatial feature Scalar BEV 12288BEV Region Altitude map and aerial view map 3x64x64Table 4: The details of features in the observation space.In the case of roa, the objec-tive is to promote natural naviga-tion and forestall the agent fromencountering obstacles, such asstones and trees.。Regarding theimplementation, penalties areimposed on the agent for devia-tions from the optimal path.。Thisoptimal trajectory is determinedby assessing the cosine similar-ity between the agent’s currentmovement direction, a 2D unitvector, provided as an environ-mental signal, and the expectedobstacle-free trajectory derivedfrom the action sequence in thetrajectory:roat=denvt·d⋆t∥denvt∥2∗ ∥d⋆t∥2−1,(8)where denvt the actual move-ment direction of the agent thatcan be directly accessed fromthe environment feedback, d⋆tthe expected movement direc-tion, which is derived by com-bining the expected movementdirection from the previous mo-ment with the movement actiontaken at the current moment.。Forthe convenience, we summarizethe corresponding pseudo-code in Algorithm 2.。For the line 5 in Algorithm 2, we use ∆tto denote the shifting from agentmovement direction action atto the camera vision degree ω.。To address the issue of the agent getting stuck on obstacles dueto short-term action sequences, we employ a smaller γfor the corresponding value head.。Specifically, we set γto 0.92.。Thisadjustment helps mitigate the impact of the obstacle avoidance reward on long-term credit assignment, allowing for a morebalanced consideration of immediate and future rewards in the agent’s decision-making process.。As for the goal-achievingreward, we’ve introduced in the main text, please refer to Section 4.2.B.. \n",
      "the piece 741\n",
      " \n",
      " rotation, relative position, distance 162.Global feature Scalar Includes circle, event, door and supply 65Status State,pain,radius 4Circle Position Blue circle, white circle 6Time existence time, rest time, total time, delay time, appear time 5Attribute Type,damage,elapsed time 8Event Position Occurred position 3Attribute Type,state 5Door Pose Position,relative position,rotation 8Status Type,size,quantity,etc.。19Supply Position Position,relative position,distance 73.Invisible enemy feature Scalar Invisible (nearby) enemy feature only for value estimation 104Attribute Character ID,team ID,size,skills 28Invisible enemies Status HP,oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。434.Spatial feature Scalar BEV 12288BEV Region Altitude map and aerial view map 3x64x64Table 4: The details of features in the observation space.In the case of roa, the objec-tive is to promote natural naviga-tion and forestall the agent fromencountering obstacles, such asstones and trees.。Regarding theimplementation, penalties areimposed on the agent for devia-tions from the optimal path.。Thisoptimal trajectory is determinedby assessing the cosine similar-ity between the agent’s currentmovement direction, a 2D unitvector, provided as an environ-mental signal, and the expectedobstacle-free trajectory derivedfrom the action sequence in thetrajectory:roat=denvt·d⋆t∥denvt∥2∗ ∥d⋆t∥2−1,(8)where denvt the actual move-ment direction of the agent thatcan be directly accessed fromthe environment feedback, d⋆tthe expected movement direc-tion, which is derived by com-bining the expected movementdirection from the previous mo-ment with the movement actiontaken at the current moment.。Forthe convenience, we summarizethe corresponding pseudo-code in Algorithm 2.。For the line 5 in Algorithm 2, we use ∆tto denote the shifting from agentmovement direction action atto the camera vision degree ω.。To address the issue of the agent getting stuck on obstacles dueto short-term action sequences, we employ a smaller γfor the corresponding value head.。Specifically, we set γto 0.92.。Thisadjustment helps mitigate the impact of the obstacle avoidance reward on long-term credit assignment, allowing for a morebalanced consideration of immediate and future rewards in the agent’s decision-making process.。As for the goal-achievingreward, we’ve introduced in the main text, please refer to Section 4.2.B.。Environmental State Abstraction and Goal SpaceFor a comprehensive understanding of the game environment, a language model undergoes a fine-tuning process dueto the scarcity of textual information within the simulation environment.. \n",
      "the piece 742\n",
      " \n",
      "ain,radius 4Circle Position Blue circle, white circle 6Time existence time, rest time, total time, delay time, appear time 5Attribute Type,damage,elapsed time 8Event Position Occurred position 3Attribute Type,state 5Door Pose Position,relative position,rotation 8Status Type,size,quantity,etc.。19Supply Position Position,relative position,distance 73.Invisible enemy feature Scalar Invisible (nearby) enemy feature only for value estimation 104Attribute Character ID,team ID,size,skills 28Invisible enemies Status HP,oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。434.Spatial feature Scalar BEV 12288BEV Region Altitude map and aerial view map 3x64x64Table 4: The details of features in the observation space.In the case of roa, the objec-tive is to promote natural naviga-tion and forestall the agent fromencountering obstacles, such asstones and trees.。Regarding theimplementation, penalties areimposed on the agent for devia-tions from the optimal path.。Thisoptimal trajectory is determinedby assessing the cosine similar-ity between the agent’s currentmovement direction, a 2D unitvector, provided as an environ-mental signal, and the expectedobstacle-free trajectory derivedfrom the action sequence in thetrajectory:roat=denvt·d⋆t∥denvt∥2∗ ∥d⋆t∥2−1,(8)where denvt the actual move-ment direction of the agent thatcan be directly accessed fromthe environment feedback, d⋆tthe expected movement direc-tion, which is derived by com-bining the expected movementdirection from the previous mo-ment with the movement actiontaken at the current moment.。Forthe convenience, we summarizethe corresponding pseudo-code in Algorithm 2.。For the line 5 in Algorithm 2, we use ∆tto denote the shifting from agentmovement direction action atto the camera vision degree ω.。To address the issue of the agent getting stuck on obstacles dueto short-term action sequences, we employ a smaller γfor the corresponding value head.。Specifically, we set γto 0.92.。Thisadjustment helps mitigate the impact of the obstacle avoidance reward on long-term credit assignment, allowing for a morebalanced consideration of immediate and future rewards in the agent’s decision-making process.。As for the goal-achievingreward, we’ve introduced in the main text, please refer to Section 4.2.B.。Environmental State Abstraction and Goal SpaceFor a comprehensive understanding of the game environment, a language model undergoes a fine-tuning process dueto the scarcity of textual information within the simulation environment.。The need arises to articulate non-linguisticelements, and the interaction between an agent and the simulation environment is centered on continuously observ-ing the environment’s state and generating corresponding actions.. \n",
      "the piece 743\n",
      " \n",
      "ce time, rest time, total time, delay time, appear time 5Attribute Type,damage,elapsed time 8Event Position Occurred position 3Attribute Type,state 5Door Pose Position,relative position,rotation 8Status Type,size,quantity,etc.。19Supply Position Position,relative position,distance 73.Invisible enemy feature Scalar Invisible (nearby) enemy feature only for value estimation 104Attribute Character ID,team ID,size,skills 28Invisible enemies Status HP,oxygen,speed, peek type, alive state, body state,etc.。33Pose Position, rotation, camera position, camera rotation, etc.。434.Spatial feature Scalar BEV 12288BEV Region Altitude map and aerial view map 3x64x64Table 4: The details of features in the observation space.In the case of roa, the objec-tive is to promote natural naviga-tion and forestall the agent fromencountering obstacles, such asstones and trees.。Regarding theimplementation, penalties areimposed on the agent for devia-tions from the optimal path.。Thisoptimal trajectory is determinedby assessing the cosine similar-ity between the agent’s currentmovement direction, a 2D unitvector, provided as an environ-mental signal, and the expectedobstacle-free trajectory derivedfrom the action sequence in thetrajectory:roat=denvt·d⋆t∥denvt∥2∗ ∥d⋆t∥2−1,(8)where denvt the actual move-ment direction of the agent thatcan be directly accessed fromthe environment feedback, d⋆tthe expected movement direc-tion, which is derived by com-bining the expected movementdirection from the previous mo-ment with the movement actiontaken at the current moment.。Forthe convenience, we summarizethe corresponding pseudo-code in Algorithm 2.。For the line 5 in Algorithm 2, we use ∆tto denote the shifting from agentmovement direction action atto the camera vision degree ω.。To address the issue of the agent getting stuck on obstacles dueto short-term action sequences, we employ a smaller γfor the corresponding value head.。Specifically, we set γto 0.92.。Thisadjustment helps mitigate the impact of the obstacle avoidance reward on long-term credit assignment, allowing for a morebalanced consideration of immediate and future rewards in the agent’s decision-making process.。As for the goal-achievingreward, we’ve introduced in the main text, please refer to Section 4.2.B.。Environmental State Abstraction and Goal SpaceFor a comprehensive understanding of the game environment, a language model undergoes a fine-tuning process dueto the scarcity of textual information within the simulation environment.。The need arises to articulate non-linguisticelements, and the interaction between an agent and the simulation environment is centered on continuously observ-ing the environment’s state and generating corresponding actions.。Therefore, the key aspects requiring verbalizationprimarily involve the state and actions of the agent.. \n",
      "the piece 744\n",
      " \n",
      "ition, rotation, camera position, camera rotation, etc.。434.Spatial feature Scalar BEV 12288BEV Region Altitude map and aerial view map 3x64x64Table 4: The details of features in the observation space.In the case of roa, the objec-tive is to promote natural naviga-tion and forestall the agent fromencountering obstacles, such asstones and trees.。Regarding theimplementation, penalties areimposed on the agent for devia-tions from the optimal path.。Thisoptimal trajectory is determinedby assessing the cosine similar-ity between the agent’s currentmovement direction, a 2D unitvector, provided as an environ-mental signal, and the expectedobstacle-free trajectory derivedfrom the action sequence in thetrajectory:roat=denvt·d⋆t∥denvt∥2∗ ∥d⋆t∥2−1,(8)where denvt the actual move-ment direction of the agent thatcan be directly accessed fromthe environment feedback, d⋆tthe expected movement direc-tion, which is derived by com-bining the expected movementdirection from the previous mo-ment with the movement actiontaken at the current moment.。Forthe convenience, we summarizethe corresponding pseudo-code in Algorithm 2.。For the line 5 in Algorithm 2, we use ∆tto denote the shifting from agentmovement direction action atto the camera vision degree ω.。To address the issue of the agent getting stuck on obstacles dueto short-term action sequences, we employ a smaller γfor the corresponding value head.。Specifically, we set γto 0.92.。Thisadjustment helps mitigate the impact of the obstacle avoidance reward on long-term credit assignment, allowing for a morebalanced consideration of immediate and future rewards in the agent’s decision-making process.。As for the goal-achievingreward, we’ve introduced in the main text, please refer to Section 4.2.B.。Environmental State Abstraction and Goal SpaceFor a comprehensive understanding of the game environment, a language model undergoes a fine-tuning process dueto the scarcity of textual information within the simulation environment.。The need arises to articulate non-linguisticelements, and the interaction between an agent and the simulation environment is centered on continuously observ-ing the environment’s state and generating corresponding actions.。Therefore, the key aspects requiring verbalizationprimarily involve the state and actions of the agent.。However, given the abundance of possible observation states inthe simulation environment, it is impractical to use all of these states directly as prompts for the language model, es-pecially considering token limitations.. \n",
      "the piece 745\n",
      " \n",
      "stall the agent fromencountering obstacles, such asstones and trees.。Regarding theimplementation, penalties areimposed on the agent for devia-tions from the optimal path.。Thisoptimal trajectory is determinedby assessing the cosine similar-ity between the agent’s currentmovement direction, a 2D unitvector, provided as an environ-mental signal, and the expectedobstacle-free trajectory derivedfrom the action sequence in thetrajectory:roat=denvt·d⋆t∥denvt∥2∗ ∥d⋆t∥2−1,(8)where denvt the actual move-ment direction of the agent thatcan be directly accessed fromthe environment feedback, d⋆tthe expected movement direc-tion, which is derived by com-bining the expected movementdirection from the previous mo-ment with the movement actiontaken at the current moment.。Forthe convenience, we summarizethe corresponding pseudo-code in Algorithm 2.。For the line 5 in Algorithm 2, we use ∆tto denote the shifting from agentmovement direction action atto the camera vision degree ω.。To address the issue of the agent getting stuck on obstacles dueto short-term action sequences, we employ a smaller γfor the corresponding value head.。Specifically, we set γto 0.92.。Thisadjustment helps mitigate the impact of the obstacle avoidance reward on long-term credit assignment, allowing for a morebalanced consideration of immediate and future rewards in the agent’s decision-making process.。As for the goal-achievingreward, we’ve introduced in the main text, please refer to Section 4.2.B.。Environmental State Abstraction and Goal SpaceFor a comprehensive understanding of the game environment, a language model undergoes a fine-tuning process dueto the scarcity of textual information within the simulation environment.。The need arises to articulate non-linguisticelements, and the interaction between an agent and the simulation environment is centered on continuously observ-ing the environment’s state and generating corresponding actions.。Therefore, the key aspects requiring verbalizationprimarily involve the state and actions of the agent.。However, given the abundance of possible observation states inthe simulation environment, it is impractical to use all of these states directly as prompts for the language model, es-pecially considering token limitations.。Consequently, there is a crucial need to extract and linguistically transformthe most significant meta-states to facilitate successful model interaction.. \n",
      "the piece 746\n",
      " \n",
      ".。Regarding theimplementation, penalties areimposed on the agent for devia-tions from the optimal path.。Thisoptimal trajectory is determinedby assessing the cosine similar-ity between the agent’s currentmovement direction, a 2D unitvector, provided as an environ-mental signal, and the expectedobstacle-free trajectory derivedfrom the action sequence in thetrajectory:roat=denvt·d⋆t∥denvt∥2∗ ∥d⋆t∥2−1,(8)where denvt the actual move-ment direction of the agent thatcan be directly accessed fromthe environment feedback, d⋆tthe expected movement direc-tion, which is derived by com-bining the expected movementdirection from the previous mo-ment with the movement actiontaken at the current moment.。Forthe convenience, we summarizethe corresponding pseudo-code in Algorithm 2.。For the line 5 in Algorithm 2, we use ∆tto denote the shifting from agentmovement direction action atto the camera vision degree ω.。To address the issue of the agent getting stuck on obstacles dueto short-term action sequences, we employ a smaller γfor the corresponding value head.。Specifically, we set γto 0.92.。Thisadjustment helps mitigate the impact of the obstacle avoidance reward on long-term credit assignment, allowing for a morebalanced consideration of immediate and future rewards in the agent’s decision-making process.。As for the goal-achievingreward, we’ve introduced in the main text, please refer to Section 4.2.B.。Environmental State Abstraction and Goal SpaceFor a comprehensive understanding of the game environment, a language model undergoes a fine-tuning process dueto the scarcity of textual information within the simulation environment.。The need arises to articulate non-linguisticelements, and the interaction between an agent and the simulation environment is centered on continuously observ-ing the environment’s state and generating corresponding actions.。Therefore, the key aspects requiring verbalizationprimarily involve the state and actions of the agent.。However, given the abundance of possible observation states inthe simulation environment, it is impractical to use all of these states directly as prompts for the language model, es-pecially considering token limitations.。Consequently, there is a crucial need to extract and linguistically transformthe most significant meta-states to facilitate successful model interaction.。It is noteworthy that smaller languageBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 2 CALCULATION OF d⋆tInput: d⋆t−1,atOutput: d⋆t1:ifat∈movement direction action space then2:d⋆t←\u0014cos(at)sin(at)\u00153:else if at∈yaw direction action space then4:5: ∆t=at−ω6:d⋆t←\u0014cos(∆ t)−sin(∆ t)sin(∆ t) cos(∆ t)\u0015d⋆t−17:else if at∈{stop; rescue; stop and fire; stop adjust and fire } then8:d⋆t←\u00140\u00159:else10: d⋆t←d⋆t−111:end ifmodels have limitations in comprehending and manipulating numerical values effectively.. \n",
      "the piece 747\n",
      " \n",
      "edobstacle-free trajectory derivedfrom the action sequence in thetrajectory:roat=denvt·d⋆t∥denvt∥2∗ ∥d⋆t∥2−1,(8)where denvt the actual move-ment direction of the agent thatcan be directly accessed fromthe environment feedback, d⋆tthe expected movement direc-tion, which is derived by com-bining the expected movementdirection from the previous mo-ment with the movement actiontaken at the current moment.。Forthe convenience, we summarizethe corresponding pseudo-code in Algorithm 2.。For the line 5 in Algorithm 2, we use ∆tto denote the shifting from agentmovement direction action atto the camera vision degree ω.。To address the issue of the agent getting stuck on obstacles dueto short-term action sequences, we employ a smaller γfor the corresponding value head.。Specifically, we set γto 0.92.。Thisadjustment helps mitigate the impact of the obstacle avoidance reward on long-term credit assignment, allowing for a morebalanced consideration of immediate and future rewards in the agent’s decision-making process.。As for the goal-achievingreward, we’ve introduced in the main text, please refer to Section 4.2.B.。Environmental State Abstraction and Goal SpaceFor a comprehensive understanding of the game environment, a language model undergoes a fine-tuning process dueto the scarcity of textual information within the simulation environment.。The need arises to articulate non-linguisticelements, and the interaction between an agent and the simulation environment is centered on continuously observ-ing the environment’s state and generating corresponding actions.。Therefore, the key aspects requiring verbalizationprimarily involve the state and actions of the agent.。However, given the abundance of possible observation states inthe simulation environment, it is impractical to use all of these states directly as prompts for the language model, es-pecially considering token limitations.。Consequently, there is a crucial need to extract and linguistically transformthe most significant meta-states to facilitate successful model interaction.。It is noteworthy that smaller languageBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 2 CALCULATION OF d⋆tInput: d⋆t−1,atOutput: d⋆t1:ifat∈movement direction action space then2:d⋆t←\u0014cos(at)sin(at)\u00153:else if at∈yaw direction action space then4:5: ∆t=at−ω6:d⋆t←\u0014cos(∆ t)−sin(∆ t)sin(∆ t) cos(∆ t)\u0015d⋆t−17:else if at∈{stop; rescue; stop and fire; stop adjust and fire } then8:d⋆t←\u00140\u00159:else10: d⋆t←d⋆t−111:end ifmodels have limitations in comprehending and manipulating numerical values effectively.。To address this challenge,a deliberate effort is made to minimize the use of numerical values during the environmental linguistics process.Sub Action Space Dim Sizemovement direction 16yaw direction 16pitch direction 3body action 9basic action 7switch weapon action 3Table 6: Action space.For example, instead of specifying an agent’s speed with specific numeric metricslike “speed: 1m/s →3m/s” a qualitative representation such as “speed: slower →faster” is adopted.. \n",
      "the piece 748\n",
      " \n",
      "t∥2−1,(8)where denvt the actual move-ment direction of the agent thatcan be directly accessed fromthe environment feedback, d⋆tthe expected movement direc-tion, which is derived by com-bining the expected movementdirection from the previous mo-ment with the movement actiontaken at the current moment.。Forthe convenience, we summarizethe corresponding pseudo-code in Algorithm 2.。For the line 5 in Algorithm 2, we use ∆tto denote the shifting from agentmovement direction action atto the camera vision degree ω.。To address the issue of the agent getting stuck on obstacles dueto short-term action sequences, we employ a smaller γfor the corresponding value head.。Specifically, we set γto 0.92.。Thisadjustment helps mitigate the impact of the obstacle avoidance reward on long-term credit assignment, allowing for a morebalanced consideration of immediate and future rewards in the agent’s decision-making process.。As for the goal-achievingreward, we’ve introduced in the main text, please refer to Section 4.2.B.。Environmental State Abstraction and Goal SpaceFor a comprehensive understanding of the game environment, a language model undergoes a fine-tuning process dueto the scarcity of textual information within the simulation environment.。The need arises to articulate non-linguisticelements, and the interaction between an agent and the simulation environment is centered on continuously observ-ing the environment’s state and generating corresponding actions.。Therefore, the key aspects requiring verbalizationprimarily involve the state and actions of the agent.。However, given the abundance of possible observation states inthe simulation environment, it is impractical to use all of these states directly as prompts for the language model, es-pecially considering token limitations.。Consequently, there is a crucial need to extract and linguistically transformthe most significant meta-states to facilitate successful model interaction.。It is noteworthy that smaller languageBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 2 CALCULATION OF d⋆tInput: d⋆t−1,atOutput: d⋆t1:ifat∈movement direction action space then2:d⋆t←\u0014cos(at)sin(at)\u00153:else if at∈yaw direction action space then4:5: ∆t=at−ω6:d⋆t←\u0014cos(∆ t)−sin(∆ t)sin(∆ t) cos(∆ t)\u0015d⋆t−17:else if at∈{stop; rescue; stop and fire; stop adjust and fire } then8:d⋆t←\u00140\u00159:else10: d⋆t←d⋆t−111:end ifmodels have limitations in comprehending and manipulating numerical values effectively.。To address this challenge,a deliberate effort is made to minimize the use of numerical values during the environmental linguistics process.Sub Action Space Dim Sizemovement direction 16yaw direction 16pitch direction 3body action 9basic action 7switch weapon action 3Table 6: Action space.For example, instead of specifying an agent’s speed with specific numeric metricslike “speed: 1m/s →3m/s” a qualitative representation such as “speed: slower →faster” is adopted.。This technique transforms the original continuous state into alimited, discrete meta-state, thereby enhancing the language model’s understanding.Similarly, for expediting language model understanding, a discrete action space isadopted, with each action accompanied by a succinct artificial language description.This discreet articulation of actions contributes to the overall interpretability of thelanguage model within the simulation environment.. \n",
      "the piece 749\n",
      " \n",
      "on sequences, we employ a smaller γfor the corresponding value head.。Specifically, we set γto 0.92.。Thisadjustment helps mitigate the impact of the obstacle avoidance reward on long-term credit assignment, allowing for a morebalanced consideration of immediate and future rewards in the agent’s decision-making process.。As for the goal-achievingreward, we’ve introduced in the main text, please refer to Section 4.2.B.。Environmental State Abstraction and Goal SpaceFor a comprehensive understanding of the game environment, a language model undergoes a fine-tuning process dueto the scarcity of textual information within the simulation environment.。The need arises to articulate non-linguisticelements, and the interaction between an agent and the simulation environment is centered on continuously observ-ing the environment’s state and generating corresponding actions.。Therefore, the key aspects requiring verbalizationprimarily involve the state and actions of the agent.。However, given the abundance of possible observation states inthe simulation environment, it is impractical to use all of these states directly as prompts for the language model, es-pecially considering token limitations.。Consequently, there is a crucial need to extract and linguistically transformthe most significant meta-states to facilitate successful model interaction.。It is noteworthy that smaller languageBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 2 CALCULATION OF d⋆tInput: d⋆t−1,atOutput: d⋆t1:ifat∈movement direction action space then2:d⋆t←\u0014cos(at)sin(at)\u00153:else if at∈yaw direction action space then4:5: ∆t=at−ω6:d⋆t←\u0014cos(∆ t)−sin(∆ t)sin(∆ t) cos(∆ t)\u0015d⋆t−17:else if at∈{stop; rescue; stop and fire; stop adjust and fire } then8:d⋆t←\u00140\u00159:else10: d⋆t←d⋆t−111:end ifmodels have limitations in comprehending and manipulating numerical values effectively.。To address this challenge,a deliberate effort is made to minimize the use of numerical values during the environmental linguistics process.Sub Action Space Dim Sizemovement direction 16yaw direction 16pitch direction 3body action 9basic action 7switch weapon action 3Table 6: Action space.For example, instead of specifying an agent’s speed with specific numeric metricslike “speed: 1m/s →3m/s” a qualitative representation such as “speed: slower →faster” is adopted.。This technique transforms the original continuous state into alimited, discrete meta-state, thereby enhancing the language model’s understanding.Similarly, for expediting language model understanding, a discrete action space isadopted, with each action accompanied by a succinct artificial language description.This discreet articulation of actions contributes to the overall interpretability of thelanguage model within the simulation environment.。We list the details in Table 7.C.. \n",
      "the piece 750\n",
      " \n",
      "ly, we set γto 0.92.。Thisadjustment helps mitigate the impact of the obstacle avoidance reward on long-term credit assignment, allowing for a morebalanced consideration of immediate and future rewards in the agent’s decision-making process.。As for the goal-achievingreward, we’ve introduced in the main text, please refer to Section 4.2.B.。Environmental State Abstraction and Goal SpaceFor a comprehensive understanding of the game environment, a language model undergoes a fine-tuning process dueto the scarcity of textual information within the simulation environment.。The need arises to articulate non-linguisticelements, and the interaction between an agent and the simulation environment is centered on continuously observ-ing the environment’s state and generating corresponding actions.。Therefore, the key aspects requiring verbalizationprimarily involve the state and actions of the agent.。However, given the abundance of possible observation states inthe simulation environment, it is impractical to use all of these states directly as prompts for the language model, es-pecially considering token limitations.。Consequently, there is a crucial need to extract and linguistically transformthe most significant meta-states to facilitate successful model interaction.。It is noteworthy that smaller languageBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 2 CALCULATION OF d⋆tInput: d⋆t−1,atOutput: d⋆t1:ifat∈movement direction action space then2:d⋆t←\u0014cos(at)sin(at)\u00153:else if at∈yaw direction action space then4:5: ∆t=at−ω6:d⋆t←\u0014cos(∆ t)−sin(∆ t)sin(∆ t) cos(∆ t)\u0015d⋆t−17:else if at∈{stop; rescue; stop and fire; stop adjust and fire } then8:d⋆t←\u00140\u00159:else10: d⋆t←d⋆t−111:end ifmodels have limitations in comprehending and manipulating numerical values effectively.。To address this challenge,a deliberate effort is made to minimize the use of numerical values during the environmental linguistics process.Sub Action Space Dim Sizemovement direction 16yaw direction 16pitch direction 3body action 9basic action 7switch weapon action 3Table 6: Action space.For example, instead of specifying an agent’s speed with specific numeric metricslike “speed: 1m/s →3m/s” a qualitative representation such as “speed: slower →faster” is adopted.。This technique transforms the original continuous state into alimited, discrete meta-state, thereby enhancing the language model’s understanding.Similarly, for expediting language model understanding, a discrete action space isadopted, with each action accompanied by a succinct artificial language description.This discreet articulation of actions contributes to the overall interpretability of thelanguage model within the simulation environment.。We list the details in Table 7.C.。Policy NetworkFigure 5 is the network architecture used for non-goal RL, corresponding to thepolicy π.. \n",
      "the piece 751\n",
      " \n",
      "ng for a morebalanced consideration of immediate and future rewards in the agent’s decision-making process.。As for the goal-achievingreward, we’ve introduced in the main text, please refer to Section 4.2.B.。Environmental State Abstraction and Goal SpaceFor a comprehensive understanding of the game environment, a language model undergoes a fine-tuning process dueto the scarcity of textual information within the simulation environment.。The need arises to articulate non-linguisticelements, and the interaction between an agent and the simulation environment is centered on continuously observ-ing the environment’s state and generating corresponding actions.。Therefore, the key aspects requiring verbalizationprimarily involve the state and actions of the agent.。However, given the abundance of possible observation states inthe simulation environment, it is impractical to use all of these states directly as prompts for the language model, es-pecially considering token limitations.。Consequently, there is a crucial need to extract and linguistically transformthe most significant meta-states to facilitate successful model interaction.。It is noteworthy that smaller languageBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 2 CALCULATION OF d⋆tInput: d⋆t−1,atOutput: d⋆t1:ifat∈movement direction action space then2:d⋆t←\u0014cos(at)sin(at)\u00153:else if at∈yaw direction action space then4:5: ∆t=at−ω6:d⋆t←\u0014cos(∆ t)−sin(∆ t)sin(∆ t) cos(∆ t)\u0015d⋆t−17:else if at∈{stop; rescue; stop and fire; stop adjust and fire } then8:d⋆t←\u00140\u00159:else10: d⋆t←d⋆t−111:end ifmodels have limitations in comprehending and manipulating numerical values effectively.。To address this challenge,a deliberate effort is made to minimize the use of numerical values during the environmental linguistics process.Sub Action Space Dim Sizemovement direction 16yaw direction 16pitch direction 3body action 9basic action 7switch weapon action 3Table 6: Action space.For example, instead of specifying an agent’s speed with specific numeric metricslike “speed: 1m/s →3m/s” a qualitative representation such as “speed: slower →faster” is adopted.。This technique transforms the original continuous state into alimited, discrete meta-state, thereby enhancing the language model’s understanding.Similarly, for expediting language model understanding, a discrete action space isadopted, with each action accompanied by a succinct artificial language description.This discreet articulation of actions contributes to the overall interpretability of thelanguage model within the simulation environment.。We list the details in Table 7.C.。Policy NetworkFigure 5 is the network architecture used for non-goal RL, corresponding to thepolicy π.。On top of observation preprocessing in Appendix A, we introduce abackbone implemented with a fully-connected layer followed by three Residual Blocks.. \n",
      "the piece 752\n",
      " \n",
      "duced in the main text, please refer to Section 4.2.B.。Environmental State Abstraction and Goal SpaceFor a comprehensive understanding of the game environment, a language model undergoes a fine-tuning process dueto the scarcity of textual information within the simulation environment.。The need arises to articulate non-linguisticelements, and the interaction between an agent and the simulation environment is centered on continuously observ-ing the environment’s state and generating corresponding actions.。Therefore, the key aspects requiring verbalizationprimarily involve the state and actions of the agent.。However, given the abundance of possible observation states inthe simulation environment, it is impractical to use all of these states directly as prompts for the language model, es-pecially considering token limitations.。Consequently, there is a crucial need to extract and linguistically transformthe most significant meta-states to facilitate successful model interaction.。It is noteworthy that smaller languageBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 2 CALCULATION OF d⋆tInput: d⋆t−1,atOutput: d⋆t1:ifat∈movement direction action space then2:d⋆t←\u0014cos(at)sin(at)\u00153:else if at∈yaw direction action space then4:5: ∆t=at−ω6:d⋆t←\u0014cos(∆ t)−sin(∆ t)sin(∆ t) cos(∆ t)\u0015d⋆t−17:else if at∈{stop; rescue; stop and fire; stop adjust and fire } then8:d⋆t←\u00140\u00159:else10: d⋆t←d⋆t−111:end ifmodels have limitations in comprehending and manipulating numerical values effectively.。To address this challenge,a deliberate effort is made to minimize the use of numerical values during the environmental linguistics process.Sub Action Space Dim Sizemovement direction 16yaw direction 16pitch direction 3body action 9basic action 7switch weapon action 3Table 6: Action space.For example, instead of specifying an agent’s speed with specific numeric metricslike “speed: 1m/s →3m/s” a qualitative representation such as “speed: slower →faster” is adopted.。This technique transforms the original continuous state into alimited, discrete meta-state, thereby enhancing the language model’s understanding.Similarly, for expediting language model understanding, a discrete action space isadopted, with each action accompanied by a succinct artificial language description.This discreet articulation of actions contributes to the overall interpretability of thelanguage model within the simulation environment.。We list the details in Table 7.C.。Policy NetworkFigure 5 is the network architecture used for non-goal RL, corresponding to thepolicy π.。On top of observation preprocessing in Appendix A, we introduce abackbone implemented with a fully-connected layer followed by three Residual Blocks.。As for the policy head and threevalue heads, we implemented each of them as two connected Residual Blocks.. \n",
      "the piece 753\n",
      " \n",
      "efer to Section 4.2.B.。Environmental State Abstraction and Goal SpaceFor a comprehensive understanding of the game environment, a language model undergoes a fine-tuning process dueto the scarcity of textual information within the simulation environment.。The need arises to articulate non-linguisticelements, and the interaction between an agent and the simulation environment is centered on continuously observ-ing the environment’s state and generating corresponding actions.。Therefore, the key aspects requiring verbalizationprimarily involve the state and actions of the agent.。However, given the abundance of possible observation states inthe simulation environment, it is impractical to use all of these states directly as prompts for the language model, es-pecially considering token limitations.。Consequently, there is a crucial need to extract and linguistically transformthe most significant meta-states to facilitate successful model interaction.。It is noteworthy that smaller languageBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 2 CALCULATION OF d⋆tInput: d⋆t−1,atOutput: d⋆t1:ifat∈movement direction action space then2:d⋆t←\u0014cos(at)sin(at)\u00153:else if at∈yaw direction action space then4:5: ∆t=at−ω6:d⋆t←\u0014cos(∆ t)−sin(∆ t)sin(∆ t) cos(∆ t)\u0015d⋆t−17:else if at∈{stop; rescue; stop and fire; stop adjust and fire } then8:d⋆t←\u00140\u00159:else10: d⋆t←d⋆t−111:end ifmodels have limitations in comprehending and manipulating numerical values effectively.。To address this challenge,a deliberate effort is made to minimize the use of numerical values during the environmental linguistics process.Sub Action Space Dim Sizemovement direction 16yaw direction 16pitch direction 3body action 9basic action 7switch weapon action 3Table 6: Action space.For example, instead of specifying an agent’s speed with specific numeric metricslike “speed: 1m/s →3m/s” a qualitative representation such as “speed: slower →faster” is adopted.。This technique transforms the original continuous state into alimited, discrete meta-state, thereby enhancing the language model’s understanding.Similarly, for expediting language model understanding, a discrete action space isadopted, with each action accompanied by a succinct artificial language description.This discreet articulation of actions contributes to the overall interpretability of thelanguage model within the simulation environment.。We list the details in Table 7.C.。Policy NetworkFigure 5 is the network architecture used for non-goal RL, corresponding to thepolicy π.。On top of observation preprocessing in Appendix A, we introduce abackbone implemented with a fully-connected layer followed by three Residual Blocks.。As for the policy head and threevalue heads, we implemented each of them as two connected Residual Blocks.。It is noteworthy that the invisible enemyinformation, such as the nearest enemy’s location, has also been introduced as an input to the value estimation, for theconsideration of stabilizing the policy learning (Vinyals et al., 2019).Figure 5: Network structure of our proposed policy.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationFeature Weight Descriptionenemy discovery 0.02 reward for see an enemydetected by enemy -0.002 punishment for being seen by an enemyscout 0.0001 reward for search for an enemyno-op -0.0002 punishment for stopping and doing nothingbullet 0.015 reward for using and refilling bulletshealth point 0.03 reward for health point changesbe knocked down -2.5 punishment for being knocked downdead -3.5 punishment for being killeddamage enemy 0.1 reward for damaging an enemyknock down enemy 4.5 reward for knocking down an enemykill enemy 3.5 reward for killing an enemyapproach a downed teammate 0.001 reward for approaching a downed teammatehelp a downed teammate up 0.8 reward for helping up a downed teammatenot save a downed teammate -0.5 punishment for not saving a downed teammatego to blue circle 0.00015 reward for going to blue circlebe in white circle -0.00005 small punishment for being outside of white circleoutside blue circle -0.012 punishment for being outside of blue circleteammate damage enemy 0.03 reward from teammate damaging enemiesteammate get up 0.6 reward from teammate getting upI help teammate up 4 reward for helping teammate upinterrupt helping teammate up -0.05 punishment for the interruption to help teammate upobstacle avoidance 0.012 punishment for being stuckgoal 1 reward of goal completionTable 5: The introduction of different rewards.Date Iteration #params Change4/14/2023 1 1802702 Experiment started4/27/2023 1808552 1802702 Env-init: Random weapons5/8/2023 2829170 1803087 Action: Add a fire action for long distance5/10/2023 3034011 1803087 Env-init:Random safe area in the whole map5/11/2023 3130353 1803855 Observation: Add number of remaining players in the game5/12/2023 3198564 2412975 Observation: Add BEV feature5/16/2023 3673506 2418111 Observation: Add history rotation feature5/22/2023 4519567 2418368 Observation: Add rotation change feature5/29/2023 5442025 2418368 Reward: Add rewards for teamwork6/2/2023 5899503 2418368 Update new game version6/13/2023 7306607 3013409 Network: Add obstacle avoidance reward and corresponding value head6/14/2023 7404118 3015457 Observation: Add distance feature to nearby obstacles6/16/2023 7628098 3015457 Env-init: Player numbers per team increased to 46/19/2023 7974450 3109267 Action: Use attention to select target to attackTable 8: The major changes in the training procedure.Goal Processing.. \n",
      "the piece 754\n",
      " \n",
      "thin the simulation environment.。The need arises to articulate non-linguisticelements, and the interaction between an agent and the simulation environment is centered on continuously observ-ing the environment’s state and generating corresponding actions.。Therefore, the key aspects requiring verbalizationprimarily involve the state and actions of the agent.。However, given the abundance of possible observation states inthe simulation environment, it is impractical to use all of these states directly as prompts for the language model, es-pecially considering token limitations.。Consequently, there is a crucial need to extract and linguistically transformthe most significant meta-states to facilitate successful model interaction.。It is noteworthy that smaller languageBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 2 CALCULATION OF d⋆tInput: d⋆t−1,atOutput: d⋆t1:ifat∈movement direction action space then2:d⋆t←\u0014cos(at)sin(at)\u00153:else if at∈yaw direction action space then4:5: ∆t=at−ω6:d⋆t←\u0014cos(∆ t)−sin(∆ t)sin(∆ t) cos(∆ t)\u0015d⋆t−17:else if at∈{stop; rescue; stop and fire; stop adjust and fire } then8:d⋆t←\u00140\u00159:else10: d⋆t←d⋆t−111:end ifmodels have limitations in comprehending and manipulating numerical values effectively.。To address this challenge,a deliberate effort is made to minimize the use of numerical values during the environmental linguistics process.Sub Action Space Dim Sizemovement direction 16yaw direction 16pitch direction 3body action 9basic action 7switch weapon action 3Table 6: Action space.For example, instead of specifying an agent’s speed with specific numeric metricslike “speed: 1m/s →3m/s” a qualitative representation such as “speed: slower →faster” is adopted.。This technique transforms the original continuous state into alimited, discrete meta-state, thereby enhancing the language model’s understanding.Similarly, for expediting language model understanding, a discrete action space isadopted, with each action accompanied by a succinct artificial language description.This discreet articulation of actions contributes to the overall interpretability of thelanguage model within the simulation environment.。We list the details in Table 7.C.。Policy NetworkFigure 5 is the network architecture used for non-goal RL, corresponding to thepolicy π.。On top of observation preprocessing in Appendix A, we introduce abackbone implemented with a fully-connected layer followed by three Residual Blocks.。As for the policy head and threevalue heads, we implemented each of them as two connected Residual Blocks.。It is noteworthy that the invisible enemyinformation, such as the nearest enemy’s location, has also been introduced as an input to the value estimation, for theconsideration of stabilizing the policy learning (Vinyals et al., 2019).Figure 5: Network structure of our proposed policy.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationFeature Weight Descriptionenemy discovery 0.02 reward for see an enemydetected by enemy -0.002 punishment for being seen by an enemyscout 0.0001 reward for search for an enemyno-op -0.0002 punishment for stopping and doing nothingbullet 0.015 reward for using and refilling bulletshealth point 0.03 reward for health point changesbe knocked down -2.5 punishment for being knocked downdead -3.5 punishment for being killeddamage enemy 0.1 reward for damaging an enemyknock down enemy 4.5 reward for knocking down an enemykill enemy 3.5 reward for killing an enemyapproach a downed teammate 0.001 reward for approaching a downed teammatehelp a downed teammate up 0.8 reward for helping up a downed teammatenot save a downed teammate -0.5 punishment for not saving a downed teammatego to blue circle 0.00015 reward for going to blue circlebe in white circle -0.00005 small punishment for being outside of white circleoutside blue circle -0.012 punishment for being outside of blue circleteammate damage enemy 0.03 reward from teammate damaging enemiesteammate get up 0.6 reward from teammate getting upI help teammate up 4 reward for helping teammate upinterrupt helping teammate up -0.05 punishment for the interruption to help teammate upobstacle avoidance 0.012 punishment for being stuckgoal 1 reward of goal completionTable 5: The introduction of different rewards.Date Iteration #params Change4/14/2023 1 1802702 Experiment started4/27/2023 1808552 1802702 Env-init: Random weapons5/8/2023 2829170 1803087 Action: Add a fire action for long distance5/10/2023 3034011 1803087 Env-init:Random safe area in the whole map5/11/2023 3130353 1803855 Observation: Add number of remaining players in the game5/12/2023 3198564 2412975 Observation: Add BEV feature5/16/2023 3673506 2418111 Observation: Add history rotation feature5/22/2023 4519567 2418368 Observation: Add rotation change feature5/29/2023 5442025 2418368 Reward: Add rewards for teamwork6/2/2023 5899503 2418368 Update new game version6/13/2023 7306607 3013409 Network: Add obstacle avoidance reward and corresponding value head6/14/2023 7404118 3015457 Observation: Add distance feature to nearby obstacles6/16/2023 7628098 3015457 Env-init: Player numbers per team increased to 46/19/2023 7974450 3109267 Action: Use attention to select target to attackTable 8: The major changes in the training procedure.Goal Processing.。To construct the goal-conditioned policy πgon top of π, we need toencode a goal gthat is generated from Grnd,GopandGllm.. \n",
      "the piece 755\n",
      " \n",
      "action between an agent and the simulation environment is centered on continuously observ-ing the environment’s state and generating corresponding actions.。Therefore, the key aspects requiring verbalizationprimarily involve the state and actions of the agent.。However, given the abundance of possible observation states inthe simulation environment, it is impractical to use all of these states directly as prompts for the language model, es-pecially considering token limitations.。Consequently, there is a crucial need to extract and linguistically transformthe most significant meta-states to facilitate successful model interaction.。It is noteworthy that smaller languageBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 2 CALCULATION OF d⋆tInput: d⋆t−1,atOutput: d⋆t1:ifat∈movement direction action space then2:d⋆t←\u0014cos(at)sin(at)\u00153:else if at∈yaw direction action space then4:5: ∆t=at−ω6:d⋆t←\u0014cos(∆ t)−sin(∆ t)sin(∆ t) cos(∆ t)\u0015d⋆t−17:else if at∈{stop; rescue; stop and fire; stop adjust and fire } then8:d⋆t←\u00140\u00159:else10: d⋆t←d⋆t−111:end ifmodels have limitations in comprehending and manipulating numerical values effectively.。To address this challenge,a deliberate effort is made to minimize the use of numerical values during the environmental linguistics process.Sub Action Space Dim Sizemovement direction 16yaw direction 16pitch direction 3body action 9basic action 7switch weapon action 3Table 6: Action space.For example, instead of specifying an agent’s speed with specific numeric metricslike “speed: 1m/s →3m/s” a qualitative representation such as “speed: slower →faster” is adopted.。This technique transforms the original continuous state into alimited, discrete meta-state, thereby enhancing the language model’s understanding.Similarly, for expediting language model understanding, a discrete action space isadopted, with each action accompanied by a succinct artificial language description.This discreet articulation of actions contributes to the overall interpretability of thelanguage model within the simulation environment.。We list the details in Table 7.C.。Policy NetworkFigure 5 is the network architecture used for non-goal RL, corresponding to thepolicy π.。On top of observation preprocessing in Appendix A, we introduce abackbone implemented with a fully-connected layer followed by three Residual Blocks.。As for the policy head and threevalue heads, we implemented each of them as two connected Residual Blocks.。It is noteworthy that the invisible enemyinformation, such as the nearest enemy’s location, has also been introduced as an input to the value estimation, for theconsideration of stabilizing the policy learning (Vinyals et al., 2019).Figure 5: Network structure of our proposed policy.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationFeature Weight Descriptionenemy discovery 0.02 reward for see an enemydetected by enemy -0.002 punishment for being seen by an enemyscout 0.0001 reward for search for an enemyno-op -0.0002 punishment for stopping and doing nothingbullet 0.015 reward for using and refilling bulletshealth point 0.03 reward for health point changesbe knocked down -2.5 punishment for being knocked downdead -3.5 punishment for being killeddamage enemy 0.1 reward for damaging an enemyknock down enemy 4.5 reward for knocking down an enemykill enemy 3.5 reward for killing an enemyapproach a downed teammate 0.001 reward for approaching a downed teammatehelp a downed teammate up 0.8 reward for helping up a downed teammatenot save a downed teammate -0.5 punishment for not saving a downed teammatego to blue circle 0.00015 reward for going to blue circlebe in white circle -0.00005 small punishment for being outside of white circleoutside blue circle -0.012 punishment for being outside of blue circleteammate damage enemy 0.03 reward from teammate damaging enemiesteammate get up 0.6 reward from teammate getting upI help teammate up 4 reward for helping teammate upinterrupt helping teammate up -0.05 punishment for the interruption to help teammate upobstacle avoidance 0.012 punishment for being stuckgoal 1 reward of goal completionTable 5: The introduction of different rewards.Date Iteration #params Change4/14/2023 1 1802702 Experiment started4/27/2023 1808552 1802702 Env-init: Random weapons5/8/2023 2829170 1803087 Action: Add a fire action for long distance5/10/2023 3034011 1803087 Env-init:Random safe area in the whole map5/11/2023 3130353 1803855 Observation: Add number of remaining players in the game5/12/2023 3198564 2412975 Observation: Add BEV feature5/16/2023 3673506 2418111 Observation: Add history rotation feature5/22/2023 4519567 2418368 Observation: Add rotation change feature5/29/2023 5442025 2418368 Reward: Add rewards for teamwork6/2/2023 5899503 2418368 Update new game version6/13/2023 7306607 3013409 Network: Add obstacle avoidance reward and corresponding value head6/14/2023 7404118 3015457 Observation: Add distance feature to nearby obstacles6/16/2023 7628098 3015457 Env-init: Player numbers per team increased to 46/19/2023 7974450 3109267 Action: Use attention to select target to attackTable 8: The major changes in the training procedure.Goal Processing.。To construct the goal-conditioned policy πgon top of π, we need toencode a goal gthat is generated from Grnd,GopandGllm.。Thus, we propose a network Fas anencoder to achieve that.. \n",
      "the piece 756\n",
      " \n",
      "e and actions of the agent.。However, given the abundance of possible observation states inthe simulation environment, it is impractical to use all of these states directly as prompts for the language model, es-pecially considering token limitations.。Consequently, there is a crucial need to extract and linguistically transformthe most significant meta-states to facilitate successful model interaction.。It is noteworthy that smaller languageBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 2 CALCULATION OF d⋆tInput: d⋆t−1,atOutput: d⋆t1:ifat∈movement direction action space then2:d⋆t←\u0014cos(at)sin(at)\u00153:else if at∈yaw direction action space then4:5: ∆t=at−ω6:d⋆t←\u0014cos(∆ t)−sin(∆ t)sin(∆ t) cos(∆ t)\u0015d⋆t−17:else if at∈{stop; rescue; stop and fire; stop adjust and fire } then8:d⋆t←\u00140\u00159:else10: d⋆t←d⋆t−111:end ifmodels have limitations in comprehending and manipulating numerical values effectively.。To address this challenge,a deliberate effort is made to minimize the use of numerical values during the environmental linguistics process.Sub Action Space Dim Sizemovement direction 16yaw direction 16pitch direction 3body action 9basic action 7switch weapon action 3Table 6: Action space.For example, instead of specifying an agent’s speed with specific numeric metricslike “speed: 1m/s →3m/s” a qualitative representation such as “speed: slower →faster” is adopted.。This technique transforms the original continuous state into alimited, discrete meta-state, thereby enhancing the language model’s understanding.Similarly, for expediting language model understanding, a discrete action space isadopted, with each action accompanied by a succinct artificial language description.This discreet articulation of actions contributes to the overall interpretability of thelanguage model within the simulation environment.。We list the details in Table 7.C.。Policy NetworkFigure 5 is the network architecture used for non-goal RL, corresponding to thepolicy π.。On top of observation preprocessing in Appendix A, we introduce abackbone implemented with a fully-connected layer followed by three Residual Blocks.。As for the policy head and threevalue heads, we implemented each of them as two connected Residual Blocks.。It is noteworthy that the invisible enemyinformation, such as the nearest enemy’s location, has also been introduced as an input to the value estimation, for theconsideration of stabilizing the policy learning (Vinyals et al., 2019).Figure 5: Network structure of our proposed policy.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationFeature Weight Descriptionenemy discovery 0.02 reward for see an enemydetected by enemy -0.002 punishment for being seen by an enemyscout 0.0001 reward for search for an enemyno-op -0.0002 punishment for stopping and doing nothingbullet 0.015 reward for using and refilling bulletshealth point 0.03 reward for health point changesbe knocked down -2.5 punishment for being knocked downdead -3.5 punishment for being killeddamage enemy 0.1 reward for damaging an enemyknock down enemy 4.5 reward for knocking down an enemykill enemy 3.5 reward for killing an enemyapproach a downed teammate 0.001 reward for approaching a downed teammatehelp a downed teammate up 0.8 reward for helping up a downed teammatenot save a downed teammate -0.5 punishment for not saving a downed teammatego to blue circle 0.00015 reward for going to blue circlebe in white circle -0.00005 small punishment for being outside of white circleoutside blue circle -0.012 punishment for being outside of blue circleteammate damage enemy 0.03 reward from teammate damaging enemiesteammate get up 0.6 reward from teammate getting upI help teammate up 4 reward for helping teammate upinterrupt helping teammate up -0.05 punishment for the interruption to help teammate upobstacle avoidance 0.012 punishment for being stuckgoal 1 reward of goal completionTable 5: The introduction of different rewards.Date Iteration #params Change4/14/2023 1 1802702 Experiment started4/27/2023 1808552 1802702 Env-init: Random weapons5/8/2023 2829170 1803087 Action: Add a fire action for long distance5/10/2023 3034011 1803087 Env-init:Random safe area in the whole map5/11/2023 3130353 1803855 Observation: Add number of remaining players in the game5/12/2023 3198564 2412975 Observation: Add BEV feature5/16/2023 3673506 2418111 Observation: Add history rotation feature5/22/2023 4519567 2418368 Observation: Add rotation change feature5/29/2023 5442025 2418368 Reward: Add rewards for teamwork6/2/2023 5899503 2418368 Update new game version6/13/2023 7306607 3013409 Network: Add obstacle avoidance reward and corresponding value head6/14/2023 7404118 3015457 Observation: Add distance feature to nearby obstacles6/16/2023 7628098 3015457 Env-init: Player numbers per team increased to 46/19/2023 7974450 3109267 Action: Use attention to select target to attackTable 8: The major changes in the training procedure.Goal Processing.。To construct the goal-conditioned policy πgon top of π, we need toencode a goal gthat is generated from Grnd,GopandGllm.。Thus, we propose a network Fas anencoder to achieve that.。In practice, to improvethe representation, we further include other inputsbesides gas (1) goal_mode_info : a 10-dimensionalvector that indicates whether current learning isgoal-conditioned, the achieved times of g, andthe rest time to complete this goal (as we presetthe maximum timestep for goal completion is30s), short in vinfo; (2) sub_goal_masking : a68-dimensional 0-1 vector that indicates whichsub-goals are masked for their corresponding values are 0, short in vmask ; (3) expected_goal_of_current_state : namely, forstatt, we generate a goal g′=PROJ(st)which is a 68-dimensional vector as the same as g, short in g′.. \n",
      "the piece 757\n",
      " \n",
      "dering token limitations.。Consequently, there is a crucial need to extract and linguistically transformthe most significant meta-states to facilitate successful model interaction.。It is noteworthy that smaller languageBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 2 CALCULATION OF d⋆tInput: d⋆t−1,atOutput: d⋆t1:ifat∈movement direction action space then2:d⋆t←\u0014cos(at)sin(at)\u00153:else if at∈yaw direction action space then4:5: ∆t=at−ω6:d⋆t←\u0014cos(∆ t)−sin(∆ t)sin(∆ t) cos(∆ t)\u0015d⋆t−17:else if at∈{stop; rescue; stop and fire; stop adjust and fire } then8:d⋆t←\u00140\u00159:else10: d⋆t←d⋆t−111:end ifmodels have limitations in comprehending and manipulating numerical values effectively.。To address this challenge,a deliberate effort is made to minimize the use of numerical values during the environmental linguistics process.Sub Action Space Dim Sizemovement direction 16yaw direction 16pitch direction 3body action 9basic action 7switch weapon action 3Table 6: Action space.For example, instead of specifying an agent’s speed with specific numeric metricslike “speed: 1m/s →3m/s” a qualitative representation such as “speed: slower →faster” is adopted.。This technique transforms the original continuous state into alimited, discrete meta-state, thereby enhancing the language model’s understanding.Similarly, for expediting language model understanding, a discrete action space isadopted, with each action accompanied by a succinct artificial language description.This discreet articulation of actions contributes to the overall interpretability of thelanguage model within the simulation environment.。We list the details in Table 7.C.。Policy NetworkFigure 5 is the network architecture used for non-goal RL, corresponding to thepolicy π.。On top of observation preprocessing in Appendix A, we introduce abackbone implemented with a fully-connected layer followed by three Residual Blocks.。As for the policy head and threevalue heads, we implemented each of them as two connected Residual Blocks.。It is noteworthy that the invisible enemyinformation, such as the nearest enemy’s location, has also been introduced as an input to the value estimation, for theconsideration of stabilizing the policy learning (Vinyals et al., 2019).Figure 5: Network structure of our proposed policy.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationFeature Weight Descriptionenemy discovery 0.02 reward for see an enemydetected by enemy -0.002 punishment for being seen by an enemyscout 0.0001 reward for search for an enemyno-op -0.0002 punishment for stopping and doing nothingbullet 0.015 reward for using and refilling bulletshealth point 0.03 reward for health point changesbe knocked down -2.5 punishment for being knocked downdead -3.5 punishment for being killeddamage enemy 0.1 reward for damaging an enemyknock down enemy 4.5 reward for knocking down an enemykill enemy 3.5 reward for killing an enemyapproach a downed teammate 0.001 reward for approaching a downed teammatehelp a downed teammate up 0.8 reward for helping up a downed teammatenot save a downed teammate -0.5 punishment for not saving a downed teammatego to blue circle 0.00015 reward for going to blue circlebe in white circle -0.00005 small punishment for being outside of white circleoutside blue circle -0.012 punishment for being outside of blue circleteammate damage enemy 0.03 reward from teammate damaging enemiesteammate get up 0.6 reward from teammate getting upI help teammate up 4 reward for helping teammate upinterrupt helping teammate up -0.05 punishment for the interruption to help teammate upobstacle avoidance 0.012 punishment for being stuckgoal 1 reward of goal completionTable 5: The introduction of different rewards.Date Iteration #params Change4/14/2023 1 1802702 Experiment started4/27/2023 1808552 1802702 Env-init: Random weapons5/8/2023 2829170 1803087 Action: Add a fire action for long distance5/10/2023 3034011 1803087 Env-init:Random safe area in the whole map5/11/2023 3130353 1803855 Observation: Add number of remaining players in the game5/12/2023 3198564 2412975 Observation: Add BEV feature5/16/2023 3673506 2418111 Observation: Add history rotation feature5/22/2023 4519567 2418368 Observation: Add rotation change feature5/29/2023 5442025 2418368 Reward: Add rewards for teamwork6/2/2023 5899503 2418368 Update new game version6/13/2023 7306607 3013409 Network: Add obstacle avoidance reward and corresponding value head6/14/2023 7404118 3015457 Observation: Add distance feature to nearby obstacles6/16/2023 7628098 3015457 Env-init: Player numbers per team increased to 46/19/2023 7974450 3109267 Action: Use attention to select target to attackTable 8: The major changes in the training procedure.Goal Processing.。To construct the goal-conditioned policy πgon top of π, we need toencode a goal gthat is generated from Grnd,GopandGllm.。Thus, we propose a network Fas anencoder to achieve that.。In practice, to improvethe representation, we further include other inputsbesides gas (1) goal_mode_info : a 10-dimensionalvector that indicates whether current learning isgoal-conditioned, the achieved times of g, andthe rest time to complete this goal (as we presetthe maximum timestep for goal completion is30s), short in vinfo; (2) sub_goal_masking : a68-dimensional 0-1 vector that indicates whichsub-goals are masked for their corresponding values are 0, short in vmask ; (3) expected_goal_of_current_state : namely, forstatt, we generate a goal g′=PROJ(st)which is a 68-dimensional vector as the same as g, short in g′.。Then, we form theinput for Fasx= (g, g′, vinfo, vmask).. \n",
      "the piece 758\n",
      " \n",
      "e most significant meta-states to facilitate successful model interaction.。It is noteworthy that smaller languageBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationAlgorithm 2 CALCULATION OF d⋆tInput: d⋆t−1,atOutput: d⋆t1:ifat∈movement direction action space then2:d⋆t←\u0014cos(at)sin(at)\u00153:else if at∈yaw direction action space then4:5: ∆t=at−ω6:d⋆t←\u0014cos(∆ t)−sin(∆ t)sin(∆ t) cos(∆ t)\u0015d⋆t−17:else if at∈{stop; rescue; stop and fire; stop adjust and fire } then8:d⋆t←\u00140\u00159:else10: d⋆t←d⋆t−111:end ifmodels have limitations in comprehending and manipulating numerical values effectively.。To address this challenge,a deliberate effort is made to minimize the use of numerical values during the environmental linguistics process.Sub Action Space Dim Sizemovement direction 16yaw direction 16pitch direction 3body action 9basic action 7switch weapon action 3Table 6: Action space.For example, instead of specifying an agent’s speed with specific numeric metricslike “speed: 1m/s →3m/s” a qualitative representation such as “speed: slower →faster” is adopted.。This technique transforms the original continuous state into alimited, discrete meta-state, thereby enhancing the language model’s understanding.Similarly, for expediting language model understanding, a discrete action space isadopted, with each action accompanied by a succinct artificial language description.This discreet articulation of actions contributes to the overall interpretability of thelanguage model within the simulation environment.。We list the details in Table 7.C.。Policy NetworkFigure 5 is the network architecture used for non-goal RL, corresponding to thepolicy π.。On top of observation preprocessing in Appendix A, we introduce abackbone implemented with a fully-connected layer followed by three Residual Blocks.。As for the policy head and threevalue heads, we implemented each of them as two connected Residual Blocks.。It is noteworthy that the invisible enemyinformation, such as the nearest enemy’s location, has also been introduced as an input to the value estimation, for theconsideration of stabilizing the policy learning (Vinyals et al., 2019).Figure 5: Network structure of our proposed policy.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationFeature Weight Descriptionenemy discovery 0.02 reward for see an enemydetected by enemy -0.002 punishment for being seen by an enemyscout 0.0001 reward for search for an enemyno-op -0.0002 punishment for stopping and doing nothingbullet 0.015 reward for using and refilling bulletshealth point 0.03 reward for health point changesbe knocked down -2.5 punishment for being knocked downdead -3.5 punishment for being killeddamage enemy 0.1 reward for damaging an enemyknock down enemy 4.5 reward for knocking down an enemykill enemy 3.5 reward for killing an enemyapproach a downed teammate 0.001 reward for approaching a downed teammatehelp a downed teammate up 0.8 reward for helping up a downed teammatenot save a downed teammate -0.5 punishment for not saving a downed teammatego to blue circle 0.00015 reward for going to blue circlebe in white circle -0.00005 small punishment for being outside of white circleoutside blue circle -0.012 punishment for being outside of blue circleteammate damage enemy 0.03 reward from teammate damaging enemiesteammate get up 0.6 reward from teammate getting upI help teammate up 4 reward for helping teammate upinterrupt helping teammate up -0.05 punishment for the interruption to help teammate upobstacle avoidance 0.012 punishment for being stuckgoal 1 reward of goal completionTable 5: The introduction of different rewards.Date Iteration #params Change4/14/2023 1 1802702 Experiment started4/27/2023 1808552 1802702 Env-init: Random weapons5/8/2023 2829170 1803087 Action: Add a fire action for long distance5/10/2023 3034011 1803087 Env-init:Random safe area in the whole map5/11/2023 3130353 1803855 Observation: Add number of remaining players in the game5/12/2023 3198564 2412975 Observation: Add BEV feature5/16/2023 3673506 2418111 Observation: Add history rotation feature5/22/2023 4519567 2418368 Observation: Add rotation change feature5/29/2023 5442025 2418368 Reward: Add rewards for teamwork6/2/2023 5899503 2418368 Update new game version6/13/2023 7306607 3013409 Network: Add obstacle avoidance reward and corresponding value head6/14/2023 7404118 3015457 Observation: Add distance feature to nearby obstacles6/16/2023 7628098 3015457 Env-init: Player numbers per team increased to 46/19/2023 7974450 3109267 Action: Use attention to select target to attackTable 8: The major changes in the training procedure.Goal Processing.。To construct the goal-conditioned policy πgon top of π, we need toencode a goal gthat is generated from Grnd,GopandGllm.。Thus, we propose a network Fas anencoder to achieve that.。In practice, to improvethe representation, we further include other inputsbesides gas (1) goal_mode_info : a 10-dimensionalvector that indicates whether current learning isgoal-conditioned, the achieved times of g, andthe rest time to complete this goal (as we presetthe maximum timestep for goal completion is30s), short in vinfo; (2) sub_goal_masking : a68-dimensional 0-1 vector that indicates whichsub-goals are masked for their corresponding values are 0, short in vmask ; (3) expected_goal_of_current_state : namely, forstatt, we generate a goal g′=PROJ(st)which is a 68-dimensional vector as the same as g, short in g′.。Then, we form theinput for Fasx= (g, g′, vinfo, vmask).。Considering that each item in xis heterogeneous on the data type, so we transformthem with independent FC layers and then follows a ResNet block for each of them.. \n",
      "the piece 759\n",
      " \n",
      "nput: d⋆t−1,atOutput: d⋆t1:ifat∈movement direction action space then2:d⋆t←\u0014cos(at)sin(at)\u00153:else if at∈yaw direction action space then4:5: ∆t=at−ω6:d⋆t←\u0014cos(∆ t)−sin(∆ t)sin(∆ t) cos(∆ t)\u0015d⋆t−17:else if at∈{stop; rescue; stop and fire; stop adjust and fire } then8:d⋆t←\u00140\u00159:else10: d⋆t←d⋆t−111:end ifmodels have limitations in comprehending and manipulating numerical values effectively.。To address this challenge,a deliberate effort is made to minimize the use of numerical values during the environmental linguistics process.Sub Action Space Dim Sizemovement direction 16yaw direction 16pitch direction 3body action 9basic action 7switch weapon action 3Table 6: Action space.For example, instead of specifying an agent’s speed with specific numeric metricslike “speed: 1m/s →3m/s” a qualitative representation such as “speed: slower →faster” is adopted.。This technique transforms the original continuous state into alimited, discrete meta-state, thereby enhancing the language model’s understanding.Similarly, for expediting language model understanding, a discrete action space isadopted, with each action accompanied by a succinct artificial language description.This discreet articulation of actions contributes to the overall interpretability of thelanguage model within the simulation environment.。We list the details in Table 7.C.。Policy NetworkFigure 5 is the network architecture used for non-goal RL, corresponding to thepolicy π.。On top of observation preprocessing in Appendix A, we introduce abackbone implemented with a fully-connected layer followed by three Residual Blocks.。As for the policy head and threevalue heads, we implemented each of them as two connected Residual Blocks.。It is noteworthy that the invisible enemyinformation, such as the nearest enemy’s location, has also been introduced as an input to the value estimation, for theconsideration of stabilizing the policy learning (Vinyals et al., 2019).Figure 5: Network structure of our proposed policy.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationFeature Weight Descriptionenemy discovery 0.02 reward for see an enemydetected by enemy -0.002 punishment for being seen by an enemyscout 0.0001 reward for search for an enemyno-op -0.0002 punishment for stopping and doing nothingbullet 0.015 reward for using and refilling bulletshealth point 0.03 reward for health point changesbe knocked down -2.5 punishment for being knocked downdead -3.5 punishment for being killeddamage enemy 0.1 reward for damaging an enemyknock down enemy 4.5 reward for knocking down an enemykill enemy 3.5 reward for killing an enemyapproach a downed teammate 0.001 reward for approaching a downed teammatehelp a downed teammate up 0.8 reward for helping up a downed teammatenot save a downed teammate -0.5 punishment for not saving a downed teammatego to blue circle 0.00015 reward for going to blue circlebe in white circle -0.00005 small punishment for being outside of white circleoutside blue circle -0.012 punishment for being outside of blue circleteammate damage enemy 0.03 reward from teammate damaging enemiesteammate get up 0.6 reward from teammate getting upI help teammate up 4 reward for helping teammate upinterrupt helping teammate up -0.05 punishment for the interruption to help teammate upobstacle avoidance 0.012 punishment for being stuckgoal 1 reward of goal completionTable 5: The introduction of different rewards.Date Iteration #params Change4/14/2023 1 1802702 Experiment started4/27/2023 1808552 1802702 Env-init: Random weapons5/8/2023 2829170 1803087 Action: Add a fire action for long distance5/10/2023 3034011 1803087 Env-init:Random safe area in the whole map5/11/2023 3130353 1803855 Observation: Add number of remaining players in the game5/12/2023 3198564 2412975 Observation: Add BEV feature5/16/2023 3673506 2418111 Observation: Add history rotation feature5/22/2023 4519567 2418368 Observation: Add rotation change feature5/29/2023 5442025 2418368 Reward: Add rewards for teamwork6/2/2023 5899503 2418368 Update new game version6/13/2023 7306607 3013409 Network: Add obstacle avoidance reward and corresponding value head6/14/2023 7404118 3015457 Observation: Add distance feature to nearby obstacles6/16/2023 7628098 3015457 Env-init: Player numbers per team increased to 46/19/2023 7974450 3109267 Action: Use attention to select target to attackTable 8: The major changes in the training procedure.Goal Processing.。To construct the goal-conditioned policy πgon top of π, we need toencode a goal gthat is generated from Grnd,GopandGllm.。Thus, we propose a network Fas anencoder to achieve that.。In practice, to improvethe representation, we further include other inputsbesides gas (1) goal_mode_info : a 10-dimensionalvector that indicates whether current learning isgoal-conditioned, the achieved times of g, andthe rest time to complete this goal (as we presetthe maximum timestep for goal completion is30s), short in vinfo; (2) sub_goal_masking : a68-dimensional 0-1 vector that indicates whichsub-goals are masked for their corresponding values are 0, short in vmask ; (3) expected_goal_of_current_state : namely, forstatt, we generate a goal g′=PROJ(st)which is a 68-dimensional vector as the same as g, short in g′.。Then, we form theinput for Fasx= (g, g′, vinfo, vmask).。Considering that each item in xis heterogeneous on the data type, so we transformthem with independent FC layers and then follows a ResNet block for each of them.。We can express this process aseg=RESNET(FC(g)), e g′=RESNET(FC(g′)), e info=RESNET(FC(vinfo)), e mask =RESNET(FC(vmask)),where eg,eg′,einfo,emask are the embedding representation corresponding to each input item, and all of them are the samein dimension size.. \n",
      "the piece 760\n",
      " \n",
      "s(∆ t)−sin(∆ t)sin(∆ t) cos(∆ t)\u0015d⋆t−17:else if at∈{stop; rescue; stop and fire; stop adjust and fire } then8:d⋆t←\u00140\u00159:else10: d⋆t←d⋆t−111:end ifmodels have limitations in comprehending and manipulating numerical values effectively.。To address this challenge,a deliberate effort is made to minimize the use of numerical values during the environmental linguistics process.Sub Action Space Dim Sizemovement direction 16yaw direction 16pitch direction 3body action 9basic action 7switch weapon action 3Table 6: Action space.For example, instead of specifying an agent’s speed with specific numeric metricslike “speed: 1m/s →3m/s” a qualitative representation such as “speed: slower →faster” is adopted.。This technique transforms the original continuous state into alimited, discrete meta-state, thereby enhancing the language model’s understanding.Similarly, for expediting language model understanding, a discrete action space isadopted, with each action accompanied by a succinct artificial language description.This discreet articulation of actions contributes to the overall interpretability of thelanguage model within the simulation environment.。We list the details in Table 7.C.。Policy NetworkFigure 5 is the network architecture used for non-goal RL, corresponding to thepolicy π.。On top of observation preprocessing in Appendix A, we introduce abackbone implemented with a fully-connected layer followed by three Residual Blocks.。As for the policy head and threevalue heads, we implemented each of them as two connected Residual Blocks.。It is noteworthy that the invisible enemyinformation, such as the nearest enemy’s location, has also been introduced as an input to the value estimation, for theconsideration of stabilizing the policy learning (Vinyals et al., 2019).Figure 5: Network structure of our proposed policy.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationFeature Weight Descriptionenemy discovery 0.02 reward for see an enemydetected by enemy -0.002 punishment for being seen by an enemyscout 0.0001 reward for search for an enemyno-op -0.0002 punishment for stopping and doing nothingbullet 0.015 reward for using and refilling bulletshealth point 0.03 reward for health point changesbe knocked down -2.5 punishment for being knocked downdead -3.5 punishment for being killeddamage enemy 0.1 reward for damaging an enemyknock down enemy 4.5 reward for knocking down an enemykill enemy 3.5 reward for killing an enemyapproach a downed teammate 0.001 reward for approaching a downed teammatehelp a downed teammate up 0.8 reward for helping up a downed teammatenot save a downed teammate -0.5 punishment for not saving a downed teammatego to blue circle 0.00015 reward for going to blue circlebe in white circle -0.00005 small punishment for being outside of white circleoutside blue circle -0.012 punishment for being outside of blue circleteammate damage enemy 0.03 reward from teammate damaging enemiesteammate get up 0.6 reward from teammate getting upI help teammate up 4 reward for helping teammate upinterrupt helping teammate up -0.05 punishment for the interruption to help teammate upobstacle avoidance 0.012 punishment for being stuckgoal 1 reward of goal completionTable 5: The introduction of different rewards.Date Iteration #params Change4/14/2023 1 1802702 Experiment started4/27/2023 1808552 1802702 Env-init: Random weapons5/8/2023 2829170 1803087 Action: Add a fire action for long distance5/10/2023 3034011 1803087 Env-init:Random safe area in the whole map5/11/2023 3130353 1803855 Observation: Add number of remaining players in the game5/12/2023 3198564 2412975 Observation: Add BEV feature5/16/2023 3673506 2418111 Observation: Add history rotation feature5/22/2023 4519567 2418368 Observation: Add rotation change feature5/29/2023 5442025 2418368 Reward: Add rewards for teamwork6/2/2023 5899503 2418368 Update new game version6/13/2023 7306607 3013409 Network: Add obstacle avoidance reward and corresponding value head6/14/2023 7404118 3015457 Observation: Add distance feature to nearby obstacles6/16/2023 7628098 3015457 Env-init: Player numbers per team increased to 46/19/2023 7974450 3109267 Action: Use attention to select target to attackTable 8: The major changes in the training procedure.Goal Processing.。To construct the goal-conditioned policy πgon top of π, we need toencode a goal gthat is generated from Grnd,GopandGllm.。Thus, we propose a network Fas anencoder to achieve that.。In practice, to improvethe representation, we further include other inputsbesides gas (1) goal_mode_info : a 10-dimensionalvector that indicates whether current learning isgoal-conditioned, the achieved times of g, andthe rest time to complete this goal (as we presetthe maximum timestep for goal completion is30s), short in vinfo; (2) sub_goal_masking : a68-dimensional 0-1 vector that indicates whichsub-goals are masked for their corresponding values are 0, short in vmask ; (3) expected_goal_of_current_state : namely, forstatt, we generate a goal g′=PROJ(st)which is a 68-dimensional vector as the same as g, short in g′.。Then, we form theinput for Fasx= (g, g′, vinfo, vmask).。Considering that each item in xis heterogeneous on the data type, so we transformthem with independent FC layers and then follows a ResNet block for each of them.。We can express this process aseg=RESNET(FC(g)), e g′=RESNET(FC(g′)), e info=RESNET(FC(vinfo)), e mask =RESNET(FC(vmask)),where eg,eg′,einfo,emask are the embedding representation corresponding to each input item, and all of them are the samein dimension size.。Then, we aggregate them with computing the average of them as to get a fusion embedding asefusion = (eg+eg′+einfo+emask)/4.With surgery, we now concatenate the embedding of backbone of πwithefusion and fusing them via a FC layer, to form abackbone for πg.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub-goal Class CandidatesDamage to enemy [Zero,Low,Little low,Medium,Little high,High]Whether knock down enemy [True,False]Whether kill enemy [True,False]Whether seen enemy [True,False]Whether seen by enemy [True,False]Number of enemies have ever seen [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]Length of distance moved [No movement,Short,Medium,Long,Very long]Average velocity [Static,Slow,Medium,Fast,Falling]Horizontal direction of movement [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Horizontal direction of view [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Pitch direction of view [Low,Little low,Medium,Little high,High]Health level [Empty,Low,Medium,High,Full]Whether to restore health [True,False]Whether the health is damaged [True,False]Whether rescued teammate [True,False]Whether be knocked down [True,False]Whether prone position [True,False]Whether have a gun [True,False]Whether have bullets [True,False]Whether have medical kits [True,False]Distance with nearest enemy [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether closer with nearest enemy [True,False]Whether crouch position [True,False]Whether hold a gun [True,False]Length of distance from agent to teammate [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether seen by teammate [True,False]Teammate’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Whether follow with the views of teammate [True,False]Whether target the same enemy as teammate [True,False]Whether follow with the movement direction of teammate [True,False]Horizontal direction of movement of enemy [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Velocity of enemy [Static,Slow,Medium,Fast,Falling,None]Enemy’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Table 7: Overview of sub-goal classes, we show a part of them here.D.. \n",
      "the piece 761\n",
      " \n",
      "stead of specifying an agent’s speed with specific numeric metricslike “speed: 1m/s →3m/s” a qualitative representation such as “speed: slower →faster” is adopted.。This technique transforms the original continuous state into alimited, discrete meta-state, thereby enhancing the language model’s understanding.Similarly, for expediting language model understanding, a discrete action space isadopted, with each action accompanied by a succinct artificial language description.This discreet articulation of actions contributes to the overall interpretability of thelanguage model within the simulation environment.。We list the details in Table 7.C.。Policy NetworkFigure 5 is the network architecture used for non-goal RL, corresponding to thepolicy π.。On top of observation preprocessing in Appendix A, we introduce abackbone implemented with a fully-connected layer followed by three Residual Blocks.。As for the policy head and threevalue heads, we implemented each of them as two connected Residual Blocks.。It is noteworthy that the invisible enemyinformation, such as the nearest enemy’s location, has also been introduced as an input to the value estimation, for theconsideration of stabilizing the policy learning (Vinyals et al., 2019).Figure 5: Network structure of our proposed policy.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationFeature Weight Descriptionenemy discovery 0.02 reward for see an enemydetected by enemy -0.002 punishment for being seen by an enemyscout 0.0001 reward for search for an enemyno-op -0.0002 punishment for stopping and doing nothingbullet 0.015 reward for using and refilling bulletshealth point 0.03 reward for health point changesbe knocked down -2.5 punishment for being knocked downdead -3.5 punishment for being killeddamage enemy 0.1 reward for damaging an enemyknock down enemy 4.5 reward for knocking down an enemykill enemy 3.5 reward for killing an enemyapproach a downed teammate 0.001 reward for approaching a downed teammatehelp a downed teammate up 0.8 reward for helping up a downed teammatenot save a downed teammate -0.5 punishment for not saving a downed teammatego to blue circle 0.00015 reward for going to blue circlebe in white circle -0.00005 small punishment for being outside of white circleoutside blue circle -0.012 punishment for being outside of blue circleteammate damage enemy 0.03 reward from teammate damaging enemiesteammate get up 0.6 reward from teammate getting upI help teammate up 4 reward for helping teammate upinterrupt helping teammate up -0.05 punishment for the interruption to help teammate upobstacle avoidance 0.012 punishment for being stuckgoal 1 reward of goal completionTable 5: The introduction of different rewards.Date Iteration #params Change4/14/2023 1 1802702 Experiment started4/27/2023 1808552 1802702 Env-init: Random weapons5/8/2023 2829170 1803087 Action: Add a fire action for long distance5/10/2023 3034011 1803087 Env-init:Random safe area in the whole map5/11/2023 3130353 1803855 Observation: Add number of remaining players in the game5/12/2023 3198564 2412975 Observation: Add BEV feature5/16/2023 3673506 2418111 Observation: Add history rotation feature5/22/2023 4519567 2418368 Observation: Add rotation change feature5/29/2023 5442025 2418368 Reward: Add rewards for teamwork6/2/2023 5899503 2418368 Update new game version6/13/2023 7306607 3013409 Network: Add obstacle avoidance reward and corresponding value head6/14/2023 7404118 3015457 Observation: Add distance feature to nearby obstacles6/16/2023 7628098 3015457 Env-init: Player numbers per team increased to 46/19/2023 7974450 3109267 Action: Use attention to select target to attackTable 8: The major changes in the training procedure.Goal Processing.。To construct the goal-conditioned policy πgon top of π, we need toencode a goal gthat is generated from Grnd,GopandGllm.。Thus, we propose a network Fas anencoder to achieve that.。In practice, to improvethe representation, we further include other inputsbesides gas (1) goal_mode_info : a 10-dimensionalvector that indicates whether current learning isgoal-conditioned, the achieved times of g, andthe rest time to complete this goal (as we presetthe maximum timestep for goal completion is30s), short in vinfo; (2) sub_goal_masking : a68-dimensional 0-1 vector that indicates whichsub-goals are masked for their corresponding values are 0, short in vmask ; (3) expected_goal_of_current_state : namely, forstatt, we generate a goal g′=PROJ(st)which is a 68-dimensional vector as the same as g, short in g′.。Then, we form theinput for Fasx= (g, g′, vinfo, vmask).。Considering that each item in xis heterogeneous on the data type, so we transformthem with independent FC layers and then follows a ResNet block for each of them.。We can express this process aseg=RESNET(FC(g)), e g′=RESNET(FC(g′)), e info=RESNET(FC(vinfo)), e mask =RESNET(FC(vmask)),where eg,eg′,einfo,emask are the embedding representation corresponding to each input item, and all of them are the samein dimension size.。Then, we aggregate them with computing the average of them as to get a fusion embedding asefusion = (eg+eg′+einfo+emask)/4.With surgery, we now concatenate the embedding of backbone of πwithefusion and fusing them via a FC layer, to form abackbone for πg.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub-goal Class CandidatesDamage to enemy [Zero,Low,Little low,Medium,Little high,High]Whether knock down enemy [True,False]Whether kill enemy [True,False]Whether seen enemy [True,False]Whether seen by enemy [True,False]Number of enemies have ever seen [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]Length of distance moved [No movement,Short,Medium,Long,Very long]Average velocity [Static,Slow,Medium,Fast,Falling]Horizontal direction of movement [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Horizontal direction of view [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Pitch direction of view [Low,Little low,Medium,Little high,High]Health level [Empty,Low,Medium,High,Full]Whether to restore health [True,False]Whether the health is damaged [True,False]Whether rescued teammate [True,False]Whether be knocked down [True,False]Whether prone position [True,False]Whether have a gun [True,False]Whether have bullets [True,False]Whether have medical kits [True,False]Distance with nearest enemy [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether closer with nearest enemy [True,False]Whether crouch position [True,False]Whether hold a gun [True,False]Length of distance from agent to teammate [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether seen by teammate [True,False]Teammate’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Whether follow with the views of teammate [True,False]Whether target the same enemy as teammate [True,False]Whether follow with the movement direction of teammate [True,False]Horizontal direction of movement of enemy [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Velocity of enemy [Static,Slow,Medium,Fast,Falling,None]Enemy’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Table 7: Overview of sub-goal classes, we show a part of them here.D.。SurgeryAs the project proceeded, Contra was continuously improved to satisfy richer featured environment dynamics.. \n",
      "the piece 762\n",
      " \n",
      "ption.This discreet articulation of actions contributes to the overall interpretability of thelanguage model within the simulation environment.。We list the details in Table 7.C.。Policy NetworkFigure 5 is the network architecture used for non-goal RL, corresponding to thepolicy π.。On top of observation preprocessing in Appendix A, we introduce abackbone implemented with a fully-connected layer followed by three Residual Blocks.。As for the policy head and threevalue heads, we implemented each of them as two connected Residual Blocks.。It is noteworthy that the invisible enemyinformation, such as the nearest enemy’s location, has also been introduced as an input to the value estimation, for theconsideration of stabilizing the policy learning (Vinyals et al., 2019).Figure 5: Network structure of our proposed policy.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationFeature Weight Descriptionenemy discovery 0.02 reward for see an enemydetected by enemy -0.002 punishment for being seen by an enemyscout 0.0001 reward for search for an enemyno-op -0.0002 punishment for stopping and doing nothingbullet 0.015 reward for using and refilling bulletshealth point 0.03 reward for health point changesbe knocked down -2.5 punishment for being knocked downdead -3.5 punishment for being killeddamage enemy 0.1 reward for damaging an enemyknock down enemy 4.5 reward for knocking down an enemykill enemy 3.5 reward for killing an enemyapproach a downed teammate 0.001 reward for approaching a downed teammatehelp a downed teammate up 0.8 reward for helping up a downed teammatenot save a downed teammate -0.5 punishment for not saving a downed teammatego to blue circle 0.00015 reward for going to blue circlebe in white circle -0.00005 small punishment for being outside of white circleoutside blue circle -0.012 punishment for being outside of blue circleteammate damage enemy 0.03 reward from teammate damaging enemiesteammate get up 0.6 reward from teammate getting upI help teammate up 4 reward for helping teammate upinterrupt helping teammate up -0.05 punishment for the interruption to help teammate upobstacle avoidance 0.012 punishment for being stuckgoal 1 reward of goal completionTable 5: The introduction of different rewards.Date Iteration #params Change4/14/2023 1 1802702 Experiment started4/27/2023 1808552 1802702 Env-init: Random weapons5/8/2023 2829170 1803087 Action: Add a fire action for long distance5/10/2023 3034011 1803087 Env-init:Random safe area in the whole map5/11/2023 3130353 1803855 Observation: Add number of remaining players in the game5/12/2023 3198564 2412975 Observation: Add BEV feature5/16/2023 3673506 2418111 Observation: Add history rotation feature5/22/2023 4519567 2418368 Observation: Add rotation change feature5/29/2023 5442025 2418368 Reward: Add rewards for teamwork6/2/2023 5899503 2418368 Update new game version6/13/2023 7306607 3013409 Network: Add obstacle avoidance reward and corresponding value head6/14/2023 7404118 3015457 Observation: Add distance feature to nearby obstacles6/16/2023 7628098 3015457 Env-init: Player numbers per team increased to 46/19/2023 7974450 3109267 Action: Use attention to select target to attackTable 8: The major changes in the training procedure.Goal Processing.。To construct the goal-conditioned policy πgon top of π, we need toencode a goal gthat is generated from Grnd,GopandGllm.。Thus, we propose a network Fas anencoder to achieve that.。In practice, to improvethe representation, we further include other inputsbesides gas (1) goal_mode_info : a 10-dimensionalvector that indicates whether current learning isgoal-conditioned, the achieved times of g, andthe rest time to complete this goal (as we presetthe maximum timestep for goal completion is30s), short in vinfo; (2) sub_goal_masking : a68-dimensional 0-1 vector that indicates whichsub-goals are masked for their corresponding values are 0, short in vmask ; (3) expected_goal_of_current_state : namely, forstatt, we generate a goal g′=PROJ(st)which is a 68-dimensional vector as the same as g, short in g′.。Then, we form theinput for Fasx= (g, g′, vinfo, vmask).。Considering that each item in xis heterogeneous on the data type, so we transformthem with independent FC layers and then follows a ResNet block for each of them.。We can express this process aseg=RESNET(FC(g)), e g′=RESNET(FC(g′)), e info=RESNET(FC(vinfo)), e mask =RESNET(FC(vmask)),where eg,eg′,einfo,emask are the embedding representation corresponding to each input item, and all of them are the samein dimension size.。Then, we aggregate them with computing the average of them as to get a fusion embedding asefusion = (eg+eg′+einfo+emask)/4.With surgery, we now concatenate the embedding of backbone of πwithefusion and fusing them via a FC layer, to form abackbone for πg.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub-goal Class CandidatesDamage to enemy [Zero,Low,Little low,Medium,Little high,High]Whether knock down enemy [True,False]Whether kill enemy [True,False]Whether seen enemy [True,False]Whether seen by enemy [True,False]Number of enemies have ever seen [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]Length of distance moved [No movement,Short,Medium,Long,Very long]Average velocity [Static,Slow,Medium,Fast,Falling]Horizontal direction of movement [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Horizontal direction of view [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Pitch direction of view [Low,Little low,Medium,Little high,High]Health level [Empty,Low,Medium,High,Full]Whether to restore health [True,False]Whether the health is damaged [True,False]Whether rescued teammate [True,False]Whether be knocked down [True,False]Whether prone position [True,False]Whether have a gun [True,False]Whether have bullets [True,False]Whether have medical kits [True,False]Distance with nearest enemy [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether closer with nearest enemy [True,False]Whether crouch position [True,False]Whether hold a gun [True,False]Length of distance from agent to teammate [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether seen by teammate [True,False]Teammate’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Whether follow with the views of teammate [True,False]Whether target the same enemy as teammate [True,False]Whether follow with the movement direction of teammate [True,False]Horizontal direction of movement of enemy [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Velocity of enemy [Static,Slow,Medium,Fast,Falling,None]Enemy’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Table 7: Overview of sub-goal classes, we show a part of them here.D.。SurgeryAs the project proceeded, Contra was continuously improved to satisfy richer featured environment dynamics.。However,such an iterative development poses some challenges to the research of open-ended learning in an embodied situation, asthe changes in API and environment attributes will make the training be non-stationary.. \n",
      "the piece 763\n",
      " \n",
      "ead and threevalue heads, we implemented each of them as two connected Residual Blocks.。It is noteworthy that the invisible enemyinformation, such as the nearest enemy’s location, has also been introduced as an input to the value estimation, for theconsideration of stabilizing the policy learning (Vinyals et al., 2019).Figure 5: Network structure of our proposed policy.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationFeature Weight Descriptionenemy discovery 0.02 reward for see an enemydetected by enemy -0.002 punishment for being seen by an enemyscout 0.0001 reward for search for an enemyno-op -0.0002 punishment for stopping and doing nothingbullet 0.015 reward for using and refilling bulletshealth point 0.03 reward for health point changesbe knocked down -2.5 punishment for being knocked downdead -3.5 punishment for being killeddamage enemy 0.1 reward for damaging an enemyknock down enemy 4.5 reward for knocking down an enemykill enemy 3.5 reward for killing an enemyapproach a downed teammate 0.001 reward for approaching a downed teammatehelp a downed teammate up 0.8 reward for helping up a downed teammatenot save a downed teammate -0.5 punishment for not saving a downed teammatego to blue circle 0.00015 reward for going to blue circlebe in white circle -0.00005 small punishment for being outside of white circleoutside blue circle -0.012 punishment for being outside of blue circleteammate damage enemy 0.03 reward from teammate damaging enemiesteammate get up 0.6 reward from teammate getting upI help teammate up 4 reward for helping teammate upinterrupt helping teammate up -0.05 punishment for the interruption to help teammate upobstacle avoidance 0.012 punishment for being stuckgoal 1 reward of goal completionTable 5: The introduction of different rewards.Date Iteration #params Change4/14/2023 1 1802702 Experiment started4/27/2023 1808552 1802702 Env-init: Random weapons5/8/2023 2829170 1803087 Action: Add a fire action for long distance5/10/2023 3034011 1803087 Env-init:Random safe area in the whole map5/11/2023 3130353 1803855 Observation: Add number of remaining players in the game5/12/2023 3198564 2412975 Observation: Add BEV feature5/16/2023 3673506 2418111 Observation: Add history rotation feature5/22/2023 4519567 2418368 Observation: Add rotation change feature5/29/2023 5442025 2418368 Reward: Add rewards for teamwork6/2/2023 5899503 2418368 Update new game version6/13/2023 7306607 3013409 Network: Add obstacle avoidance reward and corresponding value head6/14/2023 7404118 3015457 Observation: Add distance feature to nearby obstacles6/16/2023 7628098 3015457 Env-init: Player numbers per team increased to 46/19/2023 7974450 3109267 Action: Use attention to select target to attackTable 8: The major changes in the training procedure.Goal Processing.。To construct the goal-conditioned policy πgon top of π, we need toencode a goal gthat is generated from Grnd,GopandGllm.。Thus, we propose a network Fas anencoder to achieve that.。In practice, to improvethe representation, we further include other inputsbesides gas (1) goal_mode_info : a 10-dimensionalvector that indicates whether current learning isgoal-conditioned, the achieved times of g, andthe rest time to complete this goal (as we presetthe maximum timestep for goal completion is30s), short in vinfo; (2) sub_goal_masking : a68-dimensional 0-1 vector that indicates whichsub-goals are masked for their corresponding values are 0, short in vmask ; (3) expected_goal_of_current_state : namely, forstatt, we generate a goal g′=PROJ(st)which is a 68-dimensional vector as the same as g, short in g′.。Then, we form theinput for Fasx= (g, g′, vinfo, vmask).。Considering that each item in xis heterogeneous on the data type, so we transformthem with independent FC layers and then follows a ResNet block for each of them.。We can express this process aseg=RESNET(FC(g)), e g′=RESNET(FC(g′)), e info=RESNET(FC(vinfo)), e mask =RESNET(FC(vmask)),where eg,eg′,einfo,emask are the embedding representation corresponding to each input item, and all of them are the samein dimension size.。Then, we aggregate them with computing the average of them as to get a fusion embedding asefusion = (eg+eg′+einfo+emask)/4.With surgery, we now concatenate the embedding of backbone of πwithefusion and fusing them via a FC layer, to form abackbone for πg.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub-goal Class CandidatesDamage to enemy [Zero,Low,Little low,Medium,Little high,High]Whether knock down enemy [True,False]Whether kill enemy [True,False]Whether seen enemy [True,False]Whether seen by enemy [True,False]Number of enemies have ever seen [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]Length of distance moved [No movement,Short,Medium,Long,Very long]Average velocity [Static,Slow,Medium,Fast,Falling]Horizontal direction of movement [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Horizontal direction of view [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Pitch direction of view [Low,Little low,Medium,Little high,High]Health level [Empty,Low,Medium,High,Full]Whether to restore health [True,False]Whether the health is damaged [True,False]Whether rescued teammate [True,False]Whether be knocked down [True,False]Whether prone position [True,False]Whether have a gun [True,False]Whether have bullets [True,False]Whether have medical kits [True,False]Distance with nearest enemy [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether closer with nearest enemy [True,False]Whether crouch position [True,False]Whether hold a gun [True,False]Length of distance from agent to teammate [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether seen by teammate [True,False]Teammate’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Whether follow with the views of teammate [True,False]Whether target the same enemy as teammate [True,False]Whether follow with the movement direction of teammate [True,False]Horizontal direction of movement of enemy [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Velocity of enemy [Static,Slow,Medium,Fast,Falling,None]Enemy’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Table 7: Overview of sub-goal classes, we show a part of them here.D.。SurgeryAs the project proceeded, Contra was continuously improved to satisfy richer featured environment dynamics.。However,such an iterative development poses some challenges to the research of open-ended learning in an embodied situation, asthe changes in API and environment attributes will make the training be non-stationary.。A popular solution to resolve thisissue is the surgery introduced by Berner et al.. \n",
      "the piece 764\n",
      " \n",
      "ented each of them as two connected Residual Blocks.。It is noteworthy that the invisible enemyinformation, such as the nearest enemy’s location, has also been introduced as an input to the value estimation, for theconsideration of stabilizing the policy learning (Vinyals et al., 2019).Figure 5: Network structure of our proposed policy.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationFeature Weight Descriptionenemy discovery 0.02 reward for see an enemydetected by enemy -0.002 punishment for being seen by an enemyscout 0.0001 reward for search for an enemyno-op -0.0002 punishment for stopping and doing nothingbullet 0.015 reward for using and refilling bulletshealth point 0.03 reward for health point changesbe knocked down -2.5 punishment for being knocked downdead -3.5 punishment for being killeddamage enemy 0.1 reward for damaging an enemyknock down enemy 4.5 reward for knocking down an enemykill enemy 3.5 reward for killing an enemyapproach a downed teammate 0.001 reward for approaching a downed teammatehelp a downed teammate up 0.8 reward for helping up a downed teammatenot save a downed teammate -0.5 punishment for not saving a downed teammatego to blue circle 0.00015 reward for going to blue circlebe in white circle -0.00005 small punishment for being outside of white circleoutside blue circle -0.012 punishment for being outside of blue circleteammate damage enemy 0.03 reward from teammate damaging enemiesteammate get up 0.6 reward from teammate getting upI help teammate up 4 reward for helping teammate upinterrupt helping teammate up -0.05 punishment for the interruption to help teammate upobstacle avoidance 0.012 punishment for being stuckgoal 1 reward of goal completionTable 5: The introduction of different rewards.Date Iteration #params Change4/14/2023 1 1802702 Experiment started4/27/2023 1808552 1802702 Env-init: Random weapons5/8/2023 2829170 1803087 Action: Add a fire action for long distance5/10/2023 3034011 1803087 Env-init:Random safe area in the whole map5/11/2023 3130353 1803855 Observation: Add number of remaining players in the game5/12/2023 3198564 2412975 Observation: Add BEV feature5/16/2023 3673506 2418111 Observation: Add history rotation feature5/22/2023 4519567 2418368 Observation: Add rotation change feature5/29/2023 5442025 2418368 Reward: Add rewards for teamwork6/2/2023 5899503 2418368 Update new game version6/13/2023 7306607 3013409 Network: Add obstacle avoidance reward and corresponding value head6/14/2023 7404118 3015457 Observation: Add distance feature to nearby obstacles6/16/2023 7628098 3015457 Env-init: Player numbers per team increased to 46/19/2023 7974450 3109267 Action: Use attention to select target to attackTable 8: The major changes in the training procedure.Goal Processing.。To construct the goal-conditioned policy πgon top of π, we need toencode a goal gthat is generated from Grnd,GopandGllm.。Thus, we propose a network Fas anencoder to achieve that.。In practice, to improvethe representation, we further include other inputsbesides gas (1) goal_mode_info : a 10-dimensionalvector that indicates whether current learning isgoal-conditioned, the achieved times of g, andthe rest time to complete this goal (as we presetthe maximum timestep for goal completion is30s), short in vinfo; (2) sub_goal_masking : a68-dimensional 0-1 vector that indicates whichsub-goals are masked for their corresponding values are 0, short in vmask ; (3) expected_goal_of_current_state : namely, forstatt, we generate a goal g′=PROJ(st)which is a 68-dimensional vector as the same as g, short in g′.。Then, we form theinput for Fasx= (g, g′, vinfo, vmask).。Considering that each item in xis heterogeneous on the data type, so we transformthem with independent FC layers and then follows a ResNet block for each of them.。We can express this process aseg=RESNET(FC(g)), e g′=RESNET(FC(g′)), e info=RESNET(FC(vinfo)), e mask =RESNET(FC(vmask)),where eg,eg′,einfo,emask are the embedding representation corresponding to each input item, and all of them are the samein dimension size.。Then, we aggregate them with computing the average of them as to get a fusion embedding asefusion = (eg+eg′+einfo+emask)/4.With surgery, we now concatenate the embedding of backbone of πwithefusion and fusing them via a FC layer, to form abackbone for πg.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub-goal Class CandidatesDamage to enemy [Zero,Low,Little low,Medium,Little high,High]Whether knock down enemy [True,False]Whether kill enemy [True,False]Whether seen enemy [True,False]Whether seen by enemy [True,False]Number of enemies have ever seen [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]Length of distance moved [No movement,Short,Medium,Long,Very long]Average velocity [Static,Slow,Medium,Fast,Falling]Horizontal direction of movement [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Horizontal direction of view [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Pitch direction of view [Low,Little low,Medium,Little high,High]Health level [Empty,Low,Medium,High,Full]Whether to restore health [True,False]Whether the health is damaged [True,False]Whether rescued teammate [True,False]Whether be knocked down [True,False]Whether prone position [True,False]Whether have a gun [True,False]Whether have bullets [True,False]Whether have medical kits [True,False]Distance with nearest enemy [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether closer with nearest enemy [True,False]Whether crouch position [True,False]Whether hold a gun [True,False]Length of distance from agent to teammate [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether seen by teammate [True,False]Teammate’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Whether follow with the views of teammate [True,False]Whether target the same enemy as teammate [True,False]Whether follow with the movement direction of teammate [True,False]Horizontal direction of movement of enemy [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Velocity of enemy [Static,Slow,Medium,Fast,Falling,None]Enemy’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Table 7: Overview of sub-goal classes, we show a part of them here.D.。SurgeryAs the project proceeded, Contra was continuously improved to satisfy richer featured environment dynamics.。However,such an iterative development poses some challenges to the research of open-ended learning in an embodied situation, asthe changes in API and environment attributes will make the training be non-stationary.。A popular solution to resolve thisissue is the surgery introduced by Berner et al.。(2019), which significantly reduces training time by maximizing retention ofpreviously learned abilities.. \n",
      "the piece 765\n",
      " \n",
      "n, such as the nearest enemy’s location, has also been introduced as an input to the value estimation, for theconsideration of stabilizing the policy learning (Vinyals et al., 2019).Figure 5: Network structure of our proposed policy.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationFeature Weight Descriptionenemy discovery 0.02 reward for see an enemydetected by enemy -0.002 punishment for being seen by an enemyscout 0.0001 reward for search for an enemyno-op -0.0002 punishment for stopping and doing nothingbullet 0.015 reward for using and refilling bulletshealth point 0.03 reward for health point changesbe knocked down -2.5 punishment for being knocked downdead -3.5 punishment for being killeddamage enemy 0.1 reward for damaging an enemyknock down enemy 4.5 reward for knocking down an enemykill enemy 3.5 reward for killing an enemyapproach a downed teammate 0.001 reward for approaching a downed teammatehelp a downed teammate up 0.8 reward for helping up a downed teammatenot save a downed teammate -0.5 punishment for not saving a downed teammatego to blue circle 0.00015 reward for going to blue circlebe in white circle -0.00005 small punishment for being outside of white circleoutside blue circle -0.012 punishment for being outside of blue circleteammate damage enemy 0.03 reward from teammate damaging enemiesteammate get up 0.6 reward from teammate getting upI help teammate up 4 reward for helping teammate upinterrupt helping teammate up -0.05 punishment for the interruption to help teammate upobstacle avoidance 0.012 punishment for being stuckgoal 1 reward of goal completionTable 5: The introduction of different rewards.Date Iteration #params Change4/14/2023 1 1802702 Experiment started4/27/2023 1808552 1802702 Env-init: Random weapons5/8/2023 2829170 1803087 Action: Add a fire action for long distance5/10/2023 3034011 1803087 Env-init:Random safe area in the whole map5/11/2023 3130353 1803855 Observation: Add number of remaining players in the game5/12/2023 3198564 2412975 Observation: Add BEV feature5/16/2023 3673506 2418111 Observation: Add history rotation feature5/22/2023 4519567 2418368 Observation: Add rotation change feature5/29/2023 5442025 2418368 Reward: Add rewards for teamwork6/2/2023 5899503 2418368 Update new game version6/13/2023 7306607 3013409 Network: Add obstacle avoidance reward and corresponding value head6/14/2023 7404118 3015457 Observation: Add distance feature to nearby obstacles6/16/2023 7628098 3015457 Env-init: Player numbers per team increased to 46/19/2023 7974450 3109267 Action: Use attention to select target to attackTable 8: The major changes in the training procedure.Goal Processing.。To construct the goal-conditioned policy πgon top of π, we need toencode a goal gthat is generated from Grnd,GopandGllm.。Thus, we propose a network Fas anencoder to achieve that.。In practice, to improvethe representation, we further include other inputsbesides gas (1) goal_mode_info : a 10-dimensionalvector that indicates whether current learning isgoal-conditioned, the achieved times of g, andthe rest time to complete this goal (as we presetthe maximum timestep for goal completion is30s), short in vinfo; (2) sub_goal_masking : a68-dimensional 0-1 vector that indicates whichsub-goals are masked for their corresponding values are 0, short in vmask ; (3) expected_goal_of_current_state : namely, forstatt, we generate a goal g′=PROJ(st)which is a 68-dimensional vector as the same as g, short in g′.。Then, we form theinput for Fasx= (g, g′, vinfo, vmask).。Considering that each item in xis heterogeneous on the data type, so we transformthem with independent FC layers and then follows a ResNet block for each of them.。We can express this process aseg=RESNET(FC(g)), e g′=RESNET(FC(g′)), e info=RESNET(FC(vinfo)), e mask =RESNET(FC(vmask)),where eg,eg′,einfo,emask are the embedding representation corresponding to each input item, and all of them are the samein dimension size.。Then, we aggregate them with computing the average of them as to get a fusion embedding asefusion = (eg+eg′+einfo+emask)/4.With surgery, we now concatenate the embedding of backbone of πwithefusion and fusing them via a FC layer, to form abackbone for πg.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub-goal Class CandidatesDamage to enemy [Zero,Low,Little low,Medium,Little high,High]Whether knock down enemy [True,False]Whether kill enemy [True,False]Whether seen enemy [True,False]Whether seen by enemy [True,False]Number of enemies have ever seen [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]Length of distance moved [No movement,Short,Medium,Long,Very long]Average velocity [Static,Slow,Medium,Fast,Falling]Horizontal direction of movement [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Horizontal direction of view [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Pitch direction of view [Low,Little low,Medium,Little high,High]Health level [Empty,Low,Medium,High,Full]Whether to restore health [True,False]Whether the health is damaged [True,False]Whether rescued teammate [True,False]Whether be knocked down [True,False]Whether prone position [True,False]Whether have a gun [True,False]Whether have bullets [True,False]Whether have medical kits [True,False]Distance with nearest enemy [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether closer with nearest enemy [True,False]Whether crouch position [True,False]Whether hold a gun [True,False]Length of distance from agent to teammate [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether seen by teammate [True,False]Teammate’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Whether follow with the views of teammate [True,False]Whether target the same enemy as teammate [True,False]Whether follow with the movement direction of teammate [True,False]Horizontal direction of movement of enemy [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Velocity of enemy [Static,Slow,Medium,Fast,Falling,None]Enemy’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Table 7: Overview of sub-goal classes, we show a part of them here.D.。SurgeryAs the project proceeded, Contra was continuously improved to satisfy richer featured environment dynamics.。However,such an iterative development poses some challenges to the research of open-ended learning in an embodied situation, asthe changes in API and environment attributes will make the training be non-stationary.。A popular solution to resolve thisissue is the surgery introduced by Berner et al.。(2019), which significantly reduces training time by maximizing retention ofpreviously learned abilities.。Similarly, we leverage surgery in four aspects to ensure the training adapts to the new changes,including model architecture, observation space, action space, and reward functions.. \n",
      "the piece 766\n",
      " \n",
      "earning (Vinyals et al., 2019).Figure 5: Network structure of our proposed policy.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationFeature Weight Descriptionenemy discovery 0.02 reward for see an enemydetected by enemy -0.002 punishment for being seen by an enemyscout 0.0001 reward for search for an enemyno-op -0.0002 punishment for stopping and doing nothingbullet 0.015 reward for using and refilling bulletshealth point 0.03 reward for health point changesbe knocked down -2.5 punishment for being knocked downdead -3.5 punishment for being killeddamage enemy 0.1 reward for damaging an enemyknock down enemy 4.5 reward for knocking down an enemykill enemy 3.5 reward for killing an enemyapproach a downed teammate 0.001 reward for approaching a downed teammatehelp a downed teammate up 0.8 reward for helping up a downed teammatenot save a downed teammate -0.5 punishment for not saving a downed teammatego to blue circle 0.00015 reward for going to blue circlebe in white circle -0.00005 small punishment for being outside of white circleoutside blue circle -0.012 punishment for being outside of blue circleteammate damage enemy 0.03 reward from teammate damaging enemiesteammate get up 0.6 reward from teammate getting upI help teammate up 4 reward for helping teammate upinterrupt helping teammate up -0.05 punishment for the interruption to help teammate upobstacle avoidance 0.012 punishment for being stuckgoal 1 reward of goal completionTable 5: The introduction of different rewards.Date Iteration #params Change4/14/2023 1 1802702 Experiment started4/27/2023 1808552 1802702 Env-init: Random weapons5/8/2023 2829170 1803087 Action: Add a fire action for long distance5/10/2023 3034011 1803087 Env-init:Random safe area in the whole map5/11/2023 3130353 1803855 Observation: Add number of remaining players in the game5/12/2023 3198564 2412975 Observation: Add BEV feature5/16/2023 3673506 2418111 Observation: Add history rotation feature5/22/2023 4519567 2418368 Observation: Add rotation change feature5/29/2023 5442025 2418368 Reward: Add rewards for teamwork6/2/2023 5899503 2418368 Update new game version6/13/2023 7306607 3013409 Network: Add obstacle avoidance reward and corresponding value head6/14/2023 7404118 3015457 Observation: Add distance feature to nearby obstacles6/16/2023 7628098 3015457 Env-init: Player numbers per team increased to 46/19/2023 7974450 3109267 Action: Use attention to select target to attackTable 8: The major changes in the training procedure.Goal Processing.。To construct the goal-conditioned policy πgon top of π, we need toencode a goal gthat is generated from Grnd,GopandGllm.。Thus, we propose a network Fas anencoder to achieve that.。In practice, to improvethe representation, we further include other inputsbesides gas (1) goal_mode_info : a 10-dimensionalvector that indicates whether current learning isgoal-conditioned, the achieved times of g, andthe rest time to complete this goal (as we presetthe maximum timestep for goal completion is30s), short in vinfo; (2) sub_goal_masking : a68-dimensional 0-1 vector that indicates whichsub-goals are masked for their corresponding values are 0, short in vmask ; (3) expected_goal_of_current_state : namely, forstatt, we generate a goal g′=PROJ(st)which is a 68-dimensional vector as the same as g, short in g′.。Then, we form theinput for Fasx= (g, g′, vinfo, vmask).。Considering that each item in xis heterogeneous on the data type, so we transformthem with independent FC layers and then follows a ResNet block for each of them.。We can express this process aseg=RESNET(FC(g)), e g′=RESNET(FC(g′)), e info=RESNET(FC(vinfo)), e mask =RESNET(FC(vmask)),where eg,eg′,einfo,emask are the embedding representation corresponding to each input item, and all of them are the samein dimension size.。Then, we aggregate them with computing the average of them as to get a fusion embedding asefusion = (eg+eg′+einfo+emask)/4.With surgery, we now concatenate the embedding of backbone of πwithefusion and fusing them via a FC layer, to form abackbone for πg.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub-goal Class CandidatesDamage to enemy [Zero,Low,Little low,Medium,Little high,High]Whether knock down enemy [True,False]Whether kill enemy [True,False]Whether seen enemy [True,False]Whether seen by enemy [True,False]Number of enemies have ever seen [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]Length of distance moved [No movement,Short,Medium,Long,Very long]Average velocity [Static,Slow,Medium,Fast,Falling]Horizontal direction of movement [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Horizontal direction of view [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Pitch direction of view [Low,Little low,Medium,Little high,High]Health level [Empty,Low,Medium,High,Full]Whether to restore health [True,False]Whether the health is damaged [True,False]Whether rescued teammate [True,False]Whether be knocked down [True,False]Whether prone position [True,False]Whether have a gun [True,False]Whether have bullets [True,False]Whether have medical kits [True,False]Distance with nearest enemy [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether closer with nearest enemy [True,False]Whether crouch position [True,False]Whether hold a gun [True,False]Length of distance from agent to teammate [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether seen by teammate [True,False]Teammate’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Whether follow with the views of teammate [True,False]Whether target the same enemy as teammate [True,False]Whether follow with the movement direction of teammate [True,False]Horizontal direction of movement of enemy [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Velocity of enemy [Static,Slow,Medium,Fast,Falling,None]Enemy’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Table 7: Overview of sub-goal classes, we show a part of them here.D.。SurgeryAs the project proceeded, Contra was continuously improved to satisfy richer featured environment dynamics.。However,such an iterative development poses some challenges to the research of open-ended learning in an embodied situation, asthe changes in API and environment attributes will make the training be non-stationary.。A popular solution to resolve thisissue is the surgery introduced by Berner et al.。(2019), which significantly reduces training time by maximizing retention ofpreviously learned abilities.。Similarly, we leverage surgery in four aspects to ensure the training adapts to the new changes,including model architecture, observation space, action space, and reward functions.。Table 8 illustrates the main changeswe conducted and the corresponding parameters.. \n",
      "the piece 767\n",
      " \n",
      "ed Agent via Language-Policy Bidirectional AdaptationFeature Weight Descriptionenemy discovery 0.02 reward for see an enemydetected by enemy -0.002 punishment for being seen by an enemyscout 0.0001 reward for search for an enemyno-op -0.0002 punishment for stopping and doing nothingbullet 0.015 reward for using and refilling bulletshealth point 0.03 reward for health point changesbe knocked down -2.5 punishment for being knocked downdead -3.5 punishment for being killeddamage enemy 0.1 reward for damaging an enemyknock down enemy 4.5 reward for knocking down an enemykill enemy 3.5 reward for killing an enemyapproach a downed teammate 0.001 reward for approaching a downed teammatehelp a downed teammate up 0.8 reward for helping up a downed teammatenot save a downed teammate -0.5 punishment for not saving a downed teammatego to blue circle 0.00015 reward for going to blue circlebe in white circle -0.00005 small punishment for being outside of white circleoutside blue circle -0.012 punishment for being outside of blue circleteammate damage enemy 0.03 reward from teammate damaging enemiesteammate get up 0.6 reward from teammate getting upI help teammate up 4 reward for helping teammate upinterrupt helping teammate up -0.05 punishment for the interruption to help teammate upobstacle avoidance 0.012 punishment for being stuckgoal 1 reward of goal completionTable 5: The introduction of different rewards.Date Iteration #params Change4/14/2023 1 1802702 Experiment started4/27/2023 1808552 1802702 Env-init: Random weapons5/8/2023 2829170 1803087 Action: Add a fire action for long distance5/10/2023 3034011 1803087 Env-init:Random safe area in the whole map5/11/2023 3130353 1803855 Observation: Add number of remaining players in the game5/12/2023 3198564 2412975 Observation: Add BEV feature5/16/2023 3673506 2418111 Observation: Add history rotation feature5/22/2023 4519567 2418368 Observation: Add rotation change feature5/29/2023 5442025 2418368 Reward: Add rewards for teamwork6/2/2023 5899503 2418368 Update new game version6/13/2023 7306607 3013409 Network: Add obstacle avoidance reward and corresponding value head6/14/2023 7404118 3015457 Observation: Add distance feature to nearby obstacles6/16/2023 7628098 3015457 Env-init: Player numbers per team increased to 46/19/2023 7974450 3109267 Action: Use attention to select target to attackTable 8: The major changes in the training procedure.Goal Processing.。To construct the goal-conditioned policy πgon top of π, we need toencode a goal gthat is generated from Grnd,GopandGllm.。Thus, we propose a network Fas anencoder to achieve that.。In practice, to improvethe representation, we further include other inputsbesides gas (1) goal_mode_info : a 10-dimensionalvector that indicates whether current learning isgoal-conditioned, the achieved times of g, andthe rest time to complete this goal (as we presetthe maximum timestep for goal completion is30s), short in vinfo; (2) sub_goal_masking : a68-dimensional 0-1 vector that indicates whichsub-goals are masked for their corresponding values are 0, short in vmask ; (3) expected_goal_of_current_state : namely, forstatt, we generate a goal g′=PROJ(st)which is a 68-dimensional vector as the same as g, short in g′.。Then, we form theinput for Fasx= (g, g′, vinfo, vmask).。Considering that each item in xis heterogeneous on the data type, so we transformthem with independent FC layers and then follows a ResNet block for each of them.。We can express this process aseg=RESNET(FC(g)), e g′=RESNET(FC(g′)), e info=RESNET(FC(vinfo)), e mask =RESNET(FC(vmask)),where eg,eg′,einfo,emask are the embedding representation corresponding to each input item, and all of them are the samein dimension size.。Then, we aggregate them with computing the average of them as to get a fusion embedding asefusion = (eg+eg′+einfo+emask)/4.With surgery, we now concatenate the embedding of backbone of πwithefusion and fusing them via a FC layer, to form abackbone for πg.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub-goal Class CandidatesDamage to enemy [Zero,Low,Little low,Medium,Little high,High]Whether knock down enemy [True,False]Whether kill enemy [True,False]Whether seen enemy [True,False]Whether seen by enemy [True,False]Number of enemies have ever seen [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]Length of distance moved [No movement,Short,Medium,Long,Very long]Average velocity [Static,Slow,Medium,Fast,Falling]Horizontal direction of movement [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Horizontal direction of view [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Pitch direction of view [Low,Little low,Medium,Little high,High]Health level [Empty,Low,Medium,High,Full]Whether to restore health [True,False]Whether the health is damaged [True,False]Whether rescued teammate [True,False]Whether be knocked down [True,False]Whether prone position [True,False]Whether have a gun [True,False]Whether have bullets [True,False]Whether have medical kits [True,False]Distance with nearest enemy [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether closer with nearest enemy [True,False]Whether crouch position [True,False]Whether hold a gun [True,False]Length of distance from agent to teammate [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether seen by teammate [True,False]Teammate’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Whether follow with the views of teammate [True,False]Whether target the same enemy as teammate [True,False]Whether follow with the movement direction of teammate [True,False]Horizontal direction of movement of enemy [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Velocity of enemy [Static,Slow,Medium,Fast,Falling,None]Enemy’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Table 7: Overview of sub-goal classes, we show a part of them here.D.。SurgeryAs the project proceeded, Contra was continuously improved to satisfy richer featured environment dynamics.。However,such an iterative development poses some challenges to the research of open-ended learning in an embodied situation, asthe changes in API and environment attributes will make the training be non-stationary.。A popular solution to resolve thisissue is the surgery introduced by Berner et al.。(2019), which significantly reduces training time by maximizing retention ofpreviously learned abilities.。Similarly, we leverage surgery in four aspects to ensure the training adapts to the new changes,including model architecture, observation space, action space, and reward functions.。Table 8 illustrates the main changeswe conducted and the corresponding parameters.。For the surgery of observation space and model architecture, we haveintroduced a decoupled encoding in Appendix A; for the surgery of action space, we directly extend the policy head in widthto satisfy the new action space; for the reward functions, the essentials are to include the newly introduced features whichcan contribute to the learning, as we introduced in Appendix A, a linear combination has been considered to satisfy thisrequirement.. \n",
      "the piece 768\n",
      " \n",
      "at indicates whether current learning isgoal-conditioned, the achieved times of g, andthe rest time to complete this goal (as we presetthe maximum timestep for goal completion is30s), short in vinfo; (2) sub_goal_masking : a68-dimensional 0-1 vector that indicates whichsub-goals are masked for their corresponding values are 0, short in vmask ; (3) expected_goal_of_current_state : namely, forstatt, we generate a goal g′=PROJ(st)which is a 68-dimensional vector as the same as g, short in g′.。Then, we form theinput for Fasx= (g, g′, vinfo, vmask).。Considering that each item in xis heterogeneous on the data type, so we transformthem with independent FC layers and then follows a ResNet block for each of them.。We can express this process aseg=RESNET(FC(g)), e g′=RESNET(FC(g′)), e info=RESNET(FC(vinfo)), e mask =RESNET(FC(vmask)),where eg,eg′,einfo,emask are the embedding representation corresponding to each input item, and all of them are the samein dimension size.。Then, we aggregate them with computing the average of them as to get a fusion embedding asefusion = (eg+eg′+einfo+emask)/4.With surgery, we now concatenate the embedding of backbone of πwithefusion and fusing them via a FC layer, to form abackbone for πg.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub-goal Class CandidatesDamage to enemy [Zero,Low,Little low,Medium,Little high,High]Whether knock down enemy [True,False]Whether kill enemy [True,False]Whether seen enemy [True,False]Whether seen by enemy [True,False]Number of enemies have ever seen [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]Length of distance moved [No movement,Short,Medium,Long,Very long]Average velocity [Static,Slow,Medium,Fast,Falling]Horizontal direction of movement [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Horizontal direction of view [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Pitch direction of view [Low,Little low,Medium,Little high,High]Health level [Empty,Low,Medium,High,Full]Whether to restore health [True,False]Whether the health is damaged [True,False]Whether rescued teammate [True,False]Whether be knocked down [True,False]Whether prone position [True,False]Whether have a gun [True,False]Whether have bullets [True,False]Whether have medical kits [True,False]Distance with nearest enemy [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether closer with nearest enemy [True,False]Whether crouch position [True,False]Whether hold a gun [True,False]Length of distance from agent to teammate [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether seen by teammate [True,False]Teammate’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Whether follow with the views of teammate [True,False]Whether target the same enemy as teammate [True,False]Whether follow with the movement direction of teammate [True,False]Horizontal direction of movement of enemy [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Velocity of enemy [Static,Slow,Medium,Fast,Falling,None]Enemy’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Table 7: Overview of sub-goal classes, we show a part of them here.D.。SurgeryAs the project proceeded, Contra was continuously improved to satisfy richer featured environment dynamics.。However,such an iterative development poses some challenges to the research of open-ended learning in an embodied situation, asthe changes in API and environment attributes will make the training be non-stationary.。A popular solution to resolve thisissue is the surgery introduced by Berner et al.。(2019), which significantly reduces training time by maximizing retention ofpreviously learned abilities.。Similarly, we leverage surgery in four aspects to ensure the training adapts to the new changes,including model architecture, observation space, action space, and reward functions.。Table 8 illustrates the main changeswe conducted and the corresponding parameters.。For the surgery of observation space and model architecture, we haveintroduced a decoupled encoding in Appendix A; for the surgery of action space, we directly extend the policy head in widthto satisfy the new action space; for the reward functions, the essentials are to include the newly introduced features whichcan contribute to the learning, as we introduced in Appendix A, a linear combination has been considered to satisfy thisrequirement.。In our experiment, we propose three novel surgery methods, where two for model architecture and one forobservation space.. \n",
      "the piece 769\n",
      " \n",
      "(as we presetthe maximum timestep for goal completion is30s), short in vinfo; (2) sub_goal_masking : a68-dimensional 0-1 vector that indicates whichsub-goals are masked for their corresponding values are 0, short in vmask ; (3) expected_goal_of_current_state : namely, forstatt, we generate a goal g′=PROJ(st)which is a 68-dimensional vector as the same as g, short in g′.。Then, we form theinput for Fasx= (g, g′, vinfo, vmask).。Considering that each item in xis heterogeneous on the data type, so we transformthem with independent FC layers and then follows a ResNet block for each of them.。We can express this process aseg=RESNET(FC(g)), e g′=RESNET(FC(g′)), e info=RESNET(FC(vinfo)), e mask =RESNET(FC(vmask)),where eg,eg′,einfo,emask are the embedding representation corresponding to each input item, and all of them are the samein dimension size.。Then, we aggregate them with computing the average of them as to get a fusion embedding asefusion = (eg+eg′+einfo+emask)/4.With surgery, we now concatenate the embedding of backbone of πwithefusion and fusing them via a FC layer, to form abackbone for πg.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub-goal Class CandidatesDamage to enemy [Zero,Low,Little low,Medium,Little high,High]Whether knock down enemy [True,False]Whether kill enemy [True,False]Whether seen enemy [True,False]Whether seen by enemy [True,False]Number of enemies have ever seen [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]Length of distance moved [No movement,Short,Medium,Long,Very long]Average velocity [Static,Slow,Medium,Fast,Falling]Horizontal direction of movement [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Horizontal direction of view [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Pitch direction of view [Low,Little low,Medium,Little high,High]Health level [Empty,Low,Medium,High,Full]Whether to restore health [True,False]Whether the health is damaged [True,False]Whether rescued teammate [True,False]Whether be knocked down [True,False]Whether prone position [True,False]Whether have a gun [True,False]Whether have bullets [True,False]Whether have medical kits [True,False]Distance with nearest enemy [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether closer with nearest enemy [True,False]Whether crouch position [True,False]Whether hold a gun [True,False]Length of distance from agent to teammate [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether seen by teammate [True,False]Teammate’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Whether follow with the views of teammate [True,False]Whether target the same enemy as teammate [True,False]Whether follow with the movement direction of teammate [True,False]Horizontal direction of movement of enemy [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Velocity of enemy [Static,Slow,Medium,Fast,Falling,None]Enemy’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Table 7: Overview of sub-goal classes, we show a part of them here.D.。SurgeryAs the project proceeded, Contra was continuously improved to satisfy richer featured environment dynamics.。However,such an iterative development poses some challenges to the research of open-ended learning in an embodied situation, asthe changes in API and environment attributes will make the training be non-stationary.。A popular solution to resolve thisissue is the surgery introduced by Berner et al.。(2019), which significantly reduces training time by maximizing retention ofpreviously learned abilities.。Similarly, we leverage surgery in four aspects to ensure the training adapts to the new changes,including model architecture, observation space, action space, and reward functions.。Table 8 illustrates the main changeswe conducted and the corresponding parameters.。For the surgery of observation space and model architecture, we haveintroduced a decoupled encoding in Appendix A; for the surgery of action space, we directly extend the policy head in widthto satisfy the new action space; for the reward functions, the essentials are to include the newly introduced features whichcan contribute to the learning, as we introduced in Appendix A, a linear combination has been considered to satisfy thisrequirement.。In our experiment, we propose three novel surgery methods, where two for model architecture and one forobservation space.。The game environment has changed several times since the training started.. \n",
      "the piece 770\n",
      " \n",
      "), short in vinfo; (2) sub_goal_masking : a68-dimensional 0-1 vector that indicates whichsub-goals are masked for their corresponding values are 0, short in vmask ; (3) expected_goal_of_current_state : namely, forstatt, we generate a goal g′=PROJ(st)which is a 68-dimensional vector as the same as g, short in g′.。Then, we form theinput for Fasx= (g, g′, vinfo, vmask).。Considering that each item in xis heterogeneous on the data type, so we transformthem with independent FC layers and then follows a ResNet block for each of them.。We can express this process aseg=RESNET(FC(g)), e g′=RESNET(FC(g′)), e info=RESNET(FC(vinfo)), e mask =RESNET(FC(vmask)),where eg,eg′,einfo,emask are the embedding representation corresponding to each input item, and all of them are the samein dimension size.。Then, we aggregate them with computing the average of them as to get a fusion embedding asefusion = (eg+eg′+einfo+emask)/4.With surgery, we now concatenate the embedding of backbone of πwithefusion and fusing them via a FC layer, to form abackbone for πg.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub-goal Class CandidatesDamage to enemy [Zero,Low,Little low,Medium,Little high,High]Whether knock down enemy [True,False]Whether kill enemy [True,False]Whether seen enemy [True,False]Whether seen by enemy [True,False]Number of enemies have ever seen [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]Length of distance moved [No movement,Short,Medium,Long,Very long]Average velocity [Static,Slow,Medium,Fast,Falling]Horizontal direction of movement [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Horizontal direction of view [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Pitch direction of view [Low,Little low,Medium,Little high,High]Health level [Empty,Low,Medium,High,Full]Whether to restore health [True,False]Whether the health is damaged [True,False]Whether rescued teammate [True,False]Whether be knocked down [True,False]Whether prone position [True,False]Whether have a gun [True,False]Whether have bullets [True,False]Whether have medical kits [True,False]Distance with nearest enemy [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether closer with nearest enemy [True,False]Whether crouch position [True,False]Whether hold a gun [True,False]Length of distance from agent to teammate [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether seen by teammate [True,False]Teammate’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Whether follow with the views of teammate [True,False]Whether target the same enemy as teammate [True,False]Whether follow with the movement direction of teammate [True,False]Horizontal direction of movement of enemy [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Velocity of enemy [Static,Slow,Medium,Fast,Falling,None]Enemy’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Table 7: Overview of sub-goal classes, we show a part of them here.D.。SurgeryAs the project proceeded, Contra was continuously improved to satisfy richer featured environment dynamics.。However,such an iterative development poses some challenges to the research of open-ended learning in an embodied situation, asthe changes in API and environment attributes will make the training be non-stationary.。A popular solution to resolve thisissue is the surgery introduced by Berner et al.。(2019), which significantly reduces training time by maximizing retention ofpreviously learned abilities.。Similarly, we leverage surgery in four aspects to ensure the training adapts to the new changes,including model architecture, observation space, action space, and reward functions.。Table 8 illustrates the main changeswe conducted and the corresponding parameters.。For the surgery of observation space and model architecture, we haveintroduced a decoupled encoding in Appendix A; for the surgery of action space, we directly extend the policy head in widthto satisfy the new action space; for the reward functions, the essentials are to include the newly introduced features whichcan contribute to the learning, as we introduced in Appendix A, a linear combination has been considered to satisfy thisrequirement.。In our experiment, we propose three novel surgery methods, where two for model architecture and one forobservation space.。The game environment has changed several times since the training started.。The changes are mainlyabout adding player characters, adding player skills, adding weapon, modifying the map, etc.. \n",
      "the piece 771\n",
      " \n",
      "e mask =RESNET(FC(vmask)),where eg,eg′,einfo,emask are the embedding representation corresponding to each input item, and all of them are the samein dimension size.。Then, we aggregate them with computing the average of them as to get a fusion embedding asefusion = (eg+eg′+einfo+emask)/4.With surgery, we now concatenate the embedding of backbone of πwithefusion and fusing them via a FC layer, to form abackbone for πg.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub-goal Class CandidatesDamage to enemy [Zero,Low,Little low,Medium,Little high,High]Whether knock down enemy [True,False]Whether kill enemy [True,False]Whether seen enemy [True,False]Whether seen by enemy [True,False]Number of enemies have ever seen [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]Length of distance moved [No movement,Short,Medium,Long,Very long]Average velocity [Static,Slow,Medium,Fast,Falling]Horizontal direction of movement [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Horizontal direction of view [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Pitch direction of view [Low,Little low,Medium,Little high,High]Health level [Empty,Low,Medium,High,Full]Whether to restore health [True,False]Whether the health is damaged [True,False]Whether rescued teammate [True,False]Whether be knocked down [True,False]Whether prone position [True,False]Whether have a gun [True,False]Whether have bullets [True,False]Whether have medical kits [True,False]Distance with nearest enemy [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether closer with nearest enemy [True,False]Whether crouch position [True,False]Whether hold a gun [True,False]Length of distance from agent to teammate [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether seen by teammate [True,False]Teammate’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Whether follow with the views of teammate [True,False]Whether target the same enemy as teammate [True,False]Whether follow with the movement direction of teammate [True,False]Horizontal direction of movement of enemy [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Velocity of enemy [Static,Slow,Medium,Fast,Falling,None]Enemy’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Table 7: Overview of sub-goal classes, we show a part of them here.D.。SurgeryAs the project proceeded, Contra was continuously improved to satisfy richer featured environment dynamics.。However,such an iterative development poses some challenges to the research of open-ended learning in an embodied situation, asthe changes in API and environment attributes will make the training be non-stationary.。A popular solution to resolve thisissue is the surgery introduced by Berner et al.。(2019), which significantly reduces training time by maximizing retention ofpreviously learned abilities.。Similarly, we leverage surgery in four aspects to ensure the training adapts to the new changes,including model architecture, observation space, action space, and reward functions.。Table 8 illustrates the main changeswe conducted and the corresponding parameters.。For the surgery of observation space and model architecture, we haveintroduced a decoupled encoding in Appendix A; for the surgery of action space, we directly extend the policy head in widthto satisfy the new action space; for the reward functions, the essentials are to include the newly introduced features whichcan contribute to the learning, as we introduced in Appendix A, a linear combination has been considered to satisfy thisrequirement.。In our experiment, we propose three novel surgery methods, where two for model architecture and one forobservation space.。The game environment has changed several times since the training started.。The changes are mainlyabout adding player characters, adding player skills, adding weapon, modifying the map, etc.。For all these changes, theproportion of new environments in which we train our policy grows slowly from 0%to100% .. \n",
      "the piece 772\n",
      " \n",
      "e embedding representation corresponding to each input item, and all of them are the samein dimension size.。Then, we aggregate them with computing the average of them as to get a fusion embedding asefusion = (eg+eg′+einfo+emask)/4.With surgery, we now concatenate the embedding of backbone of πwithefusion and fusing them via a FC layer, to form abackbone for πg.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub-goal Class CandidatesDamage to enemy [Zero,Low,Little low,Medium,Little high,High]Whether knock down enemy [True,False]Whether kill enemy [True,False]Whether seen enemy [True,False]Whether seen by enemy [True,False]Number of enemies have ever seen [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]Length of distance moved [No movement,Short,Medium,Long,Very long]Average velocity [Static,Slow,Medium,Fast,Falling]Horizontal direction of movement [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Horizontal direction of view [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Pitch direction of view [Low,Little low,Medium,Little high,High]Health level [Empty,Low,Medium,High,Full]Whether to restore health [True,False]Whether the health is damaged [True,False]Whether rescued teammate [True,False]Whether be knocked down [True,False]Whether prone position [True,False]Whether have a gun [True,False]Whether have bullets [True,False]Whether have medical kits [True,False]Distance with nearest enemy [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether closer with nearest enemy [True,False]Whether crouch position [True,False]Whether hold a gun [True,False]Length of distance from agent to teammate [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether seen by teammate [True,False]Teammate’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Whether follow with the views of teammate [True,False]Whether target the same enemy as teammate [True,False]Whether follow with the movement direction of teammate [True,False]Horizontal direction of movement of enemy [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Velocity of enemy [Static,Slow,Medium,Fast,Falling,None]Enemy’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Table 7: Overview of sub-goal classes, we show a part of them here.D.。SurgeryAs the project proceeded, Contra was continuously improved to satisfy richer featured environment dynamics.。However,such an iterative development poses some challenges to the research of open-ended learning in an embodied situation, asthe changes in API and environment attributes will make the training be non-stationary.。A popular solution to resolve thisissue is the surgery introduced by Berner et al.。(2019), which significantly reduces training time by maximizing retention ofpreviously learned abilities.。Similarly, we leverage surgery in four aspects to ensure the training adapts to the new changes,including model architecture, observation space, action space, and reward functions.。Table 8 illustrates the main changeswe conducted and the corresponding parameters.。For the surgery of observation space and model architecture, we haveintroduced a decoupled encoding in Appendix A; for the surgery of action space, we directly extend the policy head in widthto satisfy the new action space; for the reward functions, the essentials are to include the newly introduced features whichcan contribute to the learning, as we introduced in Appendix A, a linear combination has been considered to satisfy thisrequirement.。In our experiment, we propose three novel surgery methods, where two for model architecture and one forobservation space.。The game environment has changed several times since the training started.。The changes are mainlyabout adding player characters, adding player skills, adding weapon, modifying the map, etc.。For all these changes, theproportion of new environments in which we train our policy grows slowly from 0%to100% .。In this case, the variance isrelatively small and the performance would quickly recover in the new environment.. \n",
      "the piece 773\n",
      " \n",
      "em as to get a fusion embedding asefusion = (eg+eg′+einfo+emask)/4.With surgery, we now concatenate the embedding of backbone of πwithefusion and fusing them via a FC layer, to form abackbone for πg.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub-goal Class CandidatesDamage to enemy [Zero,Low,Little low,Medium,Little high,High]Whether knock down enemy [True,False]Whether kill enemy [True,False]Whether seen enemy [True,False]Whether seen by enemy [True,False]Number of enemies have ever seen [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]Length of distance moved [No movement,Short,Medium,Long,Very long]Average velocity [Static,Slow,Medium,Fast,Falling]Horizontal direction of movement [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Horizontal direction of view [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Pitch direction of view [Low,Little low,Medium,Little high,High]Health level [Empty,Low,Medium,High,Full]Whether to restore health [True,False]Whether the health is damaged [True,False]Whether rescued teammate [True,False]Whether be knocked down [True,False]Whether prone position [True,False]Whether have a gun [True,False]Whether have bullets [True,False]Whether have medical kits [True,False]Distance with nearest enemy [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether closer with nearest enemy [True,False]Whether crouch position [True,False]Whether hold a gun [True,False]Length of distance from agent to teammate [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether seen by teammate [True,False]Teammate’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Whether follow with the views of teammate [True,False]Whether target the same enemy as teammate [True,False]Whether follow with the movement direction of teammate [True,False]Horizontal direction of movement of enemy [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Velocity of enemy [Static,Slow,Medium,Fast,Falling,None]Enemy’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Table 7: Overview of sub-goal classes, we show a part of them here.D.。SurgeryAs the project proceeded, Contra was continuously improved to satisfy richer featured environment dynamics.。However,such an iterative development poses some challenges to the research of open-ended learning in an embodied situation, asthe changes in API and environment attributes will make the training be non-stationary.。A popular solution to resolve thisissue is the surgery introduced by Berner et al.。(2019), which significantly reduces training time by maximizing retention ofpreviously learned abilities.。Similarly, we leverage surgery in four aspects to ensure the training adapts to the new changes,including model architecture, observation space, action space, and reward functions.。Table 8 illustrates the main changeswe conducted and the corresponding parameters.。For the surgery of observation space and model architecture, we haveintroduced a decoupled encoding in Appendix A; for the surgery of action space, we directly extend the policy head in widthto satisfy the new action space; for the reward functions, the essentials are to include the newly introduced features whichcan contribute to the learning, as we introduced in Appendix A, a linear combination has been considered to satisfy thisrequirement.。In our experiment, we propose three novel surgery methods, where two for model architecture and one forobservation space.。The game environment has changed several times since the training started.。The changes are mainlyabout adding player characters, adding player skills, adding weapon, modifying the map, etc.。For all these changes, theproportion of new environments in which we train our policy grows slowly from 0%to100% .。In this case, the variance isrelatively small and the performance would quickly recover in the new environment.。Figure 6 evaluates the utility of surgery,illustrating the changes in basic value during the training.. \n",
      "the piece 774\n",
      " \n",
      "tional AdaptationSub-goal Class CandidatesDamage to enemy [Zero,Low,Little low,Medium,Little high,High]Whether knock down enemy [True,False]Whether kill enemy [True,False]Whether seen enemy [True,False]Whether seen by enemy [True,False]Number of enemies have ever seen [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]Length of distance moved [No movement,Short,Medium,Long,Very long]Average velocity [Static,Slow,Medium,Fast,Falling]Horizontal direction of movement [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Horizontal direction of view [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Pitch direction of view [Low,Little low,Medium,Little high,High]Health level [Empty,Low,Medium,High,Full]Whether to restore health [True,False]Whether the health is damaged [True,False]Whether rescued teammate [True,False]Whether be knocked down [True,False]Whether prone position [True,False]Whether have a gun [True,False]Whether have bullets [True,False]Whether have medical kits [True,False]Distance with nearest enemy [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether closer with nearest enemy [True,False]Whether crouch position [True,False]Whether hold a gun [True,False]Length of distance from agent to teammate [Touch,Nearby,Moderate,Far,Out of reach,Extreme Far]Whether seen by teammate [True,False]Teammate’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]Whether follow with the views of teammate [True,False]Whether target the same enemy as teammate [True,False]Whether follow with the movement direction of teammate [True,False]Horizontal direction of movement of enemy [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Velocity of enemy [Static,Slow,Medium,Fast,Falling,None]Enemy’s position relative to agent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]Table 7: Overview of sub-goal classes, we show a part of them here.D.。SurgeryAs the project proceeded, Contra was continuously improved to satisfy richer featured environment dynamics.。However,such an iterative development poses some challenges to the research of open-ended learning in an embodied situation, asthe changes in API and environment attributes will make the training be non-stationary.。A popular solution to resolve thisissue is the surgery introduced by Berner et al.。(2019), which significantly reduces training time by maximizing retention ofpreviously learned abilities.。Similarly, we leverage surgery in four aspects to ensure the training adapts to the new changes,including model architecture, observation space, action space, and reward functions.。Table 8 illustrates the main changeswe conducted and the corresponding parameters.。For the surgery of observation space and model architecture, we haveintroduced a decoupled encoding in Appendix A; for the surgery of action space, we directly extend the policy head in widthto satisfy the new action space; for the reward functions, the essentials are to include the newly introduced features whichcan contribute to the learning, as we introduced in Appendix A, a linear combination has been considered to satisfy thisrequirement.。In our experiment, we propose three novel surgery methods, where two for model architecture and one forobservation space.。The game environment has changed several times since the training started.。The changes are mainlyabout adding player characters, adding player skills, adding weapon, modifying the map, etc.。For all these changes, theproportion of new environments in which we train our policy grows slowly from 0%to100% .。In this case, the variance isrelatively small and the performance would quickly recover in the new environment.。Figure 6 evaluates the utility of surgery,illustrating the changes in basic value during the training.。It can be seen that the values change smoothly for most surgeries.Meanwhile, the values remain stable after the surgeries.. \n",
      "the piece 775\n",
      " \n",
      "stationary.。A popular solution to resolve thisissue is the surgery introduced by Berner et al.。(2019), which significantly reduces training time by maximizing retention ofpreviously learned abilities.。Similarly, we leverage surgery in four aspects to ensure the training adapts to the new changes,including model architecture, observation space, action space, and reward functions.。Table 8 illustrates the main changeswe conducted and the corresponding parameters.。For the surgery of observation space and model architecture, we haveintroduced a decoupled encoding in Appendix A; for the surgery of action space, we directly extend the policy head in widthto satisfy the new action space; for the reward functions, the essentials are to include the newly introduced features whichcan contribute to the learning, as we introduced in Appendix A, a linear combination has been considered to satisfy thisrequirement.。In our experiment, we propose three novel surgery methods, where two for model architecture and one forobservation space.。The game environment has changed several times since the training started.。The changes are mainlyabout adding player characters, adding player skills, adding weapon, modifying the map, etc.。For all these changes, theproportion of new environments in which we train our policy grows slowly from 0%to100% .。In this case, the variance isrelatively small and the performance would quickly recover in the new environment.。Figure 6 evaluates the utility of surgery,illustrating the changes in basic value during the training.。It can be seen that the values change smoothly for most surgeries.Meanwhile, the values remain stable after the surgeries.。These results prove the effectiveness of our surgery.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation04/14 04/28 05/12 05/26 06/09 06/23 07/07Dates-1012345Basic ValueFigure 6: The value changes during the training process.E.. \n",
      "the piece 776\n",
      " \n",
      "cantly reduces training time by maximizing retention ofpreviously learned abilities.。Similarly, we leverage surgery in four aspects to ensure the training adapts to the new changes,including model architecture, observation space, action space, and reward functions.。Table 8 illustrates the main changeswe conducted and the corresponding parameters.。For the surgery of observation space and model architecture, we haveintroduced a decoupled encoding in Appendix A; for the surgery of action space, we directly extend the policy head in widthto satisfy the new action space; for the reward functions, the essentials are to include the newly introduced features whichcan contribute to the learning, as we introduced in Appendix A, a linear combination has been considered to satisfy thisrequirement.。In our experiment, we propose three novel surgery methods, where two for model architecture and one forobservation space.。The game environment has changed several times since the training started.。The changes are mainlyabout adding player characters, adding player skills, adding weapon, modifying the map, etc.。For all these changes, theproportion of new environments in which we train our policy grows slowly from 0%to100% .。In this case, the variance isrelatively small and the performance would quickly recover in the new environment.。Figure 6 evaluates the utility of surgery,illustrating the changes in basic value during the training.。It can be seen that the values change smoothly for most surgeries.Meanwhile, the values remain stable after the surgeries.。These results prove the effectiveness of our surgery.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation04/14 04/28 05/12 05/26 06/09 06/23 07/07Dates-1012345Basic ValueFigure 6: The value changes during the training process.E.。Datasets ConstructionThe process of fine-tuning the language model is operationalized through a question and answer paradigm.. \n",
      "the piece 777\n",
      " \n",
      "vation space, action space, and reward functions.。Table 8 illustrates the main changeswe conducted and the corresponding parameters.。For the surgery of observation space and model architecture, we haveintroduced a decoupled encoding in Appendix A; for the surgery of action space, we directly extend the policy head in widthto satisfy the new action space; for the reward functions, the essentials are to include the newly introduced features whichcan contribute to the learning, as we introduced in Appendix A, a linear combination has been considered to satisfy thisrequirement.。In our experiment, we propose three novel surgery methods, where two for model architecture and one forobservation space.。The game environment has changed several times since the training started.。The changes are mainlyabout adding player characters, adding player skills, adding weapon, modifying the map, etc.。For all these changes, theproportion of new environments in which we train our policy grows slowly from 0%to100% .。In this case, the variance isrelatively small and the performance would quickly recover in the new environment.。Figure 6 evaluates the utility of surgery,illustrating the changes in basic value during the training.。It can be seen that the values change smoothly for most surgeries.Meanwhile, the values remain stable after the surgeries.。These results prove the effectiveness of our surgery.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation04/14 04/28 05/12 05/26 06/09 06/23 07/07Dates-1012345Basic ValueFigure 6: The value changes during the training process.E.。Datasets ConstructionThe process of fine-tuning the language model is operationalized through a question and answer paradigm.。In thisframework, we provide the language model with a comprehensive depiction of the present conditions pertaining to theagent, its companions, and adversaries.. \n",
      "the piece 778\n",
      " \n",
      "eswe conducted and the corresponding parameters.。For the surgery of observation space and model architecture, we haveintroduced a decoupled encoding in Appendix A; for the surgery of action space, we directly extend the policy head in widthto satisfy the new action space; for the reward functions, the essentials are to include the newly introduced features whichcan contribute to the learning, as we introduced in Appendix A, a linear combination has been considered to satisfy thisrequirement.。In our experiment, we propose three novel surgery methods, where two for model architecture and one forobservation space.。The game environment has changed several times since the training started.。The changes are mainlyabout adding player characters, adding player skills, adding weapon, modifying the map, etc.。For all these changes, theproportion of new environments in which we train our policy grows slowly from 0%to100% .。In this case, the variance isrelatively small and the performance would quickly recover in the new environment.。Figure 6 evaluates the utility of surgery,illustrating the changes in basic value during the training.。It can be seen that the values change smoothly for most surgeries.Meanwhile, the values remain stable after the surgeries.。These results prove the effectiveness of our surgery.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation04/14 04/28 05/12 05/26 06/09 06/23 07/07Dates-1012345Basic ValueFigure 6: The value changes during the training process.E.。Datasets ConstructionThe process of fine-tuning the language model is operationalized through a question and answer paradigm.。In thisframework, we provide the language model with a comprehensive depiction of the present conditions pertaining to theagent, its companions, and adversaries.。Additionally, we furnish the model with the linguistic instructions furnished byits teammates.. \n",
      "the piece 779\n",
      " \n",
      "e, we haveintroduced a decoupled encoding in Appendix A; for the surgery of action space, we directly extend the policy head in widthto satisfy the new action space; for the reward functions, the essentials are to include the newly introduced features whichcan contribute to the learning, as we introduced in Appendix A, a linear combination has been considered to satisfy thisrequirement.。In our experiment, we propose three novel surgery methods, where two for model architecture and one forobservation space.。The game environment has changed several times since the training started.。The changes are mainlyabout adding player characters, adding player skills, adding weapon, modifying the map, etc.。For all these changes, theproportion of new environments in which we train our policy grows slowly from 0%to100% .。In this case, the variance isrelatively small and the performance would quickly recover in the new environment.。Figure 6 evaluates the utility of surgery,illustrating the changes in basic value during the training.。It can be seen that the values change smoothly for most surgeries.Meanwhile, the values remain stable after the surgeries.。These results prove the effectiveness of our surgery.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation04/14 04/28 05/12 05/26 06/09 06/23 07/07Dates-1012345Basic ValueFigure 6: The value changes during the training process.E.。Datasets ConstructionThe process of fine-tuning the language model is operationalized through a question and answer paradigm.。In thisframework, we provide the language model with a comprehensive depiction of the present conditions pertaining to theagent, its companions, and adversaries.。Additionally, we furnish the model with the linguistic instructions furnished byits teammates.。Subsequently, the language model is tasked with generating the agent’s subsequent target meta state inaccordance with the specifics outlined in the question and answer dataset, as elucidated in Table 17.. \n",
      "the piece 780\n",
      " \n",
      "unctions, the essentials are to include the newly introduced features whichcan contribute to the learning, as we introduced in Appendix A, a linear combination has been considered to satisfy thisrequirement.。In our experiment, we propose three novel surgery methods, where two for model architecture and one forobservation space.。The game environment has changed several times since the training started.。The changes are mainlyabout adding player characters, adding player skills, adding weapon, modifying the map, etc.。For all these changes, theproportion of new environments in which we train our policy grows slowly from 0%to100% .。In this case, the variance isrelatively small and the performance would quickly recover in the new environment.。Figure 6 evaluates the utility of surgery,illustrating the changes in basic value during the training.。It can be seen that the values change smoothly for most surgeries.Meanwhile, the values remain stable after the surgeries.。These results prove the effectiveness of our surgery.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation04/14 04/28 05/12 05/26 06/09 06/23 07/07Dates-1012345Basic ValueFigure 6: The value changes during the training process.E.。Datasets ConstructionThe process of fine-tuning the language model is operationalized through a question and answer paradigm.。In thisframework, we provide the language model with a comprehensive depiction of the present conditions pertaining to theagent, its companions, and adversaries.。Additionally, we furnish the model with the linguistic instructions furnished byits teammates.。Subsequently, the language model is tasked with generating the agent’s subsequent target meta state inaccordance with the specifics outlined in the question and answer dataset, as elucidated in Table 17.。The response is generatedby GPT-4 and subsequently subjected to parsing and formatting processes facilitated by the rule-based coding mechanism.To ensure alignment between the responses generated by GPT-4 and the targeted meta-state format, a comprehensive promptengineering endeavor was conducted to establish a set of predetermined rules for GPT-4.. \n",
      "the piece 781\n",
      " \n",
      "ibute to the learning, as we introduced in Appendix A, a linear combination has been considered to satisfy thisrequirement.。In our experiment, we propose three novel surgery methods, where two for model architecture and one forobservation space.。The game environment has changed several times since the training started.。The changes are mainlyabout adding player characters, adding player skills, adding weapon, modifying the map, etc.。For all these changes, theproportion of new environments in which we train our policy grows slowly from 0%to100% .。In this case, the variance isrelatively small and the performance would quickly recover in the new environment.。Figure 6 evaluates the utility of surgery,illustrating the changes in basic value during the training.。It can be seen that the values change smoothly for most surgeries.Meanwhile, the values remain stable after the surgeries.。These results prove the effectiveness of our surgery.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation04/14 04/28 05/12 05/26 06/09 06/23 07/07Dates-1012345Basic ValueFigure 6: The value changes during the training process.E.。Datasets ConstructionThe process of fine-tuning the language model is operationalized through a question and answer paradigm.。In thisframework, we provide the language model with a comprehensive depiction of the present conditions pertaining to theagent, its companions, and adversaries.。Additionally, we furnish the model with the linguistic instructions furnished byits teammates.。Subsequently, the language model is tasked with generating the agent’s subsequent target meta state inaccordance with the specifics outlined in the question and answer dataset, as elucidated in Table 17.。The response is generatedby GPT-4 and subsequently subjected to parsing and formatting processes facilitated by the rule-based coding mechanism.To ensure alignment between the responses generated by GPT-4 and the targeted meta-state format, a comprehensive promptengineering endeavor was conducted to establish a set of predetermined rules for GPT-4.。The rule-based prompts, whichwere employed to guide GPT-4’s responses, are meticulously documented in Table 10.In an effort to mitigate the risk of overfitting our model to concise formatted outputs, thereby preserving its capacity forenvironmental reasoning, we augmented our dataset with a substantial volume of Chain of Thought data.. \n",
      "the piece 782\n",
      " \n",
      " changes, theproportion of new environments in which we train our policy grows slowly from 0%to100% .。In this case, the variance isrelatively small and the performance would quickly recover in the new environment.。Figure 6 evaluates the utility of surgery,illustrating the changes in basic value during the training.。It can be seen that the values change smoothly for most surgeries.Meanwhile, the values remain stable after the surgeries.。These results prove the effectiveness of our surgery.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation04/14 04/28 05/12 05/26 06/09 06/23 07/07Dates-1012345Basic ValueFigure 6: The value changes during the training process.E.。Datasets ConstructionThe process of fine-tuning the language model is operationalized through a question and answer paradigm.。In thisframework, we provide the language model with a comprehensive depiction of the present conditions pertaining to theagent, its companions, and adversaries.。Additionally, we furnish the model with the linguistic instructions furnished byits teammates.。Subsequently, the language model is tasked with generating the agent’s subsequent target meta state inaccordance with the specifics outlined in the question and answer dataset, as elucidated in Table 17.。The response is generatedby GPT-4 and subsequently subjected to parsing and formatting processes facilitated by the rule-based coding mechanism.To ensure alignment between the responses generated by GPT-4 and the targeted meta-state format, a comprehensive promptengineering endeavor was conducted to establish a set of predetermined rules for GPT-4.。The rule-based prompts, whichwere employed to guide GPT-4’s responses, are meticulously documented in Table 10.In an effort to mitigate the risk of overfitting our model to concise formatted outputs, thereby preserving its capacity forenvironmental reasoning, we augmented our dataset with a substantial volume of Chain of Thought data.。This augmentationapproach entails a systematic procedure whereby the large language model is guided through a step-by-step ideationprocess, ultimately culminating in the attainment of the intended target state.. \n",
      "the piece 783\n",
      " \n",
      "iance isrelatively small and the performance would quickly recover in the new environment.。Figure 6 evaluates the utility of surgery,illustrating the changes in basic value during the training.。It can be seen that the values change smoothly for most surgeries.Meanwhile, the values remain stable after the surgeries.。These results prove the effectiveness of our surgery.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation04/14 04/28 05/12 05/26 06/09 06/23 07/07Dates-1012345Basic ValueFigure 6: The value changes during the training process.E.。Datasets ConstructionThe process of fine-tuning the language model is operationalized through a question and answer paradigm.。In thisframework, we provide the language model with a comprehensive depiction of the present conditions pertaining to theagent, its companions, and adversaries.。Additionally, we furnish the model with the linguistic instructions furnished byits teammates.。Subsequently, the language model is tasked with generating the agent’s subsequent target meta state inaccordance with the specifics outlined in the question and answer dataset, as elucidated in Table 17.。The response is generatedby GPT-4 and subsequently subjected to parsing and formatting processes facilitated by the rule-based coding mechanism.To ensure alignment between the responses generated by GPT-4 and the targeted meta-state format, a comprehensive promptengineering endeavor was conducted to establish a set of predetermined rules for GPT-4.。The rule-based prompts, whichwere employed to guide GPT-4’s responses, are meticulously documented in Table 10.In an effort to mitigate the risk of overfitting our model to concise formatted outputs, thereby preserving its capacity forenvironmental reasoning, we augmented our dataset with a substantial volume of Chain of Thought data.。This augmentationapproach entails a systematic procedure whereby the large language model is guided through a step-by-step ideationprocess, ultimately culminating in the attainment of the intended target state.。Concretely, our methodology commenceswith an initial inquiry into the semantic interpretation of the given instruction, followed by the identification of pertinentstates, contemplation of state adjustments, analysis of action propensities, and an estimation of the requisite temporalconsiderations.. \n",
      "the piece 784\n",
      " \n",
      "w environment.。Figure 6 evaluates the utility of surgery,illustrating the changes in basic value during the training.。It can be seen that the values change smoothly for most surgeries.Meanwhile, the values remain stable after the surgeries.。These results prove the effectiveness of our surgery.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation04/14 04/28 05/12 05/26 06/09 06/23 07/07Dates-1012345Basic ValueFigure 6: The value changes during the training process.E.。Datasets ConstructionThe process of fine-tuning the language model is operationalized through a question and answer paradigm.。In thisframework, we provide the language model with a comprehensive depiction of the present conditions pertaining to theagent, its companions, and adversaries.。Additionally, we furnish the model with the linguistic instructions furnished byits teammates.。Subsequently, the language model is tasked with generating the agent’s subsequent target meta state inaccordance with the specifics outlined in the question and answer dataset, as elucidated in Table 17.。The response is generatedby GPT-4 and subsequently subjected to parsing and formatting processes facilitated by the rule-based coding mechanism.To ensure alignment between the responses generated by GPT-4 and the targeted meta-state format, a comprehensive promptengineering endeavor was conducted to establish a set of predetermined rules for GPT-4.。The rule-based prompts, whichwere employed to guide GPT-4’s responses, are meticulously documented in Table 10.In an effort to mitigate the risk of overfitting our model to concise formatted outputs, thereby preserving its capacity forenvironmental reasoning, we augmented our dataset with a substantial volume of Chain of Thought data.。This augmentationapproach entails a systematic procedure whereby the large language model is guided through a step-by-step ideationprocess, ultimately culminating in the attainment of the intended target state.。Concretely, our methodology commenceswith an initial inquiry into the semantic interpretation of the given instruction, followed by the identification of pertinentstates, contemplation of state adjustments, analysis of action propensities, and an estimation of the requisite temporalconsiderations.。Comprehensive documentation of the detailed prompts and ensuing responses derived from the Chainof Thought procedure can be found in Tables 16 and 9.. \n",
      "the piece 785\n",
      " \n",
      ".。It can be seen that the values change smoothly for most surgeries.Meanwhile, the values remain stable after the surgeries.。These results prove the effectiveness of our surgery.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation04/14 04/28 05/12 05/26 06/09 06/23 07/07Dates-1012345Basic ValueFigure 6: The value changes during the training process.E.。Datasets ConstructionThe process of fine-tuning the language model is operationalized through a question and answer paradigm.。In thisframework, we provide the language model with a comprehensive depiction of the present conditions pertaining to theagent, its companions, and adversaries.。Additionally, we furnish the model with the linguistic instructions furnished byits teammates.。Subsequently, the language model is tasked with generating the agent’s subsequent target meta state inaccordance with the specifics outlined in the question and answer dataset, as elucidated in Table 17.。The response is generatedby GPT-4 and subsequently subjected to parsing and formatting processes facilitated by the rule-based coding mechanism.To ensure alignment between the responses generated by GPT-4 and the targeted meta-state format, a comprehensive promptengineering endeavor was conducted to establish a set of predetermined rules for GPT-4.。The rule-based prompts, whichwere employed to guide GPT-4’s responses, are meticulously documented in Table 10.In an effort to mitigate the risk of overfitting our model to concise formatted outputs, thereby preserving its capacity forenvironmental reasoning, we augmented our dataset with a substantial volume of Chain of Thought data.。This augmentationapproach entails a systematic procedure whereby the large language model is guided through a step-by-step ideationprocess, ultimately culminating in the attainment of the intended target state.。Concretely, our methodology commenceswith an initial inquiry into the semantic interpretation of the given instruction, followed by the identification of pertinentstates, contemplation of state adjustments, analysis of action propensities, and an estimation of the requisite temporalconsiderations.。Comprehensive documentation of the detailed prompts and ensuing responses derived from the Chainof Thought procedure can be found in Tables 16 and 9.。It is noteworthy that traditional Chain of Thought processes inexisting large language models often generate sequential thoughts, a method characterized by a relatively protracted temporaltrajectory.. \n",
      "the piece 786\n",
      " \n",
      "rgeries.。These results prove the effectiveness of our surgery.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation04/14 04/28 05/12 05/26 06/09 06/23 07/07Dates-1012345Basic ValueFigure 6: The value changes during the training process.E.。Datasets ConstructionThe process of fine-tuning the language model is operationalized through a question and answer paradigm.。In thisframework, we provide the language model with a comprehensive depiction of the present conditions pertaining to theagent, its companions, and adversaries.。Additionally, we furnish the model with the linguistic instructions furnished byits teammates.。Subsequently, the language model is tasked with generating the agent’s subsequent target meta state inaccordance with the specifics outlined in the question and answer dataset, as elucidated in Table 17.。The response is generatedby GPT-4 and subsequently subjected to parsing and formatting processes facilitated by the rule-based coding mechanism.To ensure alignment between the responses generated by GPT-4 and the targeted meta-state format, a comprehensive promptengineering endeavor was conducted to establish a set of predetermined rules for GPT-4.。The rule-based prompts, whichwere employed to guide GPT-4’s responses, are meticulously documented in Table 10.In an effort to mitigate the risk of overfitting our model to concise formatted outputs, thereby preserving its capacity forenvironmental reasoning, we augmented our dataset with a substantial volume of Chain of Thought data.。This augmentationapproach entails a systematic procedure whereby the large language model is guided through a step-by-step ideationprocess, ultimately culminating in the attainment of the intended target state.。Concretely, our methodology commenceswith an initial inquiry into the semantic interpretation of the given instruction, followed by the identification of pertinentstates, contemplation of state adjustments, analysis of action propensities, and an estimation of the requisite temporalconsiderations.。Comprehensive documentation of the detailed prompts and ensuing responses derived from the Chainof Thought procedure can be found in Tables 16 and 9.。It is noteworthy that traditional Chain of Thought processes inexisting large language models often generate sequential thoughts, a method characterized by a relatively protracted temporaltrajectory.。This sequential reasoning approach may not be well-suited to the high real-time demands typically encounteredin first-person shooter (FPS) games.. \n",
      "the piece 787\n",
      " \n",
      "icy Bidirectional Adaptation04/14 04/28 05/12 05/26 06/09 06/23 07/07Dates-1012345Basic ValueFigure 6: The value changes during the training process.E.。Datasets ConstructionThe process of fine-tuning the language model is operationalized through a question and answer paradigm.。In thisframework, we provide the language model with a comprehensive depiction of the present conditions pertaining to theagent, its companions, and adversaries.。Additionally, we furnish the model with the linguistic instructions furnished byits teammates.。Subsequently, the language model is tasked with generating the agent’s subsequent target meta state inaccordance with the specifics outlined in the question and answer dataset, as elucidated in Table 17.。The response is generatedby GPT-4 and subsequently subjected to parsing and formatting processes facilitated by the rule-based coding mechanism.To ensure alignment between the responses generated by GPT-4 and the targeted meta-state format, a comprehensive promptengineering endeavor was conducted to establish a set of predetermined rules for GPT-4.。The rule-based prompts, whichwere employed to guide GPT-4’s responses, are meticulously documented in Table 10.In an effort to mitigate the risk of overfitting our model to concise formatted outputs, thereby preserving its capacity forenvironmental reasoning, we augmented our dataset with a substantial volume of Chain of Thought data.。This augmentationapproach entails a systematic procedure whereby the large language model is guided through a step-by-step ideationprocess, ultimately culminating in the attainment of the intended target state.。Concretely, our methodology commenceswith an initial inquiry into the semantic interpretation of the given instruction, followed by the identification of pertinentstates, contemplation of state adjustments, analysis of action propensities, and an estimation of the requisite temporalconsiderations.。Comprehensive documentation of the detailed prompts and ensuing responses derived from the Chainof Thought procedure can be found in Tables 16 and 9.。It is noteworthy that traditional Chain of Thought processes inexisting large language models often generate sequential thoughts, a method characterized by a relatively protracted temporaltrajectory.。This sequential reasoning approach may not be well-suited to the high real-time demands typically encounteredin first-person shooter (FPS) games.。Furthermore, the singular-step reasoning capabilities inherent in smaller languagemodels are intrinsically modest and prone to errors.. \n",
      "the piece 788\n",
      " \n",
      "he value changes during the training process.E.。Datasets ConstructionThe process of fine-tuning the language model is operationalized through a question and answer paradigm.。In thisframework, we provide the language model with a comprehensive depiction of the present conditions pertaining to theagent, its companions, and adversaries.。Additionally, we furnish the model with the linguistic instructions furnished byits teammates.。Subsequently, the language model is tasked with generating the agent’s subsequent target meta state inaccordance with the specifics outlined in the question and answer dataset, as elucidated in Table 17.。The response is generatedby GPT-4 and subsequently subjected to parsing and formatting processes facilitated by the rule-based coding mechanism.To ensure alignment between the responses generated by GPT-4 and the targeted meta-state format, a comprehensive promptengineering endeavor was conducted to establish a set of predetermined rules for GPT-4.。The rule-based prompts, whichwere employed to guide GPT-4’s responses, are meticulously documented in Table 10.In an effort to mitigate the risk of overfitting our model to concise formatted outputs, thereby preserving its capacity forenvironmental reasoning, we augmented our dataset with a substantial volume of Chain of Thought data.。This augmentationapproach entails a systematic procedure whereby the large language model is guided through a step-by-step ideationprocess, ultimately culminating in the attainment of the intended target state.。Concretely, our methodology commenceswith an initial inquiry into the semantic interpretation of the given instruction, followed by the identification of pertinentstates, contemplation of state adjustments, analysis of action propensities, and an estimation of the requisite temporalconsiderations.。Comprehensive documentation of the detailed prompts and ensuing responses derived from the Chainof Thought procedure can be found in Tables 16 and 9.。It is noteworthy that traditional Chain of Thought processes inexisting large language models often generate sequential thoughts, a method characterized by a relatively protracted temporaltrajectory.。This sequential reasoning approach may not be well-suited to the high real-time demands typically encounteredin first-person shooter (FPS) games.。Furthermore, the singular-step reasoning capabilities inherent in smaller languagemodels are intrinsically modest and prone to errors.。Consequently, the amplification of error probabilities within the Chainof Thought reasoning process may not yield superior outcomes.. \n",
      "the piece 789\n",
      " \n",
      "ionalized through a question and answer paradigm.。In thisframework, we provide the language model with a comprehensive depiction of the present conditions pertaining to theagent, its companions, and adversaries.。Additionally, we furnish the model with the linguistic instructions furnished byits teammates.。Subsequently, the language model is tasked with generating the agent’s subsequent target meta state inaccordance with the specifics outlined in the question and answer dataset, as elucidated in Table 17.。The response is generatedby GPT-4 and subsequently subjected to parsing and formatting processes facilitated by the rule-based coding mechanism.To ensure alignment between the responses generated by GPT-4 and the targeted meta-state format, a comprehensive promptengineering endeavor was conducted to establish a set of predetermined rules for GPT-4.。The rule-based prompts, whichwere employed to guide GPT-4’s responses, are meticulously documented in Table 10.In an effort to mitigate the risk of overfitting our model to concise formatted outputs, thereby preserving its capacity forenvironmental reasoning, we augmented our dataset with a substantial volume of Chain of Thought data.。This augmentationapproach entails a systematic procedure whereby the large language model is guided through a step-by-step ideationprocess, ultimately culminating in the attainment of the intended target state.。Concretely, our methodology commenceswith an initial inquiry into the semantic interpretation of the given instruction, followed by the identification of pertinentstates, contemplation of state adjustments, analysis of action propensities, and an estimation of the requisite temporalconsiderations.。Comprehensive documentation of the detailed prompts and ensuing responses derived from the Chainof Thought procedure can be found in Tables 16 and 9.。It is noteworthy that traditional Chain of Thought processes inexisting large language models often generate sequential thoughts, a method characterized by a relatively protracted temporaltrajectory.。This sequential reasoning approach may not be well-suited to the high real-time demands typically encounteredin first-person shooter (FPS) games.。Furthermore, the singular-step reasoning capabilities inherent in smaller languagemodels are intrinsically modest and prone to errors.。Consequently, the amplification of error probabilities within the Chainof Thought reasoning process may not yield superior outcomes.。In light of these considerations, we have undertaken astrategy that amalgamates Chain of Thought data with the final target state data, thereby enhancing the fine-tuning of ourlanguage model.. \n",
      "the piece 790\n",
      " \n",
      "inguistic instructions furnished byits teammates.。Subsequently, the language model is tasked with generating the agent’s subsequent target meta state inaccordance with the specifics outlined in the question and answer dataset, as elucidated in Table 17.。The response is generatedby GPT-4 and subsequently subjected to parsing and formatting processes facilitated by the rule-based coding mechanism.To ensure alignment between the responses generated by GPT-4 and the targeted meta-state format, a comprehensive promptengineering endeavor was conducted to establish a set of predetermined rules for GPT-4.。The rule-based prompts, whichwere employed to guide GPT-4’s responses, are meticulously documented in Table 10.In an effort to mitigate the risk of overfitting our model to concise formatted outputs, thereby preserving its capacity forenvironmental reasoning, we augmented our dataset with a substantial volume of Chain of Thought data.。This augmentationapproach entails a systematic procedure whereby the large language model is guided through a step-by-step ideationprocess, ultimately culminating in the attainment of the intended target state.。Concretely, our methodology commenceswith an initial inquiry into the semantic interpretation of the given instruction, followed by the identification of pertinentstates, contemplation of state adjustments, analysis of action propensities, and an estimation of the requisite temporalconsiderations.。Comprehensive documentation of the detailed prompts and ensuing responses derived from the Chainof Thought procedure can be found in Tables 16 and 9.。It is noteworthy that traditional Chain of Thought processes inexisting large language models often generate sequential thoughts, a method characterized by a relatively protracted temporaltrajectory.。This sequential reasoning approach may not be well-suited to the high real-time demands typically encounteredin first-person shooter (FPS) games.。Furthermore, the singular-step reasoning capabilities inherent in smaller languagemodels are intrinsically modest and prone to errors.。Consequently, the amplification of error probabilities within the Chainof Thought reasoning process may not yield superior outcomes.。In light of these considerations, we have undertaken astrategy that amalgamates Chain of Thought data with the final target state data, thereby enhancing the fine-tuning of ourlanguage model.。In the course of test reasoning exercises, the language model promptly generates the ultimate target state,with the Chain of Thought information being implicitly encoded within the neural network parameters.Instruction Datasets.. \n",
      "the piece 791\n",
      " \n",
      "uent target meta state inaccordance with the specifics outlined in the question and answer dataset, as elucidated in Table 17.。The response is generatedby GPT-4 and subsequently subjected to parsing and formatting processes facilitated by the rule-based coding mechanism.To ensure alignment between the responses generated by GPT-4 and the targeted meta-state format, a comprehensive promptengineering endeavor was conducted to establish a set of predetermined rules for GPT-4.。The rule-based prompts, whichwere employed to guide GPT-4’s responses, are meticulously documented in Table 10.In an effort to mitigate the risk of overfitting our model to concise formatted outputs, thereby preserving its capacity forenvironmental reasoning, we augmented our dataset with a substantial volume of Chain of Thought data.。This augmentationapproach entails a systematic procedure whereby the large language model is guided through a step-by-step ideationprocess, ultimately culminating in the attainment of the intended target state.。Concretely, our methodology commenceswith an initial inquiry into the semantic interpretation of the given instruction, followed by the identification of pertinentstates, contemplation of state adjustments, analysis of action propensities, and an estimation of the requisite temporalconsiderations.。Comprehensive documentation of the detailed prompts and ensuing responses derived from the Chainof Thought procedure can be found in Tables 16 and 9.。It is noteworthy that traditional Chain of Thought processes inexisting large language models often generate sequential thoughts, a method characterized by a relatively protracted temporaltrajectory.。This sequential reasoning approach may not be well-suited to the high real-time demands typically encounteredin first-person shooter (FPS) games.。Furthermore, the singular-step reasoning capabilities inherent in smaller languagemodels are intrinsically modest and prone to errors.。Consequently, the amplification of error probabilities within the Chainof Thought reasoning process may not yield superior outcomes.。In light of these considerations, we have undertaken astrategy that amalgamates Chain of Thought data with the final target state data, thereby enhancing the fine-tuning of ourlanguage model.。In the course of test reasoning exercises, the language model promptly generates the ultimate target state,with the Chain of Thought information being implicitly encoded within the neural network parameters.Instruction Datasets.。To cover a comprehensive range of instruction types and state distributions, we generated fourtypes of instruction sets, which, when combined with states sampled from the environment, result in four different datasets.These are the HI (Human Instruction) dataset, constructed based on human-annotated commands; the SI (State Instruction)dataset, built by reverse-generating commands based on state transitions specified by the intelligent agent; the AI (AgentInstruction) dataset, constructed by main kinds of instruction which can be complete by pre-trained Agent; and the RI(Random Instruction) dataset, generated through random sampling of agent state transitions and random commands.•IH(Human Instructions).. \n",
      "the piece 792\n",
      " \n",
      "d subsequently subjected to parsing and formatting processes facilitated by the rule-based coding mechanism.To ensure alignment between the responses generated by GPT-4 and the targeted meta-state format, a comprehensive promptengineering endeavor was conducted to establish a set of predetermined rules for GPT-4.。The rule-based prompts, whichwere employed to guide GPT-4’s responses, are meticulously documented in Table 10.In an effort to mitigate the risk of overfitting our model to concise formatted outputs, thereby preserving its capacity forenvironmental reasoning, we augmented our dataset with a substantial volume of Chain of Thought data.。This augmentationapproach entails a systematic procedure whereby the large language model is guided through a step-by-step ideationprocess, ultimately culminating in the attainment of the intended target state.。Concretely, our methodology commenceswith an initial inquiry into the semantic interpretation of the given instruction, followed by the identification of pertinentstates, contemplation of state adjustments, analysis of action propensities, and an estimation of the requisite temporalconsiderations.。Comprehensive documentation of the detailed prompts and ensuing responses derived from the Chainof Thought procedure can be found in Tables 16 and 9.。It is noteworthy that traditional Chain of Thought processes inexisting large language models often generate sequential thoughts, a method characterized by a relatively protracted temporaltrajectory.。This sequential reasoning approach may not be well-suited to the high real-time demands typically encounteredin first-person shooter (FPS) games.。Furthermore, the singular-step reasoning capabilities inherent in smaller languagemodels are intrinsically modest and prone to errors.。Consequently, the amplification of error probabilities within the Chainof Thought reasoning process may not yield superior outcomes.。In light of these considerations, we have undertaken astrategy that amalgamates Chain of Thought data with the final target state data, thereby enhancing the fine-tuning of ourlanguage model.。In the course of test reasoning exercises, the language model promptly generates the ultimate target state,with the Chain of Thought information being implicitly encoded within the neural network parameters.Instruction Datasets.。To cover a comprehensive range of instruction types and state distributions, we generated fourtypes of instruction sets, which, when combined with states sampled from the environment, result in four different datasets.These are the HI (Human Instruction) dataset, constructed based on human-annotated commands; the SI (State Instruction)dataset, built by reverse-generating commands based on state transitions specified by the intelligent agent; the AI (AgentInstruction) dataset, constructed by main kinds of instruction which can be complete by pre-trained Agent; and the RI(Random Instruction) dataset, generated through random sampling of agent state transitions and random commands.•IH(Human Instructions).。We generate open-ended instructions manually, while the corresponding states are sampledfrom the intelligent agent’s interaction logs.. \n",
      "the piece 793\n",
      " \n",
      "g mechanism.To ensure alignment between the responses generated by GPT-4 and the targeted meta-state format, a comprehensive promptengineering endeavor was conducted to establish a set of predetermined rules for GPT-4.。The rule-based prompts, whichwere employed to guide GPT-4’s responses, are meticulously documented in Table 10.In an effort to mitigate the risk of overfitting our model to concise formatted outputs, thereby preserving its capacity forenvironmental reasoning, we augmented our dataset with a substantial volume of Chain of Thought data.。This augmentationapproach entails a systematic procedure whereby the large language model is guided through a step-by-step ideationprocess, ultimately culminating in the attainment of the intended target state.。Concretely, our methodology commenceswith an initial inquiry into the semantic interpretation of the given instruction, followed by the identification of pertinentstates, contemplation of state adjustments, analysis of action propensities, and an estimation of the requisite temporalconsiderations.。Comprehensive documentation of the detailed prompts and ensuing responses derived from the Chainof Thought procedure can be found in Tables 16 and 9.。It is noteworthy that traditional Chain of Thought processes inexisting large language models often generate sequential thoughts, a method characterized by a relatively protracted temporaltrajectory.。This sequential reasoning approach may not be well-suited to the high real-time demands typically encounteredin first-person shooter (FPS) games.。Furthermore, the singular-step reasoning capabilities inherent in smaller languagemodels are intrinsically modest and prone to errors.。Consequently, the amplification of error probabilities within the Chainof Thought reasoning process may not yield superior outcomes.。In light of these considerations, we have undertaken astrategy that amalgamates Chain of Thought data with the final target state data, thereby enhancing the fine-tuning of ourlanguage model.。In the course of test reasoning exercises, the language model promptly generates the ultimate target state,with the Chain of Thought information being implicitly encoded within the neural network parameters.Instruction Datasets.。To cover a comprehensive range of instruction types and state distributions, we generated fourtypes of instruction sets, which, when combined with states sampled from the environment, result in four different datasets.These are the HI (Human Instruction) dataset, constructed based on human-annotated commands; the SI (State Instruction)dataset, built by reverse-generating commands based on state transitions specified by the intelligent agent; the AI (AgentInstruction) dataset, constructed by main kinds of instruction which can be complete by pre-trained Agent; and the RI(Random Instruction) dataset, generated through random sampling of agent state transitions and random commands.•IH(Human Instructions).。We generate open-ended instructions manually, while the corresponding states are sampledfrom the intelligent agent’s interaction logs.。These are combined and annotated using GPT-4 based on the promptingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationmethod previously described.. \n",
      "the piece 794\n",
      " \n",
      "es for GPT-4.。The rule-based prompts, whichwere employed to guide GPT-4’s responses, are meticulously documented in Table 10.In an effort to mitigate the risk of overfitting our model to concise formatted outputs, thereby preserving its capacity forenvironmental reasoning, we augmented our dataset with a substantial volume of Chain of Thought data.。This augmentationapproach entails a systematic procedure whereby the large language model is guided through a step-by-step ideationprocess, ultimately culminating in the attainment of the intended target state.。Concretely, our methodology commenceswith an initial inquiry into the semantic interpretation of the given instruction, followed by the identification of pertinentstates, contemplation of state adjustments, analysis of action propensities, and an estimation of the requisite temporalconsiderations.。Comprehensive documentation of the detailed prompts and ensuing responses derived from the Chainof Thought procedure can be found in Tables 16 and 9.。It is noteworthy that traditional Chain of Thought processes inexisting large language models often generate sequential thoughts, a method characterized by a relatively protracted temporaltrajectory.。This sequential reasoning approach may not be well-suited to the high real-time demands typically encounteredin first-person shooter (FPS) games.。Furthermore, the singular-step reasoning capabilities inherent in smaller languagemodels are intrinsically modest and prone to errors.。Consequently, the amplification of error probabilities within the Chainof Thought reasoning process may not yield superior outcomes.。In light of these considerations, we have undertaken astrategy that amalgamates Chain of Thought data with the final target state data, thereby enhancing the fine-tuning of ourlanguage model.。In the course of test reasoning exercises, the language model promptly generates the ultimate target state,with the Chain of Thought information being implicitly encoded within the neural network parameters.Instruction Datasets.。To cover a comprehensive range of instruction types and state distributions, we generated fourtypes of instruction sets, which, when combined with states sampled from the environment, result in four different datasets.These are the HI (Human Instruction) dataset, constructed based on human-annotated commands; the SI (State Instruction)dataset, built by reverse-generating commands based on state transitions specified by the intelligent agent; the AI (AgentInstruction) dataset, constructed by main kinds of instruction which can be complete by pre-trained Agent; and the RI(Random Instruction) dataset, generated through random sampling of agent state transitions and random commands.•IH(Human Instructions).。We generate open-ended instructions manually, while the corresponding states are sampledfrom the intelligent agent’s interaction logs.。These are combined and annotated using GPT-4 based on the promptingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationmethod previously described.。We found that due to varying frequencies of state changes during the agent’s interactions,some states are difficult to capture comprehensively only using random sampling.. \n",
      "the piece 795\n",
      " \n",
      "his augmentationapproach entails a systematic procedure whereby the large language model is guided through a step-by-step ideationprocess, ultimately culminating in the attainment of the intended target state.。Concretely, our methodology commenceswith an initial inquiry into the semantic interpretation of the given instruction, followed by the identification of pertinentstates, contemplation of state adjustments, analysis of action propensities, and an estimation of the requisite temporalconsiderations.。Comprehensive documentation of the detailed prompts and ensuing responses derived from the Chainof Thought procedure can be found in Tables 16 and 9.。It is noteworthy that traditional Chain of Thought processes inexisting large language models often generate sequential thoughts, a method characterized by a relatively protracted temporaltrajectory.。This sequential reasoning approach may not be well-suited to the high real-time demands typically encounteredin first-person shooter (FPS) games.。Furthermore, the singular-step reasoning capabilities inherent in smaller languagemodels are intrinsically modest and prone to errors.。Consequently, the amplification of error probabilities within the Chainof Thought reasoning process may not yield superior outcomes.。In light of these considerations, we have undertaken astrategy that amalgamates Chain of Thought data with the final target state data, thereby enhancing the fine-tuning of ourlanguage model.。In the course of test reasoning exercises, the language model promptly generates the ultimate target state,with the Chain of Thought information being implicitly encoded within the neural network parameters.Instruction Datasets.。To cover a comprehensive range of instruction types and state distributions, we generated fourtypes of instruction sets, which, when combined with states sampled from the environment, result in four different datasets.These are the HI (Human Instruction) dataset, constructed based on human-annotated commands; the SI (State Instruction)dataset, built by reverse-generating commands based on state transitions specified by the intelligent agent; the AI (AgentInstruction) dataset, constructed by main kinds of instruction which can be complete by pre-trained Agent; and the RI(Random Instruction) dataset, generated through random sampling of agent state transitions and random commands.•IH(Human Instructions).。We generate open-ended instructions manually, while the corresponding states are sampledfrom the intelligent agent’s interaction logs.。These are combined and annotated using GPT-4 based on the promptingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationmethod previously described.。We found that due to varying frequencies of state changes during the agent’s interactions,some states are difficult to capture comprehensively only using random sampling.。To ensure a more comprehensivedistribution of states in the data and to facilitate better understanding by the language model, we employ a multi-roundrejection sampling approach to construct state set.. \n",
      "the piece 796\n",
      " \n",
      " by the identification of pertinentstates, contemplation of state adjustments, analysis of action propensities, and an estimation of the requisite temporalconsiderations.。Comprehensive documentation of the detailed prompts and ensuing responses derived from the Chainof Thought procedure can be found in Tables 16 and 9.。It is noteworthy that traditional Chain of Thought processes inexisting large language models often generate sequential thoughts, a method characterized by a relatively protracted temporaltrajectory.。This sequential reasoning approach may not be well-suited to the high real-time demands typically encounteredin first-person shooter (FPS) games.。Furthermore, the singular-step reasoning capabilities inherent in smaller languagemodels are intrinsically modest and prone to errors.。Consequently, the amplification of error probabilities within the Chainof Thought reasoning process may not yield superior outcomes.。In light of these considerations, we have undertaken astrategy that amalgamates Chain of Thought data with the final target state data, thereby enhancing the fine-tuning of ourlanguage model.。In the course of test reasoning exercises, the language model promptly generates the ultimate target state,with the Chain of Thought information being implicitly encoded within the neural network parameters.Instruction Datasets.。To cover a comprehensive range of instruction types and state distributions, we generated fourtypes of instruction sets, which, when combined with states sampled from the environment, result in four different datasets.These are the HI (Human Instruction) dataset, constructed based on human-annotated commands; the SI (State Instruction)dataset, built by reverse-generating commands based on state transitions specified by the intelligent agent; the AI (AgentInstruction) dataset, constructed by main kinds of instruction which can be complete by pre-trained Agent; and the RI(Random Instruction) dataset, generated through random sampling of agent state transitions and random commands.•IH(Human Instructions).。We generate open-ended instructions manually, while the corresponding states are sampledfrom the intelligent agent’s interaction logs.。These are combined and annotated using GPT-4 based on the promptingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationmethod previously described.。We found that due to varying frequencies of state changes during the agent’s interactions,some states are difficult to capture comprehensively only using random sampling.。To ensure a more comprehensivedistribution of states in the data and to facilitate better understanding by the language model, we employ a multi-roundrejection sampling approach to construct state set.。Let Sbe the set of states waiting to be sampled.. \n",
      "the piece 797\n",
      " \n",
      "ed prompts and ensuing responses derived from the Chainof Thought procedure can be found in Tables 16 and 9.。It is noteworthy that traditional Chain of Thought processes inexisting large language models often generate sequential thoughts, a method characterized by a relatively protracted temporaltrajectory.。This sequential reasoning approach may not be well-suited to the high real-time demands typically encounteredin first-person shooter (FPS) games.。Furthermore, the singular-step reasoning capabilities inherent in smaller languagemodels are intrinsically modest and prone to errors.。Consequently, the amplification of error probabilities within the Chainof Thought reasoning process may not yield superior outcomes.。In light of these considerations, we have undertaken astrategy that amalgamates Chain of Thought data with the final target state data, thereby enhancing the fine-tuning of ourlanguage model.。In the course of test reasoning exercises, the language model promptly generates the ultimate target state,with the Chain of Thought information being implicitly encoded within the neural network parameters.Instruction Datasets.。To cover a comprehensive range of instruction types and state distributions, we generated fourtypes of instruction sets, which, when combined with states sampled from the environment, result in four different datasets.These are the HI (Human Instruction) dataset, constructed based on human-annotated commands; the SI (State Instruction)dataset, built by reverse-generating commands based on state transitions specified by the intelligent agent; the AI (AgentInstruction) dataset, constructed by main kinds of instruction which can be complete by pre-trained Agent; and the RI(Random Instruction) dataset, generated through random sampling of agent state transitions and random commands.•IH(Human Instructions).。We generate open-ended instructions manually, while the corresponding states are sampledfrom the intelligent agent’s interaction logs.。These are combined and annotated using GPT-4 based on the promptingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationmethod previously described.。We found that due to varying frequencies of state changes during the agent’s interactions,some states are difficult to capture comprehensively only using random sampling.。To ensure a more comprehensivedistribution of states in the data and to facilitate better understanding by the language model, we employ a multi-roundrejection sampling approach to construct state set.。Let Sbe the set of states waiting to be sampled.。We perform multiplerounds of sampling on S, with Sgetirepresenting the set of states sampled in the i-th round, initially empty.. \n",
      "the piece 798\n",
      " \n",
      "jectory.。This sequential reasoning approach may not be well-suited to the high real-time demands typically encounteredin first-person shooter (FPS) games.。Furthermore, the singular-step reasoning capabilities inherent in smaller languagemodels are intrinsically modest and prone to errors.。Consequently, the amplification of error probabilities within the Chainof Thought reasoning process may not yield superior outcomes.。In light of these considerations, we have undertaken astrategy that amalgamates Chain of Thought data with the final target state data, thereby enhancing the fine-tuning of ourlanguage model.。In the course of test reasoning exercises, the language model promptly generates the ultimate target state,with the Chain of Thought information being implicitly encoded within the neural network parameters.Instruction Datasets.。To cover a comprehensive range of instruction types and state distributions, we generated fourtypes of instruction sets, which, when combined with states sampled from the environment, result in four different datasets.These are the HI (Human Instruction) dataset, constructed based on human-annotated commands; the SI (State Instruction)dataset, built by reverse-generating commands based on state transitions specified by the intelligent agent; the AI (AgentInstruction) dataset, constructed by main kinds of instruction which can be complete by pre-trained Agent; and the RI(Random Instruction) dataset, generated through random sampling of agent state transitions and random commands.•IH(Human Instructions).。We generate open-ended instructions manually, while the corresponding states are sampledfrom the intelligent agent’s interaction logs.。These are combined and annotated using GPT-4 based on the promptingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationmethod previously described.。We found that due to varying frequencies of state changes during the agent’s interactions,some states are difficult to capture comprehensively only using random sampling.。To ensure a more comprehensivedistribution of states in the data and to facilitate better understanding by the language model, we employ a multi-roundrejection sampling approach to construct state set.。Let Sbe the set of states waiting to be sampled.。We perform multiplerounds of sampling on S, with Sgetirepresenting the set of states sampled in the i-th round, initially empty.。Next, wesample a state sfrom Swithout replacement and check whether shas any state values not present in Sgeti.. \n",
      "the piece 799\n",
      " \n",
      "es.。Furthermore, the singular-step reasoning capabilities inherent in smaller languagemodels are intrinsically modest and prone to errors.。Consequently, the amplification of error probabilities within the Chainof Thought reasoning process may not yield superior outcomes.。In light of these considerations, we have undertaken astrategy that amalgamates Chain of Thought data with the final target state data, thereby enhancing the fine-tuning of ourlanguage model.。In the course of test reasoning exercises, the language model promptly generates the ultimate target state,with the Chain of Thought information being implicitly encoded within the neural network parameters.Instruction Datasets.。To cover a comprehensive range of instruction types and state distributions, we generated fourtypes of instruction sets, which, when combined with states sampled from the environment, result in four different datasets.These are the HI (Human Instruction) dataset, constructed based on human-annotated commands; the SI (State Instruction)dataset, built by reverse-generating commands based on state transitions specified by the intelligent agent; the AI (AgentInstruction) dataset, constructed by main kinds of instruction which can be complete by pre-trained Agent; and the RI(Random Instruction) dataset, generated through random sampling of agent state transitions and random commands.•IH(Human Instructions).。We generate open-ended instructions manually, while the corresponding states are sampledfrom the intelligent agent’s interaction logs.。These are combined and annotated using GPT-4 based on the promptingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationmethod previously described.。We found that due to varying frequencies of state changes during the agent’s interactions,some states are difficult to capture comprehensively only using random sampling.。To ensure a more comprehensivedistribution of states in the data and to facilitate better understanding by the language model, we employ a multi-roundrejection sampling approach to construct state set.。Let Sbe the set of states waiting to be sampled.。We perform multiplerounds of sampling on S, with Sgetirepresenting the set of states sampled in the i-th round, initially empty.。Next, wesample a state sfrom Swithout replacement and check whether shas any state values not present in Sgeti.。If it does,we accept it and add it to Sgeti, otherwise we reject it.. \n",
      "the piece 800\n",
      " \n",
      "the Chainof Thought reasoning process may not yield superior outcomes.。In light of these considerations, we have undertaken astrategy that amalgamates Chain of Thought data with the final target state data, thereby enhancing the fine-tuning of ourlanguage model.。In the course of test reasoning exercises, the language model promptly generates the ultimate target state,with the Chain of Thought information being implicitly encoded within the neural network parameters.Instruction Datasets.。To cover a comprehensive range of instruction types and state distributions, we generated fourtypes of instruction sets, which, when combined with states sampled from the environment, result in four different datasets.These are the HI (Human Instruction) dataset, constructed based on human-annotated commands; the SI (State Instruction)dataset, built by reverse-generating commands based on state transitions specified by the intelligent agent; the AI (AgentInstruction) dataset, constructed by main kinds of instruction which can be complete by pre-trained Agent; and the RI(Random Instruction) dataset, generated through random sampling of agent state transitions and random commands.•IH(Human Instructions).。We generate open-ended instructions manually, while the corresponding states are sampledfrom the intelligent agent’s interaction logs.。These are combined and annotated using GPT-4 based on the promptingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationmethod previously described.。We found that due to varying frequencies of state changes during the agent’s interactions,some states are difficult to capture comprehensively only using random sampling.。To ensure a more comprehensivedistribution of states in the data and to facilitate better understanding by the language model, we employ a multi-roundrejection sampling approach to construct state set.。Let Sbe the set of states waiting to be sampled.。We perform multiplerounds of sampling on S, with Sgetirepresenting the set of states sampled in the i-th round, initially empty.。Next, wesample a state sfrom Swithout replacement and check whether shas any state values not present in Sgeti.。If it does,we accept it and add it to Sgeti, otherwise we reject it.。Once all states in Shave been sampled, one round is completed.Sgetiis the result of i-th round’s sampling, and Swill be reset for the next round.. \n",
      "the piece 801\n",
      " \n",
      "tes Chain of Thought data with the final target state data, thereby enhancing the fine-tuning of ourlanguage model.。In the course of test reasoning exercises, the language model promptly generates the ultimate target state,with the Chain of Thought information being implicitly encoded within the neural network parameters.Instruction Datasets.。To cover a comprehensive range of instruction types and state distributions, we generated fourtypes of instruction sets, which, when combined with states sampled from the environment, result in four different datasets.These are the HI (Human Instruction) dataset, constructed based on human-annotated commands; the SI (State Instruction)dataset, built by reverse-generating commands based on state transitions specified by the intelligent agent; the AI (AgentInstruction) dataset, constructed by main kinds of instruction which can be complete by pre-trained Agent; and the RI(Random Instruction) dataset, generated through random sampling of agent state transitions and random commands.•IH(Human Instructions).。We generate open-ended instructions manually, while the corresponding states are sampledfrom the intelligent agent’s interaction logs.。These are combined and annotated using GPT-4 based on the promptingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationmethod previously described.。We found that due to varying frequencies of state changes during the agent’s interactions,some states are difficult to capture comprehensively only using random sampling.。To ensure a more comprehensivedistribution of states in the data and to facilitate better understanding by the language model, we employ a multi-roundrejection sampling approach to construct state set.。Let Sbe the set of states waiting to be sampled.。We perform multiplerounds of sampling on S, with Sgetirepresenting the set of states sampled in the i-th round, initially empty.。Next, wesample a state sfrom Swithout replacement and check whether shas any state values not present in Sgeti.。If it does,we accept it and add it to Sgeti, otherwise we reject it.。Once all states in Shave been sampled, one round is completed.Sgetiis the result of i-th round’s sampling, and Swill be reset for the next round.。This sampling method is employed toenhance the comprehensiveness of state coverage in all datasets except the Random Instruction dataset.•IS(State Instructions).. \n",
      "the piece 802\n",
      " \n",
      "t reasoning exercises, the language model promptly generates the ultimate target state,with the Chain of Thought information being implicitly encoded within the neural network parameters.Instruction Datasets.。To cover a comprehensive range of instruction types and state distributions, we generated fourtypes of instruction sets, which, when combined with states sampled from the environment, result in four different datasets.These are the HI (Human Instruction) dataset, constructed based on human-annotated commands; the SI (State Instruction)dataset, built by reverse-generating commands based on state transitions specified by the intelligent agent; the AI (AgentInstruction) dataset, constructed by main kinds of instruction which can be complete by pre-trained Agent; and the RI(Random Instruction) dataset, generated through random sampling of agent state transitions and random commands.•IH(Human Instructions).。We generate open-ended instructions manually, while the corresponding states are sampledfrom the intelligent agent’s interaction logs.。These are combined and annotated using GPT-4 based on the promptingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationmethod previously described.。We found that due to varying frequencies of state changes during the agent’s interactions,some states are difficult to capture comprehensively only using random sampling.。To ensure a more comprehensivedistribution of states in the data and to facilitate better understanding by the language model, we employ a multi-roundrejection sampling approach to construct state set.。Let Sbe the set of states waiting to be sampled.。We perform multiplerounds of sampling on S, with Sgetirepresenting the set of states sampled in the i-th round, initially empty.。Next, wesample a state sfrom Swithout replacement and check whether shas any state values not present in Sgeti.。If it does,we accept it and add it to Sgeti, otherwise we reject it.。Once all states in Shave been sampled, one round is completed.Sgetiis the result of i-th round’s sampling, and Swill be reset for the next round.。This sampling method is employed toenhance the comprehensiveness of state coverage in all datasets except the Random Instruction dataset.•IS(State Instructions).。We aim to cover a broader range of state changes in the instructions to enhance the languagemodel’s understanding of various state transitions.. \n",
      "the piece 803\n",
      " \n",
      "licitly encoded within the neural network parameters.Instruction Datasets.。To cover a comprehensive range of instruction types and state distributions, we generated fourtypes of instruction sets, which, when combined with states sampled from the environment, result in four different datasets.These are the HI (Human Instruction) dataset, constructed based on human-annotated commands; the SI (State Instruction)dataset, built by reverse-generating commands based on state transitions specified by the intelligent agent; the AI (AgentInstruction) dataset, constructed by main kinds of instruction which can be complete by pre-trained Agent; and the RI(Random Instruction) dataset, generated through random sampling of agent state transitions and random commands.•IH(Human Instructions).。We generate open-ended instructions manually, while the corresponding states are sampledfrom the intelligent agent’s interaction logs.。These are combined and annotated using GPT-4 based on the promptingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationmethod previously described.。We found that due to varying frequencies of state changes during the agent’s interactions,some states are difficult to capture comprehensively only using random sampling.。To ensure a more comprehensivedistribution of states in the data and to facilitate better understanding by the language model, we employ a multi-roundrejection sampling approach to construct state set.。Let Sbe the set of states waiting to be sampled.。We perform multiplerounds of sampling on S, with Sgetirepresenting the set of states sampled in the i-th round, initially empty.。Next, wesample a state sfrom Swithout replacement and check whether shas any state values not present in Sgeti.。If it does,we accept it and add it to Sgeti, otherwise we reject it.。Once all states in Shave been sampled, one round is completed.Sgetiis the result of i-th round’s sampling, and Swill be reset for the next round.。This sampling method is employed toenhance the comprehensiveness of state coverage in all datasets except the Random Instruction dataset.•IS(State Instructions).。We aim to cover a broader range of state changes in the instructions to enhance the languagemodel’s understanding of various state transitions.。To achieve this, we design corresponding goals and instructions forall states.. \n",
      "the piece 804\n",
      " \n",
      "s, which, when combined with states sampled from the environment, result in four different datasets.These are the HI (Human Instruction) dataset, constructed based on human-annotated commands; the SI (State Instruction)dataset, built by reverse-generating commands based on state transitions specified by the intelligent agent; the AI (AgentInstruction) dataset, constructed by main kinds of instruction which can be complete by pre-trained Agent; and the RI(Random Instruction) dataset, generated through random sampling of agent state transitions and random commands.•IH(Human Instructions).。We generate open-ended instructions manually, while the corresponding states are sampledfrom the intelligent agent’s interaction logs.。These are combined and annotated using GPT-4 based on the promptingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationmethod previously described.。We found that due to varying frequencies of state changes during the agent’s interactions,some states are difficult to capture comprehensively only using random sampling.。To ensure a more comprehensivedistribution of states in the data and to facilitate better understanding by the language model, we employ a multi-roundrejection sampling approach to construct state set.。Let Sbe the set of states waiting to be sampled.。We perform multiplerounds of sampling on S, with Sgetirepresenting the set of states sampled in the i-th round, initially empty.。Next, wesample a state sfrom Swithout replacement and check whether shas any state values not present in Sgeti.。If it does,we accept it and add it to Sgeti, otherwise we reject it.。Once all states in Shave been sampled, one round is completed.Sgetiis the result of i-th round’s sampling, and Swill be reset for the next round.。This sampling method is employed toenhance the comprehensiveness of state coverage in all datasets except the Random Instruction dataset.•IS(State Instructions).。We aim to cover a broader range of state changes in the instructions to enhance the languagemodel’s understanding of various state transitions.。To achieve this, we design corresponding goals and instructions forall states.。Specifically, for each value of each state, we generate a series of instructions that require the correspondingstate and value.. \n",
      "the piece 805\n",
      " \n",
      "ilt by reverse-generating commands based on state transitions specified by the intelligent agent; the AI (AgentInstruction) dataset, constructed by main kinds of instruction which can be complete by pre-trained Agent; and the RI(Random Instruction) dataset, generated through random sampling of agent state transitions and random commands.•IH(Human Instructions).。We generate open-ended instructions manually, while the corresponding states are sampledfrom the intelligent agent’s interaction logs.。These are combined and annotated using GPT-4 based on the promptingBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationmethod previously described.。We found that due to varying frequencies of state changes during the agent’s interactions,some states are difficult to capture comprehensively only using random sampling.。To ensure a more comprehensivedistribution of states in the data and to facilitate better understanding by the language model, we employ a multi-roundrejection sampling approach to construct state set.。Let Sbe the set of states waiting to be sampled.。We perform multiplerounds of sampling on S, with Sgetirepresenting the set of states sampled in the i-th round, initially empty.。Next, wesample a state sfrom Swithout replacement and check whether shas any state values not present in Sgeti.。If it does,we accept it and add it to Sgeti, otherwise we reject it.。Once all states in Shave been sampled, one round is completed.Sgetiis the result of i-th round’s sampling, and Swill be reset for the next round.。This sampling method is employed toenhance the comprehensiveness of state coverage in all datasets except the Random Instruction dataset.•IS(State Instructions).。We aim to cover a broader range of state changes in the instructions to enhance the languagemodel’s understanding of various state transitions.。To achieve this, we design corresponding goals and instructions forall states.。Specifically, for each value of each state, we generate a series of instructions that require the correspondingstate and value.。These are then annotated using GPT-4 based on the prompting methods previously described.. \n",
      "the piece 806\n",
      " \n",
      "s of state changes during the agent’s interactions,some states are difficult to capture comprehensively only using random sampling.。To ensure a more comprehensivedistribution of states in the data and to facilitate better understanding by the language model, we employ a multi-roundrejection sampling approach to construct state set.。Let Sbe the set of states waiting to be sampled.。We perform multiplerounds of sampling on S, with Sgetirepresenting the set of states sampled in the i-th round, initially empty.。Next, wesample a state sfrom Swithout replacement and check whether shas any state values not present in Sgeti.。If it does,we accept it and add it to Sgeti, otherwise we reject it.。Once all states in Shave been sampled, one round is completed.Sgetiis the result of i-th round’s sampling, and Swill be reset for the next round.。This sampling method is employed toenhance the comprehensiveness of state coverage in all datasets except the Random Instruction dataset.•IS(State Instructions).。We aim to cover a broader range of state changes in the instructions to enhance the languagemodel’s understanding of various state transitions.。To achieve this, we design corresponding goals and instructions forall states.。Specifically, for each value of each state, we generate a series of instructions that require the correspondingstate and value.。These are then annotated using GPT-4 based on the prompting methods previously described.。Theannotated results are checked; if they do not have corresponding states and values, manual annotation and modificationare performed to include the relevant states.•IA(Agent Instructions).. \n",
      "the piece 807\n",
      " \n",
      "nsure a more comprehensivedistribution of states in the data and to facilitate better understanding by the language model, we employ a multi-roundrejection sampling approach to construct state set.。Let Sbe the set of states waiting to be sampled.。We perform multiplerounds of sampling on S, with Sgetirepresenting the set of states sampled in the i-th round, initially empty.。Next, wesample a state sfrom Swithout replacement and check whether shas any state values not present in Sgeti.。If it does,we accept it and add it to Sgeti, otherwise we reject it.。Once all states in Shave been sampled, one round is completed.Sgetiis the result of i-th round’s sampling, and Swill be reset for the next round.。This sampling method is employed toenhance the comprehensiveness of state coverage in all datasets except the Random Instruction dataset.•IS(State Instructions).。We aim to cover a broader range of state changes in the instructions to enhance the languagemodel’s understanding of various state transitions.。To achieve this, we design corresponding goals and instructions forall states.。Specifically, for each value of each state, we generate a series of instructions that require the correspondingstate and value.。These are then annotated using GPT-4 based on the prompting methods previously described.。Theannotated results are checked; if they do not have corresponding states and values, manual annotation and modificationare performed to include the relevant states.•IA(Agent Instructions).。We aim to initially align the planning capabilities of the language model with the pre-trainedabilities of an intelligent agent based on reinforcement learning policies.. \n",
      "the piece 808\n",
      " \n",
      " construct state set.。Let Sbe the set of states waiting to be sampled.。We perform multiplerounds of sampling on S, with Sgetirepresenting the set of states sampled in the i-th round, initially empty.。Next, wesample a state sfrom Swithout replacement and check whether shas any state values not present in Sgeti.。If it does,we accept it and add it to Sgeti, otherwise we reject it.。Once all states in Shave been sampled, one round is completed.Sgetiis the result of i-th round’s sampling, and Swill be reset for the next round.。This sampling method is employed toenhance the comprehensiveness of state coverage in all datasets except the Random Instruction dataset.•IS(State Instructions).。We aim to cover a broader range of state changes in the instructions to enhance the languagemodel’s understanding of various state transitions.。To achieve this, we design corresponding goals and instructions forall states.。Specifically, for each value of each state, we generate a series of instructions that require the correspondingstate and value.。These are then annotated using GPT-4 based on the prompting methods previously described.。Theannotated results are checked; if they do not have corresponding states and values, manual annotation and modificationare performed to include the relevant states.•IA(Agent Instructions).。We aim to initially align the planning capabilities of the language model with the pre-trainedabilities of an intelligent agent based on reinforcement learning policies.。To do so, we generate potential correspondinginstructions based on actual state changes in agent interactions.. \n",
      "the piece 809\n",
      " \n",
      "-th round, initially empty.。Next, wesample a state sfrom Swithout replacement and check whether shas any state values not present in Sgeti.。If it does,we accept it and add it to Sgeti, otherwise we reject it.。Once all states in Shave been sampled, one round is completed.Sgetiis the result of i-th round’s sampling, and Swill be reset for the next round.。This sampling method is employed toenhance the comprehensiveness of state coverage in all datasets except the Random Instruction dataset.•IS(State Instructions).。We aim to cover a broader range of state changes in the instructions to enhance the languagemodel’s understanding of various state transitions.。To achieve this, we design corresponding goals and instructions forall states.。Specifically, for each value of each state, we generate a series of instructions that require the correspondingstate and value.。These are then annotated using GPT-4 based on the prompting methods previously described.。Theannotated results are checked; if they do not have corresponding states and values, manual annotation and modificationare performed to include the relevant states.•IA(Agent Instructions).。We aim to initially align the planning capabilities of the language model with the pre-trainedabilities of an intelligent agent based on reinforcement learning policies.。To do so, we generate potential correspondinginstructions based on actual state changes in agent interactions.。Specifically, we first sample a series of agent state pairsat 5-second intervals.. \n",
      "the piece 810\n",
      " \n",
      "t it.。Once all states in Shave been sampled, one round is completed.Sgetiis the result of i-th round’s sampling, and Swill be reset for the next round.。This sampling method is employed toenhance the comprehensiveness of state coverage in all datasets except the Random Instruction dataset.•IS(State Instructions).。We aim to cover a broader range of state changes in the instructions to enhance the languagemodel’s understanding of various state transitions.。To achieve this, we design corresponding goals and instructions forall states.。Specifically, for each value of each state, we generate a series of instructions that require the correspondingstate and value.。These are then annotated using GPT-4 based on the prompting methods previously described.。Theannotated results are checked; if they do not have corresponding states and values, manual annotation and modificationare performed to include the relevant states.•IA(Agent Instructions).。We aim to initially align the planning capabilities of the language model with the pre-trainedabilities of an intelligent agent based on reinforcement learning policies.。To do so, we generate potential correspondinginstructions based on actual state changes in agent interactions.。Specifically, we first sample a series of agent state pairsat 5-second intervals.。For a subset of these, we manually annotate possible corresponding instructions.. \n",
      "the piece 811\n",
      " \n",
      "ound is completed.Sgetiis the result of i-th round’s sampling, and Swill be reset for the next round.。This sampling method is employed toenhance the comprehensiveness of state coverage in all datasets except the Random Instruction dataset.•IS(State Instructions).。We aim to cover a broader range of state changes in the instructions to enhance the languagemodel’s understanding of various state transitions.。To achieve this, we design corresponding goals and instructions forall states.。Specifically, for each value of each state, we generate a series of instructions that require the correspondingstate and value.。These are then annotated using GPT-4 based on the prompting methods previously described.。Theannotated results are checked; if they do not have corresponding states and values, manual annotation and modificationare performed to include the relevant states.•IA(Agent Instructions).。We aim to initially align the planning capabilities of the language model with the pre-trainedabilities of an intelligent agent based on reinforcement learning policies.。To do so, we generate potential correspondinginstructions based on actual state changes in agent interactions.。Specifically, we first sample a series of agent state pairsat 5-second intervals.。For a subset of these, we manually annotate possible corresponding instructions.。We then usethese manual annotations as a knowledge base and employ the \"langchain\" method to use these examples to guidethe annotation of the remaining data using ChatGPT-3.5.. \n",
      "the piece 812\n",
      " \n",
      "oyed toenhance the comprehensiveness of state coverage in all datasets except the Random Instruction dataset.•IS(State Instructions).。We aim to cover a broader range of state changes in the instructions to enhance the languagemodel’s understanding of various state transitions.。To achieve this, we design corresponding goals and instructions forall states.。Specifically, for each value of each state, we generate a series of instructions that require the correspondingstate and value.。These are then annotated using GPT-4 based on the prompting methods previously described.。Theannotated results are checked; if they do not have corresponding states and values, manual annotation and modificationare performed to include the relevant states.•IA(Agent Instructions).。We aim to initially align the planning capabilities of the language model with the pre-trainedabilities of an intelligent agent based on reinforcement learning policies.。To do so, we generate potential correspondinginstructions based on actual state changes in agent interactions.。Specifically, we first sample a series of agent state pairsat 5-second intervals.。For a subset of these, we manually annotate possible corresponding instructions.。We then usethese manual annotations as a knowledge base and employ the \"langchain\" method to use these examples to guidethe annotation of the remaining data using ChatGPT-3.5.。Finally, we represent all the instructions as vectors usingOpenAI’s embedding API and perform clustering.. \n",
      "the piece 813\n",
      " \n",
      "State Instructions).。We aim to cover a broader range of state changes in the instructions to enhance the languagemodel’s understanding of various state transitions.。To achieve this, we design corresponding goals and instructions forall states.。Specifically, for each value of each state, we generate a series of instructions that require the correspondingstate and value.。These are then annotated using GPT-4 based on the prompting methods previously described.。Theannotated results are checked; if they do not have corresponding states and values, manual annotation and modificationare performed to include the relevant states.•IA(Agent Instructions).。We aim to initially align the planning capabilities of the language model with the pre-trainedabilities of an intelligent agent based on reinforcement learning policies.。To do so, we generate potential correspondinginstructions based on actual state changes in agent interactions.。Specifically, we first sample a series of agent state pairsat 5-second intervals.。For a subset of these, we manually annotate possible corresponding instructions.。We then usethese manual annotations as a knowledge base and employ the \"langchain\" method to use these examples to guidethe annotation of the remaining data using ChatGPT-3.5.。Finally, we represent all the instructions as vectors usingOpenAI’s embedding API and perform clustering.。We select the 14 most representative types of instructions and pairthem cyclically with two rounds of sampled states, ultimately constructing a dataset that better reflects the fundamentalexecution capabilities of the intelligent agent.•IR(Random Instructions).. \n",
      "the piece 814\n",
      " \n",
      "in the instructions to enhance the languagemodel’s understanding of various state transitions.。To achieve this, we design corresponding goals and instructions forall states.。Specifically, for each value of each state, we generate a series of instructions that require the correspondingstate and value.。These are then annotated using GPT-4 based on the prompting methods previously described.。Theannotated results are checked; if they do not have corresponding states and values, manual annotation and modificationare performed to include the relevant states.•IA(Agent Instructions).。We aim to initially align the planning capabilities of the language model with the pre-trainedabilities of an intelligent agent based on reinforcement learning policies.。To do so, we generate potential correspondinginstructions based on actual state changes in agent interactions.。Specifically, we first sample a series of agent state pairsat 5-second intervals.。For a subset of these, we manually annotate possible corresponding instructions.。We then usethese manual annotations as a knowledge base and employ the \"langchain\" method to use these examples to guidethe annotation of the remaining data using ChatGPT-3.5.。Finally, we represent all the instructions as vectors usingOpenAI’s embedding API and perform clustering.。We select the 14 most representative types of instructions and pairthem cyclically with two rounds of sampled states, ultimately constructing a dataset that better reflects the fundamentalexecution capabilities of the intelligent agent.•IR(Random Instructions).。This set is primarily designed to enrich the data distribution.. \n",
      "the piece 815\n",
      " \n",
      "nstructions forall states.。Specifically, for each value of each state, we generate a series of instructions that require the correspondingstate and value.。These are then annotated using GPT-4 based on the prompting methods previously described.。Theannotated results are checked; if they do not have corresponding states and values, manual annotation and modificationare performed to include the relevant states.•IA(Agent Instructions).。We aim to initially align the planning capabilities of the language model with the pre-trainedabilities of an intelligent agent based on reinforcement learning policies.。To do so, we generate potential correspondinginstructions based on actual state changes in agent interactions.。Specifically, we first sample a series of agent state pairsat 5-second intervals.。For a subset of these, we manually annotate possible corresponding instructions.。We then usethese manual annotations as a knowledge base and employ the \"langchain\" method to use these examples to guidethe annotation of the remaining data using ChatGPT-3.5.。Finally, we represent all the instructions as vectors usingOpenAI’s embedding API and perform clustering.。We select the 14 most representative types of instructions and pairthem cyclically with two rounds of sampled states, ultimately constructing a dataset that better reflects the fundamentalexecution capabilities of the intelligent agent.•IR(Random Instructions).。This set is primarily designed to enrich the data distribution.。It is constructed by randomlygenerating instructions and fully randomly sampling states, and then annotated using GPT-4 based on the promptingmethods previously described.The quantaty of the aforementioned four types of datasets is 507 for HI, 1098 for SI, 1441 for AI and 1382 for RI.. \n",
      "the piece 816\n",
      " \n",
      "e then annotated using GPT-4 based on the prompting methods previously described.。Theannotated results are checked; if they do not have corresponding states and values, manual annotation and modificationare performed to include the relevant states.•IA(Agent Instructions).。We aim to initially align the planning capabilities of the language model with the pre-trainedabilities of an intelligent agent based on reinforcement learning policies.。To do so, we generate potential correspondinginstructions based on actual state changes in agent interactions.。Specifically, we first sample a series of agent state pairsat 5-second intervals.。For a subset of these, we manually annotate possible corresponding instructions.。We then usethese manual annotations as a knowledge base and employ the \"langchain\" method to use these examples to guidethe annotation of the remaining data using ChatGPT-3.5.。Finally, we represent all the instructions as vectors usingOpenAI’s embedding API and perform clustering.。We select the 14 most representative types of instructions and pairthem cyclically with two rounds of sampled states, ultimately constructing a dataset that better reflects the fundamentalexecution capabilities of the intelligent agent.•IR(Random Instructions).。This set is primarily designed to enrich the data distribution.。It is constructed by randomlygenerating instructions and fully randomly sampling states, and then annotated using GPT-4 based on the promptingmethods previously described.The quantaty of the aforementioned four types of datasets is 507 for HI, 1098 for SI, 1441 for AI and 1382 for RI.。Moreover,the test dataset construct instructions that differ from those used in the training data, then utilize GPT-4 to generate draftlabels of goals and modified with manually filtered and annotated.. \n",
      "the piece 817\n",
      " \n",
      "ding states and values, manual annotation and modificationare performed to include the relevant states.•IA(Agent Instructions).。We aim to initially align the planning capabilities of the language model with the pre-trainedabilities of an intelligent agent based on reinforcement learning policies.。To do so, we generate potential correspondinginstructions based on actual state changes in agent interactions.。Specifically, we first sample a series of agent state pairsat 5-second intervals.。For a subset of these, we manually annotate possible corresponding instructions.。We then usethese manual annotations as a knowledge base and employ the \"langchain\" method to use these examples to guidethe annotation of the remaining data using ChatGPT-3.5.。Finally, we represent all the instructions as vectors usingOpenAI’s embedding API and perform clustering.。We select the 14 most representative types of instructions and pairthem cyclically with two rounds of sampled states, ultimately constructing a dataset that better reflects the fundamentalexecution capabilities of the intelligent agent.•IR(Random Instructions).。This set is primarily designed to enrich the data distribution.。It is constructed by randomlygenerating instructions and fully randomly sampling states, and then annotated using GPT-4 based on the promptingmethods previously described.The quantaty of the aforementioned four types of datasets is 507 for HI, 1098 for SI, 1441 for AI and 1382 for RI.。Moreover,the test dataset construct instructions that differ from those used in the training data, then utilize GPT-4 to generate draftlabels of goals and modified with manually filtered and annotated.。This test dataset used for evaluating the model’s ability toplan reasonably in response to instructions.. \n",
      "the piece 818\n",
      " \n",
      "de the relevant states.•IA(Agent Instructions).。We aim to initially align the planning capabilities of the language model with the pre-trainedabilities of an intelligent agent based on reinforcement learning policies.。To do so, we generate potential correspondinginstructions based on actual state changes in agent interactions.。Specifically, we first sample a series of agent state pairsat 5-second intervals.。For a subset of these, we manually annotate possible corresponding instructions.。We then usethese manual annotations as a knowledge base and employ the \"langchain\" method to use these examples to guidethe annotation of the remaining data using ChatGPT-3.5.。Finally, we represent all the instructions as vectors usingOpenAI’s embedding API and perform clustering.。We select the 14 most representative types of instructions and pairthem cyclically with two rounds of sampled states, ultimately constructing a dataset that better reflects the fundamentalexecution capabilities of the intelligent agent.•IR(Random Instructions).。This set is primarily designed to enrich the data distribution.。It is constructed by randomlygenerating instructions and fully randomly sampling states, and then annotated using GPT-4 based on the promptingmethods previously described.The quantaty of the aforementioned four types of datasets is 507 for HI, 1098 for SI, 1441 for AI and 1382 for RI.。Moreover,the test dataset construct instructions that differ from those used in the training data, then utilize GPT-4 to generate draftlabels of goals and modified with manually filtered and annotated.。This test dataset used for evaluating the model’s ability toplan reasonably in response to instructions.。And the size of dataset for each tuning step is 26,568 for CoT-assited fine-tuning,4,428 for supervised fine-tuning, and 4,994 for ensembling fine-tuning.F.. \n",
      "the piece 819\n",
      " \n",
      "e pre-trainedabilities of an intelligent agent based on reinforcement learning policies.。To do so, we generate potential correspondinginstructions based on actual state changes in agent interactions.。Specifically, we first sample a series of agent state pairsat 5-second intervals.。For a subset of these, we manually annotate possible corresponding instructions.。We then usethese manual annotations as a knowledge base and employ the \"langchain\" method to use these examples to guidethe annotation of the remaining data using ChatGPT-3.5.。Finally, we represent all the instructions as vectors usingOpenAI’s embedding API and perform clustering.。We select the 14 most representative types of instructions and pairthem cyclically with two rounds of sampled states, ultimately constructing a dataset that better reflects the fundamentalexecution capabilities of the intelligent agent.•IR(Random Instructions).。This set is primarily designed to enrich the data distribution.。It is constructed by randomlygenerating instructions and fully randomly sampling states, and then annotated using GPT-4 based on the promptingmethods previously described.The quantaty of the aforementioned four types of datasets is 507 for HI, 1098 for SI, 1441 for AI and 1382 for RI.。Moreover,the test dataset construct instructions that differ from those used in the training data, then utilize GPT-4 to generate draftlabels of goals and modified with manually filtered and annotated.。This test dataset used for evaluating the model’s ability toplan reasonably in response to instructions.。And the size of dataset for each tuning step is 26,568 for CoT-assited fine-tuning,4,428 for supervised fine-tuning, and 4,994 for ensembling fine-tuning.F.。Distributed Training FrameworkTo improve the training efficiency, we adopt a distributed training system, shown in Figure 7.. \n",
      "the piece 820\n",
      " \n",
      " do so, we generate potential correspondinginstructions based on actual state changes in agent interactions.。Specifically, we first sample a series of agent state pairsat 5-second intervals.。For a subset of these, we manually annotate possible corresponding instructions.。We then usethese manual annotations as a knowledge base and employ the \"langchain\" method to use these examples to guidethe annotation of the remaining data using ChatGPT-3.5.。Finally, we represent all the instructions as vectors usingOpenAI’s embedding API and perform clustering.。We select the 14 most representative types of instructions and pairthem cyclically with two rounds of sampled states, ultimately constructing a dataset that better reflects the fundamentalexecution capabilities of the intelligent agent.•IR(Random Instructions).。This set is primarily designed to enrich the data distribution.。It is constructed by randomlygenerating instructions and fully randomly sampling states, and then annotated using GPT-4 based on the promptingmethods previously described.The quantaty of the aforementioned four types of datasets is 507 for HI, 1098 for SI, 1441 for AI and 1382 for RI.。Moreover,the test dataset construct instructions that differ from those used in the training data, then utilize GPT-4 to generate draftlabels of goals and modified with manually filtered and annotated.。This test dataset used for evaluating the model’s ability toplan reasonably in response to instructions.。And the size of dataset for each tuning step is 26,568 for CoT-assited fine-tuning,4,428 for supervised fine-tuning, and 4,994 for ensembling fine-tuning.F.。Distributed Training FrameworkTo improve the training efficiency, we adopt a distributed training system, shown in Figure 7.。In this system, the Actors runover CPU nodes to collect training data, then send the collected data to the Learner which is deployed on a GPU node.. \n",
      "the piece 821\n",
      " \n",
      "or a subset of these, we manually annotate possible corresponding instructions.。We then usethese manual annotations as a knowledge base and employ the \"langchain\" method to use these examples to guidethe annotation of the remaining data using ChatGPT-3.5.。Finally, we represent all the instructions as vectors usingOpenAI’s embedding API and perform clustering.。We select the 14 most representative types of instructions and pairthem cyclically with two rounds of sampled states, ultimately constructing a dataset that better reflects the fundamentalexecution capabilities of the intelligent agent.•IR(Random Instructions).。This set is primarily designed to enrich the data distribution.。It is constructed by randomlygenerating instructions and fully randomly sampling states, and then annotated using GPT-4 based on the promptingmethods previously described.The quantaty of the aforementioned four types of datasets is 507 for HI, 1098 for SI, 1441 for AI and 1382 for RI.。Moreover,the test dataset construct instructions that differ from those used in the training data, then utilize GPT-4 to generate draftlabels of goals and modified with manually filtered and annotated.。This test dataset used for evaluating the model’s ability toplan reasonably in response to instructions.。And the size of dataset for each tuning step is 26,568 for CoT-assited fine-tuning,4,428 for supervised fine-tuning, and 4,994 for ensembling fine-tuning.F.。Distributed Training FrameworkTo improve the training efficiency, we adopt a distributed training system, shown in Figure 7.。In this system, the Actors runover CPU nodes to collect training data, then send the collected data to the Learner which is deployed on a GPU node.。Wefurther take a LLM server to enable multiple replicas of LLM for goal generation, which improve the throughput of rolloutwhen the RL training is switch to goal-conditioned cases.G.. \n",
      "the piece 822\n",
      " \n",
      "o use these examples to guidethe annotation of the remaining data using ChatGPT-3.5.。Finally, we represent all the instructions as vectors usingOpenAI’s embedding API and perform clustering.。We select the 14 most representative types of instructions and pairthem cyclically with two rounds of sampled states, ultimately constructing a dataset that better reflects the fundamentalexecution capabilities of the intelligent agent.•IR(Random Instructions).。This set is primarily designed to enrich the data distribution.。It is constructed by randomlygenerating instructions and fully randomly sampling states, and then annotated using GPT-4 based on the promptingmethods previously described.The quantaty of the aforementioned four types of datasets is 507 for HI, 1098 for SI, 1441 for AI and 1382 for RI.。Moreover,the test dataset construct instructions that differ from those used in the training data, then utilize GPT-4 to generate draftlabels of goals and modified with manually filtered and annotated.。This test dataset used for evaluating the model’s ability toplan reasonably in response to instructions.。And the size of dataset for each tuning step is 26,568 for CoT-assited fine-tuning,4,428 for supervised fine-tuning, and 4,994 for ensembling fine-tuning.F.。Distributed Training FrameworkTo improve the training efficiency, we adopt a distributed training system, shown in Figure 7.。In this system, the Actors runover CPU nodes to collect training data, then send the collected data to the Learner which is deployed on a GPU node.。Wefurther take a LLM server to enable multiple replicas of LLM for goal generation, which improve the throughput of rolloutwhen the RL training is switch to goal-conditioned cases.G.。Parameter SettingsSome of the hyper-parameters used in our experiment are illustrated in Table 11 and other dynamic hyper-parameters areintroduced their corresponding parts.H.. \n",
      "the piece 823\n",
      " \n",
      "he instructions as vectors usingOpenAI’s embedding API and perform clustering.。We select the 14 most representative types of instructions and pairthem cyclically with two rounds of sampled states, ultimately constructing a dataset that better reflects the fundamentalexecution capabilities of the intelligent agent.•IR(Random Instructions).。This set is primarily designed to enrich the data distribution.。It is constructed by randomlygenerating instructions and fully randomly sampling states, and then annotated using GPT-4 based on the promptingmethods previously described.The quantaty of the aforementioned four types of datasets is 507 for HI, 1098 for SI, 1441 for AI and 1382 for RI.。Moreover,the test dataset construct instructions that differ from those used in the training data, then utilize GPT-4 to generate draftlabels of goals and modified with manually filtered and annotated.。This test dataset used for evaluating the model’s ability toplan reasonably in response to instructions.。And the size of dataset for each tuning step is 26,568 for CoT-assited fine-tuning,4,428 for supervised fine-tuning, and 4,994 for ensembling fine-tuning.F.。Distributed Training FrameworkTo improve the training efficiency, we adopt a distributed training system, shown in Figure 7.。In this system, the Actors runover CPU nodes to collect training data, then send the collected data to the Learner which is deployed on a GPU node.。Wefurther take a LLM server to enable multiple replicas of LLM for goal generation, which improve the throughput of rolloutwhen the RL training is switch to goal-conditioned cases.G.。Parameter SettingsSome of the hyper-parameters used in our experiment are illustrated in Table 11 and other dynamic hyper-parameters areintroduced their corresponding parts.H.。AlgorithmsAlgorithm 3 lists the pseudo-code of goal-conditioned RL procedures in Stage I, where Rbt,RoatandRgtrepresent thediscounted basic return, obstacle avoidance return and goal-reaching return from time step ttill the termination, respectively.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationLearning tasksLearner LeagueTraining outcomes+-Model parameters --+ Redis ,.---' --------..., 'I,..)\"'----------- \"\\ 'IlLLM Replica !,I V\\ I / \\ I '\"------------ ./I II II Goals / I ;'State ab�tractions ,;--1 , ActionsI / ---------1 /;___ --��====���\\\\\\Checkpoints \\ ActorsRollout outcomesModel parameters --+ EnvironmentsFigure 7: This training system has four key parts: Actor, Learner, League and LLM replicas.. \n",
      "the piece 824\n",
      " \n",
      "elect the 14 most representative types of instructions and pairthem cyclically with two rounds of sampled states, ultimately constructing a dataset that better reflects the fundamentalexecution capabilities of the intelligent agent.•IR(Random Instructions).。This set is primarily designed to enrich the data distribution.。It is constructed by randomlygenerating instructions and fully randomly sampling states, and then annotated using GPT-4 based on the promptingmethods previously described.The quantaty of the aforementioned four types of datasets is 507 for HI, 1098 for SI, 1441 for AI and 1382 for RI.。Moreover,the test dataset construct instructions that differ from those used in the training data, then utilize GPT-4 to generate draftlabels of goals and modified with manually filtered and annotated.。This test dataset used for evaluating the model’s ability toplan reasonably in response to instructions.。And the size of dataset for each tuning step is 26,568 for CoT-assited fine-tuning,4,428 for supervised fine-tuning, and 4,994 for ensembling fine-tuning.F.。Distributed Training FrameworkTo improve the training efficiency, we adopt a distributed training system, shown in Figure 7.。In this system, the Actors runover CPU nodes to collect training data, then send the collected data to the Learner which is deployed on a GPU node.。Wefurther take a LLM server to enable multiple replicas of LLM for goal generation, which improve the throughput of rolloutwhen the RL training is switch to goal-conditioned cases.G.。Parameter SettingsSome of the hyper-parameters used in our experiment are illustrated in Table 11 and other dynamic hyper-parameters areintroduced their corresponding parts.H.。AlgorithmsAlgorithm 3 lists the pseudo-code of goal-conditioned RL procedures in Stage I, where Rbt,RoatandRgtrepresent thediscounted basic return, obstacle avoidance return and goal-reaching return from time step ttill the termination, respectively.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationLearning tasksLearner LeagueTraining outcomes+-Model parameters --+ Redis ,.---' --------..., 'I,..)\"'----------- \"\\ 'IlLLM Replica !,I V\\ I / \\ I '\"------------ ./I II II Goals / I ;'State ab�tractions ,;--1 , ActionsI / ---------1 /;___ --��====���\\\\\\Checkpoints \\ ActorsRollout outcomesModel parameters --+ EnvironmentsFigure 7: This training system has four key parts: Actor, Learner, League and LLM replicas.。Actors are responsible fordata collection, the Learner trains the policy model using this data, the League coordinates the overall training process anddisplays results, and the LLM Replicas handle goal generation and distribute them to downstream workers.Algorithm 3 GOAL-CONDITIONED REINFORCEMENT LEARNING1:Input: θparameterizes policy πandζ={ζ1, ζ2, ζ3}parameterizes value heads, goal generators GrndandGop2:fork=1, 2, ... do3: Reset environment with returned initial state s0∼P(s)4: Sample a goal: g∼(Grnd∪Gop)(s0,∆t,∆V)5: Run policy πθkin environment until be terminated6: Actors collect trajectories Dτand send them to the Learner7: Update the θktoθk+1with Equation (6)8: Update ζbymax ζEs∼Dτh∥Rbt−Vbζ1(st)∥2+∥Roat−Voaζ2(st)∥2+1gt̸=∅∥Rgt−Vgζ3(st)∥2i9:end forThe computation of each can be expressed as follows:Rbt=TXi=tγrb(si, ai), Roat=TXi=tγroa(si, ai), Rgt=TXi=tγrg(si, ai, g).. \n",
      "the piece 825\n",
      " \n",
      "h two rounds of sampled states, ultimately constructing a dataset that better reflects the fundamentalexecution capabilities of the intelligent agent.•IR(Random Instructions).。This set is primarily designed to enrich the data distribution.。It is constructed by randomlygenerating instructions and fully randomly sampling states, and then annotated using GPT-4 based on the promptingmethods previously described.The quantaty of the aforementioned four types of datasets is 507 for HI, 1098 for SI, 1441 for AI and 1382 for RI.。Moreover,the test dataset construct instructions that differ from those used in the training data, then utilize GPT-4 to generate draftlabels of goals and modified with manually filtered and annotated.。This test dataset used for evaluating the model’s ability toplan reasonably in response to instructions.。And the size of dataset for each tuning step is 26,568 for CoT-assited fine-tuning,4,428 for supervised fine-tuning, and 4,994 for ensembling fine-tuning.F.。Distributed Training FrameworkTo improve the training efficiency, we adopt a distributed training system, shown in Figure 7.。In this system, the Actors runover CPU nodes to collect training data, then send the collected data to the Learner which is deployed on a GPU node.。Wefurther take a LLM server to enable multiple replicas of LLM for goal generation, which improve the throughput of rolloutwhen the RL training is switch to goal-conditioned cases.G.。Parameter SettingsSome of the hyper-parameters used in our experiment are illustrated in Table 11 and other dynamic hyper-parameters areintroduced their corresponding parts.H.。AlgorithmsAlgorithm 3 lists the pseudo-code of goal-conditioned RL procedures in Stage I, where Rbt,RoatandRgtrepresent thediscounted basic return, obstacle avoidance return and goal-reaching return from time step ttill the termination, respectively.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationLearning tasksLearner LeagueTraining outcomes+-Model parameters --+ Redis ,.---' --------..., 'I,..)\"'----------- \"\\ 'IlLLM Replica !,I V\\ I / \\ I '\"------------ ./I II II Goals / I ;'State ab�tractions ,;--1 , ActionsI / ---------1 /;___ --��====���\\\\\\Checkpoints \\ ActorsRollout outcomesModel parameters --+ EnvironmentsFigure 7: This training system has four key parts: Actor, Learner, League and LLM replicas.。Actors are responsible fordata collection, the Learner trains the policy model using this data, the League coordinates the overall training process anddisplays results, and the LLM Replicas handle goal generation and distribute them to downstream workers.Algorithm 3 GOAL-CONDITIONED REINFORCEMENT LEARNING1:Input: θparameterizes policy πandζ={ζ1, ζ2, ζ3}parameterizes value heads, goal generators GrndandGop2:fork=1, 2, ... do3: Reset environment with returned initial state s0∼P(s)4: Sample a goal: g∼(Grnd∪Gop)(s0,∆t,∆V)5: Run policy πθkin environment until be terminated6: Actors collect trajectories Dτand send them to the Learner7: Update the θktoθk+1with Equation (6)8: Update ζbymax ζEs∼Dτh∥Rbt−Vbζ1(st)∥2+∥Roat−Voaζ2(st)∥2+1gt̸=∅∥Rgt−Vgζ3(st)∥2i9:end forThe computation of each can be expressed as follows:Rbt=TXi=tγrb(si, ai), Roat=TXi=tγroa(si, ai), Rgt=TXi=tγrg(si, ai, g).。(9)As for J(πg,θ)in Equation (6), we follow the computation of policy loss in PPO to express it asJ(πg,θ) = min\u0012πg,θ(s, a)πg,θk(s, a)Aπg,θk(s, a), g(ϵ, Aπg,θk(s, a))\u0013,where (s, a)∼ D τ, g(ϵ, A) =((1 +ϵ)A, A ≥0(1−ϵ)A, A > 0.. \n",
      "the piece 826\n",
      " \n",
      "his set is primarily designed to enrich the data distribution.。It is constructed by randomlygenerating instructions and fully randomly sampling states, and then annotated using GPT-4 based on the promptingmethods previously described.The quantaty of the aforementioned four types of datasets is 507 for HI, 1098 for SI, 1441 for AI and 1382 for RI.。Moreover,the test dataset construct instructions that differ from those used in the training data, then utilize GPT-4 to generate draftlabels of goals and modified with manually filtered and annotated.。This test dataset used for evaluating the model’s ability toplan reasonably in response to instructions.。And the size of dataset for each tuning step is 26,568 for CoT-assited fine-tuning,4,428 for supervised fine-tuning, and 4,994 for ensembling fine-tuning.F.。Distributed Training FrameworkTo improve the training efficiency, we adopt a distributed training system, shown in Figure 7.。In this system, the Actors runover CPU nodes to collect training data, then send the collected data to the Learner which is deployed on a GPU node.。Wefurther take a LLM server to enable multiple replicas of LLM for goal generation, which improve the throughput of rolloutwhen the RL training is switch to goal-conditioned cases.G.。Parameter SettingsSome of the hyper-parameters used in our experiment are illustrated in Table 11 and other dynamic hyper-parameters areintroduced their corresponding parts.H.。AlgorithmsAlgorithm 3 lists the pseudo-code of goal-conditioned RL procedures in Stage I, where Rbt,RoatandRgtrepresent thediscounted basic return, obstacle avoidance return and goal-reaching return from time step ttill the termination, respectively.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationLearning tasksLearner LeagueTraining outcomes+-Model parameters --+ Redis ,.---' --------..., 'I,..)\"'----------- \"\\ 'IlLLM Replica !,I V\\ I / \\ I '\"------------ ./I II II Goals / I ;'State ab�tractions ,;--1 , ActionsI / ---------1 /;___ --��====���\\\\\\Checkpoints \\ ActorsRollout outcomesModel parameters --+ EnvironmentsFigure 7: This training system has four key parts: Actor, Learner, League and LLM replicas.。Actors are responsible fordata collection, the Learner trains the policy model using this data, the League coordinates the overall training process anddisplays results, and the LLM Replicas handle goal generation and distribute them to downstream workers.Algorithm 3 GOAL-CONDITIONED REINFORCEMENT LEARNING1:Input: θparameterizes policy πandζ={ζ1, ζ2, ζ3}parameterizes value heads, goal generators GrndandGop2:fork=1, 2, ... do3: Reset environment with returned initial state s0∼P(s)4: Sample a goal: g∼(Grnd∪Gop)(s0,∆t,∆V)5: Run policy πθkin environment until be terminated6: Actors collect trajectories Dτand send them to the Learner7: Update the θktoθk+1with Equation (6)8: Update ζbymax ζEs∼Dτh∥Rbt−Vbζ1(st)∥2+∥Roat−Voaζ2(st)∥2+1gt̸=∅∥Rgt−Vgζ3(st)∥2i9:end forThe computation of each can be expressed as follows:Rbt=TXi=tγrb(si, ai), Roat=TXi=tγroa(si, ai), Rgt=TXi=tγrg(si, ai, g).。(9)As for J(πg,θ)in Equation (6), we follow the computation of policy loss in PPO to express it asJ(πg,θ) = min\u0012πg,θ(s, a)πg,θk(s, a)Aπg,θk(s, a), g(ϵ, Aπg,θk(s, a))\u0013,where (s, a)∼ D τ, g(ϵ, A) =((1 +ϵ)A, A ≥0(1−ϵ)A, A > 0.。(10)Aπ(s, a)indicates the advantage is computed under the condition of policy π.I.. \n",
      "the piece 827\n",
      " \n",
      "ructions and fully randomly sampling states, and then annotated using GPT-4 based on the promptingmethods previously described.The quantaty of the aforementioned four types of datasets is 507 for HI, 1098 for SI, 1441 for AI and 1382 for RI.。Moreover,the test dataset construct instructions that differ from those used in the training data, then utilize GPT-4 to generate draftlabels of goals and modified with manually filtered and annotated.。This test dataset used for evaluating the model’s ability toplan reasonably in response to instructions.。And the size of dataset for each tuning step is 26,568 for CoT-assited fine-tuning,4,428 for supervised fine-tuning, and 4,994 for ensembling fine-tuning.F.。Distributed Training FrameworkTo improve the training efficiency, we adopt a distributed training system, shown in Figure 7.。In this system, the Actors runover CPU nodes to collect training data, then send the collected data to the Learner which is deployed on a GPU node.。Wefurther take a LLM server to enable multiple replicas of LLM for goal generation, which improve the throughput of rolloutwhen the RL training is switch to goal-conditioned cases.G.。Parameter SettingsSome of the hyper-parameters used in our experiment are illustrated in Table 11 and other dynamic hyper-parameters areintroduced their corresponding parts.H.。AlgorithmsAlgorithm 3 lists the pseudo-code of goal-conditioned RL procedures in Stage I, where Rbt,RoatandRgtrepresent thediscounted basic return, obstacle avoidance return and goal-reaching return from time step ttill the termination, respectively.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationLearning tasksLearner LeagueTraining outcomes+-Model parameters --+ Redis ,.---' --------..., 'I,..)\"'----------- \"\\ 'IlLLM Replica !,I V\\ I / \\ I '\"------------ ./I II II Goals / I ;'State ab�tractions ,;--1 , ActionsI / ---------1 /;___ --��====���\\\\\\Checkpoints \\ ActorsRollout outcomesModel parameters --+ EnvironmentsFigure 7: This training system has four key parts: Actor, Learner, League and LLM replicas.。Actors are responsible fordata collection, the Learner trains the policy model using this data, the League coordinates the overall training process anddisplays results, and the LLM Replicas handle goal generation and distribute them to downstream workers.Algorithm 3 GOAL-CONDITIONED REINFORCEMENT LEARNING1:Input: θparameterizes policy πandζ={ζ1, ζ2, ζ3}parameterizes value heads, goal generators GrndandGop2:fork=1, 2, ... do3: Reset environment with returned initial state s0∼P(s)4: Sample a goal: g∼(Grnd∪Gop)(s0,∆t,∆V)5: Run policy πθkin environment until be terminated6: Actors collect trajectories Dτand send them to the Learner7: Update the θktoθk+1with Equation (6)8: Update ζbymax ζEs∼Dτh∥Rbt−Vbζ1(st)∥2+∥Roat−Voaζ2(st)∥2+1gt̸=∅∥Rgt−Vgζ3(st)∥2i9:end forThe computation of each can be expressed as follows:Rbt=TXi=tγrb(si, ai), Roat=TXi=tγroa(si, ai), Rgt=TXi=tγrg(si, ai, g).。(9)As for J(πg,θ)in Equation (6), we follow the computation of policy loss in PPO to express it asJ(πg,θ) = min\u0012πg,θ(s, a)πg,θk(s, a)Aπg,θk(s, a), g(ϵ, Aπg,θk(s, a))\u0013,where (s, a)∼ D τ, g(ϵ, A) =((1 +ϵ)A, A ≥0(1−ϵ)A, A > 0.。(10)Aπ(s, a)indicates the advantage is computed under the condition of policy π.I.。Open-ended Goal GenerationInspired by Hindsight Experience Replay (HER) (Andrychowicz et al., 2017), we adopt a similar method to utilizethe collected trajectories for learning a goal generator Gopwhich accepts a state as input.. \n",
      "the piece 828\n",
      " \n",
      "aset construct instructions that differ from those used in the training data, then utilize GPT-4 to generate draftlabels of goals and modified with manually filtered and annotated.。This test dataset used for evaluating the model’s ability toplan reasonably in response to instructions.。And the size of dataset for each tuning step is 26,568 for CoT-assited fine-tuning,4,428 for supervised fine-tuning, and 4,994 for ensembling fine-tuning.F.。Distributed Training FrameworkTo improve the training efficiency, we adopt a distributed training system, shown in Figure 7.。In this system, the Actors runover CPU nodes to collect training data, then send the collected data to the Learner which is deployed on a GPU node.。Wefurther take a LLM server to enable multiple replicas of LLM for goal generation, which improve the throughput of rolloutwhen the RL training is switch to goal-conditioned cases.G.。Parameter SettingsSome of the hyper-parameters used in our experiment are illustrated in Table 11 and other dynamic hyper-parameters areintroduced their corresponding parts.H.。AlgorithmsAlgorithm 3 lists the pseudo-code of goal-conditioned RL procedures in Stage I, where Rbt,RoatandRgtrepresent thediscounted basic return, obstacle avoidance return and goal-reaching return from time step ttill the termination, respectively.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationLearning tasksLearner LeagueTraining outcomes+-Model parameters --+ Redis ,.---' --------..., 'I,..)\"'----------- \"\\ 'IlLLM Replica !,I V\\ I / \\ I '\"------------ ./I II II Goals / I ;'State ab�tractions ,;--1 , ActionsI / ---------1 /;___ --��====���\\\\\\Checkpoints \\ ActorsRollout outcomesModel parameters --+ EnvironmentsFigure 7: This training system has four key parts: Actor, Learner, League and LLM replicas.。Actors are responsible fordata collection, the Learner trains the policy model using this data, the League coordinates the overall training process anddisplays results, and the LLM Replicas handle goal generation and distribute them to downstream workers.Algorithm 3 GOAL-CONDITIONED REINFORCEMENT LEARNING1:Input: θparameterizes policy πandζ={ζ1, ζ2, ζ3}parameterizes value heads, goal generators GrndandGop2:fork=1, 2, ... do3: Reset environment with returned initial state s0∼P(s)4: Sample a goal: g∼(Grnd∪Gop)(s0,∆t,∆V)5: Run policy πθkin environment until be terminated6: Actors collect trajectories Dτand send them to the Learner7: Update the θktoθk+1with Equation (6)8: Update ζbymax ζEs∼Dτh∥Rbt−Vbζ1(st)∥2+∥Roat−Voaζ2(st)∥2+1gt̸=∅∥Rgt−Vgζ3(st)∥2i9:end forThe computation of each can be expressed as follows:Rbt=TXi=tγrb(si, ai), Roat=TXi=tγroa(si, ai), Rgt=TXi=tγrg(si, ai, g).。(9)As for J(πg,θ)in Equation (6), we follow the computation of policy loss in PPO to express it asJ(πg,θ) = min\u0012πg,θ(s, a)πg,θk(s, a)Aπg,θk(s, a), g(ϵ, Aπg,θk(s, a))\u0013,where (s, a)∼ D τ, g(ϵ, A) =((1 +ϵ)A, A ≥0(1−ϵ)A, A > 0.。(10)Aπ(s, a)indicates the advantage is computed under the condition of policy π.I.。Open-ended Goal GenerationInspired by Hindsight Experience Replay (HER) (Andrychowicz et al., 2017), we adopt a similar method to utilizethe collected trajectories for learning a goal generator Gopwhich accepts a state as input.。We conclude its train-ing in two steps: (1) constructing (s, g)pairs with collected trajectories as illustrated in Figure 11(b); (2) supervisedtraining Gopwith the above pairs and an MSE loss between the labeled goals and predicted goals.. \n",
      "the piece 829\n",
      " \n",
      "aining data, then utilize GPT-4 to generate draftlabels of goals and modified with manually filtered and annotated.。This test dataset used for evaluating the model’s ability toplan reasonably in response to instructions.。And the size of dataset for each tuning step is 26,568 for CoT-assited fine-tuning,4,428 for supervised fine-tuning, and 4,994 for ensembling fine-tuning.F.。Distributed Training FrameworkTo improve the training efficiency, we adopt a distributed training system, shown in Figure 7.。In this system, the Actors runover CPU nodes to collect training data, then send the collected data to the Learner which is deployed on a GPU node.。Wefurther take a LLM server to enable multiple replicas of LLM for goal generation, which improve the throughput of rolloutwhen the RL training is switch to goal-conditioned cases.G.。Parameter SettingsSome of the hyper-parameters used in our experiment are illustrated in Table 11 and other dynamic hyper-parameters areintroduced their corresponding parts.H.。AlgorithmsAlgorithm 3 lists the pseudo-code of goal-conditioned RL procedures in Stage I, where Rbt,RoatandRgtrepresent thediscounted basic return, obstacle avoidance return and goal-reaching return from time step ttill the termination, respectively.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationLearning tasksLearner LeagueTraining outcomes+-Model parameters --+ Redis ,.---' --------..., 'I,..)\"'----------- \"\\ 'IlLLM Replica !,I V\\ I / \\ I '\"------------ ./I II II Goals / I ;'State ab�tractions ,;--1 , ActionsI / ---------1 /;___ --��====���\\\\\\Checkpoints \\ ActorsRollout outcomesModel parameters --+ EnvironmentsFigure 7: This training system has four key parts: Actor, Learner, League and LLM replicas.。Actors are responsible fordata collection, the Learner trains the policy model using this data, the League coordinates the overall training process anddisplays results, and the LLM Replicas handle goal generation and distribute them to downstream workers.Algorithm 3 GOAL-CONDITIONED REINFORCEMENT LEARNING1:Input: θparameterizes policy πandζ={ζ1, ζ2, ζ3}parameterizes value heads, goal generators GrndandGop2:fork=1, 2, ... do3: Reset environment with returned initial state s0∼P(s)4: Sample a goal: g∼(Grnd∪Gop)(s0,∆t,∆V)5: Run policy πθkin environment until be terminated6: Actors collect trajectories Dτand send them to the Learner7: Update the θktoθk+1with Equation (6)8: Update ζbymax ζEs∼Dτh∥Rbt−Vbζ1(st)∥2+∥Roat−Voaζ2(st)∥2+1gt̸=∅∥Rgt−Vgζ3(st)∥2i9:end forThe computation of each can be expressed as follows:Rbt=TXi=tγrb(si, ai), Roat=TXi=tγroa(si, ai), Rgt=TXi=tγrg(si, ai, g).。(9)As for J(πg,θ)in Equation (6), we follow the computation of policy loss in PPO to express it asJ(πg,θ) = min\u0012πg,θ(s, a)πg,θk(s, a)Aπg,θk(s, a), g(ϵ, Aπg,θk(s, a))\u0013,where (s, a)∼ D τ, g(ϵ, A) =((1 +ϵ)A, A ≥0(1−ϵ)A, A > 0.。(10)Aπ(s, a)indicates the advantage is computed under the condition of policy π.I.。Open-ended Goal GenerationInspired by Hindsight Experience Replay (HER) (Andrychowicz et al., 2017), we adopt a similar method to utilizethe collected trajectories for learning a goal generator Gopwhich accepts a state as input.。We conclude its train-ing in two steps: (1) constructing (s, g)pairs with collected trajectories as illustrated in Figure 11(b); (2) supervisedtraining Gopwith the above pairs and an MSE loss between the labeled goals and predicted goals.。For step (1),we split trajectories into many segments with length of 200 timesteps.. \n",
      "the piece 830\n",
      " \n",
      "ited fine-tuning,4,428 for supervised fine-tuning, and 4,994 for ensembling fine-tuning.F.。Distributed Training FrameworkTo improve the training efficiency, we adopt a distributed training system, shown in Figure 7.。In this system, the Actors runover CPU nodes to collect training data, then send the collected data to the Learner which is deployed on a GPU node.。Wefurther take a LLM server to enable multiple replicas of LLM for goal generation, which improve the throughput of rolloutwhen the RL training is switch to goal-conditioned cases.G.。Parameter SettingsSome of the hyper-parameters used in our experiment are illustrated in Table 11 and other dynamic hyper-parameters areintroduced their corresponding parts.H.。AlgorithmsAlgorithm 3 lists the pseudo-code of goal-conditioned RL procedures in Stage I, where Rbt,RoatandRgtrepresent thediscounted basic return, obstacle avoidance return and goal-reaching return from time step ttill the termination, respectively.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationLearning tasksLearner LeagueTraining outcomes+-Model parameters --+ Redis ,.---' --------..., 'I,..)\"'----------- \"\\ 'IlLLM Replica !,I V\\ I / \\ I '\"------------ ./I II II Goals / I ;'State ab�tractions ,;--1 , ActionsI / ---------1 /;___ --��====���\\\\\\Checkpoints \\ ActorsRollout outcomesModel parameters --+ EnvironmentsFigure 7: This training system has four key parts: Actor, Learner, League and LLM replicas.。Actors are responsible fordata collection, the Learner trains the policy model using this data, the League coordinates the overall training process anddisplays results, and the LLM Replicas handle goal generation and distribute them to downstream workers.Algorithm 3 GOAL-CONDITIONED REINFORCEMENT LEARNING1:Input: θparameterizes policy πandζ={ζ1, ζ2, ζ3}parameterizes value heads, goal generators GrndandGop2:fork=1, 2, ... do3: Reset environment with returned initial state s0∼P(s)4: Sample a goal: g∼(Grnd∪Gop)(s0,∆t,∆V)5: Run policy πθkin environment until be terminated6: Actors collect trajectories Dτand send them to the Learner7: Update the θktoθk+1with Equation (6)8: Update ζbymax ζEs∼Dτh∥Rbt−Vbζ1(st)∥2+∥Roat−Voaζ2(st)∥2+1gt̸=∅∥Rgt−Vgζ3(st)∥2i9:end forThe computation of each can be expressed as follows:Rbt=TXi=tγrb(si, ai), Roat=TXi=tγroa(si, ai), Rgt=TXi=tγrg(si, ai, g).。(9)As for J(πg,θ)in Equation (6), we follow the computation of policy loss in PPO to express it asJ(πg,θ) = min\u0012πg,θ(s, a)πg,θk(s, a)Aπg,θk(s, a), g(ϵ, Aπg,θk(s, a))\u0013,where (s, a)∼ D τ, g(ϵ, A) =((1 +ϵ)A, A ≥0(1−ϵ)A, A > 0.。(10)Aπ(s, a)indicates the advantage is computed under the condition of policy π.I.。Open-ended Goal GenerationInspired by Hindsight Experience Replay (HER) (Andrychowicz et al., 2017), we adopt a similar method to utilizethe collected trajectories for learning a goal generator Gopwhich accepts a state as input.。We conclude its train-ing in two steps: (1) constructing (s, g)pairs with collected trajectories as illustrated in Figure 11(b); (2) supervisedtraining Gopwith the above pairs and an MSE loss between the labeled goals and predicted goals.。For step (1),we split trajectories into many segments with length of 200 timesteps.。Then, we randomly sample a state sfromthe first 150 steps and sample a state s′from the last 20 steps to derive a goal g=PROJ(s′), with a distributionproportional to their basic value Vbasic(s′).. \n",
      "the piece 831\n",
      " \n",
      "in Figure 7.。In this system, the Actors runover CPU nodes to collect training data, then send the collected data to the Learner which is deployed on a GPU node.。Wefurther take a LLM server to enable multiple replicas of LLM for goal generation, which improve the throughput of rolloutwhen the RL training is switch to goal-conditioned cases.G.。Parameter SettingsSome of the hyper-parameters used in our experiment are illustrated in Table 11 and other dynamic hyper-parameters areintroduced their corresponding parts.H.。AlgorithmsAlgorithm 3 lists the pseudo-code of goal-conditioned RL procedures in Stage I, where Rbt,RoatandRgtrepresent thediscounted basic return, obstacle avoidance return and goal-reaching return from time step ttill the termination, respectively.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationLearning tasksLearner LeagueTraining outcomes+-Model parameters --+ Redis ,.---' --------..., 'I,..)\"'----------- \"\\ 'IlLLM Replica !,I V\\ I / \\ I '\"------------ ./I II II Goals / I ;'State ab�tractions ,;--1 , ActionsI / ---------1 /;___ --��====���\\\\\\Checkpoints \\ ActorsRollout outcomesModel parameters --+ EnvironmentsFigure 7: This training system has four key parts: Actor, Learner, League and LLM replicas.。Actors are responsible fordata collection, the Learner trains the policy model using this data, the League coordinates the overall training process anddisplays results, and the LLM Replicas handle goal generation and distribute them to downstream workers.Algorithm 3 GOAL-CONDITIONED REINFORCEMENT LEARNING1:Input: θparameterizes policy πandζ={ζ1, ζ2, ζ3}parameterizes value heads, goal generators GrndandGop2:fork=1, 2, ... do3: Reset environment with returned initial state s0∼P(s)4: Sample a goal: g∼(Grnd∪Gop)(s0,∆t,∆V)5: Run policy πθkin environment until be terminated6: Actors collect trajectories Dτand send them to the Learner7: Update the θktoθk+1with Equation (6)8: Update ζbymax ζEs∼Dτh∥Rbt−Vbζ1(st)∥2+∥Roat−Voaζ2(st)∥2+1gt̸=∅∥Rgt−Vgζ3(st)∥2i9:end forThe computation of each can be expressed as follows:Rbt=TXi=tγrb(si, ai), Roat=TXi=tγroa(si, ai), Rgt=TXi=tγrg(si, ai, g).。(9)As for J(πg,θ)in Equation (6), we follow the computation of policy loss in PPO to express it asJ(πg,θ) = min\u0012πg,θ(s, a)πg,θk(s, a)Aπg,θk(s, a), g(ϵ, Aπg,θk(s, a))\u0013,where (s, a)∼ D τ, g(ϵ, A) =((1 +ϵ)A, A ≥0(1−ϵ)A, A > 0.。(10)Aπ(s, a)indicates the advantage is computed under the condition of policy π.I.。Open-ended Goal GenerationInspired by Hindsight Experience Replay (HER) (Andrychowicz et al., 2017), we adopt a similar method to utilizethe collected trajectories for learning a goal generator Gopwhich accepts a state as input.。We conclude its train-ing in two steps: (1) constructing (s, g)pairs with collected trajectories as illustrated in Figure 11(b); (2) supervisedtraining Gopwith the above pairs and an MSE loss between the labeled goals and predicted goals.。For step (1),we split trajectories into many segments with length of 200 timesteps.。Then, we randomly sample a state sfromthe first 150 steps and sample a state s′from the last 20 steps to derive a goal g=PROJ(s′), with a distributionproportional to their basic value Vbasic(s′).。For step (2), we train Gopwith s,∆t,Vbasic(s)andVbasic(s′)as in-put to generate goals, where ∆tthe time slot of goal completion, Vbasic(s′)andVbasic(s)the basic state value.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationCoFT... ... ... ...StatesCoTQuestionsCoTResponses  HI      RI    SI     AIChatGLM LoRA-FT+tuning... ... ... ...SFTStatesGoalsInstructions  HI      RI    SI     AIChatGLM LoRA-FT+tuningEFTStateResponse 1ChatGLMLoRA-FTLoRA-CK1LoRA-CK2+......Response 2 Response NEnsemble R esponse generatingSFTvotingInstructionChatGLM LoRA-FT +LoRA-CT RL ModelENVRewardsStateGoal Completion RewardExamination RewardFormat RewardInstruction ResponseExam SetInteractionRewardComplete statusPPO TuningFormat recognitionselectingCo-TrainingStateFigure 8: Overview of the training framework with LLM.. \n",
      "the piece 832\n",
      " \n",
      "d data to the Learner which is deployed on a GPU node.。Wefurther take a LLM server to enable multiple replicas of LLM for goal generation, which improve the throughput of rolloutwhen the RL training is switch to goal-conditioned cases.G.。Parameter SettingsSome of the hyper-parameters used in our experiment are illustrated in Table 11 and other dynamic hyper-parameters areintroduced their corresponding parts.H.。AlgorithmsAlgorithm 3 lists the pseudo-code of goal-conditioned RL procedures in Stage I, where Rbt,RoatandRgtrepresent thediscounted basic return, obstacle avoidance return and goal-reaching return from time step ttill the termination, respectively.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationLearning tasksLearner LeagueTraining outcomes+-Model parameters --+ Redis ,.---' --------..., 'I,..)\"'----------- \"\\ 'IlLLM Replica !,I V\\ I / \\ I '\"------------ ./I II II Goals / I ;'State ab�tractions ,;--1 , ActionsI / ---------1 /;___ --��====���\\\\\\Checkpoints \\ ActorsRollout outcomesModel parameters --+ EnvironmentsFigure 7: This training system has four key parts: Actor, Learner, League and LLM replicas.。Actors are responsible fordata collection, the Learner trains the policy model using this data, the League coordinates the overall training process anddisplays results, and the LLM Replicas handle goal generation and distribute them to downstream workers.Algorithm 3 GOAL-CONDITIONED REINFORCEMENT LEARNING1:Input: θparameterizes policy πandζ={ζ1, ζ2, ζ3}parameterizes value heads, goal generators GrndandGop2:fork=1, 2, ... do3: Reset environment with returned initial state s0∼P(s)4: Sample a goal: g∼(Grnd∪Gop)(s0,∆t,∆V)5: Run policy πθkin environment until be terminated6: Actors collect trajectories Dτand send them to the Learner7: Update the θktoθk+1with Equation (6)8: Update ζbymax ζEs∼Dτh∥Rbt−Vbζ1(st)∥2+∥Roat−Voaζ2(st)∥2+1gt̸=∅∥Rgt−Vgζ3(st)∥2i9:end forThe computation of each can be expressed as follows:Rbt=TXi=tγrb(si, ai), Roat=TXi=tγroa(si, ai), Rgt=TXi=tγrg(si, ai, g).。(9)As for J(πg,θ)in Equation (6), we follow the computation of policy loss in PPO to express it asJ(πg,θ) = min\u0012πg,θ(s, a)πg,θk(s, a)Aπg,θk(s, a), g(ϵ, Aπg,θk(s, a))\u0013,where (s, a)∼ D τ, g(ϵ, A) =((1 +ϵ)A, A ≥0(1−ϵ)A, A > 0.。(10)Aπ(s, a)indicates the advantage is computed under the condition of policy π.I.。Open-ended Goal GenerationInspired by Hindsight Experience Replay (HER) (Andrychowicz et al., 2017), we adopt a similar method to utilizethe collected trajectories for learning a goal generator Gopwhich accepts a state as input.。We conclude its train-ing in two steps: (1) constructing (s, g)pairs with collected trajectories as illustrated in Figure 11(b); (2) supervisedtraining Gopwith the above pairs and an MSE loss between the labeled goals and predicted goals.。For step (1),we split trajectories into many segments with length of 200 timesteps.。Then, we randomly sample a state sfromthe first 150 steps and sample a state s′from the last 20 steps to derive a goal g=PROJ(s′), with a distributionproportional to their basic value Vbasic(s′).。For step (2), we train Gopwith s,∆t,Vbasic(s)andVbasic(s′)as in-put to generate goals, where ∆tthe time slot of goal completion, Vbasic(s′)andVbasic(s)the basic state value.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationCoFT... ... ... ...StatesCoTQuestionsCoTResponses  HI      RI    SI     AIChatGLM LoRA-FT+tuning... ... ... ...SFTStatesGoalsInstructions  HI      RI    SI     AIChatGLM LoRA-FT+tuningEFTStateResponse 1ChatGLMLoRA-FTLoRA-CK1LoRA-CK2+......Response 2 Response NEnsemble R esponse generatingSFTvotingInstructionChatGLM LoRA-FT +LoRA-CT RL ModelENVRewardsStateGoal Completion RewardExamination RewardFormat RewardInstruction ResponseExam SetInteractionRewardComplete statusPPO TuningFormat recognitionselectingCo-TrainingStateFigure 8: Overview of the training framework with LLM.。This training framework has three kinds of LLM tuning approaches:CoFT (Chain of Thoughts assisted Fine-Tuning), SFT (Supervised Fine-Tuning), EFT (Ensemble Fine-Tuning); and oneLLM-RL co-training approach.PPO clip eps 0.2Optimizer AdamLearning rate 0.0001Batch size 20480Number of CPUs 5120 (AMD EPYC 7H12 64-Core)Number of GPUs 2 (A100)γ(basic ) 0.995γ(oa) 0.92γ(goal) 0.993λ 0.95Entropy coefficient 0.025Unroll length 20Sample max use times 3Gradient clip threshold 10Table 11: Parameter settings for RL.Validation.. \n",
      "the piece 833\n",
      " \n",
      "hroughput of rolloutwhen the RL training is switch to goal-conditioned cases.G.。Parameter SettingsSome of the hyper-parameters used in our experiment are illustrated in Table 11 and other dynamic hyper-parameters areintroduced their corresponding parts.H.。AlgorithmsAlgorithm 3 lists the pseudo-code of goal-conditioned RL procedures in Stage I, where Rbt,RoatandRgtrepresent thediscounted basic return, obstacle avoidance return and goal-reaching return from time step ttill the termination, respectively.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationLearning tasksLearner LeagueTraining outcomes+-Model parameters --+ Redis ,.---' --------..., 'I,..)\"'----------- \"\\ 'IlLLM Replica !,I V\\ I / \\ I '\"------------ ./I II II Goals / I ;'State ab�tractions ,;--1 , ActionsI / ---------1 /;___ --��====���\\\\\\Checkpoints \\ ActorsRollout outcomesModel parameters --+ EnvironmentsFigure 7: This training system has four key parts: Actor, Learner, League and LLM replicas.。Actors are responsible fordata collection, the Learner trains the policy model using this data, the League coordinates the overall training process anddisplays results, and the LLM Replicas handle goal generation and distribute them to downstream workers.Algorithm 3 GOAL-CONDITIONED REINFORCEMENT LEARNING1:Input: θparameterizes policy πandζ={ζ1, ζ2, ζ3}parameterizes value heads, goal generators GrndandGop2:fork=1, 2, ... do3: Reset environment with returned initial state s0∼P(s)4: Sample a goal: g∼(Grnd∪Gop)(s0,∆t,∆V)5: Run policy πθkin environment until be terminated6: Actors collect trajectories Dτand send them to the Learner7: Update the θktoθk+1with Equation (6)8: Update ζbymax ζEs∼Dτh∥Rbt−Vbζ1(st)∥2+∥Roat−Voaζ2(st)∥2+1gt̸=∅∥Rgt−Vgζ3(st)∥2i9:end forThe computation of each can be expressed as follows:Rbt=TXi=tγrb(si, ai), Roat=TXi=tγroa(si, ai), Rgt=TXi=tγrg(si, ai, g).。(9)As for J(πg,θ)in Equation (6), we follow the computation of policy loss in PPO to express it asJ(πg,θ) = min\u0012πg,θ(s, a)πg,θk(s, a)Aπg,θk(s, a), g(ϵ, Aπg,θk(s, a))\u0013,where (s, a)∼ D τ, g(ϵ, A) =((1 +ϵ)A, A ≥0(1−ϵ)A, A > 0.。(10)Aπ(s, a)indicates the advantage is computed under the condition of policy π.I.。Open-ended Goal GenerationInspired by Hindsight Experience Replay (HER) (Andrychowicz et al., 2017), we adopt a similar method to utilizethe collected trajectories for learning a goal generator Gopwhich accepts a state as input.。We conclude its train-ing in two steps: (1) constructing (s, g)pairs with collected trajectories as illustrated in Figure 11(b); (2) supervisedtraining Gopwith the above pairs and an MSE loss between the labeled goals and predicted goals.。For step (1),we split trajectories into many segments with length of 200 timesteps.。Then, we randomly sample a state sfromthe first 150 steps and sample a state s′from the last 20 steps to derive a goal g=PROJ(s′), with a distributionproportional to their basic value Vbasic(s′).。For step (2), we train Gopwith s,∆t,Vbasic(s)andVbasic(s′)as in-put to generate goals, where ∆tthe time slot of goal completion, Vbasic(s′)andVbasic(s)the basic state value.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationCoFT... ... ... ...StatesCoTQuestionsCoTResponses  HI      RI    SI     AIChatGLM LoRA-FT+tuning... ... ... ...SFTStatesGoalsInstructions  HI      RI    SI     AIChatGLM LoRA-FT+tuningEFTStateResponse 1ChatGLMLoRA-FTLoRA-CK1LoRA-CK2+......Response 2 Response NEnsemble R esponse generatingSFTvotingInstructionChatGLM LoRA-FT +LoRA-CT RL ModelENVRewardsStateGoal Completion RewardExamination RewardFormat RewardInstruction ResponseExam SetInteractionRewardComplete statusPPO TuningFormat recognitionselectingCo-TrainingStateFigure 8: Overview of the training framework with LLM.。This training framework has three kinds of LLM tuning approaches:CoFT (Chain of Thoughts assisted Fine-Tuning), SFT (Supervised Fine-Tuning), EFT (Ensemble Fine-Tuning); and oneLLM-RL co-training approach.PPO clip eps 0.2Optimizer AdamLearning rate 0.0001Batch size 20480Number of CPUs 5120 (AMD EPYC 7H12 64-Core)Number of GPUs 2 (A100)γ(basic ) 0.995γ(oa) 0.92γ(goal) 0.993λ 0.95Entropy coefficient 0.025Unroll length 20Sample max use times 3Gradient clip threshold 10Table 11: Parameter settings for RL.Validation.。To validate the efficiency of Gop, we conduct a com-parison between generated goals and oracles w.r.t distribution, asillustrated in Figure 10.. \n",
      "the piece 834\n",
      " \n",
      " used in our experiment are illustrated in Table 11 and other dynamic hyper-parameters areintroduced their corresponding parts.H.。AlgorithmsAlgorithm 3 lists the pseudo-code of goal-conditioned RL procedures in Stage I, where Rbt,RoatandRgtrepresent thediscounted basic return, obstacle avoidance return and goal-reaching return from time step ttill the termination, respectively.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationLearning tasksLearner LeagueTraining outcomes+-Model parameters --+ Redis ,.---' --------..., 'I,..)\"'----------- \"\\ 'IlLLM Replica !,I V\\ I / \\ I '\"------------ ./I II II Goals / I ;'State ab�tractions ,;--1 , ActionsI / ---------1 /;___ --��====���\\\\\\Checkpoints \\ ActorsRollout outcomesModel parameters --+ EnvironmentsFigure 7: This training system has four key parts: Actor, Learner, League and LLM replicas.。Actors are responsible fordata collection, the Learner trains the policy model using this data, the League coordinates the overall training process anddisplays results, and the LLM Replicas handle goal generation and distribute them to downstream workers.Algorithm 3 GOAL-CONDITIONED REINFORCEMENT LEARNING1:Input: θparameterizes policy πandζ={ζ1, ζ2, ζ3}parameterizes value heads, goal generators GrndandGop2:fork=1, 2, ... do3: Reset environment with returned initial state s0∼P(s)4: Sample a goal: g∼(Grnd∪Gop)(s0,∆t,∆V)5: Run policy πθkin environment until be terminated6: Actors collect trajectories Dτand send them to the Learner7: Update the θktoθk+1with Equation (6)8: Update ζbymax ζEs∼Dτh∥Rbt−Vbζ1(st)∥2+∥Roat−Voaζ2(st)∥2+1gt̸=∅∥Rgt−Vgζ3(st)∥2i9:end forThe computation of each can be expressed as follows:Rbt=TXi=tγrb(si, ai), Roat=TXi=tγroa(si, ai), Rgt=TXi=tγrg(si, ai, g).。(9)As for J(πg,θ)in Equation (6), we follow the computation of policy loss in PPO to express it asJ(πg,θ) = min\u0012πg,θ(s, a)πg,θk(s, a)Aπg,θk(s, a), g(ϵ, Aπg,θk(s, a))\u0013,where (s, a)∼ D τ, g(ϵ, A) =((1 +ϵ)A, A ≥0(1−ϵ)A, A > 0.。(10)Aπ(s, a)indicates the advantage is computed under the condition of policy π.I.。Open-ended Goal GenerationInspired by Hindsight Experience Replay (HER) (Andrychowicz et al., 2017), we adopt a similar method to utilizethe collected trajectories for learning a goal generator Gopwhich accepts a state as input.。We conclude its train-ing in two steps: (1) constructing (s, g)pairs with collected trajectories as illustrated in Figure 11(b); (2) supervisedtraining Gopwith the above pairs and an MSE loss between the labeled goals and predicted goals.。For step (1),we split trajectories into many segments with length of 200 timesteps.。Then, we randomly sample a state sfromthe first 150 steps and sample a state s′from the last 20 steps to derive a goal g=PROJ(s′), with a distributionproportional to their basic value Vbasic(s′).。For step (2), we train Gopwith s,∆t,Vbasic(s)andVbasic(s′)as in-put to generate goals, where ∆tthe time slot of goal completion, Vbasic(s′)andVbasic(s)the basic state value.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationCoFT... ... ... ...StatesCoTQuestionsCoTResponses  HI      RI    SI     AIChatGLM LoRA-FT+tuning... ... ... ...SFTStatesGoalsInstructions  HI      RI    SI     AIChatGLM LoRA-FT+tuningEFTStateResponse 1ChatGLMLoRA-FTLoRA-CK1LoRA-CK2+......Response 2 Response NEnsemble R esponse generatingSFTvotingInstructionChatGLM LoRA-FT +LoRA-CT RL ModelENVRewardsStateGoal Completion RewardExamination RewardFormat RewardInstruction ResponseExam SetInteractionRewardComplete statusPPO TuningFormat recognitionselectingCo-TrainingStateFigure 8: Overview of the training framework with LLM.。This training framework has three kinds of LLM tuning approaches:CoFT (Chain of Thoughts assisted Fine-Tuning), SFT (Supervised Fine-Tuning), EFT (Ensemble Fine-Tuning); and oneLLM-RL co-training approach.PPO clip eps 0.2Optimizer AdamLearning rate 0.0001Batch size 20480Number of CPUs 5120 (AMD EPYC 7H12 64-Core)Number of GPUs 2 (A100)γ(basic ) 0.995γ(oa) 0.92γ(goal) 0.993λ 0.95Entropy coefficient 0.025Unroll length 20Sample max use times 3Gradient clip threshold 10Table 11: Parameter settings for RL.Validation.。To validate the efficiency of Gop, we conduct a com-parison between generated goals and oracles w.r.t distribution, asillustrated in Figure 10.。Empirically, we first construct a test datasetof goals (we label them as Oracles) with trajectories that are sampledwith multiple checkpoints of non-goal policy, to ensure the diversity.Then, the tuples of (s,∆t,∆V)that corresponding to the oracles areused to generate goal predictions (we label them as Prediction) withGop.. \n",
      "the piece 835\n",
      " \n",
      " 3 lists the pseudo-code of goal-conditioned RL procedures in Stage I, where Rbt,RoatandRgtrepresent thediscounted basic return, obstacle avoidance return and goal-reaching return from time step ttill the termination, respectively.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationLearning tasksLearner LeagueTraining outcomes+-Model parameters --+ Redis ,.---' --------..., 'I,..)\"'----------- \"\\ 'IlLLM Replica !,I V\\ I / \\ I '\"------------ ./I II II Goals / I ;'State ab�tractions ,;--1 , ActionsI / ---------1 /;___ --��====���\\\\\\Checkpoints \\ ActorsRollout outcomesModel parameters --+ EnvironmentsFigure 7: This training system has four key parts: Actor, Learner, League and LLM replicas.。Actors are responsible fordata collection, the Learner trains the policy model using this data, the League coordinates the overall training process anddisplays results, and the LLM Replicas handle goal generation and distribute them to downstream workers.Algorithm 3 GOAL-CONDITIONED REINFORCEMENT LEARNING1:Input: θparameterizes policy πandζ={ζ1, ζ2, ζ3}parameterizes value heads, goal generators GrndandGop2:fork=1, 2, ... do3: Reset environment with returned initial state s0∼P(s)4: Sample a goal: g∼(Grnd∪Gop)(s0,∆t,∆V)5: Run policy πθkin environment until be terminated6: Actors collect trajectories Dτand send them to the Learner7: Update the θktoθk+1with Equation (6)8: Update ζbymax ζEs∼Dτh∥Rbt−Vbζ1(st)∥2+∥Roat−Voaζ2(st)∥2+1gt̸=∅∥Rgt−Vgζ3(st)∥2i9:end forThe computation of each can be expressed as follows:Rbt=TXi=tγrb(si, ai), Roat=TXi=tγroa(si, ai), Rgt=TXi=tγrg(si, ai, g).。(9)As for J(πg,θ)in Equation (6), we follow the computation of policy loss in PPO to express it asJ(πg,θ) = min\u0012πg,θ(s, a)πg,θk(s, a)Aπg,θk(s, a), g(ϵ, Aπg,θk(s, a))\u0013,where (s, a)∼ D τ, g(ϵ, A) =((1 +ϵ)A, A ≥0(1−ϵ)A, A > 0.。(10)Aπ(s, a)indicates the advantage is computed under the condition of policy π.I.。Open-ended Goal GenerationInspired by Hindsight Experience Replay (HER) (Andrychowicz et al., 2017), we adopt a similar method to utilizethe collected trajectories for learning a goal generator Gopwhich accepts a state as input.。We conclude its train-ing in two steps: (1) constructing (s, g)pairs with collected trajectories as illustrated in Figure 11(b); (2) supervisedtraining Gopwith the above pairs and an MSE loss between the labeled goals and predicted goals.。For step (1),we split trajectories into many segments with length of 200 timesteps.。Then, we randomly sample a state sfromthe first 150 steps and sample a state s′from the last 20 steps to derive a goal g=PROJ(s′), with a distributionproportional to their basic value Vbasic(s′).。For step (2), we train Gopwith s,∆t,Vbasic(s)andVbasic(s′)as in-put to generate goals, where ∆tthe time slot of goal completion, Vbasic(s′)andVbasic(s)the basic state value.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationCoFT... ... ... ...StatesCoTQuestionsCoTResponses  HI      RI    SI     AIChatGLM LoRA-FT+tuning... ... ... ...SFTStatesGoalsInstructions  HI      RI    SI     AIChatGLM LoRA-FT+tuningEFTStateResponse 1ChatGLMLoRA-FTLoRA-CK1LoRA-CK2+......Response 2 Response NEnsemble R esponse generatingSFTvotingInstructionChatGLM LoRA-FT +LoRA-CT RL ModelENVRewardsStateGoal Completion RewardExamination RewardFormat RewardInstruction ResponseExam SetInteractionRewardComplete statusPPO TuningFormat recognitionselectingCo-TrainingStateFigure 8: Overview of the training framework with LLM.。This training framework has three kinds of LLM tuning approaches:CoFT (Chain of Thoughts assisted Fine-Tuning), SFT (Supervised Fine-Tuning), EFT (Ensemble Fine-Tuning); and oneLLM-RL co-training approach.PPO clip eps 0.2Optimizer AdamLearning rate 0.0001Batch size 20480Number of CPUs 5120 (AMD EPYC 7H12 64-Core)Number of GPUs 2 (A100)γ(basic ) 0.995γ(oa) 0.92γ(goal) 0.993λ 0.95Entropy coefficient 0.025Unroll length 20Sample max use times 3Gradient clip threshold 10Table 11: Parameter settings for RL.Validation.。To validate the efficiency of Gop, we conduct a com-parison between generated goals and oracles w.r.t distribution, asillustrated in Figure 10.。Empirically, we first construct a test datasetof goals (we label them as Oracles) with trajectories that are sampledwith multiple checkpoints of non-goal policy, to ensure the diversity.Then, the tuples of (s,∆t,∆V)that corresponding to the oracles areused to generate goal predictions (we label them as Prediction) withGop.。To visualize the goal distribution, we leverage TSNE (Van derMaaten & Hinton, 2008) to shrink the goal dimension from 68 to 1.The results in Figure 10 show that the distribution of Prediction wellmatches the distribution of Oracles.J.. \n",
      "the piece 836\n",
      " \n",
      " time step ttill the termination, respectively.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationLearning tasksLearner LeagueTraining outcomes+-Model parameters --+ Redis ,.---' --------..., 'I,..)\"'----------- \"\\ 'IlLLM Replica !,I V\\ I / \\ I '\"------------ ./I II II Goals / I ;'State ab�tractions ,;--1 , ActionsI / ---------1 /;___ --��====���\\\\\\Checkpoints \\ ActorsRollout outcomesModel parameters --+ EnvironmentsFigure 7: This training system has four key parts: Actor, Learner, League and LLM replicas.。Actors are responsible fordata collection, the Learner trains the policy model using this data, the League coordinates the overall training process anddisplays results, and the LLM Replicas handle goal generation and distribute them to downstream workers.Algorithm 3 GOAL-CONDITIONED REINFORCEMENT LEARNING1:Input: θparameterizes policy πandζ={ζ1, ζ2, ζ3}parameterizes value heads, goal generators GrndandGop2:fork=1, 2, ... do3: Reset environment with returned initial state s0∼P(s)4: Sample a goal: g∼(Grnd∪Gop)(s0,∆t,∆V)5: Run policy πθkin environment until be terminated6: Actors collect trajectories Dτand send them to the Learner7: Update the θktoθk+1with Equation (6)8: Update ζbymax ζEs∼Dτh∥Rbt−Vbζ1(st)∥2+∥Roat−Voaζ2(st)∥2+1gt̸=∅∥Rgt−Vgζ3(st)∥2i9:end forThe computation of each can be expressed as follows:Rbt=TXi=tγrb(si, ai), Roat=TXi=tγroa(si, ai), Rgt=TXi=tγrg(si, ai, g).。(9)As for J(πg,θ)in Equation (6), we follow the computation of policy loss in PPO to express it asJ(πg,θ) = min\u0012πg,θ(s, a)πg,θk(s, a)Aπg,θk(s, a), g(ϵ, Aπg,θk(s, a))\u0013,where (s, a)∼ D τ, g(ϵ, A) =((1 +ϵ)A, A ≥0(1−ϵ)A, A > 0.。(10)Aπ(s, a)indicates the advantage is computed under the condition of policy π.I.。Open-ended Goal GenerationInspired by Hindsight Experience Replay (HER) (Andrychowicz et al., 2017), we adopt a similar method to utilizethe collected trajectories for learning a goal generator Gopwhich accepts a state as input.。We conclude its train-ing in two steps: (1) constructing (s, g)pairs with collected trajectories as illustrated in Figure 11(b); (2) supervisedtraining Gopwith the above pairs and an MSE loss between the labeled goals and predicted goals.。For step (1),we split trajectories into many segments with length of 200 timesteps.。Then, we randomly sample a state sfromthe first 150 steps and sample a state s′from the last 20 steps to derive a goal g=PROJ(s′), with a distributionproportional to their basic value Vbasic(s′).。For step (2), we train Gopwith s,∆t,Vbasic(s)andVbasic(s′)as in-put to generate goals, where ∆tthe time slot of goal completion, Vbasic(s′)andVbasic(s)the basic state value.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationCoFT... ... ... ...StatesCoTQuestionsCoTResponses  HI      RI    SI     AIChatGLM LoRA-FT+tuning... ... ... ...SFTStatesGoalsInstructions  HI      RI    SI     AIChatGLM LoRA-FT+tuningEFTStateResponse 1ChatGLMLoRA-FTLoRA-CK1LoRA-CK2+......Response 2 Response NEnsemble R esponse generatingSFTvotingInstructionChatGLM LoRA-FT +LoRA-CT RL ModelENVRewardsStateGoal Completion RewardExamination RewardFormat RewardInstruction ResponseExam SetInteractionRewardComplete statusPPO TuningFormat recognitionselectingCo-TrainingStateFigure 8: Overview of the training framework with LLM.。This training framework has three kinds of LLM tuning approaches:CoFT (Chain of Thoughts assisted Fine-Tuning), SFT (Supervised Fine-Tuning), EFT (Ensemble Fine-Tuning); and oneLLM-RL co-training approach.PPO clip eps 0.2Optimizer AdamLearning rate 0.0001Batch size 20480Number of CPUs 5120 (AMD EPYC 7H12 64-Core)Number of GPUs 2 (A100)γ(basic ) 0.995γ(oa) 0.92γ(goal) 0.993λ 0.95Entropy coefficient 0.025Unroll length 20Sample max use times 3Gradient clip threshold 10Table 11: Parameter settings for RL.Validation.。To validate the efficiency of Gop, we conduct a com-parison between generated goals and oracles w.r.t distribution, asillustrated in Figure 10.。Empirically, we first construct a test datasetof goals (we label them as Oracles) with trajectories that are sampledwith multiple checkpoints of non-goal policy, to ensure the diversity.Then, the tuples of (s,∆t,∆V)that corresponding to the oracles areused to generate goal predictions (we label them as Prediction) withGop.。To visualize the goal distribution, we leverage TSNE (Van derMaaten & Hinton, 2008) to shrink the goal dimension from 68 to 1.The results in Figure 10 show that the distribution of Prediction wellmatches the distribution of Oracles.J.。Ablation Study on LLMsThe Impact of Lora Rank.. \n",
      "the piece 837\n",
      " \n",
      "l parameters --+ Redis ,.---' --------..., 'I,..)\"'----------- \"\\ 'IlLLM Replica !,I V\\ I / \\ I '\"------------ ./I II II Goals / I ;'State ab�tractions ,;--1 , ActionsI / ---------1 /;___ --��====���\\\\\\Checkpoints \\ ActorsRollout outcomesModel parameters --+ EnvironmentsFigure 7: This training system has four key parts: Actor, Learner, League and LLM replicas.。Actors are responsible fordata collection, the Learner trains the policy model using this data, the League coordinates the overall training process anddisplays results, and the LLM Replicas handle goal generation and distribute them to downstream workers.Algorithm 3 GOAL-CONDITIONED REINFORCEMENT LEARNING1:Input: θparameterizes policy πandζ={ζ1, ζ2, ζ3}parameterizes value heads, goal generators GrndandGop2:fork=1, 2, ... do3: Reset environment with returned initial state s0∼P(s)4: Sample a goal: g∼(Grnd∪Gop)(s0,∆t,∆V)5: Run policy πθkin environment until be terminated6: Actors collect trajectories Dτand send them to the Learner7: Update the θktoθk+1with Equation (6)8: Update ζbymax ζEs∼Dτh∥Rbt−Vbζ1(st)∥2+∥Roat−Voaζ2(st)∥2+1gt̸=∅∥Rgt−Vgζ3(st)∥2i9:end forThe computation of each can be expressed as follows:Rbt=TXi=tγrb(si, ai), Roat=TXi=tγroa(si, ai), Rgt=TXi=tγrg(si, ai, g).。(9)As for J(πg,θ)in Equation (6), we follow the computation of policy loss in PPO to express it asJ(πg,θ) = min\u0012πg,θ(s, a)πg,θk(s, a)Aπg,θk(s, a), g(ϵ, Aπg,θk(s, a))\u0013,where (s, a)∼ D τ, g(ϵ, A) =((1 +ϵ)A, A ≥0(1−ϵ)A, A > 0.。(10)Aπ(s, a)indicates the advantage is computed under the condition of policy π.I.。Open-ended Goal GenerationInspired by Hindsight Experience Replay (HER) (Andrychowicz et al., 2017), we adopt a similar method to utilizethe collected trajectories for learning a goal generator Gopwhich accepts a state as input.。We conclude its train-ing in two steps: (1) constructing (s, g)pairs with collected trajectories as illustrated in Figure 11(b); (2) supervisedtraining Gopwith the above pairs and an MSE loss between the labeled goals and predicted goals.。For step (1),we split trajectories into many segments with length of 200 timesteps.。Then, we randomly sample a state sfromthe first 150 steps and sample a state s′from the last 20 steps to derive a goal g=PROJ(s′), with a distributionproportional to their basic value Vbasic(s′).。For step (2), we train Gopwith s,∆t,Vbasic(s)andVbasic(s′)as in-put to generate goals, where ∆tthe time slot of goal completion, Vbasic(s′)andVbasic(s)the basic state value.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationCoFT... ... ... ...StatesCoTQuestionsCoTResponses  HI      RI    SI     AIChatGLM LoRA-FT+tuning... ... ... ...SFTStatesGoalsInstructions  HI      RI    SI     AIChatGLM LoRA-FT+tuningEFTStateResponse 1ChatGLMLoRA-FTLoRA-CK1LoRA-CK2+......Response 2 Response NEnsemble R esponse generatingSFTvotingInstructionChatGLM LoRA-FT +LoRA-CT RL ModelENVRewardsStateGoal Completion RewardExamination RewardFormat RewardInstruction ResponseExam SetInteractionRewardComplete statusPPO TuningFormat recognitionselectingCo-TrainingStateFigure 8: Overview of the training framework with LLM.。This training framework has three kinds of LLM tuning approaches:CoFT (Chain of Thoughts assisted Fine-Tuning), SFT (Supervised Fine-Tuning), EFT (Ensemble Fine-Tuning); and oneLLM-RL co-training approach.PPO clip eps 0.2Optimizer AdamLearning rate 0.0001Batch size 20480Number of CPUs 5120 (AMD EPYC 7H12 64-Core)Number of GPUs 2 (A100)γ(basic ) 0.995γ(oa) 0.92γ(goal) 0.993λ 0.95Entropy coefficient 0.025Unroll length 20Sample max use times 3Gradient clip threshold 10Table 11: Parameter settings for RL.Validation.。To validate the efficiency of Gop, we conduct a com-parison between generated goals and oracles w.r.t distribution, asillustrated in Figure 10.。Empirically, we first construct a test datasetof goals (we label them as Oracles) with trajectories that are sampledwith multiple checkpoints of non-goal policy, to ensure the diversity.Then, the tuples of (s,∆t,∆V)that corresponding to the oracles areused to generate goal predictions (we label them as Prediction) withGop.。To visualize the goal distribution, we leverage TSNE (Van derMaaten & Hinton, 2008) to shrink the goal dimension from 68 to 1.The results in Figure 10 show that the distribution of Prediction wellmatches the distribution of Oracles.J.。Ablation Study on LLMsThe Impact of Lora Rank.。We evaluate the impact of the rank parameter on performance during LoRA fine-tuning oflarge language model neural networks.. \n",
      "the piece 838\n",
      " \n",
      " goal generators GrndandGop2:fork=1, 2, ... do3: Reset environment with returned initial state s0∼P(s)4: Sample a goal: g∼(Grnd∪Gop)(s0,∆t,∆V)5: Run policy πθkin environment until be terminated6: Actors collect trajectories Dτand send them to the Learner7: Update the θktoθk+1with Equation (6)8: Update ζbymax ζEs∼Dτh∥Rbt−Vbζ1(st)∥2+∥Roat−Voaζ2(st)∥2+1gt̸=∅∥Rgt−Vgζ3(st)∥2i9:end forThe computation of each can be expressed as follows:Rbt=TXi=tγrb(si, ai), Roat=TXi=tγroa(si, ai), Rgt=TXi=tγrg(si, ai, g).。(9)As for J(πg,θ)in Equation (6), we follow the computation of policy loss in PPO to express it asJ(πg,θ) = min\u0012πg,θ(s, a)πg,θk(s, a)Aπg,θk(s, a), g(ϵ, Aπg,θk(s, a))\u0013,where (s, a)∼ D τ, g(ϵ, A) =((1 +ϵ)A, A ≥0(1−ϵ)A, A > 0.。(10)Aπ(s, a)indicates the advantage is computed under the condition of policy π.I.。Open-ended Goal GenerationInspired by Hindsight Experience Replay (HER) (Andrychowicz et al., 2017), we adopt a similar method to utilizethe collected trajectories for learning a goal generator Gopwhich accepts a state as input.。We conclude its train-ing in two steps: (1) constructing (s, g)pairs with collected trajectories as illustrated in Figure 11(b); (2) supervisedtraining Gopwith the above pairs and an MSE loss between the labeled goals and predicted goals.。For step (1),we split trajectories into many segments with length of 200 timesteps.。Then, we randomly sample a state sfromthe first 150 steps and sample a state s′from the last 20 steps to derive a goal g=PROJ(s′), with a distributionproportional to their basic value Vbasic(s′).。For step (2), we train Gopwith s,∆t,Vbasic(s)andVbasic(s′)as in-put to generate goals, where ∆tthe time slot of goal completion, Vbasic(s′)andVbasic(s)the basic state value.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationCoFT... ... ... ...StatesCoTQuestionsCoTResponses  HI      RI    SI     AIChatGLM LoRA-FT+tuning... ... ... ...SFTStatesGoalsInstructions  HI      RI    SI     AIChatGLM LoRA-FT+tuningEFTStateResponse 1ChatGLMLoRA-FTLoRA-CK1LoRA-CK2+......Response 2 Response NEnsemble R esponse generatingSFTvotingInstructionChatGLM LoRA-FT +LoRA-CT RL ModelENVRewardsStateGoal Completion RewardExamination RewardFormat RewardInstruction ResponseExam SetInteractionRewardComplete statusPPO TuningFormat recognitionselectingCo-TrainingStateFigure 8: Overview of the training framework with LLM.。This training framework has three kinds of LLM tuning approaches:CoFT (Chain of Thoughts assisted Fine-Tuning), SFT (Supervised Fine-Tuning), EFT (Ensemble Fine-Tuning); and oneLLM-RL co-training approach.PPO clip eps 0.2Optimizer AdamLearning rate 0.0001Batch size 20480Number of CPUs 5120 (AMD EPYC 7H12 64-Core)Number of GPUs 2 (A100)γ(basic ) 0.995γ(oa) 0.92γ(goal) 0.993λ 0.95Entropy coefficient 0.025Unroll length 20Sample max use times 3Gradient clip threshold 10Table 11: Parameter settings for RL.Validation.。To validate the efficiency of Gop, we conduct a com-parison between generated goals and oracles w.r.t distribution, asillustrated in Figure 10.。Empirically, we first construct a test datasetof goals (we label them as Oracles) with trajectories that are sampledwith multiple checkpoints of non-goal policy, to ensure the diversity.Then, the tuples of (s,∆t,∆V)that corresponding to the oracles areused to generate goal predictions (we label them as Prediction) withGop.。To visualize the goal distribution, we leverage TSNE (Van derMaaten & Hinton, 2008) to shrink the goal dimension from 68 to 1.The results in Figure 10 show that the distribution of Prediction wellmatches the distribution of Oracles.J.。Ablation Study on LLMsThe Impact of Lora Rank.。We evaluate the impact of the rank parameter on performance during LoRA fine-tuning oflarge language model neural networks.。Generally speaking, the larger the rank parameter, the more comprehensive andthorough the fine-tuning of the neural network, but the corresponding training time and model footprint will be larger.Rank PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)8 0.544 0.672 0.482 0.608 0.502 0.629 0.060 0.12416 0.550 0.673 0.487 0.601 0.507 0.626 0.070 0.12432 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.15964 0.547 0.675 0.501 0.616 0.519 0.635 0.070 0.124128 0.552 0.684 0.507 0.626 0.524 0.645 0.075 0.134Table 12: Evaluation on lora rank.The experimental results are shown in Table 12.. \n",
      "the piece 839\n",
      " \n",
      "drychowicz et al., 2017), we adopt a similar method to utilizethe collected trajectories for learning a goal generator Gopwhich accepts a state as input.。We conclude its train-ing in two steps: (1) constructing (s, g)pairs with collected trajectories as illustrated in Figure 11(b); (2) supervisedtraining Gopwith the above pairs and an MSE loss between the labeled goals and predicted goals.。For step (1),we split trajectories into many segments with length of 200 timesteps.。Then, we randomly sample a state sfromthe first 150 steps and sample a state s′from the last 20 steps to derive a goal g=PROJ(s′), with a distributionproportional to their basic value Vbasic(s′).。For step (2), we train Gopwith s,∆t,Vbasic(s)andVbasic(s′)as in-put to generate goals, where ∆tthe time slot of goal completion, Vbasic(s′)andVbasic(s)the basic state value.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationCoFT... ... ... ...StatesCoTQuestionsCoTResponses  HI      RI    SI     AIChatGLM LoRA-FT+tuning... ... ... ...SFTStatesGoalsInstructions  HI      RI    SI     AIChatGLM LoRA-FT+tuningEFTStateResponse 1ChatGLMLoRA-FTLoRA-CK1LoRA-CK2+......Response 2 Response NEnsemble R esponse generatingSFTvotingInstructionChatGLM LoRA-FT +LoRA-CT RL ModelENVRewardsStateGoal Completion RewardExamination RewardFormat RewardInstruction ResponseExam SetInteractionRewardComplete statusPPO TuningFormat recognitionselectingCo-TrainingStateFigure 8: Overview of the training framework with LLM.。This training framework has three kinds of LLM tuning approaches:CoFT (Chain of Thoughts assisted Fine-Tuning), SFT (Supervised Fine-Tuning), EFT (Ensemble Fine-Tuning); and oneLLM-RL co-training approach.PPO clip eps 0.2Optimizer AdamLearning rate 0.0001Batch size 20480Number of CPUs 5120 (AMD EPYC 7H12 64-Core)Number of GPUs 2 (A100)γ(basic ) 0.995γ(oa) 0.92γ(goal) 0.993λ 0.95Entropy coefficient 0.025Unroll length 20Sample max use times 3Gradient clip threshold 10Table 11: Parameter settings for RL.Validation.。To validate the efficiency of Gop, we conduct a com-parison between generated goals and oracles w.r.t distribution, asillustrated in Figure 10.。Empirically, we first construct a test datasetof goals (we label them as Oracles) with trajectories that are sampledwith multiple checkpoints of non-goal policy, to ensure the diversity.Then, the tuples of (s,∆t,∆V)that corresponding to the oracles areused to generate goal predictions (we label them as Prediction) withGop.。To visualize the goal distribution, we leverage TSNE (Van derMaaten & Hinton, 2008) to shrink the goal dimension from 68 to 1.The results in Figure 10 show that the distribution of Prediction wellmatches the distribution of Oracles.J.。Ablation Study on LLMsThe Impact of Lora Rank.。We evaluate the impact of the rank parameter on performance during LoRA fine-tuning oflarge language model neural networks.。Generally speaking, the larger the rank parameter, the more comprehensive andthorough the fine-tuning of the neural network, but the corresponding training time and model footprint will be larger.Rank PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)8 0.544 0.672 0.482 0.608 0.502 0.629 0.060 0.12416 0.550 0.673 0.487 0.601 0.507 0.626 0.070 0.12432 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.15964 0.547 0.675 0.501 0.616 0.519 0.635 0.070 0.124128 0.552 0.684 0.507 0.626 0.524 0.645 0.075 0.134Table 12: Evaluation on lora rank.The experimental results are shown in Table 12.。Thesize of lora rank has little impact on model performanceindicators, but a large rank will cause the model trainingtime and the size of the saved parameter file to increasedramatically.The Impact of Lora Target.. \n",
      "the piece 840\n",
      " \n",
      "th collected trajectories as illustrated in Figure 11(b); (2) supervisedtraining Gopwith the above pairs and an MSE loss between the labeled goals and predicted goals.。For step (1),we split trajectories into many segments with length of 200 timesteps.。Then, we randomly sample a state sfromthe first 150 steps and sample a state s′from the last 20 steps to derive a goal g=PROJ(s′), with a distributionproportional to their basic value Vbasic(s′).。For step (2), we train Gopwith s,∆t,Vbasic(s)andVbasic(s′)as in-put to generate goals, where ∆tthe time slot of goal completion, Vbasic(s′)andVbasic(s)the basic state value.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationCoFT... ... ... ...StatesCoTQuestionsCoTResponses  HI      RI    SI     AIChatGLM LoRA-FT+tuning... ... ... ...SFTStatesGoalsInstructions  HI      RI    SI     AIChatGLM LoRA-FT+tuningEFTStateResponse 1ChatGLMLoRA-FTLoRA-CK1LoRA-CK2+......Response 2 Response NEnsemble R esponse generatingSFTvotingInstructionChatGLM LoRA-FT +LoRA-CT RL ModelENVRewardsStateGoal Completion RewardExamination RewardFormat RewardInstruction ResponseExam SetInteractionRewardComplete statusPPO TuningFormat recognitionselectingCo-TrainingStateFigure 8: Overview of the training framework with LLM.。This training framework has three kinds of LLM tuning approaches:CoFT (Chain of Thoughts assisted Fine-Tuning), SFT (Supervised Fine-Tuning), EFT (Ensemble Fine-Tuning); and oneLLM-RL co-training approach.PPO clip eps 0.2Optimizer AdamLearning rate 0.0001Batch size 20480Number of CPUs 5120 (AMD EPYC 7H12 64-Core)Number of GPUs 2 (A100)γ(basic ) 0.995γ(oa) 0.92γ(goal) 0.993λ 0.95Entropy coefficient 0.025Unroll length 20Sample max use times 3Gradient clip threshold 10Table 11: Parameter settings for RL.Validation.。To validate the efficiency of Gop, we conduct a com-parison between generated goals and oracles w.r.t distribution, asillustrated in Figure 10.。Empirically, we first construct a test datasetof goals (we label them as Oracles) with trajectories that are sampledwith multiple checkpoints of non-goal policy, to ensure the diversity.Then, the tuples of (s,∆t,∆V)that corresponding to the oracles areused to generate goal predictions (we label them as Prediction) withGop.。To visualize the goal distribution, we leverage TSNE (Van derMaaten & Hinton, 2008) to shrink the goal dimension from 68 to 1.The results in Figure 10 show that the distribution of Prediction wellmatches the distribution of Oracles.J.。Ablation Study on LLMsThe Impact of Lora Rank.。We evaluate the impact of the rank parameter on performance during LoRA fine-tuning oflarge language model neural networks.。Generally speaking, the larger the rank parameter, the more comprehensive andthorough the fine-tuning of the neural network, but the corresponding training time and model footprint will be larger.Rank PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)8 0.544 0.672 0.482 0.608 0.502 0.629 0.060 0.12416 0.550 0.673 0.487 0.601 0.507 0.626 0.070 0.12432 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.15964 0.547 0.675 0.501 0.616 0.519 0.635 0.070 0.124128 0.552 0.684 0.507 0.626 0.524 0.645 0.075 0.134Table 12: Evaluation on lora rank.The experimental results are shown in Table 12.。Thesize of lora rank has little impact on model performanceindicators, but a large rank will cause the model trainingtime and the size of the saved parameter file to increasedramatically.The Impact of Lora Target.。We next verified whichneural networks in fine-tuning the ChatGLM-6B largelanguage model can achieve the best performance.. \n",
      "the piece 841\n",
      " \n",
      "with the above pairs and an MSE loss between the labeled goals and predicted goals.。For step (1),we split trajectories into many segments with length of 200 timesteps.。Then, we randomly sample a state sfromthe first 150 steps and sample a state s′from the last 20 steps to derive a goal g=PROJ(s′), with a distributionproportional to their basic value Vbasic(s′).。For step (2), we train Gopwith s,∆t,Vbasic(s)andVbasic(s′)as in-put to generate goals, where ∆tthe time slot of goal completion, Vbasic(s′)andVbasic(s)the basic state value.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationCoFT... ... ... ...StatesCoTQuestionsCoTResponses  HI      RI    SI     AIChatGLM LoRA-FT+tuning... ... ... ...SFTStatesGoalsInstructions  HI      RI    SI     AIChatGLM LoRA-FT+tuningEFTStateResponse 1ChatGLMLoRA-FTLoRA-CK1LoRA-CK2+......Response 2 Response NEnsemble R esponse generatingSFTvotingInstructionChatGLM LoRA-FT +LoRA-CT RL ModelENVRewardsStateGoal Completion RewardExamination RewardFormat RewardInstruction ResponseExam SetInteractionRewardComplete statusPPO TuningFormat recognitionselectingCo-TrainingStateFigure 8: Overview of the training framework with LLM.。This training framework has three kinds of LLM tuning approaches:CoFT (Chain of Thoughts assisted Fine-Tuning), SFT (Supervised Fine-Tuning), EFT (Ensemble Fine-Tuning); and oneLLM-RL co-training approach.PPO clip eps 0.2Optimizer AdamLearning rate 0.0001Batch size 20480Number of CPUs 5120 (AMD EPYC 7H12 64-Core)Number of GPUs 2 (A100)γ(basic ) 0.995γ(oa) 0.92γ(goal) 0.993λ 0.95Entropy coefficient 0.025Unroll length 20Sample max use times 3Gradient clip threshold 10Table 11: Parameter settings for RL.Validation.。To validate the efficiency of Gop, we conduct a com-parison between generated goals and oracles w.r.t distribution, asillustrated in Figure 10.。Empirically, we first construct a test datasetof goals (we label them as Oracles) with trajectories that are sampledwith multiple checkpoints of non-goal policy, to ensure the diversity.Then, the tuples of (s,∆t,∆V)that corresponding to the oracles areused to generate goal predictions (we label them as Prediction) withGop.。To visualize the goal distribution, we leverage TSNE (Van derMaaten & Hinton, 2008) to shrink the goal dimension from 68 to 1.The results in Figure 10 show that the distribution of Prediction wellmatches the distribution of Oracles.J.。Ablation Study on LLMsThe Impact of Lora Rank.。We evaluate the impact of the rank parameter on performance during LoRA fine-tuning oflarge language model neural networks.。Generally speaking, the larger the rank parameter, the more comprehensive andthorough the fine-tuning of the neural network, but the corresponding training time and model footprint will be larger.Rank PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)8 0.544 0.672 0.482 0.608 0.502 0.629 0.060 0.12416 0.550 0.673 0.487 0.601 0.507 0.626 0.070 0.12432 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.15964 0.547 0.675 0.501 0.616 0.519 0.635 0.070 0.124128 0.552 0.684 0.507 0.626 0.524 0.645 0.075 0.134Table 12: Evaluation on lora rank.The experimental results are shown in Table 12.。Thesize of lora rank has little impact on model performanceindicators, but a large rank will cause the model trainingtime and the size of the saved parameter file to increasedramatically.The Impact of Lora Target.。We next verified whichneural networks in fine-tuning the ChatGLM-6B largelanguage model can achieve the best performance.。TheBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(a) Altitude map (0.8m) (b) Altitude map (4m) (c) Aerial view map (0.4m)Figure 9: Illustration of BEV features in observation space.. \n",
      "the piece 842\n",
      " \n",
      "sample a state s′from the last 20 steps to derive a goal g=PROJ(s′), with a distributionproportional to their basic value Vbasic(s′).。For step (2), we train Gopwith s,∆t,Vbasic(s)andVbasic(s′)as in-put to generate goals, where ∆tthe time slot of goal completion, Vbasic(s′)andVbasic(s)the basic state value.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationCoFT... ... ... ...StatesCoTQuestionsCoTResponses  HI      RI    SI     AIChatGLM LoRA-FT+tuning... ... ... ...SFTStatesGoalsInstructions  HI      RI    SI     AIChatGLM LoRA-FT+tuningEFTStateResponse 1ChatGLMLoRA-FTLoRA-CK1LoRA-CK2+......Response 2 Response NEnsemble R esponse generatingSFTvotingInstructionChatGLM LoRA-FT +LoRA-CT RL ModelENVRewardsStateGoal Completion RewardExamination RewardFormat RewardInstruction ResponseExam SetInteractionRewardComplete statusPPO TuningFormat recognitionselectingCo-TrainingStateFigure 8: Overview of the training framework with LLM.。This training framework has three kinds of LLM tuning approaches:CoFT (Chain of Thoughts assisted Fine-Tuning), SFT (Supervised Fine-Tuning), EFT (Ensemble Fine-Tuning); and oneLLM-RL co-training approach.PPO clip eps 0.2Optimizer AdamLearning rate 0.0001Batch size 20480Number of CPUs 5120 (AMD EPYC 7H12 64-Core)Number of GPUs 2 (A100)γ(basic ) 0.995γ(oa) 0.92γ(goal) 0.993λ 0.95Entropy coefficient 0.025Unroll length 20Sample max use times 3Gradient clip threshold 10Table 11: Parameter settings for RL.Validation.。To validate the efficiency of Gop, we conduct a com-parison between generated goals and oracles w.r.t distribution, asillustrated in Figure 10.。Empirically, we first construct a test datasetof goals (we label them as Oracles) with trajectories that are sampledwith multiple checkpoints of non-goal policy, to ensure the diversity.Then, the tuples of (s,∆t,∆V)that corresponding to the oracles areused to generate goal predictions (we label them as Prediction) withGop.。To visualize the goal distribution, we leverage TSNE (Van derMaaten & Hinton, 2008) to shrink the goal dimension from 68 to 1.The results in Figure 10 show that the distribution of Prediction wellmatches the distribution of Oracles.J.。Ablation Study on LLMsThe Impact of Lora Rank.。We evaluate the impact of the rank parameter on performance during LoRA fine-tuning oflarge language model neural networks.。Generally speaking, the larger the rank parameter, the more comprehensive andthorough the fine-tuning of the neural network, but the corresponding training time and model footprint will be larger.Rank PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)8 0.544 0.672 0.482 0.608 0.502 0.629 0.060 0.12416 0.550 0.673 0.487 0.601 0.507 0.626 0.070 0.12432 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.15964 0.547 0.675 0.501 0.616 0.519 0.635 0.070 0.124128 0.552 0.684 0.507 0.626 0.524 0.645 0.075 0.134Table 12: Evaluation on lora rank.The experimental results are shown in Table 12.。Thesize of lora rank has little impact on model performanceindicators, but a large rank will cause the model trainingtime and the size of the saved parameter file to increasedramatically.The Impact of Lora Target.。We next verified whichneural networks in fine-tuning the ChatGLM-6B largelanguage model can achieve the best performance.。TheBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(a) Altitude map (0.8m) (b) Altitude map (4m) (c) Aerial view map (0.4m)Figure 9: Illustration of BEV features in observation space.。(a) and (b) are the altitude maps where bright areas are higherthan dark areas.. \n",
      "the piece 843\n",
      " \n",
      "ot of goal completion, Vbasic(s′)andVbasic(s)the basic state value.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationCoFT... ... ... ...StatesCoTQuestionsCoTResponses  HI      RI    SI     AIChatGLM LoRA-FT+tuning... ... ... ...SFTStatesGoalsInstructions  HI      RI    SI     AIChatGLM LoRA-FT+tuningEFTStateResponse 1ChatGLMLoRA-FTLoRA-CK1LoRA-CK2+......Response 2 Response NEnsemble R esponse generatingSFTvotingInstructionChatGLM LoRA-FT +LoRA-CT RL ModelENVRewardsStateGoal Completion RewardExamination RewardFormat RewardInstruction ResponseExam SetInteractionRewardComplete statusPPO TuningFormat recognitionselectingCo-TrainingStateFigure 8: Overview of the training framework with LLM.。This training framework has three kinds of LLM tuning approaches:CoFT (Chain of Thoughts assisted Fine-Tuning), SFT (Supervised Fine-Tuning), EFT (Ensemble Fine-Tuning); and oneLLM-RL co-training approach.PPO clip eps 0.2Optimizer AdamLearning rate 0.0001Batch size 20480Number of CPUs 5120 (AMD EPYC 7H12 64-Core)Number of GPUs 2 (A100)γ(basic ) 0.995γ(oa) 0.92γ(goal) 0.993λ 0.95Entropy coefficient 0.025Unroll length 20Sample max use times 3Gradient clip threshold 10Table 11: Parameter settings for RL.Validation.。To validate the efficiency of Gop, we conduct a com-parison between generated goals and oracles w.r.t distribution, asillustrated in Figure 10.。Empirically, we first construct a test datasetof goals (we label them as Oracles) with trajectories that are sampledwith multiple checkpoints of non-goal policy, to ensure the diversity.Then, the tuples of (s,∆t,∆V)that corresponding to the oracles areused to generate goal predictions (we label them as Prediction) withGop.。To visualize the goal distribution, we leverage TSNE (Van derMaaten & Hinton, 2008) to shrink the goal dimension from 68 to 1.The results in Figure 10 show that the distribution of Prediction wellmatches the distribution of Oracles.J.。Ablation Study on LLMsThe Impact of Lora Rank.。We evaluate the impact of the rank parameter on performance during LoRA fine-tuning oflarge language model neural networks.。Generally speaking, the larger the rank parameter, the more comprehensive andthorough the fine-tuning of the neural network, but the corresponding training time and model footprint will be larger.Rank PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)8 0.544 0.672 0.482 0.608 0.502 0.629 0.060 0.12416 0.550 0.673 0.487 0.601 0.507 0.626 0.070 0.12432 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.15964 0.547 0.675 0.501 0.616 0.519 0.635 0.070 0.124128 0.552 0.684 0.507 0.626 0.524 0.645 0.075 0.134Table 12: Evaluation on lora rank.The experimental results are shown in Table 12.。Thesize of lora rank has little impact on model performanceindicators, but a large rank will cause the model trainingtime and the size of the saved parameter file to increasedramatically.The Impact of Lora Target.。We next verified whichneural networks in fine-tuning the ChatGLM-6B largelanguage model can achieve the best performance.。TheBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(a) Altitude map (0.8m) (b) Altitude map (4m) (c) Aerial view map (0.4m)Figure 9: Illustration of BEV features in observation space.。(a) and (b) are the altitude maps where bright areas are higherthan dark areas.。(c) is the aerial view map where the disconnected areas are windows or doors.. \n",
      "the piece 844\n",
      " \n",
      "d Embodied Agent via Language-Policy Bidirectional AdaptationCoFT... ... ... ...StatesCoTQuestionsCoTResponses  HI      RI    SI     AIChatGLM LoRA-FT+tuning... ... ... ...SFTStatesGoalsInstructions  HI      RI    SI     AIChatGLM LoRA-FT+tuningEFTStateResponse 1ChatGLMLoRA-FTLoRA-CK1LoRA-CK2+......Response 2 Response NEnsemble R esponse generatingSFTvotingInstructionChatGLM LoRA-FT +LoRA-CT RL ModelENVRewardsStateGoal Completion RewardExamination RewardFormat RewardInstruction ResponseExam SetInteractionRewardComplete statusPPO TuningFormat recognitionselectingCo-TrainingStateFigure 8: Overview of the training framework with LLM.。This training framework has three kinds of LLM tuning approaches:CoFT (Chain of Thoughts assisted Fine-Tuning), SFT (Supervised Fine-Tuning), EFT (Ensemble Fine-Tuning); and oneLLM-RL co-training approach.PPO clip eps 0.2Optimizer AdamLearning rate 0.0001Batch size 20480Number of CPUs 5120 (AMD EPYC 7H12 64-Core)Number of GPUs 2 (A100)γ(basic ) 0.995γ(oa) 0.92γ(goal) 0.993λ 0.95Entropy coefficient 0.025Unroll length 20Sample max use times 3Gradient clip threshold 10Table 11: Parameter settings for RL.Validation.。To validate the efficiency of Gop, we conduct a com-parison between generated goals and oracles w.r.t distribution, asillustrated in Figure 10.。Empirically, we first construct a test datasetof goals (we label them as Oracles) with trajectories that are sampledwith multiple checkpoints of non-goal policy, to ensure the diversity.Then, the tuples of (s,∆t,∆V)that corresponding to the oracles areused to generate goal predictions (we label them as Prediction) withGop.。To visualize the goal distribution, we leverage TSNE (Van derMaaten & Hinton, 2008) to shrink the goal dimension from 68 to 1.The results in Figure 10 show that the distribution of Prediction wellmatches the distribution of Oracles.J.。Ablation Study on LLMsThe Impact of Lora Rank.。We evaluate the impact of the rank parameter on performance during LoRA fine-tuning oflarge language model neural networks.。Generally speaking, the larger the rank parameter, the more comprehensive andthorough the fine-tuning of the neural network, but the corresponding training time and model footprint will be larger.Rank PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)8 0.544 0.672 0.482 0.608 0.502 0.629 0.060 0.12416 0.550 0.673 0.487 0.601 0.507 0.626 0.070 0.12432 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.15964 0.547 0.675 0.501 0.616 0.519 0.635 0.070 0.124128 0.552 0.684 0.507 0.626 0.524 0.645 0.075 0.134Table 12: Evaluation on lora rank.The experimental results are shown in Table 12.。Thesize of lora rank has little impact on model performanceindicators, but a large rank will cause the model trainingtime and the size of the saved parameter file to increasedramatically.The Impact of Lora Target.。We next verified whichneural networks in fine-tuning the ChatGLM-6B largelanguage model can achieve the best performance.。TheBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(a) Altitude map (0.8m) (b) Altitude map (4m) (c) Aerial view map (0.4m)Figure 9: Illustration of BEV features in observation space.。(a) and (b) are the altitude maps where bright areas are higherthan dark areas.。(c) is the aerial view map where the disconnected areas are windows or doors.。One pixel in (a), (b) and (c)denotes 0.8 meter, 4 meters and 0.4 meter respectively.. \n",
      "the piece 845\n",
      " \n",
      "s  HI      RI    SI     AIChatGLM LoRA-FT+tuningEFTStateResponse 1ChatGLMLoRA-FTLoRA-CK1LoRA-CK2+......Response 2 Response NEnsemble R esponse generatingSFTvotingInstructionChatGLM LoRA-FT +LoRA-CT RL ModelENVRewardsStateGoal Completion RewardExamination RewardFormat RewardInstruction ResponseExam SetInteractionRewardComplete statusPPO TuningFormat recognitionselectingCo-TrainingStateFigure 8: Overview of the training framework with LLM.。This training framework has three kinds of LLM tuning approaches:CoFT (Chain of Thoughts assisted Fine-Tuning), SFT (Supervised Fine-Tuning), EFT (Ensemble Fine-Tuning); and oneLLM-RL co-training approach.PPO clip eps 0.2Optimizer AdamLearning rate 0.0001Batch size 20480Number of CPUs 5120 (AMD EPYC 7H12 64-Core)Number of GPUs 2 (A100)γ(basic ) 0.995γ(oa) 0.92γ(goal) 0.993λ 0.95Entropy coefficient 0.025Unroll length 20Sample max use times 3Gradient clip threshold 10Table 11: Parameter settings for RL.Validation.。To validate the efficiency of Gop, we conduct a com-parison between generated goals and oracles w.r.t distribution, asillustrated in Figure 10.。Empirically, we first construct a test datasetof goals (we label them as Oracles) with trajectories that are sampledwith multiple checkpoints of non-goal policy, to ensure the diversity.Then, the tuples of (s,∆t,∆V)that corresponding to the oracles areused to generate goal predictions (we label them as Prediction) withGop.。To visualize the goal distribution, we leverage TSNE (Van derMaaten & Hinton, 2008) to shrink the goal dimension from 68 to 1.The results in Figure 10 show that the distribution of Prediction wellmatches the distribution of Oracles.J.。Ablation Study on LLMsThe Impact of Lora Rank.。We evaluate the impact of the rank parameter on performance during LoRA fine-tuning oflarge language model neural networks.。Generally speaking, the larger the rank parameter, the more comprehensive andthorough the fine-tuning of the neural network, but the corresponding training time and model footprint will be larger.Rank PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)8 0.544 0.672 0.482 0.608 0.502 0.629 0.060 0.12416 0.550 0.673 0.487 0.601 0.507 0.626 0.070 0.12432 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.15964 0.547 0.675 0.501 0.616 0.519 0.635 0.070 0.124128 0.552 0.684 0.507 0.626 0.524 0.645 0.075 0.134Table 12: Evaluation on lora rank.The experimental results are shown in Table 12.。Thesize of lora rank has little impact on model performanceindicators, but a large rank will cause the model trainingtime and the size of the saved parameter file to increasedramatically.The Impact of Lora Target.。We next verified whichneural networks in fine-tuning the ChatGLM-6B largelanguage model can achieve the best performance.。TheBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(a) Altitude map (0.8m) (b) Altitude map (4m) (c) Aerial view map (0.4m)Figure 9: Illustration of BEV features in observation space.。(a) and (b) are the altitude maps where bright areas are higherthan dark areas.。(c) is the aerial view map where the disconnected areas are windows or doors.。One pixel in (a), (b) and (c)denotes 0.8 meter, 4 meters and 0.4 meter respectively.。The small yellow blocks represent player positions and small blueblocks represent enemy positions.V15T5075100125150175200225250Goal ProjectionOraclesPrediction(a) Corresponding to (∆T,∆V)50 75 100 125 150 175 200 225 250T050100Goal ProjectionOraclesPrediction (b) Corresponding to ∆T0 5 10 15V050100Goal ProjectionOraclesPrediction (c) Corresponding to ∆VFigure 10: Distribution comparison between real goals (Oracles) and goals generated by Gop(Prediction).. \n",
      "the piece 846\n",
      " \n",
      "coefficient 0.025Unroll length 20Sample max use times 3Gradient clip threshold 10Table 11: Parameter settings for RL.Validation.。To validate the efficiency of Gop, we conduct a com-parison between generated goals and oracles w.r.t distribution, asillustrated in Figure 10.。Empirically, we first construct a test datasetof goals (we label them as Oracles) with trajectories that are sampledwith multiple checkpoints of non-goal policy, to ensure the diversity.Then, the tuples of (s,∆t,∆V)that corresponding to the oracles areused to generate goal predictions (we label them as Prediction) withGop.。To visualize the goal distribution, we leverage TSNE (Van derMaaten & Hinton, 2008) to shrink the goal dimension from 68 to 1.The results in Figure 10 show that the distribution of Prediction wellmatches the distribution of Oracles.J.。Ablation Study on LLMsThe Impact of Lora Rank.。We evaluate the impact of the rank parameter on performance during LoRA fine-tuning oflarge language model neural networks.。Generally speaking, the larger the rank parameter, the more comprehensive andthorough the fine-tuning of the neural network, but the corresponding training time and model footprint will be larger.Rank PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)8 0.544 0.672 0.482 0.608 0.502 0.629 0.060 0.12416 0.550 0.673 0.487 0.601 0.507 0.626 0.070 0.12432 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.15964 0.547 0.675 0.501 0.616 0.519 0.635 0.070 0.124128 0.552 0.684 0.507 0.626 0.524 0.645 0.075 0.134Table 12: Evaluation on lora rank.The experimental results are shown in Table 12.。Thesize of lora rank has little impact on model performanceindicators, but a large rank will cause the model trainingtime and the size of the saved parameter file to increasedramatically.The Impact of Lora Target.。We next verified whichneural networks in fine-tuning the ChatGLM-6B largelanguage model can achieve the best performance.。TheBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(a) Altitude map (0.8m) (b) Altitude map (4m) (c) Aerial view map (0.4m)Figure 9: Illustration of BEV features in observation space.。(a) and (b) are the altitude maps where bright areas are higherthan dark areas.。(c) is the aerial view map where the disconnected areas are windows or doors.。One pixel in (a), (b) and (c)denotes 0.8 meter, 4 meters and 0.4 meter respectively.。The small yellow blocks represent player positions and small blueblocks represent enemy positions.V15T5075100125150175200225250Goal ProjectionOraclesPrediction(a) Corresponding to (∆T,∆V)50 75 100 125 150 175 200 225 250T050100Goal ProjectionOraclesPrediction (b) Corresponding to ∆T0 5 10 15V050100Goal ProjectionOraclesPrediction (c) Corresponding to ∆VFigure 10: Distribution comparison between real goals (Oracles) and goals generated by Gop(Prediction).。The illustrationshows that Gopgenerates goals that follows the real distribution, indicating good generalization on open-ended goalgeneration.experimental results are shown in Table 13.. \n",
      "the piece 847\n",
      " \n",
      "es areused to generate goal predictions (we label them as Prediction) withGop.。To visualize the goal distribution, we leverage TSNE (Van derMaaten & Hinton, 2008) to shrink the goal dimension from 68 to 1.The results in Figure 10 show that the distribution of Prediction wellmatches the distribution of Oracles.J.。Ablation Study on LLMsThe Impact of Lora Rank.。We evaluate the impact of the rank parameter on performance during LoRA fine-tuning oflarge language model neural networks.。Generally speaking, the larger the rank parameter, the more comprehensive andthorough the fine-tuning of the neural network, but the corresponding training time and model footprint will be larger.Rank PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)8 0.544 0.672 0.482 0.608 0.502 0.629 0.060 0.12416 0.550 0.673 0.487 0.601 0.507 0.626 0.070 0.12432 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.15964 0.547 0.675 0.501 0.616 0.519 0.635 0.070 0.124128 0.552 0.684 0.507 0.626 0.524 0.645 0.075 0.134Table 12: Evaluation on lora rank.The experimental results are shown in Table 12.。Thesize of lora rank has little impact on model performanceindicators, but a large rank will cause the model trainingtime and the size of the saved parameter file to increasedramatically.The Impact of Lora Target.。We next verified whichneural networks in fine-tuning the ChatGLM-6B largelanguage model can achieve the best performance.。TheBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(a) Altitude map (0.8m) (b) Altitude map (4m) (c) Aerial view map (0.4m)Figure 9: Illustration of BEV features in observation space.。(a) and (b) are the altitude maps where bright areas are higherthan dark areas.。(c) is the aerial view map where the disconnected areas are windows or doors.。One pixel in (a), (b) and (c)denotes 0.8 meter, 4 meters and 0.4 meter respectively.。The small yellow blocks represent player positions and small blueblocks represent enemy positions.V15T5075100125150175200225250Goal ProjectionOraclesPrediction(a) Corresponding to (∆T,∆V)50 75 100 125 150 175 200 225 250T050100Goal ProjectionOraclesPrediction (b) Corresponding to ∆T0 5 10 15V050100Goal ProjectionOraclesPrediction (c) Corresponding to ∆VFigure 10: Distribution comparison between real goals (Oracles) and goals generated by Gop(Prediction).。The illustrationshows that Gopgenerates goals that follows the real distribution, indicating good generalization on open-ended goalgeneration.experimental results are shown in Table 13.。It is worth noting that only fine-tuning the MLP network without fine-tuning theattention network can achieve the best training results.. \n",
      "the piece 848\n",
      " \n",
      "n & Hinton, 2008) to shrink the goal dimension from 68 to 1.The results in Figure 10 show that the distribution of Prediction wellmatches the distribution of Oracles.J.。Ablation Study on LLMsThe Impact of Lora Rank.。We evaluate the impact of the rank parameter on performance during LoRA fine-tuning oflarge language model neural networks.。Generally speaking, the larger the rank parameter, the more comprehensive andthorough the fine-tuning of the neural network, but the corresponding training time and model footprint will be larger.Rank PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)8 0.544 0.672 0.482 0.608 0.502 0.629 0.060 0.12416 0.550 0.673 0.487 0.601 0.507 0.626 0.070 0.12432 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.15964 0.547 0.675 0.501 0.616 0.519 0.635 0.070 0.124128 0.552 0.684 0.507 0.626 0.524 0.645 0.075 0.134Table 12: Evaluation on lora rank.The experimental results are shown in Table 12.。Thesize of lora rank has little impact on model performanceindicators, but a large rank will cause the model trainingtime and the size of the saved parameter file to increasedramatically.The Impact of Lora Target.。We next verified whichneural networks in fine-tuning the ChatGLM-6B largelanguage model can achieve the best performance.。TheBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(a) Altitude map (0.8m) (b) Altitude map (4m) (c) Aerial view map (0.4m)Figure 9: Illustration of BEV features in observation space.。(a) and (b) are the altitude maps where bright areas are higherthan dark areas.。(c) is the aerial view map where the disconnected areas are windows or doors.。One pixel in (a), (b) and (c)denotes 0.8 meter, 4 meters and 0.4 meter respectively.。The small yellow blocks represent player positions and small blueblocks represent enemy positions.V15T5075100125150175200225250Goal ProjectionOraclesPrediction(a) Corresponding to (∆T,∆V)50 75 100 125 150 175 200 225 250T050100Goal ProjectionOraclesPrediction (b) Corresponding to ∆T0 5 10 15V050100Goal ProjectionOraclesPrediction (c) Corresponding to ∆VFigure 10: Distribution comparison between real goals (Oracles) and goals generated by Gop(Prediction).。The illustrationshows that Gopgenerates goals that follows the real distribution, indicating good generalization on open-ended goalgeneration.experimental results are shown in Table 13.。It is worth noting that only fine-tuning the MLP network without fine-tuning theattention network can achieve the best training results.。Although generally speaking, the mainstream fine-tuning task oflarge language models is to fine-tune the attention layer network, but that task usually focuses more on answer semantics.. \n",
      "the piece 849\n",
      " \n",
      "ral networks.。Generally speaking, the larger the rank parameter, the more comprehensive andthorough the fine-tuning of the neural network, but the corresponding training time and model footprint will be larger.Rank PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)8 0.544 0.672 0.482 0.608 0.502 0.629 0.060 0.12416 0.550 0.673 0.487 0.601 0.507 0.626 0.070 0.12432 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.15964 0.547 0.675 0.501 0.616 0.519 0.635 0.070 0.124128 0.552 0.684 0.507 0.626 0.524 0.645 0.075 0.134Table 12: Evaluation on lora rank.The experimental results are shown in Table 12.。Thesize of lora rank has little impact on model performanceindicators, but a large rank will cause the model trainingtime and the size of the saved parameter file to increasedramatically.The Impact of Lora Target.。We next verified whichneural networks in fine-tuning the ChatGLM-6B largelanguage model can achieve the best performance.。TheBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(a) Altitude map (0.8m) (b) Altitude map (4m) (c) Aerial view map (0.4m)Figure 9: Illustration of BEV features in observation space.。(a) and (b) are the altitude maps where bright areas are higherthan dark areas.。(c) is the aerial view map where the disconnected areas are windows or doors.。One pixel in (a), (b) and (c)denotes 0.8 meter, 4 meters and 0.4 meter respectively.。The small yellow blocks represent player positions and small blueblocks represent enemy positions.V15T5075100125150175200225250Goal ProjectionOraclesPrediction(a) Corresponding to (∆T,∆V)50 75 100 125 150 175 200 225 250T050100Goal ProjectionOraclesPrediction (b) Corresponding to ∆T0 5 10 15V050100Goal ProjectionOraclesPrediction (c) Corresponding to ∆VFigure 10: Distribution comparison between real goals (Oracles) and goals generated by Gop(Prediction).。The illustrationshows that Gopgenerates goals that follows the real distribution, indicating good generalization on open-ended goalgeneration.experimental results are shown in Table 13.。It is worth noting that only fine-tuning the MLP network without fine-tuning theattention network can achieve the best training results.。Although generally speaking, the mainstream fine-tuning task oflarge language models is to fine-tune the attention layer network, but that task usually focuses more on answer semantics.。Inour task, we pay more attention to the format to meet the metastate parsing requirements, so fine-tuning the MLP networkcan achieve better results.The Impact of Dataset Scale.. \n",
      "the piece 850\n",
      " \n",
      "oice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)8 0.544 0.672 0.482 0.608 0.502 0.629 0.060 0.12416 0.550 0.673 0.487 0.601 0.507 0.626 0.070 0.12432 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.15964 0.547 0.675 0.501 0.616 0.519 0.635 0.070 0.124128 0.552 0.684 0.507 0.626 0.524 0.645 0.075 0.134Table 12: Evaluation on lora rank.The experimental results are shown in Table 12.。Thesize of lora rank has little impact on model performanceindicators, but a large rank will cause the model trainingtime and the size of the saved parameter file to increasedramatically.The Impact of Lora Target.。We next verified whichneural networks in fine-tuning the ChatGLM-6B largelanguage model can achieve the best performance.。TheBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(a) Altitude map (0.8m) (b) Altitude map (4m) (c) Aerial view map (0.4m)Figure 9: Illustration of BEV features in observation space.。(a) and (b) are the altitude maps where bright areas are higherthan dark areas.。(c) is the aerial view map where the disconnected areas are windows or doors.。One pixel in (a), (b) and (c)denotes 0.8 meter, 4 meters and 0.4 meter respectively.。The small yellow blocks represent player positions and small blueblocks represent enemy positions.V15T5075100125150175200225250Goal ProjectionOraclesPrediction(a) Corresponding to (∆T,∆V)50 75 100 125 150 175 200 225 250T050100Goal ProjectionOraclesPrediction (b) Corresponding to ∆T0 5 10 15V050100Goal ProjectionOraclesPrediction (c) Corresponding to ∆VFigure 10: Distribution comparison between real goals (Oracles) and goals generated by Gop(Prediction).。The illustrationshows that Gopgenerates goals that follows the real distribution, indicating good generalization on open-ended goalgeneration.experimental results are shown in Table 13.。It is worth noting that only fine-tuning the MLP network without fine-tuning theattention network can achieve the best training results.。Although generally speaking, the mainstream fine-tuning task oflarge language models is to fine-tune the attention layer network, but that task usually focuses more on answer semantics.。Inour task, we pay more attention to the format to meet the metastate parsing requirements, so fine-tuning the MLP networkcan achieve better results.The Impact of Dataset Scale.。We conduct experiments of various models with four percentages of fine-tuning train set,i.e., 100%, 30%, 10%, 3%, on the goal generation task.. \n",
      "the piece 851\n",
      " \n",
      "urate(Choice)8 0.544 0.672 0.482 0.608 0.502 0.629 0.060 0.12416 0.550 0.673 0.487 0.601 0.507 0.626 0.070 0.12432 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.15964 0.547 0.675 0.501 0.616 0.519 0.635 0.070 0.124128 0.552 0.684 0.507 0.626 0.524 0.645 0.075 0.134Table 12: Evaluation on lora rank.The experimental results are shown in Table 12.。Thesize of lora rank has little impact on model performanceindicators, but a large rank will cause the model trainingtime and the size of the saved parameter file to increasedramatically.The Impact of Lora Target.。We next verified whichneural networks in fine-tuning the ChatGLM-6B largelanguage model can achieve the best performance.。TheBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(a) Altitude map (0.8m) (b) Altitude map (4m) (c) Aerial view map (0.4m)Figure 9: Illustration of BEV features in observation space.。(a) and (b) are the altitude maps where bright areas are higherthan dark areas.。(c) is the aerial view map where the disconnected areas are windows or doors.。One pixel in (a), (b) and (c)denotes 0.8 meter, 4 meters and 0.4 meter respectively.。The small yellow blocks represent player positions and small blueblocks represent enemy positions.V15T5075100125150175200225250Goal ProjectionOraclesPrediction(a) Corresponding to (∆T,∆V)50 75 100 125 150 175 200 225 250T050100Goal ProjectionOraclesPrediction (b) Corresponding to ∆T0 5 10 15V050100Goal ProjectionOraclesPrediction (c) Corresponding to ∆VFigure 10: Distribution comparison between real goals (Oracles) and goals generated by Gop(Prediction).。The illustrationshows that Gopgenerates goals that follows the real distribution, indicating good generalization on open-ended goalgeneration.experimental results are shown in Table 13.。It is worth noting that only fine-tuning the MLP network without fine-tuning theattention network can achieve the best training results.。Although generally speaking, the mainstream fine-tuning task oflarge language models is to fine-tune the attention layer network, but that task usually focuses more on answer semantics.。Inour task, we pay more attention to the format to meet the metastate parsing requirements, so fine-tuning the MLP networkcan achieve better results.The Impact of Dataset Scale.。We conduct experiments of various models with four percentages of fine-tuning train set,i.e., 100%, 30%, 10%, 3%, on the goal generation task.。The results are shown in Table 15.. \n",
      "the piece 852\n",
      " \n",
      "5 0.505 0.621 0.529 0.652 0.065 0.15964 0.547 0.675 0.501 0.616 0.519 0.635 0.070 0.124128 0.552 0.684 0.507 0.626 0.524 0.645 0.075 0.134Table 12: Evaluation on lora rank.The experimental results are shown in Table 12.。Thesize of lora rank has little impact on model performanceindicators, but a large rank will cause the model trainingtime and the size of the saved parameter file to increasedramatically.The Impact of Lora Target.。We next verified whichneural networks in fine-tuning the ChatGLM-6B largelanguage model can achieve the best performance.。TheBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation(a) Altitude map (0.8m) (b) Altitude map (4m) (c) Aerial view map (0.4m)Figure 9: Illustration of BEV features in observation space.。(a) and (b) are the altitude maps where bright areas are higherthan dark areas.。(c) is the aerial view map where the disconnected areas are windows or doors.。One pixel in (a), (b) and (c)denotes 0.8 meter, 4 meters and 0.4 meter respectively.。The small yellow blocks represent player positions and small blueblocks represent enemy positions.V15T5075100125150175200225250Goal ProjectionOraclesPrediction(a) Corresponding to (∆T,∆V)50 75 100 125 150 175 200 225 250T050100Goal ProjectionOraclesPrediction (b) Corresponding to ∆T0 5 10 15V050100Goal ProjectionOraclesPrediction (c) Corresponding to ∆VFigure 10: Distribution comparison between real goals (Oracles) and goals generated by Gop(Prediction).。The illustrationshows that Gopgenerates goals that follows the real distribution, indicating good generalization on open-ended goalgeneration.experimental results are shown in Table 13.。It is worth noting that only fine-tuning the MLP network without fine-tuning theattention network can achieve the best training results.。Although generally speaking, the mainstream fine-tuning task oflarge language models is to fine-tune the attention layer network, but that task usually focuses more on answer semantics.。Inour task, we pay more attention to the format to meet the metastate parsing requirements, so fine-tuning the MLP networkcan achieve better results.The Impact of Dataset Scale.。We conduct experiments of various models with four percentages of fine-tuning train set,i.e., 100%, 30%, 10%, 3%, on the goal generation task.。The results are shown in Table 15.。It can be seen that as the amountof data gradually decreases, the performance of various training indicators gradually deteriorates under various settings.However, the smaller the amount of data, the greater the improvement brought by pre-training of our proposed CoFT method.The results show that the CoFT method we proposed can effectively collect and expand the chain of thought data related tothe final goal, thereby avoiding overfitting of the training set in the case of small data.K.. \n",
      "the piece 853\n",
      " \n",
      "directional Adaptation(a) Altitude map (0.8m) (b) Altitude map (4m) (c) Aerial view map (0.4m)Figure 9: Illustration of BEV features in observation space.。(a) and (b) are the altitude maps where bright areas are higherthan dark areas.。(c) is the aerial view map where the disconnected areas are windows or doors.。One pixel in (a), (b) and (c)denotes 0.8 meter, 4 meters and 0.4 meter respectively.。The small yellow blocks represent player positions and small blueblocks represent enemy positions.V15T5075100125150175200225250Goal ProjectionOraclesPrediction(a) Corresponding to (∆T,∆V)50 75 100 125 150 175 200 225 250T050100Goal ProjectionOraclesPrediction (b) Corresponding to ∆T0 5 10 15V050100Goal ProjectionOraclesPrediction (c) Corresponding to ∆VFigure 10: Distribution comparison between real goals (Oracles) and goals generated by Gop(Prediction).。The illustrationshows that Gopgenerates goals that follows the real distribution, indicating good generalization on open-ended goalgeneration.experimental results are shown in Table 13.。It is worth noting that only fine-tuning the MLP network without fine-tuning theattention network can achieve the best training results.。Although generally speaking, the mainstream fine-tuning task oflarge language models is to fine-tune the attention layer network, but that task usually focuses more on answer semantics.。Inour task, we pay more attention to the format to meet the metastate parsing requirements, so fine-tuning the MLP networkcan achieve better results.The Impact of Dataset Scale.。We conduct experiments of various models with four percentages of fine-tuning train set,i.e., 100%, 30%, 10%, 3%, on the goal generation task.。The results are shown in Table 15.。It can be seen that as the amountof data gradually decreases, the performance of various training indicators gradually deteriorates under various settings.However, the smaller the amount of data, the greater the improvement brought by pre-training of our proposed CoFT method.The results show that the CoFT method we proposed can effectively collect and expand the chain of thought data related tothe final goal, thereby avoiding overfitting of the training set in the case of small data.K.。Reward Functions for Co-TrainingAgent Feedback Rewards.. \n",
      "the piece 854\n",
      " \n",
      "herthan dark areas.。(c) is the aerial view map where the disconnected areas are windows or doors.。One pixel in (a), (b) and (c)denotes 0.8 meter, 4 meters and 0.4 meter respectively.。The small yellow blocks represent player positions and small blueblocks represent enemy positions.V15T5075100125150175200225250Goal ProjectionOraclesPrediction(a) Corresponding to (∆T,∆V)50 75 100 125 150 175 200 225 250T050100Goal ProjectionOraclesPrediction (b) Corresponding to ∆T0 5 10 15V050100Goal ProjectionOraclesPrediction (c) Corresponding to ∆VFigure 10: Distribution comparison between real goals (Oracles) and goals generated by Gop(Prediction).。The illustrationshows that Gopgenerates goals that follows the real distribution, indicating good generalization on open-ended goalgeneration.experimental results are shown in Table 13.。It is worth noting that only fine-tuning the MLP network without fine-tuning theattention network can achieve the best training results.。Although generally speaking, the mainstream fine-tuning task oflarge language models is to fine-tune the attention layer network, but that task usually focuses more on answer semantics.。Inour task, we pay more attention to the format to meet the metastate parsing requirements, so fine-tuning the MLP networkcan achieve better results.The Impact of Dataset Scale.。We conduct experiments of various models with four percentages of fine-tuning train set,i.e., 100%, 30%, 10%, 3%, on the goal generation task.。The results are shown in Table 15.。It can be seen that as the amountof data gradually decreases, the performance of various training indicators gradually deteriorates under various settings.However, the smaller the amount of data, the greater the improvement brought by pre-training of our proposed CoFT method.The results show that the CoFT method we proposed can effectively collect and expand the chain of thought data related tothe final goal, thereby avoiding overfitting of the training set in the case of small data.K.。Reward Functions for Co-TrainingAgent Feedback Rewards.。The calculation of the agent feedback reward is multifaceted, aiming to reflect the degree ofcompletion as feedback for the training of the LLM.. \n",
      "the piece 855\n",
      " \n",
      " (c)denotes 0.8 meter, 4 meters and 0.4 meter respectively.。The small yellow blocks represent player positions and small blueblocks represent enemy positions.V15T5075100125150175200225250Goal ProjectionOraclesPrediction(a) Corresponding to (∆T,∆V)50 75 100 125 150 175 200 225 250T050100Goal ProjectionOraclesPrediction (b) Corresponding to ∆T0 5 10 15V050100Goal ProjectionOraclesPrediction (c) Corresponding to ∆VFigure 10: Distribution comparison between real goals (Oracles) and goals generated by Gop(Prediction).。The illustrationshows that Gopgenerates goals that follows the real distribution, indicating good generalization on open-ended goalgeneration.experimental results are shown in Table 13.。It is worth noting that only fine-tuning the MLP network without fine-tuning theattention network can achieve the best training results.。Although generally speaking, the mainstream fine-tuning task oflarge language models is to fine-tune the attention layer network, but that task usually focuses more on answer semantics.。Inour task, we pay more attention to the format to meet the metastate parsing requirements, so fine-tuning the MLP networkcan achieve better results.The Impact of Dataset Scale.。We conduct experiments of various models with four percentages of fine-tuning train set,i.e., 100%, 30%, 10%, 3%, on the goal generation task.。The results are shown in Table 15.。It can be seen that as the amountof data gradually decreases, the performance of various training indicators gradually deteriorates under various settings.However, the smaller the amount of data, the greater the improvement brought by pre-training of our proposed CoFT method.The results show that the CoFT method we proposed can effectively collect and expand the chain of thought data related tothe final goal, thereby avoiding overfitting of the training set in the case of small data.K.。Reward Functions for Co-TrainingAgent Feedback Rewards.。The calculation of the agent feedback reward is multifaceted, aiming to reflect the degree ofcompletion as feedback for the training of the LLM.。Specifically, three aspects are considered to satisfy the requirements,and the total agent feedback reward is given by the sum of them:•rfg- Minimal Distance to a Goal When Satisfying Environment Termination .. \n",
      "the piece 856\n",
      " \n",
      "ion(a) Corresponding to (∆T,∆V)50 75 100 125 150 175 200 225 250T050100Goal ProjectionOraclesPrediction (b) Corresponding to ∆T0 5 10 15V050100Goal ProjectionOraclesPrediction (c) Corresponding to ∆VFigure 10: Distribution comparison between real goals (Oracles) and goals generated by Gop(Prediction).。The illustrationshows that Gopgenerates goals that follows the real distribution, indicating good generalization on open-ended goalgeneration.experimental results are shown in Table 13.。It is worth noting that only fine-tuning the MLP network without fine-tuning theattention network can achieve the best training results.。Although generally speaking, the mainstream fine-tuning task oflarge language models is to fine-tune the attention layer network, but that task usually focuses more on answer semantics.。Inour task, we pay more attention to the format to meet the metastate parsing requirements, so fine-tuning the MLP networkcan achieve better results.The Impact of Dataset Scale.。We conduct experiments of various models with four percentages of fine-tuning train set,i.e., 100%, 30%, 10%, 3%, on the goal generation task.。The results are shown in Table 15.。It can be seen that as the amountof data gradually decreases, the performance of various training indicators gradually deteriorates under various settings.However, the smaller the amount of data, the greater the improvement brought by pre-training of our proposed CoFT method.The results show that the CoFT method we proposed can effectively collect and expand the chain of thought data related tothe final goal, thereby avoiding overfitting of the training set in the case of small data.K.。Reward Functions for Co-TrainingAgent Feedback Rewards.。The calculation of the agent feedback reward is multifaceted, aiming to reflect the degree ofcompletion as feedback for the training of the LLM.。Specifically, three aspects are considered to satisfy the requirements,and the total agent feedback reward is given by the sum of them:•rfg- Minimal Distance to a Goal When Satisfying Environment Termination .。As depicted by Equation (11), theBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub goal        g4   g8   g12   g16   g20 Time in loopsL1    L2    L3    L4    L5    L6    L70.0250.0500.0750.1000.1250.1500.1750.2000.225Possibility(a)(b)Figure 11: (a) Sub-goal distribution during co-training.. \n",
      "the piece 857\n",
      " \n",
      "ctionOraclesPrediction (b) Corresponding to ∆T0 5 10 15V050100Goal ProjectionOraclesPrediction (c) Corresponding to ∆VFigure 10: Distribution comparison between real goals (Oracles) and goals generated by Gop(Prediction).。The illustrationshows that Gopgenerates goals that follows the real distribution, indicating good generalization on open-ended goalgeneration.experimental results are shown in Table 13.。It is worth noting that only fine-tuning the MLP network without fine-tuning theattention network can achieve the best training results.。Although generally speaking, the mainstream fine-tuning task oflarge language models is to fine-tune the attention layer network, but that task usually focuses more on answer semantics.。Inour task, we pay more attention to the format to meet the metastate parsing requirements, so fine-tuning the MLP networkcan achieve better results.The Impact of Dataset Scale.。We conduct experiments of various models with four percentages of fine-tuning train set,i.e., 100%, 30%, 10%, 3%, on the goal generation task.。The results are shown in Table 15.。It can be seen that as the amountof data gradually decreases, the performance of various training indicators gradually deteriorates under various settings.However, the smaller the amount of data, the greater the improvement brought by pre-training of our proposed CoFT method.The results show that the CoFT method we proposed can effectively collect and expand the chain of thought data related tothe final goal, thereby avoiding overfitting of the training set in the case of small data.K.。Reward Functions for Co-TrainingAgent Feedback Rewards.。The calculation of the agent feedback reward is multifaceted, aiming to reflect the degree ofcompletion as feedback for the training of the LLM.。Specifically, three aspects are considered to satisfy the requirements,and the total agent feedback reward is given by the sum of them:•rfg- Minimal Distance to a Goal When Satisfying Environment Termination .。As depicted by Equation (11), theBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub goal        g4   g8   g12   g16   g20 Time in loopsL1    L2    L3    L4    L5    L6    L70.0250.0500.0750.1000.1250.1500.1750.2000.225Possibility(a)(b)Figure 11: (a) Sub-goal distribution during co-training.。The 20 most frequently occurring goal meta states are filteredout and displayed.. \n",
      "the piece 858\n",
      " \n",
      "aclesPrediction (c) Corresponding to ∆VFigure 10: Distribution comparison between real goals (Oracles) and goals generated by Gop(Prediction).。The illustrationshows that Gopgenerates goals that follows the real distribution, indicating good generalization on open-ended goalgeneration.experimental results are shown in Table 13.。It is worth noting that only fine-tuning the MLP network without fine-tuning theattention network can achieve the best training results.。Although generally speaking, the mainstream fine-tuning task oflarge language models is to fine-tune the attention layer network, but that task usually focuses more on answer semantics.。Inour task, we pay more attention to the format to meet the metastate parsing requirements, so fine-tuning the MLP networkcan achieve better results.The Impact of Dataset Scale.。We conduct experiments of various models with four percentages of fine-tuning train set,i.e., 100%, 30%, 10%, 3%, on the goal generation task.。The results are shown in Table 15.。It can be seen that as the amountof data gradually decreases, the performance of various training indicators gradually deteriorates under various settings.However, the smaller the amount of data, the greater the improvement brought by pre-training of our proposed CoFT method.The results show that the CoFT method we proposed can effectively collect and expand the chain of thought data related tothe final goal, thereby avoiding overfitting of the training set in the case of small data.K.。Reward Functions for Co-TrainingAgent Feedback Rewards.。The calculation of the agent feedback reward is multifaceted, aiming to reflect the degree ofcompletion as feedback for the training of the LLM.。Specifically, three aspects are considered to satisfy the requirements,and the total agent feedback reward is given by the sum of them:•rfg- Minimal Distance to a Goal When Satisfying Environment Termination .。As depicted by Equation (11), theBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub goal        g4   g8   g12   g16   g20 Time in loopsL1    L2    L3    L4    L5    L6    L70.0250.0500.0750.1000.1250.1500.1750.2000.225Possibility(a)(b)Figure 11: (a) Sub-goal distribution during co-training.。The 20 most frequently occurring goal meta states are filteredout and displayed.。The vertical axis represents the probability of the state being output by the language model; (b) For acollected trajectory segment with length k= 200 , we firstly estimate the basic value for the last k−j+ 1states (herej= 20 ) and select one state as the goal with the probability proportional to their values.agent progressively reduces the distance between the initial state and the goal, scaling it by the magnitude of the initialstate-goal difference:Rfg=TXt=1∥|g−Proj (st−1)| − |g−Proj (st)||g−Proj (s0)|+ϵ∥1,where ϵ=1e-6 (11)•rfkeep- Reward Indicating How Long the Goal Can Be Kept .. \n",
      "the piece 859\n",
      " \n",
      " goals (Oracles) and goals generated by Gop(Prediction).。The illustrationshows that Gopgenerates goals that follows the real distribution, indicating good generalization on open-ended goalgeneration.experimental results are shown in Table 13.。It is worth noting that only fine-tuning the MLP network without fine-tuning theattention network can achieve the best training results.。Although generally speaking, the mainstream fine-tuning task oflarge language models is to fine-tune the attention layer network, but that task usually focuses more on answer semantics.。Inour task, we pay more attention to the format to meet the metastate parsing requirements, so fine-tuning the MLP networkcan achieve better results.The Impact of Dataset Scale.。We conduct experiments of various models with four percentages of fine-tuning train set,i.e., 100%, 30%, 10%, 3%, on the goal generation task.。The results are shown in Table 15.。It can be seen that as the amountof data gradually decreases, the performance of various training indicators gradually deteriorates under various settings.However, the smaller the amount of data, the greater the improvement brought by pre-training of our proposed CoFT method.The results show that the CoFT method we proposed can effectively collect and expand the chain of thought data related tothe final goal, thereby avoiding overfitting of the training set in the case of small data.K.。Reward Functions for Co-TrainingAgent Feedback Rewards.。The calculation of the agent feedback reward is multifaceted, aiming to reflect the degree ofcompletion as feedback for the training of the LLM.。Specifically, three aspects are considered to satisfy the requirements,and the total agent feedback reward is given by the sum of them:•rfg- Minimal Distance to a Goal When Satisfying Environment Termination .。As depicted by Equation (11), theBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub goal        g4   g8   g12   g16   g20 Time in loopsL1    L2    L3    L4    L5    L6    L70.0250.0500.0750.1000.1250.1500.1750.2000.225Possibility(a)(b)Figure 11: (a) Sub-goal distribution during co-training.。The 20 most frequently occurring goal meta states are filteredout and displayed.。The vertical axis represents the probability of the state being output by the language model; (b) For acollected trajectory segment with length k= 200 , we firstly estimate the basic value for the last k−j+ 1states (herej= 20 ) and select one state as the goal with the probability proportional to their values.agent progressively reduces the distance between the initial state and the goal, scaling it by the magnitude of the initialstate-goal difference:Rfg=TXt=1∥|g−Proj (st−1)| − |g−Proj (st)||g−Proj (s0)|+ϵ∥1,where ϵ=1e-6 (11)•rfkeep- Reward Indicating How Long the Goal Can Be Kept .。As depicted by Equation (12), upon accomplishing thegoal, the agent receives a reward proportional to the cumulative number of steps taken to sustain the goal state, scaledby the count of distinct sub-goals between the initial state s0and the goal g, i.e.n(g∩PROJ(s0)):Rfkeep=n(g∩PROJ(s0))·TXt=01g∩PROJ(st)̸=∅ (12)•rfrnd- Reward Indicating Whether the Generated Goal is Reachable for the Current State .. \n",
      "the piece 860\n",
      " \n",
      "dels is to fine-tune the attention layer network, but that task usually focuses more on answer semantics.。Inour task, we pay more attention to the format to meet the metastate parsing requirements, so fine-tuning the MLP networkcan achieve better results.The Impact of Dataset Scale.。We conduct experiments of various models with four percentages of fine-tuning train set,i.e., 100%, 30%, 10%, 3%, on the goal generation task.。The results are shown in Table 15.。It can be seen that as the amountof data gradually decreases, the performance of various training indicators gradually deteriorates under various settings.However, the smaller the amount of data, the greater the improvement brought by pre-training of our proposed CoFT method.The results show that the CoFT method we proposed can effectively collect and expand the chain of thought data related tothe final goal, thereby avoiding overfitting of the training set in the case of small data.K.。Reward Functions for Co-TrainingAgent Feedback Rewards.。The calculation of the agent feedback reward is multifaceted, aiming to reflect the degree ofcompletion as feedback for the training of the LLM.。Specifically, three aspects are considered to satisfy the requirements,and the total agent feedback reward is given by the sum of them:•rfg- Minimal Distance to a Goal When Satisfying Environment Termination .。As depicted by Equation (11), theBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub goal        g4   g8   g12   g16   g20 Time in loopsL1    L2    L3    L4    L5    L6    L70.0250.0500.0750.1000.1250.1500.1750.2000.225Possibility(a)(b)Figure 11: (a) Sub-goal distribution during co-training.。The 20 most frequently occurring goal meta states are filteredout and displayed.。The vertical axis represents the probability of the state being output by the language model; (b) For acollected trajectory segment with length k= 200 , we firstly estimate the basic value for the last k−j+ 1states (herej= 20 ) and select one state as the goal with the probability proportional to their values.agent progressively reduces the distance between the initial state and the goal, scaling it by the magnitude of the initialstate-goal difference:Rfg=TXt=1∥|g−Proj (st−1)| − |g−Proj (st)||g−Proj (s0)|+ϵ∥1,where ϵ=1e-6 (11)•rfkeep- Reward Indicating How Long the Goal Can Be Kept .。As depicted by Equation (12), upon accomplishing thegoal, the agent receives a reward proportional to the cumulative number of steps taken to sustain the goal state, scaledby the count of distinct sub-goals between the initial state s0and the goal g, i.e.n(g∩PROJ(s0)):Rfkeep=n(g∩PROJ(s0))·TXt=01g∩PROJ(st)̸=∅ (12)•rfrnd- Reward Indicating Whether the Generated Goal is Reachable for the Current State .。RND (Burda et al.,2018; Nikulin et al., 2023; Zhang et al., 2021; Du et al., 2023) is an effective method to measure the visiting frequencyof states or transitions in RL, where higher a RND score (reward), the more frequent a state is visited.. \n",
      "the piece 861\n",
      " \n",
      "uirements, so fine-tuning the MLP networkcan achieve better results.The Impact of Dataset Scale.。We conduct experiments of various models with four percentages of fine-tuning train set,i.e., 100%, 30%, 10%, 3%, on the goal generation task.。The results are shown in Table 15.。It can be seen that as the amountof data gradually decreases, the performance of various training indicators gradually deteriorates under various settings.However, the smaller the amount of data, the greater the improvement brought by pre-training of our proposed CoFT method.The results show that the CoFT method we proposed can effectively collect and expand the chain of thought data related tothe final goal, thereby avoiding overfitting of the training set in the case of small data.K.。Reward Functions for Co-TrainingAgent Feedback Rewards.。The calculation of the agent feedback reward is multifaceted, aiming to reflect the degree ofcompletion as feedback for the training of the LLM.。Specifically, three aspects are considered to satisfy the requirements,and the total agent feedback reward is given by the sum of them:•rfg- Minimal Distance to a Goal When Satisfying Environment Termination .。As depicted by Equation (11), theBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub goal        g4   g8   g12   g16   g20 Time in loopsL1    L2    L3    L4    L5    L6    L70.0250.0500.0750.1000.1250.1500.1750.2000.225Possibility(a)(b)Figure 11: (a) Sub-goal distribution during co-training.。The 20 most frequently occurring goal meta states are filteredout and displayed.。The vertical axis represents the probability of the state being output by the language model; (b) For acollected trajectory segment with length k= 200 , we firstly estimate the basic value for the last k−j+ 1states (herej= 20 ) and select one state as the goal with the probability proportional to their values.agent progressively reduces the distance between the initial state and the goal, scaling it by the magnitude of the initialstate-goal difference:Rfg=TXt=1∥|g−Proj (st−1)| − |g−Proj (st)||g−Proj (s0)|+ϵ∥1,where ϵ=1e-6 (11)•rfkeep- Reward Indicating How Long the Goal Can Be Kept .。As depicted by Equation (12), upon accomplishing thegoal, the agent receives a reward proportional to the cumulative number of steps taken to sustain the goal state, scaledby the count of distinct sub-goals between the initial state s0and the goal g, i.e.n(g∩PROJ(s0)):Rfkeep=n(g∩PROJ(s0))·TXt=01g∩PROJ(st)̸=∅ (12)•rfrnd- Reward Indicating Whether the Generated Goal is Reachable for the Current State .。RND (Burda et al.,2018; Nikulin et al., 2023; Zhang et al., 2021; Du et al., 2023) is an effective method to measure the visiting frequencyof states or transitions in RL, where higher a RND score (reward), the more frequent a state is visited.。Thus, we canleverage such a method to quantify how novel a state is:Rfrnd=−TXt=0∥φ(E(st, g))−φ⋆(E(st, g))∥, (13)where φ⋆a target network which shares the same architecture as the RND predictor but the network is non-trainable.Thus, we express RfasRf=Rfg+Rfkeep+Rfrnd.Examination Reward Function.. \n",
      "the piece 862\n",
      " \n",
      "with four percentages of fine-tuning train set,i.e., 100%, 30%, 10%, 3%, on the goal generation task.。The results are shown in Table 15.。It can be seen that as the amountof data gradually decreases, the performance of various training indicators gradually deteriorates under various settings.However, the smaller the amount of data, the greater the improvement brought by pre-training of our proposed CoFT method.The results show that the CoFT method we proposed can effectively collect and expand the chain of thought data related tothe final goal, thereby avoiding overfitting of the training set in the case of small data.K.。Reward Functions for Co-TrainingAgent Feedback Rewards.。The calculation of the agent feedback reward is multifaceted, aiming to reflect the degree ofcompletion as feedback for the training of the LLM.。Specifically, three aspects are considered to satisfy the requirements,and the total agent feedback reward is given by the sum of them:•rfg- Minimal Distance to a Goal When Satisfying Environment Termination .。As depicted by Equation (11), theBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub goal        g4   g8   g12   g16   g20 Time in loopsL1    L2    L3    L4    L5    L6    L70.0250.0500.0750.1000.1250.1500.1750.2000.225Possibility(a)(b)Figure 11: (a) Sub-goal distribution during co-training.。The 20 most frequently occurring goal meta states are filteredout and displayed.。The vertical axis represents the probability of the state being output by the language model; (b) For acollected trajectory segment with length k= 200 , we firstly estimate the basic value for the last k−j+ 1states (herej= 20 ) and select one state as the goal with the probability proportional to their values.agent progressively reduces the distance between the initial state and the goal, scaling it by the magnitude of the initialstate-goal difference:Rfg=TXt=1∥|g−Proj (st−1)| − |g−Proj (st)||g−Proj (s0)|+ϵ∥1,where ϵ=1e-6 (11)•rfkeep- Reward Indicating How Long the Goal Can Be Kept .。As depicted by Equation (12), upon accomplishing thegoal, the agent receives a reward proportional to the cumulative number of steps taken to sustain the goal state, scaledby the count of distinct sub-goals between the initial state s0and the goal g, i.e.n(g∩PROJ(s0)):Rfkeep=n(g∩PROJ(s0))·TXt=01g∩PROJ(st)̸=∅ (12)•rfrnd- Reward Indicating Whether the Generated Goal is Reachable for the Current State .。RND (Burda et al.,2018; Nikulin et al., 2023; Zhang et al., 2021; Du et al., 2023) is an effective method to measure the visiting frequencyof states or transitions in RL, where higher a RND score (reward), the more frequent a state is visited.。Thus, we canleverage such a method to quantify how novel a state is:Rfrnd=−TXt=0∥φ(E(st, g))−φ⋆(E(st, g))∥, (13)where φ⋆a target network which shares the same architecture as the RND predictor but the network is non-trainable.Thus, we express RfasRf=Rfg+Rfkeep+Rfrnd.Examination Reward Function.。The examination reward function is introduced as an intrinsic signal to encourage theLLM to generate goals with essential sub-goals.. \n",
      "the piece 863\n",
      " \n",
      " decreases, the performance of various training indicators gradually deteriorates under various settings.However, the smaller the amount of data, the greater the improvement brought by pre-training of our proposed CoFT method.The results show that the CoFT method we proposed can effectively collect and expand the chain of thought data related tothe final goal, thereby avoiding overfitting of the training set in the case of small data.K.。Reward Functions for Co-TrainingAgent Feedback Rewards.。The calculation of the agent feedback reward is multifaceted, aiming to reflect the degree ofcompletion as feedback for the training of the LLM.。Specifically, three aspects are considered to satisfy the requirements,and the total agent feedback reward is given by the sum of them:•rfg- Minimal Distance to a Goal When Satisfying Environment Termination .。As depicted by Equation (11), theBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub goal        g4   g8   g12   g16   g20 Time in loopsL1    L2    L3    L4    L5    L6    L70.0250.0500.0750.1000.1250.1500.1750.2000.225Possibility(a)(b)Figure 11: (a) Sub-goal distribution during co-training.。The 20 most frequently occurring goal meta states are filteredout and displayed.。The vertical axis represents the probability of the state being output by the language model; (b) For acollected trajectory segment with length k= 200 , we firstly estimate the basic value for the last k−j+ 1states (herej= 20 ) and select one state as the goal with the probability proportional to their values.agent progressively reduces the distance between the initial state and the goal, scaling it by the magnitude of the initialstate-goal difference:Rfg=TXt=1∥|g−Proj (st−1)| − |g−Proj (st)||g−Proj (s0)|+ϵ∥1,where ϵ=1e-6 (11)•rfkeep- Reward Indicating How Long the Goal Can Be Kept .。As depicted by Equation (12), upon accomplishing thegoal, the agent receives a reward proportional to the cumulative number of steps taken to sustain the goal state, scaledby the count of distinct sub-goals between the initial state s0and the goal g, i.e.n(g∩PROJ(s0)):Rfkeep=n(g∩PROJ(s0))·TXt=01g∩PROJ(st)̸=∅ (12)•rfrnd- Reward Indicating Whether the Generated Goal is Reachable for the Current State .。RND (Burda et al.,2018; Nikulin et al., 2023; Zhang et al., 2021; Du et al., 2023) is an effective method to measure the visiting frequencyof states or transitions in RL, where higher a RND score (reward), the more frequent a state is visited.。Thus, we canleverage such a method to quantify how novel a state is:Rfrnd=−TXt=0∥φ(E(st, g))−φ⋆(E(st, g))∥, (13)where φ⋆a target network which shares the same architecture as the RND predictor but the network is non-trainable.Thus, we express RfasRf=Rfg+Rfkeep+Rfrnd.Examination Reward Function.。The examination reward function is introduced as an intrinsic signal to encourage theLLM to generate goals with essential sub-goals.。We use the SI dataset as the examination set IS.. \n",
      "the piece 864\n",
      " \n",
      "ht by pre-training of our proposed CoFT method.The results show that the CoFT method we proposed can effectively collect and expand the chain of thought data related tothe final goal, thereby avoiding overfitting of the training set in the case of small data.K.。Reward Functions for Co-TrainingAgent Feedback Rewards.。The calculation of the agent feedback reward is multifaceted, aiming to reflect the degree ofcompletion as feedback for the training of the LLM.。Specifically, three aspects are considered to satisfy the requirements,and the total agent feedback reward is given by the sum of them:•rfg- Minimal Distance to a Goal When Satisfying Environment Termination .。As depicted by Equation (11), theBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub goal        g4   g8   g12   g16   g20 Time in loopsL1    L2    L3    L4    L5    L6    L70.0250.0500.0750.1000.1250.1500.1750.2000.225Possibility(a)(b)Figure 11: (a) Sub-goal distribution during co-training.。The 20 most frequently occurring goal meta states are filteredout and displayed.。The vertical axis represents the probability of the state being output by the language model; (b) For acollected trajectory segment with length k= 200 , we firstly estimate the basic value for the last k−j+ 1states (herej= 20 ) and select one state as the goal with the probability proportional to their values.agent progressively reduces the distance between the initial state and the goal, scaling it by the magnitude of the initialstate-goal difference:Rfg=TXt=1∥|g−Proj (st−1)| − |g−Proj (st)||g−Proj (s0)|+ϵ∥1,where ϵ=1e-6 (11)•rfkeep- Reward Indicating How Long the Goal Can Be Kept .。As depicted by Equation (12), upon accomplishing thegoal, the agent receives a reward proportional to the cumulative number of steps taken to sustain the goal state, scaledby the count of distinct sub-goals between the initial state s0and the goal g, i.e.n(g∩PROJ(s0)):Rfkeep=n(g∩PROJ(s0))·TXt=01g∩PROJ(st)̸=∅ (12)•rfrnd- Reward Indicating Whether the Generated Goal is Reachable for the Current State .。RND (Burda et al.,2018; Nikulin et al., 2023; Zhang et al., 2021; Du et al., 2023) is an effective method to measure the visiting frequencyof states or transitions in RL, where higher a RND score (reward), the more frequent a state is visited.。Thus, we canleverage such a method to quantify how novel a state is:Rfrnd=−TXt=0∥φ(E(st, g))−φ⋆(E(st, g))∥, (13)where φ⋆a target network which shares the same architecture as the RND predictor but the network is non-trainable.Thus, we express RfasRf=Rfg+Rfkeep+Rfrnd.Examination Reward Function.。The examination reward function is introduced as an intrinsic signal to encourage theLLM to generate goals with essential sub-goals.。We use the SI dataset as the examination set IS.。For each training iteration,a batch of instructions Itrain is randomly sampled from the full instruction dataset I, and corresponding goals garegenerated.. \n",
      "the piece 865\n",
      " \n",
      " thought data related tothe final goal, thereby avoiding overfitting of the training set in the case of small data.K.。Reward Functions for Co-TrainingAgent Feedback Rewards.。The calculation of the agent feedback reward is multifaceted, aiming to reflect the degree ofcompletion as feedback for the training of the LLM.。Specifically, three aspects are considered to satisfy the requirements,and the total agent feedback reward is given by the sum of them:•rfg- Minimal Distance to a Goal When Satisfying Environment Termination .。As depicted by Equation (11), theBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub goal        g4   g8   g12   g16   g20 Time in loopsL1    L2    L3    L4    L5    L6    L70.0250.0500.0750.1000.1250.1500.1750.2000.225Possibility(a)(b)Figure 11: (a) Sub-goal distribution during co-training.。The 20 most frequently occurring goal meta states are filteredout and displayed.。The vertical axis represents the probability of the state being output by the language model; (b) For acollected trajectory segment with length k= 200 , we firstly estimate the basic value for the last k−j+ 1states (herej= 20 ) and select one state as the goal with the probability proportional to their values.agent progressively reduces the distance between the initial state and the goal, scaling it by the magnitude of the initialstate-goal difference:Rfg=TXt=1∥|g−Proj (st−1)| − |g−Proj (st)||g−Proj (s0)|+ϵ∥1,where ϵ=1e-6 (11)•rfkeep- Reward Indicating How Long the Goal Can Be Kept .。As depicted by Equation (12), upon accomplishing thegoal, the agent receives a reward proportional to the cumulative number of steps taken to sustain the goal state, scaledby the count of distinct sub-goals between the initial state s0and the goal g, i.e.n(g∩PROJ(s0)):Rfkeep=n(g∩PROJ(s0))·TXt=01g∩PROJ(st)̸=∅ (12)•rfrnd- Reward Indicating Whether the Generated Goal is Reachable for the Current State .。RND (Burda et al.,2018; Nikulin et al., 2023; Zhang et al., 2021; Du et al., 2023) is an effective method to measure the visiting frequencyof states or transitions in RL, where higher a RND score (reward), the more frequent a state is visited.。Thus, we canleverage such a method to quantify how novel a state is:Rfrnd=−TXt=0∥φ(E(st, g))−φ⋆(E(st, g))∥, (13)where φ⋆a target network which shares the same architecture as the RND predictor but the network is non-trainable.Thus, we express RfasRf=Rfg+Rfkeep+Rfrnd.Examination Reward Function.。The examination reward function is introduced as an intrinsic signal to encourage theLLM to generate goals with essential sub-goals.。We use the SI dataset as the examination set IS.。For each training iteration,a batch of instructions Itrain is randomly sampled from the full instruction dataset I, and corresponding goals garegenerated.。After the agent finishes its rollout, the examination reward for each batch is computed based on the intersectionI∩=IS∩ Itrain .. \n",
      "the piece 866\n",
      " \n",
      "al, thereby avoiding overfitting of the training set in the case of small data.K.。Reward Functions for Co-TrainingAgent Feedback Rewards.。The calculation of the agent feedback reward is multifaceted, aiming to reflect the degree ofcompletion as feedback for the training of the LLM.。Specifically, three aspects are considered to satisfy the requirements,and the total agent feedback reward is given by the sum of them:•rfg- Minimal Distance to a Goal When Satisfying Environment Termination .。As depicted by Equation (11), theBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub goal        g4   g8   g12   g16   g20 Time in loopsL1    L2    L3    L4    L5    L6    L70.0250.0500.0750.1000.1250.1500.1750.2000.225Possibility(a)(b)Figure 11: (a) Sub-goal distribution during co-training.。The 20 most frequently occurring goal meta states are filteredout and displayed.。The vertical axis represents the probability of the state being output by the language model; (b) For acollected trajectory segment with length k= 200 , we firstly estimate the basic value for the last k−j+ 1states (herej= 20 ) and select one state as the goal with the probability proportional to their values.agent progressively reduces the distance between the initial state and the goal, scaling it by the magnitude of the initialstate-goal difference:Rfg=TXt=1∥|g−Proj (st−1)| − |g−Proj (st)||g−Proj (s0)|+ϵ∥1,where ϵ=1e-6 (11)•rfkeep- Reward Indicating How Long the Goal Can Be Kept .。As depicted by Equation (12), upon accomplishing thegoal, the agent receives a reward proportional to the cumulative number of steps taken to sustain the goal state, scaledby the count of distinct sub-goals between the initial state s0and the goal g, i.e.n(g∩PROJ(s0)):Rfkeep=n(g∩PROJ(s0))·TXt=01g∩PROJ(st)̸=∅ (12)•rfrnd- Reward Indicating Whether the Generated Goal is Reachable for the Current State .。RND (Burda et al.,2018; Nikulin et al., 2023; Zhang et al., 2021; Du et al., 2023) is an effective method to measure the visiting frequencyof states or transitions in RL, where higher a RND score (reward), the more frequent a state is visited.。Thus, we canleverage such a method to quantify how novel a state is:Rfrnd=−TXt=0∥φ(E(st, g))−φ⋆(E(st, g))∥, (13)where φ⋆a target network which shares the same architecture as the RND predictor but the network is non-trainable.Thus, we express RfasRf=Rfg+Rfkeep+Rfrnd.Examination Reward Function.。The examination reward function is introduced as an intrinsic signal to encourage theLLM to generate goals with essential sub-goals.。We use the SI dataset as the examination set IS.。For each training iteration,a batch of instructions Itrain is randomly sampled from the full instruction dataset I, and corresponding goals garegenerated.。After the agent finishes its rollout, the examination reward for each batch is computed based on the intersectionI∩=IS∩ Itrain .。For non-empty I∩, an examination reward for each instruction in I∩is computed as:Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationre(ι, g, g sub) =(+2 gsub∈g−2otherwise,∀ι∈ I∩ (14)Then, Reis calculated as Re=1|I∩|Pι∈I∩re(ι, g, g sub|g=Gllm(s, ι)).Formatting Reward Function.. \n",
      "the piece 867\n",
      " \n",
      "。As depicted by Equation (11), theBuilding Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationSub goal        g4   g8   g12   g16   g20 Time in loopsL1    L2    L3    L4    L5    L6    L70.0250.0500.0750.1000.1250.1500.1750.2000.225Possibility(a)(b)Figure 11: (a) Sub-goal distribution during co-training.。The 20 most frequently occurring goal meta states are filteredout and displayed.。The vertical axis represents the probability of the state being output by the language model; (b) For acollected trajectory segment with length k= 200 , we firstly estimate the basic value for the last k−j+ 1states (herej= 20 ) and select one state as the goal with the probability proportional to their values.agent progressively reduces the distance between the initial state and the goal, scaling it by the magnitude of the initialstate-goal difference:Rfg=TXt=1∥|g−Proj (st−1)| − |g−Proj (st)||g−Proj (s0)|+ϵ∥1,where ϵ=1e-6 (11)•rfkeep- Reward Indicating How Long the Goal Can Be Kept .。As depicted by Equation (12), upon accomplishing thegoal, the agent receives a reward proportional to the cumulative number of steps taken to sustain the goal state, scaledby the count of distinct sub-goals between the initial state s0and the goal g, i.e.n(g∩PROJ(s0)):Rfkeep=n(g∩PROJ(s0))·TXt=01g∩PROJ(st)̸=∅ (12)•rfrnd- Reward Indicating Whether the Generated Goal is Reachable for the Current State .。RND (Burda et al.,2018; Nikulin et al., 2023; Zhang et al., 2021; Du et al., 2023) is an effective method to measure the visiting frequencyof states or transitions in RL, where higher a RND score (reward), the more frequent a state is visited.。Thus, we canleverage such a method to quantify how novel a state is:Rfrnd=−TXt=0∥φ(E(st, g))−φ⋆(E(st, g))∥, (13)where φ⋆a target network which shares the same architecture as the RND predictor but the network is non-trainable.Thus, we express RfasRf=Rfg+Rfkeep+Rfrnd.Examination Reward Function.。The examination reward function is introduced as an intrinsic signal to encourage theLLM to generate goals with essential sub-goals.。We use the SI dataset as the examination set IS.。For each training iteration,a batch of instructions Itrain is randomly sampled from the full instruction dataset I, and corresponding goals garegenerated.。After the agent finishes its rollout, the examination reward for each batch is computed based on the intersectionI∩=IS∩ Itrain .。For non-empty I∩, an examination reward for each instruction in I∩is computed as:Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationre(ι, g, g sub) =(+2 gsub∈g−2otherwise,∀ι∈ I∩ (14)Then, Reis calculated as Re=1|I∩|Pι∈I∩re(ι, g, g sub|g=Gllm(s, ι)).Formatting Reward Function.。The formatting reward Rmfor each generated goal is calculated by computing an editdistance, utilizing the Wagner-Fischer algorithm (Wagner & Fischer, 1974).With the defined reward functions, RLAF is applied with a reward function R=Rf+Re+Rmand Proximal PolicyOptimization (PPO) for each data point in a batch.Figure 12: Implementation of the RND predictor network.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.. \n",
      "the piece 868\n",
      " \n",
      "odied Agent via Language-Policy Bidirectional AdaptationSub goal        g4   g8   g12   g16   g20 Time in loopsL1    L2    L3    L4    L5    L6    L70.0250.0500.0750.1000.1250.1500.1750.2000.225Possibility(a)(b)Figure 11: (a) Sub-goal distribution during co-training.。The 20 most frequently occurring goal meta states are filteredout and displayed.。The vertical axis represents the probability of the state being output by the language model; (b) For acollected trajectory segment with length k= 200 , we firstly estimate the basic value for the last k−j+ 1states (herej= 20 ) and select one state as the goal with the probability proportional to their values.agent progressively reduces the distance between the initial state and the goal, scaling it by the magnitude of the initialstate-goal difference:Rfg=TXt=1∥|g−Proj (st−1)| − |g−Proj (st)||g−Proj (s0)|+ϵ∥1,where ϵ=1e-6 (11)•rfkeep- Reward Indicating How Long the Goal Can Be Kept .。As depicted by Equation (12), upon accomplishing thegoal, the agent receives a reward proportional to the cumulative number of steps taken to sustain the goal state, scaledby the count of distinct sub-goals between the initial state s0and the goal g, i.e.n(g∩PROJ(s0)):Rfkeep=n(g∩PROJ(s0))·TXt=01g∩PROJ(st)̸=∅ (12)•rfrnd- Reward Indicating Whether the Generated Goal is Reachable for the Current State .。RND (Burda et al.,2018; Nikulin et al., 2023; Zhang et al., 2021; Du et al., 2023) is an effective method to measure the visiting frequencyof states or transitions in RL, where higher a RND score (reward), the more frequent a state is visited.。Thus, we canleverage such a method to quantify how novel a state is:Rfrnd=−TXt=0∥φ(E(st, g))−φ⋆(E(st, g))∥, (13)where φ⋆a target network which shares the same architecture as the RND predictor but the network is non-trainable.Thus, we express RfasRf=Rfg+Rfkeep+Rfrnd.Examination Reward Function.。The examination reward function is introduced as an intrinsic signal to encourage theLLM to generate goals with essential sub-goals.。We use the SI dataset as the examination set IS.。For each training iteration,a batch of instructions Itrain is randomly sampled from the full instruction dataset I, and corresponding goals garegenerated.。After the agent finishes its rollout, the examination reward for each batch is computed based on the intersectionI∩=IS∩ Itrain .。For non-empty I∩, an examination reward for each instruction in I∩is computed as:Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationre(ι, g, g sub) =(+2 gsub∈g−2otherwise,∀ι∈ I∩ (14)Then, Reis calculated as Re=1|I∩|Pι∈I∩re(ι, g, g sub|g=Gllm(s, ι)).Formatting Reward Function.。The formatting reward Rmfor each generated goal is calculated by computing an editdistance, utilizing the Wagner-Fischer algorithm (Wagner & Fischer, 1974).With the defined reward functions, RLAF is applied with a reward function R=Rf+Re+Rmand Proximal PolicyOptimization (PPO) for each data point in a batch.Figure 12: Implementation of the RND predictor network.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.。Analyze the verbal orders of teammates and players, what do you want to do?. \n",
      "the piece 869\n",
      " \n",
      " L70.0250.0500.0750.1000.1250.1500.1750.2000.225Possibility(a)(b)Figure 11: (a) Sub-goal distribution during co-training.。The 20 most frequently occurring goal meta states are filteredout and displayed.。The vertical axis represents the probability of the state being output by the language model; (b) For acollected trajectory segment with length k= 200 , we firstly estimate the basic value for the last k−j+ 1states (herej= 20 ) and select one state as the goal with the probability proportional to their values.agent progressively reduces the distance between the initial state and the goal, scaling it by the magnitude of the initialstate-goal difference:Rfg=TXt=1∥|g−Proj (st−1)| − |g−Proj (st)||g−Proj (s0)|+ϵ∥1,where ϵ=1e-6 (11)•rfkeep- Reward Indicating How Long the Goal Can Be Kept .。As depicted by Equation (12), upon accomplishing thegoal, the agent receives a reward proportional to the cumulative number of steps taken to sustain the goal state, scaledby the count of distinct sub-goals between the initial state s0and the goal g, i.e.n(g∩PROJ(s0)):Rfkeep=n(g∩PROJ(s0))·TXt=01g∩PROJ(st)̸=∅ (12)•rfrnd- Reward Indicating Whether the Generated Goal is Reachable for the Current State .。RND (Burda et al.,2018; Nikulin et al., 2023; Zhang et al., 2021; Du et al., 2023) is an effective method to measure the visiting frequencyof states or transitions in RL, where higher a RND score (reward), the more frequent a state is visited.。Thus, we canleverage such a method to quantify how novel a state is:Rfrnd=−TXt=0∥φ(E(st, g))−φ⋆(E(st, g))∥, (13)where φ⋆a target network which shares the same architecture as the RND predictor but the network is non-trainable.Thus, we express RfasRf=Rfg+Rfkeep+Rfrnd.Examination Reward Function.。The examination reward function is introduced as an intrinsic signal to encourage theLLM to generate goals with essential sub-goals.。We use the SI dataset as the examination set IS.。For each training iteration,a batch of instructions Itrain is randomly sampled from the full instruction dataset I, and corresponding goals garegenerated.。After the agent finishes its rollout, the examination reward for each batch is computed based on the intersectionI∩=IS∩ Itrain .。For non-empty I∩, an examination reward for each instruction in I∩is computed as:Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationre(ι, g, g sub) =(+2 gsub∈g−2otherwise,∀ι∈ I∩ (14)Then, Reis calculated as Re=1|I∩|Pι∈I∩re(ι, g, g sub|g=Gllm(s, ι)).Formatting Reward Function.。The formatting reward Rmfor each generated goal is calculated by computing an editdistance, utilizing the Wagner-Fischer algorithm (Wagner & Fischer, 1974).With the defined reward functions, RLAF is applied with a reward function R=Rf+Re+Rmand Proximal PolicyOptimization (PPO) for each data point in a batch.Figure 12: Implementation of the RND predictor network.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is ‘You should lie in wait’, which means teammate player wantsthe agent to set up an ambush or take a hiding position.2.. \n",
      "the piece 870\n",
      " \n",
      "ical axis represents the probability of the state being output by the language model; (b) For acollected trajectory segment with length k= 200 , we firstly estimate the basic value for the last k−j+ 1states (herej= 20 ) and select one state as the goal with the probability proportional to their values.agent progressively reduces the distance between the initial state and the goal, scaling it by the magnitude of the initialstate-goal difference:Rfg=TXt=1∥|g−Proj (st−1)| − |g−Proj (st)||g−Proj (s0)|+ϵ∥1,where ϵ=1e-6 (11)•rfkeep- Reward Indicating How Long the Goal Can Be Kept .。As depicted by Equation (12), upon accomplishing thegoal, the agent receives a reward proportional to the cumulative number of steps taken to sustain the goal state, scaledby the count of distinct sub-goals between the initial state s0and the goal g, i.e.n(g∩PROJ(s0)):Rfkeep=n(g∩PROJ(s0))·TXt=01g∩PROJ(st)̸=∅ (12)•rfrnd- Reward Indicating Whether the Generated Goal is Reachable for the Current State .。RND (Burda et al.,2018; Nikulin et al., 2023; Zhang et al., 2021; Du et al., 2023) is an effective method to measure the visiting frequencyof states or transitions in RL, where higher a RND score (reward), the more frequent a state is visited.。Thus, we canleverage such a method to quantify how novel a state is:Rfrnd=−TXt=0∥φ(E(st, g))−φ⋆(E(st, g))∥, (13)where φ⋆a target network which shares the same architecture as the RND predictor but the network is non-trainable.Thus, we express RfasRf=Rfg+Rfkeep+Rfrnd.Examination Reward Function.。The examination reward function is introduced as an intrinsic signal to encourage theLLM to generate goals with essential sub-goals.。We use the SI dataset as the examination set IS.。For each training iteration,a batch of instructions Itrain is randomly sampled from the full instruction dataset I, and corresponding goals garegenerated.。After the agent finishes its rollout, the examination reward for each batch is computed based on the intersectionI∩=IS∩ Itrain .。For non-empty I∩, an examination reward for each instruction in I∩is computed as:Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationre(ι, g, g sub) =(+2 gsub∈g−2otherwise,∀ι∈ I∩ (14)Then, Reis calculated as Re=1|I∩|Pι∈I∩re(ι, g, g sub|g=Gllm(s, ι)).Formatting Reward Function.。The formatting reward Rmfor each generated goal is calculated by computing an editdistance, utilizing the Wagner-Fischer algorithm (Wagner & Fischer, 1974).With the defined reward functions, RLAF is applied with a reward function R=Rf+Re+Rmand Proximal PolicyOptimization (PPO) for each data point in a batch.Figure 12: Implementation of the RND predictor network.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is ‘You should lie in wait’, which means teammate player wantsthe agent to set up an ambush or take a hiding position.2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.. \n",
      "the piece 871\n",
      " \n",
      "duces the distance between the initial state and the goal, scaling it by the magnitude of the initialstate-goal difference:Rfg=TXt=1∥|g−Proj (st−1)| − |g−Proj (st)||g−Proj (s0)|+ϵ∥1,where ϵ=1e-6 (11)•rfkeep- Reward Indicating How Long the Goal Can Be Kept .。As depicted by Equation (12), upon accomplishing thegoal, the agent receives a reward proportional to the cumulative number of steps taken to sustain the goal state, scaledby the count of distinct sub-goals between the initial state s0and the goal g, i.e.n(g∩PROJ(s0)):Rfkeep=n(g∩PROJ(s0))·TXt=01g∩PROJ(st)̸=∅ (12)•rfrnd- Reward Indicating Whether the Generated Goal is Reachable for the Current State .。RND (Burda et al.,2018; Nikulin et al., 2023; Zhang et al., 2021; Du et al., 2023) is an effective method to measure the visiting frequencyof states or transitions in RL, where higher a RND score (reward), the more frequent a state is visited.。Thus, we canleverage such a method to quantify how novel a state is:Rfrnd=−TXt=0∥φ(E(st, g))−φ⋆(E(st, g))∥, (13)where φ⋆a target network which shares the same architecture as the RND predictor but the network is non-trainable.Thus, we express RfasRf=Rfg+Rfkeep+Rfrnd.Examination Reward Function.。The examination reward function is introduced as an intrinsic signal to encourage theLLM to generate goals with essential sub-goals.。We use the SI dataset as the examination set IS.。For each training iteration,a batch of instructions Itrain is randomly sampled from the full instruction dataset I, and corresponding goals garegenerated.。After the agent finishes its rollout, the examination reward for each batch is computed based on the intersectionI∩=IS∩ Itrain .。For non-empty I∩, an examination reward for each instruction in I∩is computed as:Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationre(ι, g, g sub) =(+2 gsub∈g−2otherwise,∀ι∈ I∩ (14)Then, Reis calculated as Re=1|I∩|Pι∈I∩re(ι, g, g sub|g=Gllm(s, ι)).Formatting Reward Function.。The formatting reward Rmfor each generated goal is calculated by computing an editdistance, utilizing the Wagner-Fischer algorithm (Wagner & Fischer, 1974).With the defined reward functions, RLAF is applied with a reward function R=Rf+Re+Rmand Proximal PolicyOptimization (PPO) for each data point in a batch.Figure 12: Implementation of the RND predictor network.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is ‘You should lie in wait’, which means teammate player wantsthe agent to set up an ambush or take a hiding position.2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan.According to the teammate‘S command:2.1.. \n",
      "the piece 872\n",
      " \n",
      "tude of the initialstate-goal difference:Rfg=TXt=1∥|g−Proj (st−1)| − |g−Proj (st)||g−Proj (s0)|+ϵ∥1,where ϵ=1e-6 (11)•rfkeep- Reward Indicating How Long the Goal Can Be Kept .。As depicted by Equation (12), upon accomplishing thegoal, the agent receives a reward proportional to the cumulative number of steps taken to sustain the goal state, scaledby the count of distinct sub-goals between the initial state s0and the goal g, i.e.n(g∩PROJ(s0)):Rfkeep=n(g∩PROJ(s0))·TXt=01g∩PROJ(st)̸=∅ (12)•rfrnd- Reward Indicating Whether the Generated Goal is Reachable for the Current State .。RND (Burda et al.,2018; Nikulin et al., 2023; Zhang et al., 2021; Du et al., 2023) is an effective method to measure the visiting frequencyof states or transitions in RL, where higher a RND score (reward), the more frequent a state is visited.。Thus, we canleverage such a method to quantify how novel a state is:Rfrnd=−TXt=0∥φ(E(st, g))−φ⋆(E(st, g))∥, (13)where φ⋆a target network which shares the same architecture as the RND predictor but the network is non-trainable.Thus, we express RfasRf=Rfg+Rfkeep+Rfrnd.Examination Reward Function.。The examination reward function is introduced as an intrinsic signal to encourage theLLM to generate goals with essential sub-goals.。We use the SI dataset as the examination set IS.。For each training iteration,a batch of instructions Itrain is randomly sampled from the full instruction dataset I, and corresponding goals garegenerated.。After the agent finishes its rollout, the examination reward for each batch is computed based on the intersectionI∩=IS∩ Itrain .。For non-empty I∩, an examination reward for each instruction in I∩is computed as:Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationre(ι, g, g sub) =(+2 gsub∈g−2otherwise,∀ι∈ I∩ (14)Then, Reis calculated as Re=1|I∩|Pι∈I∩re(ι, g, g sub|g=Gllm(s, ι)).Formatting Reward Function.。The formatting reward Rmfor each generated goal is calculated by computing an editdistance, utilizing the Wagner-Fischer algorithm (Wagner & Fischer, 1974).With the defined reward functions, RLAF is applied with a reward function R=Rf+Re+Rmand Proximal PolicyOptimization (PPO) for each data point in a batch.Figure 12: Implementation of the RND predictor network.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is ‘You should lie in wait’, which means teammate player wantsthe agent to set up an ambush or take a hiding position.2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan.According to the teammate‘S command:2.1.。Need to hide: ‘Whether prone position’, ‘Whether crouch position’2.2.. \n",
      "the piece 873\n",
      " \n",
      "t al.,2018; Nikulin et al., 2023; Zhang et al., 2021; Du et al., 2023) is an effective method to measure the visiting frequencyof states or transitions in RL, where higher a RND score (reward), the more frequent a state is visited.。Thus, we canleverage such a method to quantify how novel a state is:Rfrnd=−TXt=0∥φ(E(st, g))−φ⋆(E(st, g))∥, (13)where φ⋆a target network which shares the same architecture as the RND predictor but the network is non-trainable.Thus, we express RfasRf=Rfg+Rfkeep+Rfrnd.Examination Reward Function.。The examination reward function is introduced as an intrinsic signal to encourage theLLM to generate goals with essential sub-goals.。We use the SI dataset as the examination set IS.。For each training iteration,a batch of instructions Itrain is randomly sampled from the full instruction dataset I, and corresponding goals garegenerated.。After the agent finishes its rollout, the examination reward for each batch is computed based on the intersectionI∩=IS∩ Itrain .。For non-empty I∩, an examination reward for each instruction in I∩is computed as:Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationre(ι, g, g sub) =(+2 gsub∈g−2otherwise,∀ι∈ I∩ (14)Then, Reis calculated as Re=1|I∩|Pι∈I∩re(ι, g, g sub|g=Gllm(s, ι)).Formatting Reward Function.。The formatting reward Rmfor each generated goal is calculated by computing an editdistance, utilizing the Wagner-Fischer algorithm (Wagner & Fischer, 1974).With the defined reward functions, RLAF is applied with a reward function R=Rf+Re+Rmand Proximal PolicyOptimization (PPO) for each data point in a batch.Figure 12: Implementation of the RND predictor network.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is ‘You should lie in wait’, which means teammate player wantsthe agent to set up an ambush or take a hiding position.2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan.According to the teammate‘S command:2.1.。Need to hide: ‘Whether prone position’, ‘Whether crouch position’2.2.。Reduce movement to stay unnoticed: ‘Average velocity’, ‘Length of distance moved’2.3.. \n",
      "the piece 874\n",
      " \n",
      "s the RND predictor but the network is non-trainable.Thus, we express RfasRf=Rfg+Rfkeep+Rfrnd.Examination Reward Function.。The examination reward function is introduced as an intrinsic signal to encourage theLLM to generate goals with essential sub-goals.。We use the SI dataset as the examination set IS.。For each training iteration,a batch of instructions Itrain is randomly sampled from the full instruction dataset I, and corresponding goals garegenerated.。After the agent finishes its rollout, the examination reward for each batch is computed based on the intersectionI∩=IS∩ Itrain .。For non-empty I∩, an examination reward for each instruction in I∩is computed as:Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationre(ι, g, g sub) =(+2 gsub∈g−2otherwise,∀ι∈ I∩ (14)Then, Reis calculated as Re=1|I∩|Pι∈I∩re(ι, g, g sub|g=Gllm(s, ι)).Formatting Reward Function.。The formatting reward Rmfor each generated goal is calculated by computing an editdistance, utilizing the Wagner-Fischer algorithm (Wagner & Fischer, 1974).With the defined reward functions, RLAF is applied with a reward function R=Rf+Re+Rmand Proximal PolicyOptimization (PPO) for each data point in a batch.Figure 12: Implementation of the RND predictor network.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is ‘You should lie in wait’, which means teammate player wantsthe agent to set up an ambush or take a hiding position.2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan.According to the teammate‘S command:2.1.。Need to hide: ‘Whether prone position’, ‘Whether crouch position’2.2.。Reduce movement to stay unnoticed: ‘Average velocity’, ‘Length of distance moved’2.3.。Ensure readiness for potential attack: ‘Whether hold a gun’3.. \n",
      "the piece 875\n",
      " \n",
      "sub-goals.。We use the SI dataset as the examination set IS.。For each training iteration,a batch of instructions Itrain is randomly sampled from the full instruction dataset I, and corresponding goals garegenerated.。After the agent finishes its rollout, the examination reward for each batch is computed based on the intersectionI∩=IS∩ Itrain .。For non-empty I∩, an examination reward for each instruction in I∩is computed as:Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationre(ι, g, g sub) =(+2 gsub∈g−2otherwise,∀ι∈ I∩ (14)Then, Reis calculated as Re=1|I∩|Pι∈I∩re(ι, g, g sub|g=Gllm(s, ι)).Formatting Reward Function.。The formatting reward Rmfor each generated goal is calculated by computing an editdistance, utilizing the Wagner-Fischer algorithm (Wagner & Fischer, 1974).With the defined reward functions, RLAF is applied with a reward function R=Rf+Re+Rmand Proximal PolicyOptimization (PPO) for each data point in a batch.Figure 12: Implementation of the RND predictor network.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is ‘You should lie in wait’, which means teammate player wantsthe agent to set up an ambush or take a hiding position.2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan.According to the teammate‘S command:2.1.。Need to hide: ‘Whether prone position’, ‘Whether crouch position’2.2.。Reduce movement to stay unnoticed: ‘Average velocity’, ‘Length of distance moved’2.3.。Ensure readiness for potential attack: ‘Whether hold a gun’3.。Plan how these key states need to be adjusted.According to the teammate‘S command:3.1.. \n",
      "the piece 876\n",
      " \n",
      "puted based on the intersectionI∩=IS∩ Itrain .。For non-empty I∩, an examination reward for each instruction in I∩is computed as:Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationre(ι, g, g sub) =(+2 gsub∈g−2otherwise,∀ι∈ I∩ (14)Then, Reis calculated as Re=1|I∩|Pι∈I∩re(ι, g, g sub|g=Gllm(s, ι)).Formatting Reward Function.。The formatting reward Rmfor each generated goal is calculated by computing an editdistance, utilizing the Wagner-Fischer algorithm (Wagner & Fischer, 1974).With the defined reward functions, RLAF is applied with a reward function R=Rf+Re+Rmand Proximal PolicyOptimization (PPO) for each data point in a batch.Figure 12: Implementation of the RND predictor network.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is ‘You should lie in wait’, which means teammate player wantsthe agent to set up an ambush or take a hiding position.2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan.According to the teammate‘S command:2.1.。Need to hide: ‘Whether prone position’, ‘Whether crouch position’2.2.。Reduce movement to stay unnoticed: ‘Average velocity’, ‘Length of distance moved’2.3.。Ensure readiness for potential attack: ‘Whether hold a gun’3.。Plan how these key states need to be adjusted.According to the teammate‘S command:3.1.。‘Whether prone position’: Need to hide: ‘False’ -> ‘True’3.2.. \n",
      "the piece 877\n",
      " \n",
      "ng Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptationre(ι, g, g sub) =(+2 gsub∈g−2otherwise,∀ι∈ I∩ (14)Then, Reis calculated as Re=1|I∩|Pι∈I∩re(ι, g, g sub|g=Gllm(s, ι)).Formatting Reward Function.。The formatting reward Rmfor each generated goal is calculated by computing an editdistance, utilizing the Wagner-Fischer algorithm (Wagner & Fischer, 1974).With the defined reward functions, RLAF is applied with a reward function R=Rf+Re+Rmand Proximal PolicyOptimization (PPO) for each data point in a batch.Figure 12: Implementation of the RND predictor network.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is ‘You should lie in wait’, which means teammate player wantsthe agent to set up an ambush or take a hiding position.2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan.According to the teammate‘S command:2.1.。Need to hide: ‘Whether prone position’, ‘Whether crouch position’2.2.。Reduce movement to stay unnoticed: ‘Average velocity’, ‘Length of distance moved’2.3.。Ensure readiness for potential attack: ‘Whether hold a gun’3.。Plan how these key states need to be adjusted.According to the teammate‘S command:3.1.。‘Whether prone position’: Need to hide: ‘False’ -> ‘True’3.2.。‘Whether crouch position’: Alternative hiding posture if not prone: ‘False’ -> ‘True’3.3.. \n",
      "the piece 878\n",
      " \n",
      "idirectional Adaptationre(ι, g, g sub) =(+2 gsub∈g−2otherwise,∀ι∈ I∩ (14)Then, Reis calculated as Re=1|I∩|Pι∈I∩re(ι, g, g sub|g=Gllm(s, ι)).Formatting Reward Function.。The formatting reward Rmfor each generated goal is calculated by computing an editdistance, utilizing the Wagner-Fischer algorithm (Wagner & Fischer, 1974).With the defined reward functions, RLAF is applied with a reward function R=Rf+Re+Rmand Proximal PolicyOptimization (PPO) for each data point in a batch.Figure 12: Implementation of the RND predictor network.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is ‘You should lie in wait’, which means teammate player wantsthe agent to set up an ambush or take a hiding position.2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan.According to the teammate‘S command:2.1.。Need to hide: ‘Whether prone position’, ‘Whether crouch position’2.2.。Reduce movement to stay unnoticed: ‘Average velocity’, ‘Length of distance moved’2.3.。Ensure readiness for potential attack: ‘Whether hold a gun’3.。Plan how these key states need to be adjusted.According to the teammate‘S command:3.1.。‘Whether prone position’: Need to hide: ‘False’ -> ‘True’3.2.。‘Whether crouch position’: Alternative hiding posture if not prone: ‘False’ -> ‘True’3.3.。‘Average velocity’: Reduce movement: ‘Fast’ -> ‘Static’3.4.. \n",
      "the piece 879\n",
      " \n",
      "d Function.。The formatting reward Rmfor each generated goal is calculated by computing an editdistance, utilizing the Wagner-Fischer algorithm (Wagner & Fischer, 1974).With the defined reward functions, RLAF is applied with a reward function R=Rf+Re+Rmand Proximal PolicyOptimization (PPO) for each data point in a batch.Figure 12: Implementation of the RND predictor network.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is ‘You should lie in wait’, which means teammate player wantsthe agent to set up an ambush or take a hiding position.2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan.According to the teammate‘S command:2.1.。Need to hide: ‘Whether prone position’, ‘Whether crouch position’2.2.。Reduce movement to stay unnoticed: ‘Average velocity’, ‘Length of distance moved’2.3.。Ensure readiness for potential attack: ‘Whether hold a gun’3.。Plan how these key states need to be adjusted.According to the teammate‘S command:3.1.。‘Whether prone position’: Need to hide: ‘False’ -> ‘True’3.2.。‘Whether crouch position’: Alternative hiding posture if not prone: ‘False’ -> ‘True’3.3.。‘Average velocity’: Reduce movement: ‘Fast’ -> ‘Static’3.4.。‘Length of distance moved’: Reduce movement: ‘Short’ -> ‘No movement’3.5.. \n",
      "the piece 880\n",
      " \n",
      "er algorithm (Wagner & Fischer, 1974).With the defined reward functions, RLAF is applied with a reward function R=Rf+Re+Rmand Proximal PolicyOptimization (PPO) for each data point in a batch.Figure 12: Implementation of the RND predictor network.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is ‘You should lie in wait’, which means teammate player wantsthe agent to set up an ambush or take a hiding position.2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan.According to the teammate‘S command:2.1.。Need to hide: ‘Whether prone position’, ‘Whether crouch position’2.2.。Reduce movement to stay unnoticed: ‘Average velocity’, ‘Length of distance moved’2.3.。Ensure readiness for potential attack: ‘Whether hold a gun’3.。Plan how these key states need to be adjusted.According to the teammate‘S command:3.1.。‘Whether prone position’: Need to hide: ‘False’ -> ‘True’3.2.。‘Whether crouch position’: Alternative hiding posture if not prone: ‘False’ -> ‘True’3.3.。‘Average velocity’: Reduce movement: ‘Fast’ -> ‘Static’3.4.。‘Length of distance moved’: Reduce movement: ‘Short’ -> ‘No movement’3.5.。‘Whether hold a gun’: Ensure readiness for potential attack: ‘True’ -> ‘True’4.. \n",
      "the piece 881\n",
      " \n",
      "ectional Adaptation1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is ‘You should lie in wait’, which means teammate player wantsthe agent to set up an ambush or take a hiding position.2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan.According to the teammate‘S command:2.1.。Need to hide: ‘Whether prone position’, ‘Whether crouch position’2.2.。Reduce movement to stay unnoticed: ‘Average velocity’, ‘Length of distance moved’2.3.。Ensure readiness for potential attack: ‘Whether hold a gun’3.。Plan how these key states need to be adjusted.According to the teammate‘S command:3.1.。‘Whether prone position’: Need to hide: ‘False’ -> ‘True’3.2.。‘Whether crouch position’: Alternative hiding posture if not prone: ‘False’ -> ‘True’3.3.。‘Average velocity’: Reduce movement: ‘Fast’ -> ‘Static’3.4.。‘Length of distance moved’: Reduce movement: ‘Short’ -> ‘No movement’3.5.。‘Whether hold a gun’: Ensure readiness for potential attack: ‘True’ -> ‘True’4.。Modify the adjustment that may be wrong, and refer to the Rules to analyze which state adjustments mayconflict, repeat or be unnecessary, and output the modified adjustment plan:According to the states adjustments in 3:4.1.. \n",
      "the piece 882\n",
      " \n",
      "mmate player.。The agentsin the unselected states will adjust themselves to complete your plan.According to the teammate‘S command:2.1.。Need to hide: ‘Whether prone position’, ‘Whether crouch position’2.2.。Reduce movement to stay unnoticed: ‘Average velocity’, ‘Length of distance moved’2.3.。Ensure readiness for potential attack: ‘Whether hold a gun’3.。Plan how these key states need to be adjusted.According to the teammate‘S command:3.1.。‘Whether prone position’: Need to hide: ‘False’ -> ‘True’3.2.。‘Whether crouch position’: Alternative hiding posture if not prone: ‘False’ -> ‘True’3.3.。‘Average velocity’: Reduce movement: ‘Fast’ -> ‘Static’3.4.。‘Length of distance moved’: Reduce movement: ‘Short’ -> ‘No movement’3.5.。‘Whether hold a gun’: Ensure readiness for potential attack: ‘True’ -> ‘True’4.。Modify the adjustment that may be wrong, and refer to the Rules to analyze which state adjustments mayconflict, repeat or be unnecessary, and output the modified adjustment plan:According to the states adjustments in 3:4.1.。‘Whether prone position’: ‘False’ -> ‘True’4.2.. \n",
      "the piece 883\n",
      " \n",
      "mplete your plan.According to the teammate‘S command:2.1.。Need to hide: ‘Whether prone position’, ‘Whether crouch position’2.2.。Reduce movement to stay unnoticed: ‘Average velocity’, ‘Length of distance moved’2.3.。Ensure readiness for potential attack: ‘Whether hold a gun’3.。Plan how these key states need to be adjusted.According to the teammate‘S command:3.1.。‘Whether prone position’: Need to hide: ‘False’ -> ‘True’3.2.。‘Whether crouch position’: Alternative hiding posture if not prone: ‘False’ -> ‘True’3.3.。‘Average velocity’: Reduce movement: ‘Fast’ -> ‘Static’3.4.。‘Length of distance moved’: Reduce movement: ‘Short’ -> ‘No movement’3.5.。‘Whether hold a gun’: Ensure readiness for potential attack: ‘True’ -> ‘True’4.。Modify the adjustment that may be wrong, and refer to the Rules to analyze which state adjustments mayconflict, repeat or be unnecessary, and output the modified adjustment plan:According to the states adjustments in 3:4.1.。‘Whether prone position’: ‘False’ -> ‘True’4.2.。‘Average velocity’: ‘Fast’ -> ‘Static’4.3.. \n",
      "the piece 884\n",
      " \n",
      " gun’3.。Plan how these key states need to be adjusted.According to the teammate‘S command:3.1.。‘Whether prone position’: Need to hide: ‘False’ -> ‘True’3.2.。‘Whether crouch position’: Alternative hiding posture if not prone: ‘False’ -> ‘True’3.3.。‘Average velocity’: Reduce movement: ‘Fast’ -> ‘Static’3.4.。‘Length of distance moved’: Reduce movement: ‘Short’ -> ‘No movement’3.5.。‘Whether hold a gun’: Ensure readiness for potential attack: ‘True’ -> ‘True’4.。Modify the adjustment that may be wrong, and refer to the Rules to analyze which state adjustments mayconflict, repeat or be unnecessary, and output the modified adjustment plan:According to the states adjustments in 3:4.1.。‘Whether prone position’: ‘False’ -> ‘True’4.2.。‘Average velocity’: ‘Fast’ -> ‘Static’4.3.。‘Length of distance moved’: ‘Short’ -> ‘No movement’4.4.. \n",
      "the piece 885\n",
      " \n",
      "hether prone position’: Need to hide: ‘False’ -> ‘True’3.2.。‘Whether crouch position’: Alternative hiding posture if not prone: ‘False’ -> ‘True’3.3.。‘Average velocity’: Reduce movement: ‘Fast’ -> ‘Static’3.4.。‘Length of distance moved’: Reduce movement: ‘Short’ -> ‘No movement’3.5.。‘Whether hold a gun’: Ensure readiness for potential attack: ‘True’ -> ‘True’4.。Modify the adjustment that may be wrong, and refer to the Rules to analyze which state adjustments mayconflict, repeat or be unnecessary, and output the modified adjustment plan:According to the states adjustments in 3:4.1.。‘Whether prone position’: ‘False’ -> ‘True’4.2.。‘Average velocity’: ‘Fast’ -> ‘Static’4.3.。‘Length of distance moved’: ‘Short’ -> ‘No movement’4.4.。‘Whether hold a gun’: ‘True’ -> ‘True’(Removed ‘Whether crouch position’ as it conflicts with ‘Whether prone position’ and prone position givesbetter cover for an ambush.)5.. \n",
      "the piece 886\n",
      " \n",
      "rone: ‘False’ -> ‘True’3.3.。‘Average velocity’: Reduce movement: ‘Fast’ -> ‘Static’3.4.。‘Length of distance moved’: Reduce movement: ‘Short’ -> ‘No movement’3.5.。‘Whether hold a gun’: Ensure readiness for potential attack: ‘True’ -> ‘True’4.。Modify the adjustment that may be wrong, and refer to the Rules to analyze which state adjustments mayconflict, repeat or be unnecessary, and output the modified adjustment plan:According to the states adjustments in 3:4.1.。‘Whether prone position’: ‘False’ -> ‘True’4.2.。‘Average velocity’: ‘Fast’ -> ‘Static’4.3.。‘Length of distance moved’: ‘Short’ -> ‘No movement’4.4.。‘Whether hold a gun’: ‘True’ -> ‘True’(Removed ‘Whether crouch position’ as it conflicts with ‘Whether prone position’ and prone position givesbetter cover for an ambush.)5.。According to the analyze and the planing of the verbal command, further analyze the behavior tendencyrequired in the adjustment process.Mobile: Need to stop all movement: 0%Offense: Must be ready to attack if necessary: 20%Waiting: The main goal is to set up an ambush and wait: 70%Supplies: No need for supplies during this phase: 0%Scouting: Need to observe the surroundings, but not actively scout: 10%6.. \n",
      "the piece 887\n",
      " \n",
      " -> ‘Static’3.4.。‘Length of distance moved’: Reduce movement: ‘Short’ -> ‘No movement’3.5.。‘Whether hold a gun’: Ensure readiness for potential attack: ‘True’ -> ‘True’4.。Modify the adjustment that may be wrong, and refer to the Rules to analyze which state adjustments mayconflict, repeat or be unnecessary, and output the modified adjustment plan:According to the states adjustments in 3:4.1.。‘Whether prone position’: ‘False’ -> ‘True’4.2.。‘Average velocity’: ‘Fast’ -> ‘Static’4.3.。‘Length of distance moved’: ‘Short’ -> ‘No movement’4.4.。‘Whether hold a gun’: ‘True’ -> ‘True’(Removed ‘Whether crouch position’ as it conflicts with ‘Whether prone position’ and prone position givesbetter cover for an ambush.)5.。According to the analyze and the planing of the verbal command, further analyze the behavior tendencyrequired in the adjustment process.Mobile: Need to stop all movement: 0%Offense: Must be ready to attack if necessary: 20%Waiting: The main goal is to set up an ambush and wait: 70%Supplies: No need for supplies during this phase: 0%Scouting: Need to observe the surroundings, but not actively scout: 10%6.。Analyze how long the current command needs to be kept.According to the command of the teammate, taking a hiding position or setting up an ambush is usually donewith the intention of waiting for an opportune moment to strike or observe the enemy‘S movement: The currentcommand needs to be kept by ‘long term’.Table 9: Chain of thought response from GPT4.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.Only select the most relevant and necessary states for planning, and the unplanned states will be adjusted bythe agent itself2.. \n",
      "the piece 888\n",
      " \n",
      ".5.。‘Whether hold a gun’: Ensure readiness for potential attack: ‘True’ -> ‘True’4.。Modify the adjustment that may be wrong, and refer to the Rules to analyze which state adjustments mayconflict, repeat or be unnecessary, and output the modified adjustment plan:According to the states adjustments in 3:4.1.。‘Whether prone position’: ‘False’ -> ‘True’4.2.。‘Average velocity’: ‘Fast’ -> ‘Static’4.3.。‘Length of distance moved’: ‘Short’ -> ‘No movement’4.4.。‘Whether hold a gun’: ‘True’ -> ‘True’(Removed ‘Whether crouch position’ as it conflicts with ‘Whether prone position’ and prone position givesbetter cover for an ambush.)5.。According to the analyze and the planing of the verbal command, further analyze the behavior tendencyrequired in the adjustment process.Mobile: Need to stop all movement: 0%Offense: Must be ready to attack if necessary: 20%Waiting: The main goal is to set up an ambush and wait: 70%Supplies: No need for supplies during this phase: 0%Scouting: Need to observe the surroundings, but not actively scout: 10%6.。Analyze how long the current command needs to be kept.According to the command of the teammate, taking a hiding position or setting up an ambush is usually donewith the intention of waiting for an opportune moment to strike or observe the enemy‘S movement: The currentcommand needs to be kept by ‘long term’.Table 9: Chain of thought response from GPT4.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.Only select the most relevant and necessary states for planning, and the unplanned states will be adjusted bythe agent itself2.。[Choose 1, Choose 2, ...] indicates the values that can be selected for the state.. \n",
      "the piece 889\n",
      " \n",
      ": ‘True’ -> ‘True’4.。Modify the adjustment that may be wrong, and refer to the Rules to analyze which state adjustments mayconflict, repeat or be unnecessary, and output the modified adjustment plan:According to the states adjustments in 3:4.1.。‘Whether prone position’: ‘False’ -> ‘True’4.2.。‘Average velocity’: ‘Fast’ -> ‘Static’4.3.。‘Length of distance moved’: ‘Short’ -> ‘No movement’4.4.。‘Whether hold a gun’: ‘True’ -> ‘True’(Removed ‘Whether crouch position’ as it conflicts with ‘Whether prone position’ and prone position givesbetter cover for an ambush.)5.。According to the analyze and the planing of the verbal command, further analyze the behavior tendencyrequired in the adjustment process.Mobile: Need to stop all movement: 0%Offense: Must be ready to attack if necessary: 20%Waiting: The main goal is to set up an ambush and wait: 70%Supplies: No need for supplies during this phase: 0%Scouting: Need to observe the surroundings, but not actively scout: 10%6.。Analyze how long the current command needs to be kept.According to the command of the teammate, taking a hiding position or setting up an ambush is usually donewith the intention of waiting for an opportune moment to strike or observe the enemy‘S movement: The currentcommand needs to be kept by ‘long term’.Table 9: Chain of thought response from GPT4.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.Only select the most relevant and necessary states for planning, and the unplanned states will be adjusted bythe agent itself2.。[Choose 1, Choose 2, ...] indicates the values that can be selected for the state.。When you plan, you can onlychoose the value of the state from it, and do not invent new value not listed in [Choice1, Choice2, ...].3.The selected state can change the current value or maintain the current value.. \n",
      "the piece 890\n",
      " \n",
      "analyze which state adjustments mayconflict, repeat or be unnecessary, and output the modified adjustment plan:According to the states adjustments in 3:4.1.。‘Whether prone position’: ‘False’ -> ‘True’4.2.。‘Average velocity’: ‘Fast’ -> ‘Static’4.3.。‘Length of distance moved’: ‘Short’ -> ‘No movement’4.4.。‘Whether hold a gun’: ‘True’ -> ‘True’(Removed ‘Whether crouch position’ as it conflicts with ‘Whether prone position’ and prone position givesbetter cover for an ambush.)5.。According to the analyze and the planing of the verbal command, further analyze the behavior tendencyrequired in the adjustment process.Mobile: Need to stop all movement: 0%Offense: Must be ready to attack if necessary: 20%Waiting: The main goal is to set up an ambush and wait: 70%Supplies: No need for supplies during this phase: 0%Scouting: Need to observe the surroundings, but not actively scout: 10%6.。Analyze how long the current command needs to be kept.According to the command of the teammate, taking a hiding position or setting up an ambush is usually donewith the intention of waiting for an opportune moment to strike or observe the enemy‘S movement: The currentcommand needs to be kept by ‘long term’.Table 9: Chain of thought response from GPT4.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.Only select the most relevant and necessary states for planning, and the unplanned states will be adjusted bythe agent itself2.。[Choose 1, Choose 2, ...] indicates the values that can be selected for the state.。When you plan, you can onlychoose the value of the state from it, and do not invent new value not listed in [Choice1, Choice2, ...].3.The selected state can change the current value or maintain the current value.。The agent will try to achieveand maintain the value of the state you choose after you give the plan.4.Agents don’t voluntarily discard items (for example guns, bullets, medical kits) unless items are reduced orset as False in your plan, so there is no need to keep them, only to choose when making changes.5.Do not plan and adjust the states of teammates and enemies, they can move freely and cannot be controlled.6.Avoid conflicts of states planing.. \n",
      "the piece 891\n",
      " \n",
      "essary, and output the modified adjustment plan:According to the states adjustments in 3:4.1.。‘Whether prone position’: ‘False’ -> ‘True’4.2.。‘Average velocity’: ‘Fast’ -> ‘Static’4.3.。‘Length of distance moved’: ‘Short’ -> ‘No movement’4.4.。‘Whether hold a gun’: ‘True’ -> ‘True’(Removed ‘Whether crouch position’ as it conflicts with ‘Whether prone position’ and prone position givesbetter cover for an ambush.)5.。According to the analyze and the planing of the verbal command, further analyze the behavior tendencyrequired in the adjustment process.Mobile: Need to stop all movement: 0%Offense: Must be ready to attack if necessary: 20%Waiting: The main goal is to set up an ambush and wait: 70%Supplies: No need for supplies during this phase: 0%Scouting: Need to observe the surroundings, but not actively scout: 10%6.。Analyze how long the current command needs to be kept.According to the command of the teammate, taking a hiding position or setting up an ambush is usually donewith the intention of waiting for an opportune moment to strike or observe the enemy‘S movement: The currentcommand needs to be kept by ‘long term’.Table 9: Chain of thought response from GPT4.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.Only select the most relevant and necessary states for planning, and the unplanned states will be adjusted bythe agent itself2.。[Choose 1, Choose 2, ...] indicates the values that can be selected for the state.。When you plan, you can onlychoose the value of the state from it, and do not invent new value not listed in [Choice1, Choice2, ...].3.The selected state can change the current value or maintain the current value.。The agent will try to achieveand maintain the value of the state you choose after you give the plan.4.Agents don’t voluntarily discard items (for example guns, bullets, medical kits) unless items are reduced orset as False in your plan, so there is no need to keep them, only to choose when making changes.5.Do not plan and adjust the states of teammates and enemies, they can move freely and cannot be controlled.6.Avoid conflicts of states planing.。For example, agent unable to move quickly when lying down, and unable tosee enemies when length of distance from agent to enemy is far away.7.Avoid the repetition of states planing.. \n",
      "the piece 892\n",
      " \n",
      "1.。‘Whether prone position’: ‘False’ -> ‘True’4.2.。‘Average velocity’: ‘Fast’ -> ‘Static’4.3.。‘Length of distance moved’: ‘Short’ -> ‘No movement’4.4.。‘Whether hold a gun’: ‘True’ -> ‘True’(Removed ‘Whether crouch position’ as it conflicts with ‘Whether prone position’ and prone position givesbetter cover for an ambush.)5.。According to the analyze and the planing of the verbal command, further analyze the behavior tendencyrequired in the adjustment process.Mobile: Need to stop all movement: 0%Offense: Must be ready to attack if necessary: 20%Waiting: The main goal is to set up an ambush and wait: 70%Supplies: No need for supplies during this phase: 0%Scouting: Need to observe the surroundings, but not actively scout: 10%6.。Analyze how long the current command needs to be kept.According to the command of the teammate, taking a hiding position or setting up an ambush is usually donewith the intention of waiting for an opportune moment to strike or observe the enemy‘S movement: The currentcommand needs to be kept by ‘long term’.Table 9: Chain of thought response from GPT4.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.Only select the most relevant and necessary states for planning, and the unplanned states will be adjusted bythe agent itself2.。[Choose 1, Choose 2, ...] indicates the values that can be selected for the state.。When you plan, you can onlychoose the value of the state from it, and do not invent new value not listed in [Choice1, Choice2, ...].3.The selected state can change the current value or maintain the current value.。The agent will try to achieveand maintain the value of the state you choose after you give the plan.4.Agents don’t voluntarily discard items (for example guns, bullets, medical kits) unless items are reduced orset as False in your plan, so there is no need to keep them, only to choose when making changes.5.Do not plan and adjust the states of teammates and enemies, they can move freely and cannot be controlled.6.Avoid conflicts of states planing.。For example, agent unable to move quickly when lying down, and unable tosee enemies when length of distance from agent to enemy is far away.7.Avoid the repetition of states planing.。For example, if the Average velocity has been adjusted to be Fast, thereis no need to adjust the Whether prone position to False, because the agent can automatically adjust state to fitoverlapping meanings.8.When it is necessary to refer to enemy or teammate information for planing, describe the specific state valueduring analysis.Table 10: Rule prompt for GPT4.Dataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)Attention 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.159Mlp 0.549 0.664 0.482 0.587 0.514 0.620 0.065 0.134All 0.529 0.642 0.471 0.581 0.485 0.596 0.069 0.119Table 13: Evaluation on LoRA target.Symbol Sub-goal Classg1Average velocityg2Horizontal direction of movementg3Whether seen enemyg4Whether hold a gung5Whether prone positiong6Length of distance movedg7Length of distance from agent to teammateg8Distance with nearest enemyg9Whether seen by enemyg10Damage to enemyg11Whether have bulletsg12Horizontal direction of viewg13Whether follow with the movement direction of teammateg14Whether crouch positiong15Whether have a gung16Whether have medical kitsg17Whether to restore healthg18Health levelg19Whether knock down enemyg20Whether target the same enemy as teammateTable 14: Top 20 sub-goals ranked by frequency.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationDataset SizeTrainingMethodPrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)100%CoFT 51.85% 64.84% 46.68% 57.91% 49.13% 61.18%CoFT→SFT 55.48% 68.52% 50.48% 62.10% 52.86% 65.15%SFT 54.70% 65.20% 49.00% 60.20% 51.70% 63.20%Improve Rate 1.42% 5.09% 3.02% 3.16% 2.25% 3.09%30%CoFT 49.43% 62.66% 44.45% 56.29% 46.81% 59.30%CoFT→SFT 49.92% 62.59% 45.51% 57.65% 47.61% 60.02%SFT 46.12% 60.96% 33.68% 45.39% 38.93% 52.03%Improve Rate 8.25% 2.68% 35.11% 27.01% 22.30% 15.34%10%CoFT 45.58% 60.84% 41.77% 53.92% 43.59% 57.17%CoFT→SFT 48.06% 61.01% 43.15% 54.31% 45.47% 57.47%SFT 42.08% 55.31% 30.86% 41.45% 35.61% 47.39%Improve Rate 14.20% 10.32% 39.80% 31.03% 27.69% 21.28%3%CoFT 39.42% 58.35% 34.28% 50.10% 36.67% 53.91%CoFT→SFT 41.61% 60.40% 34.55% 50.32% 37.75% 54.90%SFT 17.66% 38.28% 13.33% 29.47% 15.20% 33.30%Improve Rate 135.52% 57.78% 159.15% 70.79% 148.43% 64.88%Table 15: Language model performance evaluation with different sizes of fine-tuning training set.. \n",
      "the piece 893\n",
      " \n",
      "elocity’: ‘Fast’ -> ‘Static’4.3.。‘Length of distance moved’: ‘Short’ -> ‘No movement’4.4.。‘Whether hold a gun’: ‘True’ -> ‘True’(Removed ‘Whether crouch position’ as it conflicts with ‘Whether prone position’ and prone position givesbetter cover for an ambush.)5.。According to the analyze and the planing of the verbal command, further analyze the behavior tendencyrequired in the adjustment process.Mobile: Need to stop all movement: 0%Offense: Must be ready to attack if necessary: 20%Waiting: The main goal is to set up an ambush and wait: 70%Supplies: No need for supplies during this phase: 0%Scouting: Need to observe the surroundings, but not actively scout: 10%6.。Analyze how long the current command needs to be kept.According to the command of the teammate, taking a hiding position or setting up an ambush is usually donewith the intention of waiting for an opportune moment to strike or observe the enemy‘S movement: The currentcommand needs to be kept by ‘long term’.Table 9: Chain of thought response from GPT4.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.Only select the most relevant and necessary states for planning, and the unplanned states will be adjusted bythe agent itself2.。[Choose 1, Choose 2, ...] indicates the values that can be selected for the state.。When you plan, you can onlychoose the value of the state from it, and do not invent new value not listed in [Choice1, Choice2, ...].3.The selected state can change the current value or maintain the current value.。The agent will try to achieveand maintain the value of the state you choose after you give the plan.4.Agents don’t voluntarily discard items (for example guns, bullets, medical kits) unless items are reduced orset as False in your plan, so there is no need to keep them, only to choose when making changes.5.Do not plan and adjust the states of teammates and enemies, they can move freely and cannot be controlled.6.Avoid conflicts of states planing.。For example, agent unable to move quickly when lying down, and unable tosee enemies when length of distance from agent to enemy is far away.7.Avoid the repetition of states planing.。For example, if the Average velocity has been adjusted to be Fast, thereis no need to adjust the Whether prone position to False, because the agent can automatically adjust state to fitoverlapping meanings.8.When it is necessary to refer to enemy or teammate information for planing, describe the specific state valueduring analysis.Table 10: Rule prompt for GPT4.Dataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)Attention 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.159Mlp 0.549 0.664 0.482 0.587 0.514 0.620 0.065 0.134All 0.529 0.642 0.471 0.581 0.485 0.596 0.069 0.119Table 13: Evaluation on LoRA target.Symbol Sub-goal Classg1Average velocityg2Horizontal direction of movementg3Whether seen enemyg4Whether hold a gung5Whether prone positiong6Length of distance movedg7Length of distance from agent to teammateg8Distance with nearest enemyg9Whether seen by enemyg10Damage to enemyg11Whether have bulletsg12Horizontal direction of viewg13Whether follow with the movement direction of teammateg14Whether crouch positiong15Whether have a gung16Whether have medical kitsg17Whether to restore healthg18Health levelg19Whether knock down enemyg20Whether target the same enemy as teammateTable 14: Top 20 sub-goals ranked by frequency.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationDataset SizeTrainingMethodPrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)100%CoFT 51.85% 64.84% 46.68% 57.91% 49.13% 61.18%CoFT→SFT 55.48% 68.52% 50.48% 62.10% 52.86% 65.15%SFT 54.70% 65.20% 49.00% 60.20% 51.70% 63.20%Improve Rate 1.42% 5.09% 3.02% 3.16% 2.25% 3.09%30%CoFT 49.43% 62.66% 44.45% 56.29% 46.81% 59.30%CoFT→SFT 49.92% 62.59% 45.51% 57.65% 47.61% 60.02%SFT 46.12% 60.96% 33.68% 45.39% 38.93% 52.03%Improve Rate 8.25% 2.68% 35.11% 27.01% 22.30% 15.34%10%CoFT 45.58% 60.84% 41.77% 53.92% 43.59% 57.17%CoFT→SFT 48.06% 61.01% 43.15% 54.31% 45.47% 57.47%SFT 42.08% 55.31% 30.86% 41.45% 35.61% 47.39%Improve Rate 14.20% 10.32% 39.80% 31.03% 27.69% 21.28%3%CoFT 39.42% 58.35% 34.28% 50.10% 36.67% 53.91%CoFT→SFT 41.61% 60.40% 34.55% 50.32% 37.75% 54.90%SFT 17.66% 38.28% 13.33% 29.47% 15.20% 33.30%Improve Rate 135.52% 57.78% 159.15% 70.79% 148.43% 64.88%Table 15: Language model performance evaluation with different sizes of fine-tuning training set.。The underlined “ImproveRate” values represent the improvement percentage of the “CoFT →SFT” method relative to “SFT” method.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationIn order to complete the command ‘You should lie in wait’, let us plan the states of the agent step by step usingthe following template:1.. \n",
      "the piece 894\n",
      " \n",
      " movement’4.4.。‘Whether hold a gun’: ‘True’ -> ‘True’(Removed ‘Whether crouch position’ as it conflicts with ‘Whether prone position’ and prone position givesbetter cover for an ambush.)5.。According to the analyze and the planing of the verbal command, further analyze the behavior tendencyrequired in the adjustment process.Mobile: Need to stop all movement: 0%Offense: Must be ready to attack if necessary: 20%Waiting: The main goal is to set up an ambush and wait: 70%Supplies: No need for supplies during this phase: 0%Scouting: Need to observe the surroundings, but not actively scout: 10%6.。Analyze how long the current command needs to be kept.According to the command of the teammate, taking a hiding position or setting up an ambush is usually donewith the intention of waiting for an opportune moment to strike or observe the enemy‘S movement: The currentcommand needs to be kept by ‘long term’.Table 9: Chain of thought response from GPT4.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.Only select the most relevant and necessary states for planning, and the unplanned states will be adjusted bythe agent itself2.。[Choose 1, Choose 2, ...] indicates the values that can be selected for the state.。When you plan, you can onlychoose the value of the state from it, and do not invent new value not listed in [Choice1, Choice2, ...].3.The selected state can change the current value or maintain the current value.。The agent will try to achieveand maintain the value of the state you choose after you give the plan.4.Agents don’t voluntarily discard items (for example guns, bullets, medical kits) unless items are reduced orset as False in your plan, so there is no need to keep them, only to choose when making changes.5.Do not plan and adjust the states of teammates and enemies, they can move freely and cannot be controlled.6.Avoid conflicts of states planing.。For example, agent unable to move quickly when lying down, and unable tosee enemies when length of distance from agent to enemy is far away.7.Avoid the repetition of states planing.。For example, if the Average velocity has been adjusted to be Fast, thereis no need to adjust the Whether prone position to False, because the agent can automatically adjust state to fitoverlapping meanings.8.When it is necessary to refer to enemy or teammate information for planing, describe the specific state valueduring analysis.Table 10: Rule prompt for GPT4.Dataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)Attention 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.159Mlp 0.549 0.664 0.482 0.587 0.514 0.620 0.065 0.134All 0.529 0.642 0.471 0.581 0.485 0.596 0.069 0.119Table 13: Evaluation on LoRA target.Symbol Sub-goal Classg1Average velocityg2Horizontal direction of movementg3Whether seen enemyg4Whether hold a gung5Whether prone positiong6Length of distance movedg7Length of distance from agent to teammateg8Distance with nearest enemyg9Whether seen by enemyg10Damage to enemyg11Whether have bulletsg12Horizontal direction of viewg13Whether follow with the movement direction of teammateg14Whether crouch positiong15Whether have a gung16Whether have medical kitsg17Whether to restore healthg18Health levelg19Whether knock down enemyg20Whether target the same enemy as teammateTable 14: Top 20 sub-goals ranked by frequency.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationDataset SizeTrainingMethodPrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)100%CoFT 51.85% 64.84% 46.68% 57.91% 49.13% 61.18%CoFT→SFT 55.48% 68.52% 50.48% 62.10% 52.86% 65.15%SFT 54.70% 65.20% 49.00% 60.20% 51.70% 63.20%Improve Rate 1.42% 5.09% 3.02% 3.16% 2.25% 3.09%30%CoFT 49.43% 62.66% 44.45% 56.29% 46.81% 59.30%CoFT→SFT 49.92% 62.59% 45.51% 57.65% 47.61% 60.02%SFT 46.12% 60.96% 33.68% 45.39% 38.93% 52.03%Improve Rate 8.25% 2.68% 35.11% 27.01% 22.30% 15.34%10%CoFT 45.58% 60.84% 41.77% 53.92% 43.59% 57.17%CoFT→SFT 48.06% 61.01% 43.15% 54.31% 45.47% 57.47%SFT 42.08% 55.31% 30.86% 41.45% 35.61% 47.39%Improve Rate 14.20% 10.32% 39.80% 31.03% 27.69% 21.28%3%CoFT 39.42% 58.35% 34.28% 50.10% 36.67% 53.91%CoFT→SFT 41.61% 60.40% 34.55% 50.32% 37.75% 54.90%SFT 17.66% 38.28% 13.33% 29.47% 15.20% 33.30%Improve Rate 135.52% 57.78% 159.15% 70.79% 148.43% 64.88%Table 15: Language model performance evaluation with different sizes of fine-tuning training set.。The underlined “ImproveRate” values represent the improvement percentage of the “CoFT →SFT” method relative to “SFT” method.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationIn order to complete the command ‘You should lie in wait’, let us plan the states of the agent step by step usingthe following template:1.。Analyze the verbal orders of teammates and players, what do you want to do?. \n",
      "the piece 895\n",
      " \n",
      "ition’ as it conflicts with ‘Whether prone position’ and prone position givesbetter cover for an ambush.)5.。According to the analyze and the planing of the verbal command, further analyze the behavior tendencyrequired in the adjustment process.Mobile: Need to stop all movement: 0%Offense: Must be ready to attack if necessary: 20%Waiting: The main goal is to set up an ambush and wait: 70%Supplies: No need for supplies during this phase: 0%Scouting: Need to observe the surroundings, but not actively scout: 10%6.。Analyze how long the current command needs to be kept.According to the command of the teammate, taking a hiding position or setting up an ambush is usually donewith the intention of waiting for an opportune moment to strike or observe the enemy‘S movement: The currentcommand needs to be kept by ‘long term’.Table 9: Chain of thought response from GPT4.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.Only select the most relevant and necessary states for planning, and the unplanned states will be adjusted bythe agent itself2.。[Choose 1, Choose 2, ...] indicates the values that can be selected for the state.。When you plan, you can onlychoose the value of the state from it, and do not invent new value not listed in [Choice1, Choice2, ...].3.The selected state can change the current value or maintain the current value.。The agent will try to achieveand maintain the value of the state you choose after you give the plan.4.Agents don’t voluntarily discard items (for example guns, bullets, medical kits) unless items are reduced orset as False in your plan, so there is no need to keep them, only to choose when making changes.5.Do not plan and adjust the states of teammates and enemies, they can move freely and cannot be controlled.6.Avoid conflicts of states planing.。For example, agent unable to move quickly when lying down, and unable tosee enemies when length of distance from agent to enemy is far away.7.Avoid the repetition of states planing.。For example, if the Average velocity has been adjusted to be Fast, thereis no need to adjust the Whether prone position to False, because the agent can automatically adjust state to fitoverlapping meanings.8.When it is necessary to refer to enemy or teammate information for planing, describe the specific state valueduring analysis.Table 10: Rule prompt for GPT4.Dataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)Attention 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.159Mlp 0.549 0.664 0.482 0.587 0.514 0.620 0.065 0.134All 0.529 0.642 0.471 0.581 0.485 0.596 0.069 0.119Table 13: Evaluation on LoRA target.Symbol Sub-goal Classg1Average velocityg2Horizontal direction of movementg3Whether seen enemyg4Whether hold a gung5Whether prone positiong6Length of distance movedg7Length of distance from agent to teammateg8Distance with nearest enemyg9Whether seen by enemyg10Damage to enemyg11Whether have bulletsg12Horizontal direction of viewg13Whether follow with the movement direction of teammateg14Whether crouch positiong15Whether have a gung16Whether have medical kitsg17Whether to restore healthg18Health levelg19Whether knock down enemyg20Whether target the same enemy as teammateTable 14: Top 20 sub-goals ranked by frequency.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationDataset SizeTrainingMethodPrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)100%CoFT 51.85% 64.84% 46.68% 57.91% 49.13% 61.18%CoFT→SFT 55.48% 68.52% 50.48% 62.10% 52.86% 65.15%SFT 54.70% 65.20% 49.00% 60.20% 51.70% 63.20%Improve Rate 1.42% 5.09% 3.02% 3.16% 2.25% 3.09%30%CoFT 49.43% 62.66% 44.45% 56.29% 46.81% 59.30%CoFT→SFT 49.92% 62.59% 45.51% 57.65% 47.61% 60.02%SFT 46.12% 60.96% 33.68% 45.39% 38.93% 52.03%Improve Rate 8.25% 2.68% 35.11% 27.01% 22.30% 15.34%10%CoFT 45.58% 60.84% 41.77% 53.92% 43.59% 57.17%CoFT→SFT 48.06% 61.01% 43.15% 54.31% 45.47% 57.47%SFT 42.08% 55.31% 30.86% 41.45% 35.61% 47.39%Improve Rate 14.20% 10.32% 39.80% 31.03% 27.69% 21.28%3%CoFT 39.42% 58.35% 34.28% 50.10% 36.67% 53.91%CoFT→SFT 41.61% 60.40% 34.55% 50.32% 37.75% 54.90%SFT 17.66% 38.28% 13.33% 29.47% 15.20% 33.30%Improve Rate 135.52% 57.78% 159.15% 70.79% 148.43% 64.88%Table 15: Language model performance evaluation with different sizes of fine-tuning training set.。The underlined “ImproveRate” values represent the improvement percentage of the “CoFT →SFT” method relative to “SFT” method.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationIn order to complete the command ‘You should lie in wait’, let us plan the states of the agent step by step usingthe following template:1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is [Command], which means teammate player wants the agent...2.. \n",
      "the piece 896\n",
      " \n",
      "adjustment process.Mobile: Need to stop all movement: 0%Offense: Must be ready to attack if necessary: 20%Waiting: The main goal is to set up an ambush and wait: 70%Supplies: No need for supplies during this phase: 0%Scouting: Need to observe the surroundings, but not actively scout: 10%6.。Analyze how long the current command needs to be kept.According to the command of the teammate, taking a hiding position or setting up an ambush is usually donewith the intention of waiting for an opportune moment to strike or observe the enemy‘S movement: The currentcommand needs to be kept by ‘long term’.Table 9: Chain of thought response from GPT4.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.Only select the most relevant and necessary states for planning, and the unplanned states will be adjusted bythe agent itself2.。[Choose 1, Choose 2, ...] indicates the values that can be selected for the state.。When you plan, you can onlychoose the value of the state from it, and do not invent new value not listed in [Choice1, Choice2, ...].3.The selected state can change the current value or maintain the current value.。The agent will try to achieveand maintain the value of the state you choose after you give the plan.4.Agents don’t voluntarily discard items (for example guns, bullets, medical kits) unless items are reduced orset as False in your plan, so there is no need to keep them, only to choose when making changes.5.Do not plan and adjust the states of teammates and enemies, they can move freely and cannot be controlled.6.Avoid conflicts of states planing.。For example, agent unable to move quickly when lying down, and unable tosee enemies when length of distance from agent to enemy is far away.7.Avoid the repetition of states planing.。For example, if the Average velocity has been adjusted to be Fast, thereis no need to adjust the Whether prone position to False, because the agent can automatically adjust state to fitoverlapping meanings.8.When it is necessary to refer to enemy or teammate information for planing, describe the specific state valueduring analysis.Table 10: Rule prompt for GPT4.Dataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)Attention 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.159Mlp 0.549 0.664 0.482 0.587 0.514 0.620 0.065 0.134All 0.529 0.642 0.471 0.581 0.485 0.596 0.069 0.119Table 13: Evaluation on LoRA target.Symbol Sub-goal Classg1Average velocityg2Horizontal direction of movementg3Whether seen enemyg4Whether hold a gung5Whether prone positiong6Length of distance movedg7Length of distance from agent to teammateg8Distance with nearest enemyg9Whether seen by enemyg10Damage to enemyg11Whether have bulletsg12Horizontal direction of viewg13Whether follow with the movement direction of teammateg14Whether crouch positiong15Whether have a gung16Whether have medical kitsg17Whether to restore healthg18Health levelg19Whether knock down enemyg20Whether target the same enemy as teammateTable 14: Top 20 sub-goals ranked by frequency.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationDataset SizeTrainingMethodPrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)100%CoFT 51.85% 64.84% 46.68% 57.91% 49.13% 61.18%CoFT→SFT 55.48% 68.52% 50.48% 62.10% 52.86% 65.15%SFT 54.70% 65.20% 49.00% 60.20% 51.70% 63.20%Improve Rate 1.42% 5.09% 3.02% 3.16% 2.25% 3.09%30%CoFT 49.43% 62.66% 44.45% 56.29% 46.81% 59.30%CoFT→SFT 49.92% 62.59% 45.51% 57.65% 47.61% 60.02%SFT 46.12% 60.96% 33.68% 45.39% 38.93% 52.03%Improve Rate 8.25% 2.68% 35.11% 27.01% 22.30% 15.34%10%CoFT 45.58% 60.84% 41.77% 53.92% 43.59% 57.17%CoFT→SFT 48.06% 61.01% 43.15% 54.31% 45.47% 57.47%SFT 42.08% 55.31% 30.86% 41.45% 35.61% 47.39%Improve Rate 14.20% 10.32% 39.80% 31.03% 27.69% 21.28%3%CoFT 39.42% 58.35% 34.28% 50.10% 36.67% 53.91%CoFT→SFT 41.61% 60.40% 34.55% 50.32% 37.75% 54.90%SFT 17.66% 38.28% 13.33% 29.47% 15.20% 33.30%Improve Rate 135.52% 57.78% 159.15% 70.79% 148.43% 64.88%Table 15: Language model performance evaluation with different sizes of fine-tuning training set.。The underlined “ImproveRate” values represent the improvement percentage of the “CoFT →SFT” method relative to “SFT” method.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationIn order to complete the command ‘You should lie in wait’, let us plan the states of the agent step by step usingthe following template:1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is [Command], which means teammate player wants the agent...2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.. \n",
      "the piece 897\n",
      " \n",
      "ent: 0%Offense: Must be ready to attack if necessary: 20%Waiting: The main goal is to set up an ambush and wait: 70%Supplies: No need for supplies during this phase: 0%Scouting: Need to observe the surroundings, but not actively scout: 10%6.。Analyze how long the current command needs to be kept.According to the command of the teammate, taking a hiding position or setting up an ambush is usually donewith the intention of waiting for an opportune moment to strike or observe the enemy‘S movement: The currentcommand needs to be kept by ‘long term’.Table 9: Chain of thought response from GPT4.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.Only select the most relevant and necessary states for planning, and the unplanned states will be adjusted bythe agent itself2.。[Choose 1, Choose 2, ...] indicates the values that can be selected for the state.。When you plan, you can onlychoose the value of the state from it, and do not invent new value not listed in [Choice1, Choice2, ...].3.The selected state can change the current value or maintain the current value.。The agent will try to achieveand maintain the value of the state you choose after you give the plan.4.Agents don’t voluntarily discard items (for example guns, bullets, medical kits) unless items are reduced orset as False in your plan, so there is no need to keep them, only to choose when making changes.5.Do not plan and adjust the states of teammates and enemies, they can move freely and cannot be controlled.6.Avoid conflicts of states planing.。For example, agent unable to move quickly when lying down, and unable tosee enemies when length of distance from agent to enemy is far away.7.Avoid the repetition of states planing.。For example, if the Average velocity has been adjusted to be Fast, thereis no need to adjust the Whether prone position to False, because the agent can automatically adjust state to fitoverlapping meanings.8.When it is necessary to refer to enemy or teammate information for planing, describe the specific state valueduring analysis.Table 10: Rule prompt for GPT4.Dataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)Attention 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.159Mlp 0.549 0.664 0.482 0.587 0.514 0.620 0.065 0.134All 0.529 0.642 0.471 0.581 0.485 0.596 0.069 0.119Table 13: Evaluation on LoRA target.Symbol Sub-goal Classg1Average velocityg2Horizontal direction of movementg3Whether seen enemyg4Whether hold a gung5Whether prone positiong6Length of distance movedg7Length of distance from agent to teammateg8Distance with nearest enemyg9Whether seen by enemyg10Damage to enemyg11Whether have bulletsg12Horizontal direction of viewg13Whether follow with the movement direction of teammateg14Whether crouch positiong15Whether have a gung16Whether have medical kitsg17Whether to restore healthg18Health levelg19Whether knock down enemyg20Whether target the same enemy as teammateTable 14: Top 20 sub-goals ranked by frequency.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationDataset SizeTrainingMethodPrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)100%CoFT 51.85% 64.84% 46.68% 57.91% 49.13% 61.18%CoFT→SFT 55.48% 68.52% 50.48% 62.10% 52.86% 65.15%SFT 54.70% 65.20% 49.00% 60.20% 51.70% 63.20%Improve Rate 1.42% 5.09% 3.02% 3.16% 2.25% 3.09%30%CoFT 49.43% 62.66% 44.45% 56.29% 46.81% 59.30%CoFT→SFT 49.92% 62.59% 45.51% 57.65% 47.61% 60.02%SFT 46.12% 60.96% 33.68% 45.39% 38.93% 52.03%Improve Rate 8.25% 2.68% 35.11% 27.01% 22.30% 15.34%10%CoFT 45.58% 60.84% 41.77% 53.92% 43.59% 57.17%CoFT→SFT 48.06% 61.01% 43.15% 54.31% 45.47% 57.47%SFT 42.08% 55.31% 30.86% 41.45% 35.61% 47.39%Improve Rate 14.20% 10.32% 39.80% 31.03% 27.69% 21.28%3%CoFT 39.42% 58.35% 34.28% 50.10% 36.67% 53.91%CoFT→SFT 41.61% 60.40% 34.55% 50.32% 37.75% 54.90%SFT 17.66% 38.28% 13.33% 29.47% 15.20% 33.30%Improve Rate 135.52% 57.78% 159.15% 70.79% 148.43% 64.88%Table 15: Language model performance evaluation with different sizes of fine-tuning training set.。The underlined “ImproveRate” values represent the improvement percentage of the “CoFT →SFT” method relative to “SFT” method.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationIn order to complete the command ‘You should lie in wait’, let us plan the states of the agent step by step usingthe following template:1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is [Command], which means teammate player wants the agent...2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan (analyze the reason first, then select keystates one by one as few as possible and as important as possible according to the degree of importance)?According to the teammate’s command:2.1.. \n",
      "the piece 898\n",
      " \n",
      "ecessary: 20%Waiting: The main goal is to set up an ambush and wait: 70%Supplies: No need for supplies during this phase: 0%Scouting: Need to observe the surroundings, but not actively scout: 10%6.。Analyze how long the current command needs to be kept.According to the command of the teammate, taking a hiding position or setting up an ambush is usually donewith the intention of waiting for an opportune moment to strike or observe the enemy‘S movement: The currentcommand needs to be kept by ‘long term’.Table 9: Chain of thought response from GPT4.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.Only select the most relevant and necessary states for planning, and the unplanned states will be adjusted bythe agent itself2.。[Choose 1, Choose 2, ...] indicates the values that can be selected for the state.。When you plan, you can onlychoose the value of the state from it, and do not invent new value not listed in [Choice1, Choice2, ...].3.The selected state can change the current value or maintain the current value.。The agent will try to achieveand maintain the value of the state you choose after you give the plan.4.Agents don’t voluntarily discard items (for example guns, bullets, medical kits) unless items are reduced orset as False in your plan, so there is no need to keep them, only to choose when making changes.5.Do not plan and adjust the states of teammates and enemies, they can move freely and cannot be controlled.6.Avoid conflicts of states planing.。For example, agent unable to move quickly when lying down, and unable tosee enemies when length of distance from agent to enemy is far away.7.Avoid the repetition of states planing.。For example, if the Average velocity has been adjusted to be Fast, thereis no need to adjust the Whether prone position to False, because the agent can automatically adjust state to fitoverlapping meanings.8.When it is necessary to refer to enemy or teammate information for planing, describe the specific state valueduring analysis.Table 10: Rule prompt for GPT4.Dataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)Attention 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.159Mlp 0.549 0.664 0.482 0.587 0.514 0.620 0.065 0.134All 0.529 0.642 0.471 0.581 0.485 0.596 0.069 0.119Table 13: Evaluation on LoRA target.Symbol Sub-goal Classg1Average velocityg2Horizontal direction of movementg3Whether seen enemyg4Whether hold a gung5Whether prone positiong6Length of distance movedg7Length of distance from agent to teammateg8Distance with nearest enemyg9Whether seen by enemyg10Damage to enemyg11Whether have bulletsg12Horizontal direction of viewg13Whether follow with the movement direction of teammateg14Whether crouch positiong15Whether have a gung16Whether have medical kitsg17Whether to restore healthg18Health levelg19Whether knock down enemyg20Whether target the same enemy as teammateTable 14: Top 20 sub-goals ranked by frequency.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationDataset SizeTrainingMethodPrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)100%CoFT 51.85% 64.84% 46.68% 57.91% 49.13% 61.18%CoFT→SFT 55.48% 68.52% 50.48% 62.10% 52.86% 65.15%SFT 54.70% 65.20% 49.00% 60.20% 51.70% 63.20%Improve Rate 1.42% 5.09% 3.02% 3.16% 2.25% 3.09%30%CoFT 49.43% 62.66% 44.45% 56.29% 46.81% 59.30%CoFT→SFT 49.92% 62.59% 45.51% 57.65% 47.61% 60.02%SFT 46.12% 60.96% 33.68% 45.39% 38.93% 52.03%Improve Rate 8.25% 2.68% 35.11% 27.01% 22.30% 15.34%10%CoFT 45.58% 60.84% 41.77% 53.92% 43.59% 57.17%CoFT→SFT 48.06% 61.01% 43.15% 54.31% 45.47% 57.47%SFT 42.08% 55.31% 30.86% 41.45% 35.61% 47.39%Improve Rate 14.20% 10.32% 39.80% 31.03% 27.69% 21.28%3%CoFT 39.42% 58.35% 34.28% 50.10% 36.67% 53.91%CoFT→SFT 41.61% 60.40% 34.55% 50.32% 37.75% 54.90%SFT 17.66% 38.28% 13.33% 29.47% 15.20% 33.30%Improve Rate 135.52% 57.78% 159.15% 70.79% 148.43% 64.88%Table 15: Language model performance evaluation with different sizes of fine-tuning training set.。The underlined “ImproveRate” values represent the improvement percentage of the “CoFT →SFT” method relative to “SFT” method.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationIn order to complete the command ‘You should lie in wait’, let us plan the states of the agent step by step usingthe following template:1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is [Command], which means teammate player wants the agent...2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan (analyze the reason first, then select keystates one by one as few as possible and as important as possible according to the degree of importance)?According to the teammate’s command:2.1.。[Reason1]: [State1]2.2.. \n",
      "the piece 899\n",
      " \n",
      " and wait: 70%Supplies: No need for supplies during this phase: 0%Scouting: Need to observe the surroundings, but not actively scout: 10%6.。Analyze how long the current command needs to be kept.According to the command of the teammate, taking a hiding position or setting up an ambush is usually donewith the intention of waiting for an opportune moment to strike or observe the enemy‘S movement: The currentcommand needs to be kept by ‘long term’.Table 9: Chain of thought response from GPT4.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.Only select the most relevant and necessary states for planning, and the unplanned states will be adjusted bythe agent itself2.。[Choose 1, Choose 2, ...] indicates the values that can be selected for the state.。When you plan, you can onlychoose the value of the state from it, and do not invent new value not listed in [Choice1, Choice2, ...].3.The selected state can change the current value or maintain the current value.。The agent will try to achieveand maintain the value of the state you choose after you give the plan.4.Agents don’t voluntarily discard items (for example guns, bullets, medical kits) unless items are reduced orset as False in your plan, so there is no need to keep them, only to choose when making changes.5.Do not plan and adjust the states of teammates and enemies, they can move freely and cannot be controlled.6.Avoid conflicts of states planing.。For example, agent unable to move quickly when lying down, and unable tosee enemies when length of distance from agent to enemy is far away.7.Avoid the repetition of states planing.。For example, if the Average velocity has been adjusted to be Fast, thereis no need to adjust the Whether prone position to False, because the agent can automatically adjust state to fitoverlapping meanings.8.When it is necessary to refer to enemy or teammate information for planing, describe the specific state valueduring analysis.Table 10: Rule prompt for GPT4.Dataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)Attention 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.159Mlp 0.549 0.664 0.482 0.587 0.514 0.620 0.065 0.134All 0.529 0.642 0.471 0.581 0.485 0.596 0.069 0.119Table 13: Evaluation on LoRA target.Symbol Sub-goal Classg1Average velocityg2Horizontal direction of movementg3Whether seen enemyg4Whether hold a gung5Whether prone positiong6Length of distance movedg7Length of distance from agent to teammateg8Distance with nearest enemyg9Whether seen by enemyg10Damage to enemyg11Whether have bulletsg12Horizontal direction of viewg13Whether follow with the movement direction of teammateg14Whether crouch positiong15Whether have a gung16Whether have medical kitsg17Whether to restore healthg18Health levelg19Whether knock down enemyg20Whether target the same enemy as teammateTable 14: Top 20 sub-goals ranked by frequency.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationDataset SizeTrainingMethodPrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)100%CoFT 51.85% 64.84% 46.68% 57.91% 49.13% 61.18%CoFT→SFT 55.48% 68.52% 50.48% 62.10% 52.86% 65.15%SFT 54.70% 65.20% 49.00% 60.20% 51.70% 63.20%Improve Rate 1.42% 5.09% 3.02% 3.16% 2.25% 3.09%30%CoFT 49.43% 62.66% 44.45% 56.29% 46.81% 59.30%CoFT→SFT 49.92% 62.59% 45.51% 57.65% 47.61% 60.02%SFT 46.12% 60.96% 33.68% 45.39% 38.93% 52.03%Improve Rate 8.25% 2.68% 35.11% 27.01% 22.30% 15.34%10%CoFT 45.58% 60.84% 41.77% 53.92% 43.59% 57.17%CoFT→SFT 48.06% 61.01% 43.15% 54.31% 45.47% 57.47%SFT 42.08% 55.31% 30.86% 41.45% 35.61% 47.39%Improve Rate 14.20% 10.32% 39.80% 31.03% 27.69% 21.28%3%CoFT 39.42% 58.35% 34.28% 50.10% 36.67% 53.91%CoFT→SFT 41.61% 60.40% 34.55% 50.32% 37.75% 54.90%SFT 17.66% 38.28% 13.33% 29.47% 15.20% 33.30%Improve Rate 135.52% 57.78% 159.15% 70.79% 148.43% 64.88%Table 15: Language model performance evaluation with different sizes of fine-tuning training set.。The underlined “ImproveRate” values represent the improvement percentage of the “CoFT →SFT” method relative to “SFT” method.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationIn order to complete the command ‘You should lie in wait’, let us plan the states of the agent step by step usingthe following template:1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is [Command], which means teammate player wants the agent...2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan (analyze the reason first, then select keystates one by one as few as possible and as important as possible according to the degree of importance)?According to the teammate’s command:2.1.。[Reason1]: [State1]2.2.。[Reason2]: [State2]...3.. \n",
      "the piece 900\n",
      " \n",
      "d needs to be kept.According to the command of the teammate, taking a hiding position or setting up an ambush is usually donewith the intention of waiting for an opportune moment to strike or observe the enemy‘S movement: The currentcommand needs to be kept by ‘long term’.Table 9: Chain of thought response from GPT4.Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation1.Only select the most relevant and necessary states for planning, and the unplanned states will be adjusted bythe agent itself2.。[Choose 1, Choose 2, ...] indicates the values that can be selected for the state.。When you plan, you can onlychoose the value of the state from it, and do not invent new value not listed in [Choice1, Choice2, ...].3.The selected state can change the current value or maintain the current value.。The agent will try to achieveand maintain the value of the state you choose after you give the plan.4.Agents don’t voluntarily discard items (for example guns, bullets, medical kits) unless items are reduced orset as False in your plan, so there is no need to keep them, only to choose when making changes.5.Do not plan and adjust the states of teammates and enemies, they can move freely and cannot be controlled.6.Avoid conflicts of states planing.。For example, agent unable to move quickly when lying down, and unable tosee enemies when length of distance from agent to enemy is far away.7.Avoid the repetition of states planing.。For example, if the Average velocity has been adjusted to be Fast, thereis no need to adjust the Whether prone position to False, because the agent can automatically adjust state to fitoverlapping meanings.8.When it is necessary to refer to enemy or teammate information for planing, describe the specific state valueduring analysis.Table 10: Rule prompt for GPT4.Dataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)Attention 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.159Mlp 0.549 0.664 0.482 0.587 0.514 0.620 0.065 0.134All 0.529 0.642 0.471 0.581 0.485 0.596 0.069 0.119Table 13: Evaluation on LoRA target.Symbol Sub-goal Classg1Average velocityg2Horizontal direction of movementg3Whether seen enemyg4Whether hold a gung5Whether prone positiong6Length of distance movedg7Length of distance from agent to teammateg8Distance with nearest enemyg9Whether seen by enemyg10Damage to enemyg11Whether have bulletsg12Horizontal direction of viewg13Whether follow with the movement direction of teammateg14Whether crouch positiong15Whether have a gung16Whether have medical kitsg17Whether to restore healthg18Health levelg19Whether knock down enemyg20Whether target the same enemy as teammateTable 14: Top 20 sub-goals ranked by frequency.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationDataset SizeTrainingMethodPrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)100%CoFT 51.85% 64.84% 46.68% 57.91% 49.13% 61.18%CoFT→SFT 55.48% 68.52% 50.48% 62.10% 52.86% 65.15%SFT 54.70% 65.20% 49.00% 60.20% 51.70% 63.20%Improve Rate 1.42% 5.09% 3.02% 3.16% 2.25% 3.09%30%CoFT 49.43% 62.66% 44.45% 56.29% 46.81% 59.30%CoFT→SFT 49.92% 62.59% 45.51% 57.65% 47.61% 60.02%SFT 46.12% 60.96% 33.68% 45.39% 38.93% 52.03%Improve Rate 8.25% 2.68% 35.11% 27.01% 22.30% 15.34%10%CoFT 45.58% 60.84% 41.77% 53.92% 43.59% 57.17%CoFT→SFT 48.06% 61.01% 43.15% 54.31% 45.47% 57.47%SFT 42.08% 55.31% 30.86% 41.45% 35.61% 47.39%Improve Rate 14.20% 10.32% 39.80% 31.03% 27.69% 21.28%3%CoFT 39.42% 58.35% 34.28% 50.10% 36.67% 53.91%CoFT→SFT 41.61% 60.40% 34.55% 50.32% 37.75% 54.90%SFT 17.66% 38.28% 13.33% 29.47% 15.20% 33.30%Improve Rate 135.52% 57.78% 159.15% 70.79% 148.43% 64.88%Table 15: Language model performance evaluation with different sizes of fine-tuning training set.。The underlined “ImproveRate” values represent the improvement percentage of the “CoFT →SFT” method relative to “SFT” method.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationIn order to complete the command ‘You should lie in wait’, let us plan the states of the agent step by step usingthe following template:1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is [Command], which means teammate player wants the agent...2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan (analyze the reason first, then select keystates one by one as few as possible and as important as possible according to the degree of importance)?According to the teammate’s command:2.1.。[Reason1]: [State1]2.2.。[Reason2]: [State2]...3.。Plan how these key states need to be adjusted (analyze the reason first, and then make adjustments one stateby one state, the state can be changed or remain the same, and must be selected from the value range of thegame state [Choice 1, Choice 2, ...])?According to the teammate’s command:3.1.. \n",
      "the piece 901\n",
      " \n",
      "t the most relevant and necessary states for planning, and the unplanned states will be adjusted bythe agent itself2.。[Choose 1, Choose 2, ...] indicates the values that can be selected for the state.。When you plan, you can onlychoose the value of the state from it, and do not invent new value not listed in [Choice1, Choice2, ...].3.The selected state can change the current value or maintain the current value.。The agent will try to achieveand maintain the value of the state you choose after you give the plan.4.Agents don’t voluntarily discard items (for example guns, bullets, medical kits) unless items are reduced orset as False in your plan, so there is no need to keep them, only to choose when making changes.5.Do not plan and adjust the states of teammates and enemies, they can move freely and cannot be controlled.6.Avoid conflicts of states planing.。For example, agent unable to move quickly when lying down, and unable tosee enemies when length of distance from agent to enemy is far away.7.Avoid the repetition of states planing.。For example, if the Average velocity has been adjusted to be Fast, thereis no need to adjust the Whether prone position to False, because the agent can automatically adjust state to fitoverlapping meanings.8.When it is necessary to refer to enemy or teammate information for planing, describe the specific state valueduring analysis.Table 10: Rule prompt for GPT4.Dataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)Attention 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.159Mlp 0.549 0.664 0.482 0.587 0.514 0.620 0.065 0.134All 0.529 0.642 0.471 0.581 0.485 0.596 0.069 0.119Table 13: Evaluation on LoRA target.Symbol Sub-goal Classg1Average velocityg2Horizontal direction of movementg3Whether seen enemyg4Whether hold a gung5Whether prone positiong6Length of distance movedg7Length of distance from agent to teammateg8Distance with nearest enemyg9Whether seen by enemyg10Damage to enemyg11Whether have bulletsg12Horizontal direction of viewg13Whether follow with the movement direction of teammateg14Whether crouch positiong15Whether have a gung16Whether have medical kitsg17Whether to restore healthg18Health levelg19Whether knock down enemyg20Whether target the same enemy as teammateTable 14: Top 20 sub-goals ranked by frequency.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationDataset SizeTrainingMethodPrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)100%CoFT 51.85% 64.84% 46.68% 57.91% 49.13% 61.18%CoFT→SFT 55.48% 68.52% 50.48% 62.10% 52.86% 65.15%SFT 54.70% 65.20% 49.00% 60.20% 51.70% 63.20%Improve Rate 1.42% 5.09% 3.02% 3.16% 2.25% 3.09%30%CoFT 49.43% 62.66% 44.45% 56.29% 46.81% 59.30%CoFT→SFT 49.92% 62.59% 45.51% 57.65% 47.61% 60.02%SFT 46.12% 60.96% 33.68% 45.39% 38.93% 52.03%Improve Rate 8.25% 2.68% 35.11% 27.01% 22.30% 15.34%10%CoFT 45.58% 60.84% 41.77% 53.92% 43.59% 57.17%CoFT→SFT 48.06% 61.01% 43.15% 54.31% 45.47% 57.47%SFT 42.08% 55.31% 30.86% 41.45% 35.61% 47.39%Improve Rate 14.20% 10.32% 39.80% 31.03% 27.69% 21.28%3%CoFT 39.42% 58.35% 34.28% 50.10% 36.67% 53.91%CoFT→SFT 41.61% 60.40% 34.55% 50.32% 37.75% 54.90%SFT 17.66% 38.28% 13.33% 29.47% 15.20% 33.30%Improve Rate 135.52% 57.78% 159.15% 70.79% 148.43% 64.88%Table 15: Language model performance evaluation with different sizes of fine-tuning training set.。The underlined “ImproveRate” values represent the improvement percentage of the “CoFT →SFT” method relative to “SFT” method.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationIn order to complete the command ‘You should lie in wait’, let us plan the states of the agent step by step usingthe following template:1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is [Command], which means teammate player wants the agent...2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan (analyze the reason first, then select keystates one by one as few as possible and as important as possible according to the degree of importance)?According to the teammate’s command:2.1.。[Reason1]: [State1]2.2.。[Reason2]: [State2]...3.。Plan how these key states need to be adjusted (analyze the reason first, and then make adjustments one stateby one state, the state can be changed or remain the same, and must be selected from the value range of thegame state [Choice 1, Choice 2, ...])?According to the teammate’s command:3.1.。[State1]: [Reason1]: [Current_value1] ->[Target_value2]3.2.. \n",
      "the piece 902\n",
      " \n",
      "mple guns, bullets, medical kits) unless items are reduced orset as False in your plan, so there is no need to keep them, only to choose when making changes.5.Do not plan and adjust the states of teammates and enemies, they can move freely and cannot be controlled.6.Avoid conflicts of states planing.。For example, agent unable to move quickly when lying down, and unable tosee enemies when length of distance from agent to enemy is far away.7.Avoid the repetition of states planing.。For example, if the Average velocity has been adjusted to be Fast, thereis no need to adjust the Whether prone position to False, because the agent can automatically adjust state to fitoverlapping meanings.8.When it is necessary to refer to enemy or teammate information for planing, describe the specific state valueduring analysis.Table 10: Rule prompt for GPT4.Dataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)Attention 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.159Mlp 0.549 0.664 0.482 0.587 0.514 0.620 0.065 0.134All 0.529 0.642 0.471 0.581 0.485 0.596 0.069 0.119Table 13: Evaluation on LoRA target.Symbol Sub-goal Classg1Average velocityg2Horizontal direction of movementg3Whether seen enemyg4Whether hold a gung5Whether prone positiong6Length of distance movedg7Length of distance from agent to teammateg8Distance with nearest enemyg9Whether seen by enemyg10Damage to enemyg11Whether have bulletsg12Horizontal direction of viewg13Whether follow with the movement direction of teammateg14Whether crouch positiong15Whether have a gung16Whether have medical kitsg17Whether to restore healthg18Health levelg19Whether knock down enemyg20Whether target the same enemy as teammateTable 14: Top 20 sub-goals ranked by frequency.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationDataset SizeTrainingMethodPrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)100%CoFT 51.85% 64.84% 46.68% 57.91% 49.13% 61.18%CoFT→SFT 55.48% 68.52% 50.48% 62.10% 52.86% 65.15%SFT 54.70% 65.20% 49.00% 60.20% 51.70% 63.20%Improve Rate 1.42% 5.09% 3.02% 3.16% 2.25% 3.09%30%CoFT 49.43% 62.66% 44.45% 56.29% 46.81% 59.30%CoFT→SFT 49.92% 62.59% 45.51% 57.65% 47.61% 60.02%SFT 46.12% 60.96% 33.68% 45.39% 38.93% 52.03%Improve Rate 8.25% 2.68% 35.11% 27.01% 22.30% 15.34%10%CoFT 45.58% 60.84% 41.77% 53.92% 43.59% 57.17%CoFT→SFT 48.06% 61.01% 43.15% 54.31% 45.47% 57.47%SFT 42.08% 55.31% 30.86% 41.45% 35.61% 47.39%Improve Rate 14.20% 10.32% 39.80% 31.03% 27.69% 21.28%3%CoFT 39.42% 58.35% 34.28% 50.10% 36.67% 53.91%CoFT→SFT 41.61% 60.40% 34.55% 50.32% 37.75% 54.90%SFT 17.66% 38.28% 13.33% 29.47% 15.20% 33.30%Improve Rate 135.52% 57.78% 159.15% 70.79% 148.43% 64.88%Table 15: Language model performance evaluation with different sizes of fine-tuning training set.。The underlined “ImproveRate” values represent the improvement percentage of the “CoFT →SFT” method relative to “SFT” method.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationIn order to complete the command ‘You should lie in wait’, let us plan the states of the agent step by step usingthe following template:1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is [Command], which means teammate player wants the agent...2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan (analyze the reason first, then select keystates one by one as few as possible and as important as possible according to the degree of importance)?According to the teammate’s command:2.1.。[Reason1]: [State1]2.2.。[Reason2]: [State2]...3.。Plan how these key states need to be adjusted (analyze the reason first, and then make adjustments one stateby one state, the state can be changed or remain the same, and must be selected from the value range of thegame state [Choice 1, Choice 2, ...])?According to the teammate’s command:3.1.。[State1]: [Reason1]: [Current_value1] ->[Target_value2]3.2.。[State2]: [Reason2]: [Current_value1] ->[Target_value2]...4.. \n",
      "the piece 903\n",
      " \n",
      "an, so there is no need to keep them, only to choose when making changes.5.Do not plan and adjust the states of teammates and enemies, they can move freely and cannot be controlled.6.Avoid conflicts of states planing.。For example, agent unable to move quickly when lying down, and unable tosee enemies when length of distance from agent to enemy is far away.7.Avoid the repetition of states planing.。For example, if the Average velocity has been adjusted to be Fast, thereis no need to adjust the Whether prone position to False, because the agent can automatically adjust state to fitoverlapping meanings.8.When it is necessary to refer to enemy or teammate information for planing, describe the specific state valueduring analysis.Table 10: Rule prompt for GPT4.Dataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)Attention 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.159Mlp 0.549 0.664 0.482 0.587 0.514 0.620 0.065 0.134All 0.529 0.642 0.471 0.581 0.485 0.596 0.069 0.119Table 13: Evaluation on LoRA target.Symbol Sub-goal Classg1Average velocityg2Horizontal direction of movementg3Whether seen enemyg4Whether hold a gung5Whether prone positiong6Length of distance movedg7Length of distance from agent to teammateg8Distance with nearest enemyg9Whether seen by enemyg10Damage to enemyg11Whether have bulletsg12Horizontal direction of viewg13Whether follow with the movement direction of teammateg14Whether crouch positiong15Whether have a gung16Whether have medical kitsg17Whether to restore healthg18Health levelg19Whether knock down enemyg20Whether target the same enemy as teammateTable 14: Top 20 sub-goals ranked by frequency.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationDataset SizeTrainingMethodPrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)100%CoFT 51.85% 64.84% 46.68% 57.91% 49.13% 61.18%CoFT→SFT 55.48% 68.52% 50.48% 62.10% 52.86% 65.15%SFT 54.70% 65.20% 49.00% 60.20% 51.70% 63.20%Improve Rate 1.42% 5.09% 3.02% 3.16% 2.25% 3.09%30%CoFT 49.43% 62.66% 44.45% 56.29% 46.81% 59.30%CoFT→SFT 49.92% 62.59% 45.51% 57.65% 47.61% 60.02%SFT 46.12% 60.96% 33.68% 45.39% 38.93% 52.03%Improve Rate 8.25% 2.68% 35.11% 27.01% 22.30% 15.34%10%CoFT 45.58% 60.84% 41.77% 53.92% 43.59% 57.17%CoFT→SFT 48.06% 61.01% 43.15% 54.31% 45.47% 57.47%SFT 42.08% 55.31% 30.86% 41.45% 35.61% 47.39%Improve Rate 14.20% 10.32% 39.80% 31.03% 27.69% 21.28%3%CoFT 39.42% 58.35% 34.28% 50.10% 36.67% 53.91%CoFT→SFT 41.61% 60.40% 34.55% 50.32% 37.75% 54.90%SFT 17.66% 38.28% 13.33% 29.47% 15.20% 33.30%Improve Rate 135.52% 57.78% 159.15% 70.79% 148.43% 64.88%Table 15: Language model performance evaluation with different sizes of fine-tuning training set.。The underlined “ImproveRate” values represent the improvement percentage of the “CoFT →SFT” method relative to “SFT” method.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationIn order to complete the command ‘You should lie in wait’, let us plan the states of the agent step by step usingthe following template:1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is [Command], which means teammate player wants the agent...2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan (analyze the reason first, then select keystates one by one as few as possible and as important as possible according to the degree of importance)?According to the teammate’s command:2.1.。[Reason1]: [State1]2.2.。[Reason2]: [State2]...3.。Plan how these key states need to be adjusted (analyze the reason first, and then make adjustments one stateby one state, the state can be changed or remain the same, and must be selected from the value range of thegame state [Choice 1, Choice 2, ...])?According to the teammate’s command:3.1.。[State1]: [Reason1]: [Current_value1] ->[Target_value2]3.2.。[State2]: [Reason2]: [Current_value1] ->[Target_value2]...4.。Modify the adjustment that may be wrong, and refer to the Rules to analyze which state adjustments mayconflict, repeat or be unnecessary, and output the modified adjustment plan: According to the states adjustmentsin 3...4.1.. \n",
      "the piece 904\n",
      " \n",
      "ng.。For example, agent unable to move quickly when lying down, and unable tosee enemies when length of distance from agent to enemy is far away.7.Avoid the repetition of states planing.。For example, if the Average velocity has been adjusted to be Fast, thereis no need to adjust the Whether prone position to False, because the agent can automatically adjust state to fitoverlapping meanings.8.When it is necessary to refer to enemy or teammate information for planing, describe the specific state valueduring analysis.Table 10: Rule prompt for GPT4.Dataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)Attention 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.159Mlp 0.549 0.664 0.482 0.587 0.514 0.620 0.065 0.134All 0.529 0.642 0.471 0.581 0.485 0.596 0.069 0.119Table 13: Evaluation on LoRA target.Symbol Sub-goal Classg1Average velocityg2Horizontal direction of movementg3Whether seen enemyg4Whether hold a gung5Whether prone positiong6Length of distance movedg7Length of distance from agent to teammateg8Distance with nearest enemyg9Whether seen by enemyg10Damage to enemyg11Whether have bulletsg12Horizontal direction of viewg13Whether follow with the movement direction of teammateg14Whether crouch positiong15Whether have a gung16Whether have medical kitsg17Whether to restore healthg18Health levelg19Whether knock down enemyg20Whether target the same enemy as teammateTable 14: Top 20 sub-goals ranked by frequency.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationDataset SizeTrainingMethodPrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)100%CoFT 51.85% 64.84% 46.68% 57.91% 49.13% 61.18%CoFT→SFT 55.48% 68.52% 50.48% 62.10% 52.86% 65.15%SFT 54.70% 65.20% 49.00% 60.20% 51.70% 63.20%Improve Rate 1.42% 5.09% 3.02% 3.16% 2.25% 3.09%30%CoFT 49.43% 62.66% 44.45% 56.29% 46.81% 59.30%CoFT→SFT 49.92% 62.59% 45.51% 57.65% 47.61% 60.02%SFT 46.12% 60.96% 33.68% 45.39% 38.93% 52.03%Improve Rate 8.25% 2.68% 35.11% 27.01% 22.30% 15.34%10%CoFT 45.58% 60.84% 41.77% 53.92% 43.59% 57.17%CoFT→SFT 48.06% 61.01% 43.15% 54.31% 45.47% 57.47%SFT 42.08% 55.31% 30.86% 41.45% 35.61% 47.39%Improve Rate 14.20% 10.32% 39.80% 31.03% 27.69% 21.28%3%CoFT 39.42% 58.35% 34.28% 50.10% 36.67% 53.91%CoFT→SFT 41.61% 60.40% 34.55% 50.32% 37.75% 54.90%SFT 17.66% 38.28% 13.33% 29.47% 15.20% 33.30%Improve Rate 135.52% 57.78% 159.15% 70.79% 148.43% 64.88%Table 15: Language model performance evaluation with different sizes of fine-tuning training set.。The underlined “ImproveRate” values represent the improvement percentage of the “CoFT →SFT” method relative to “SFT” method.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationIn order to complete the command ‘You should lie in wait’, let us plan the states of the agent step by step usingthe following template:1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is [Command], which means teammate player wants the agent...2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan (analyze the reason first, then select keystates one by one as few as possible and as important as possible according to the degree of importance)?According to the teammate’s command:2.1.。[Reason1]: [State1]2.2.。[Reason2]: [State2]...3.。Plan how these key states need to be adjusted (analyze the reason first, and then make adjustments one stateby one state, the state can be changed or remain the same, and must be selected from the value range of thegame state [Choice 1, Choice 2, ...])?According to the teammate’s command:3.1.。[State1]: [Reason1]: [Current_value1] ->[Target_value2]3.2.。[State2]: [Reason2]: [Current_value1] ->[Target_value2]...4.。Modify the adjustment that may be wrong, and refer to the Rules to analyze which state adjustments mayconflict, repeat or be unnecessary, and output the modified adjustment plan: According to the states adjustmentsin 3...4.1.。[State1]: [Current_value1] ->[Target_value2]4.2.. \n",
      "the piece 905\n",
      " \n",
      "tion for planing, describe the specific state valueduring analysis.Table 10: Rule prompt for GPT4.Dataset PrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)AccurateAccurate(Choice)Attention 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.159Mlp 0.549 0.664 0.482 0.587 0.514 0.620 0.065 0.134All 0.529 0.642 0.471 0.581 0.485 0.596 0.069 0.119Table 13: Evaluation on LoRA target.Symbol Sub-goal Classg1Average velocityg2Horizontal direction of movementg3Whether seen enemyg4Whether hold a gung5Whether prone positiong6Length of distance movedg7Length of distance from agent to teammateg8Distance with nearest enemyg9Whether seen by enemyg10Damage to enemyg11Whether have bulletsg12Horizontal direction of viewg13Whether follow with the movement direction of teammateg14Whether crouch positiong15Whether have a gung16Whether have medical kitsg17Whether to restore healthg18Health levelg19Whether knock down enemyg20Whether target the same enemy as teammateTable 14: Top 20 sub-goals ranked by frequency.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationDataset SizeTrainingMethodPrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)100%CoFT 51.85% 64.84% 46.68% 57.91% 49.13% 61.18%CoFT→SFT 55.48% 68.52% 50.48% 62.10% 52.86% 65.15%SFT 54.70% 65.20% 49.00% 60.20% 51.70% 63.20%Improve Rate 1.42% 5.09% 3.02% 3.16% 2.25% 3.09%30%CoFT 49.43% 62.66% 44.45% 56.29% 46.81% 59.30%CoFT→SFT 49.92% 62.59% 45.51% 57.65% 47.61% 60.02%SFT 46.12% 60.96% 33.68% 45.39% 38.93% 52.03%Improve Rate 8.25% 2.68% 35.11% 27.01% 22.30% 15.34%10%CoFT 45.58% 60.84% 41.77% 53.92% 43.59% 57.17%CoFT→SFT 48.06% 61.01% 43.15% 54.31% 45.47% 57.47%SFT 42.08% 55.31% 30.86% 41.45% 35.61% 47.39%Improve Rate 14.20% 10.32% 39.80% 31.03% 27.69% 21.28%3%CoFT 39.42% 58.35% 34.28% 50.10% 36.67% 53.91%CoFT→SFT 41.61% 60.40% 34.55% 50.32% 37.75% 54.90%SFT 17.66% 38.28% 13.33% 29.47% 15.20% 33.30%Improve Rate 135.52% 57.78% 159.15% 70.79% 148.43% 64.88%Table 15: Language model performance evaluation with different sizes of fine-tuning training set.。The underlined “ImproveRate” values represent the improvement percentage of the “CoFT →SFT” method relative to “SFT” method.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationIn order to complete the command ‘You should lie in wait’, let us plan the states of the agent step by step usingthe following template:1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is [Command], which means teammate player wants the agent...2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan (analyze the reason first, then select keystates one by one as few as possible and as important as possible according to the degree of importance)?According to the teammate’s command:2.1.。[Reason1]: [State1]2.2.。[Reason2]: [State2]...3.。Plan how these key states need to be adjusted (analyze the reason first, and then make adjustments one stateby one state, the state can be changed or remain the same, and must be selected from the value range of thegame state [Choice 1, Choice 2, ...])?According to the teammate’s command:3.1.。[State1]: [Reason1]: [Current_value1] ->[Target_value2]3.2.。[State2]: [Reason2]: [Current_value1] ->[Target_value2]...4.。Modify the adjustment that may be wrong, and refer to the Rules to analyze which state adjustments mayconflict, repeat or be unnecessary, and output the modified adjustment plan: According to the states adjustmentsin 3...4.1.。[State1]: [Current_value1] ->[Target_value2]4.2.。[State2]: [Current_value1] ->[Target_value2]...5.. \n",
      "the piece 906\n",
      " \n",
      "oice)Attention 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.159Mlp 0.549 0.664 0.482 0.587 0.514 0.620 0.065 0.134All 0.529 0.642 0.471 0.581 0.485 0.596 0.069 0.119Table 13: Evaluation on LoRA target.Symbol Sub-goal Classg1Average velocityg2Horizontal direction of movementg3Whether seen enemyg4Whether hold a gung5Whether prone positiong6Length of distance movedg7Length of distance from agent to teammateg8Distance with nearest enemyg9Whether seen by enemyg10Damage to enemyg11Whether have bulletsg12Horizontal direction of viewg13Whether follow with the movement direction of teammateg14Whether crouch positiong15Whether have a gung16Whether have medical kitsg17Whether to restore healthg18Health levelg19Whether knock down enemyg20Whether target the same enemy as teammateTable 14: Top 20 sub-goals ranked by frequency.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationDataset SizeTrainingMethodPrecisionPrecision(Choice)RecallRecall(Choice)F1F1(Choice)100%CoFT 51.85% 64.84% 46.68% 57.91% 49.13% 61.18%CoFT→SFT 55.48% 68.52% 50.48% 62.10% 52.86% 65.15%SFT 54.70% 65.20% 49.00% 60.20% 51.70% 63.20%Improve Rate 1.42% 5.09% 3.02% 3.16% 2.25% 3.09%30%CoFT 49.43% 62.66% 44.45% 56.29% 46.81% 59.30%CoFT→SFT 49.92% 62.59% 45.51% 57.65% 47.61% 60.02%SFT 46.12% 60.96% 33.68% 45.39% 38.93% 52.03%Improve Rate 8.25% 2.68% 35.11% 27.01% 22.30% 15.34%10%CoFT 45.58% 60.84% 41.77% 53.92% 43.59% 57.17%CoFT→SFT 48.06% 61.01% 43.15% 54.31% 45.47% 57.47%SFT 42.08% 55.31% 30.86% 41.45% 35.61% 47.39%Improve Rate 14.20% 10.32% 39.80% 31.03% 27.69% 21.28%3%CoFT 39.42% 58.35% 34.28% 50.10% 36.67% 53.91%CoFT→SFT 41.61% 60.40% 34.55% 50.32% 37.75% 54.90%SFT 17.66% 38.28% 13.33% 29.47% 15.20% 33.30%Improve Rate 135.52% 57.78% 159.15% 70.79% 148.43% 64.88%Table 15: Language model performance evaluation with different sizes of fine-tuning training set.。The underlined “ImproveRate” values represent the improvement percentage of the “CoFT →SFT” method relative to “SFT” method.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationIn order to complete the command ‘You should lie in wait’, let us plan the states of the agent step by step usingthe following template:1.。Analyze the verbal orders of teammates and players, what do you want to do?。According to the command,also analysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is [Command], which means teammate player wants the agent...2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan (analyze the reason first, then select keystates one by one as few as possible and as important as possible according to the degree of importance)?According to the teammate’s command:2.1.。[Reason1]: [State1]2.2.。[Reason2]: [State2]...3.。Plan how these key states need to be adjusted (analyze the reason first, and then make adjustments one stateby one state, the state can be changed or remain the same, and must be selected from the value range of thegame state [Choice 1, Choice 2, ...])?According to the teammate’s command:3.1.。[State1]: [Reason1]: [Current_value1] ->[Target_value2]3.2.。[State2]: [Reason2]: [Current_value1] ->[Target_value2]...4.。Modify the adjustment that may be wrong, and refer to the Rules to analyze which state adjustments mayconflict, repeat or be unnecessary, and output the modified adjustment plan: According to the states adjustmentsin 3...4.1.。[State1]: [Current_value1] ->[Target_value2]4.2.。[State2]: [Current_value1] ->[Target_value2]...5.。According to the analyze and the planing of the verbal command, further analyze the behavior tendencyrequired in the adjustment process (the proportion of Mobile, Offense, Waiting, Supplies, Scouting, first analyzethe reason, and then calculate the percentage)Mobile: [Reason1]: [Percent1]Offense: [Reason2]: [Percent2]Waiting: [Reason3]: [Percent3]Supplies: [Reason4]: [Percent4]Scouting: [Reason5]: [Percent5]6.. \n",
      "the piece 907\n",
      " \n",
      "alysis the relevant states of teammates and enemies that need attention.The verbal command of the teammate player is [Command], which means teammate player wants the agent...2.。Analyze which states of the agents are most relevant to the verbal commands of teammate player.。The agentsin the unselected states will adjust themselves to complete your plan (analyze the reason first, then select keystates one by one as few as possible and as important as possible according to the degree of importance)?According to the teammate’s command:2.1.。[Reason1]: [State1]2.2.。[Reason2]: [State2]...3.。Plan how these key states need to be adjusted (analyze the reason first, and then make adjustments one stateby one state, the state can be changed or remain the same, and must be selected from the value range of thegame state [Choice 1, Choice 2, ...])?According to the teammate’s command:3.1.。[State1]: [Reason1]: [Current_value1] ->[Target_value2]3.2.。[State2]: [Reason2]: [Current_value1] ->[Target_value2]...4.。Modify the adjustment that may be wrong, and refer to the Rules to analyze which state adjustments mayconflict, repeat or be unnecessary, and output the modified adjustment plan: According to the states adjustmentsin 3...4.1.。[State1]: [Current_value1] ->[Target_value2]4.2.。[State2]: [Current_value1] ->[Target_value2]...5.。According to the analyze and the planing of the verbal command, further analyze the behavior tendencyrequired in the adjustment process (the proportion of Mobile, Offense, Waiting, Supplies, Scouting, first analyzethe reason, and then calculate the percentage)Mobile: [Reason1]: [Percent1]Offense: [Reason2]: [Percent2]Waiting: [Reason3]: [Percent3]Supplies: [Reason4]: [Percent4]Scouting: [Reason5]: [Percent5]6.。Analyze how long the current command needs to be kept (for example, the command of ‘killing the enemy’needs to be kept for a ‘short term’, and the command of ‘pay attention to reconnaissance’ needs to be kept for a‘long term’.. \n",
      "the piece 908\n",
      " \n",
      "your plan (analyze the reason first, then select keystates one by one as few as possible and as important as possible according to the degree of importance)?According to the teammate’s command:2.1.。[Reason1]: [State1]2.2.。[Reason2]: [State2]...3.。Plan how these key states need to be adjusted (analyze the reason first, and then make adjustments one stateby one state, the state can be changed or remain the same, and must be selected from the value range of thegame state [Choice 1, Choice 2, ...])?According to the teammate’s command:3.1.。[State1]: [Reason1]: [Current_value1] ->[Target_value2]3.2.。[State2]: [Reason2]: [Current_value1] ->[Target_value2]...4.。Modify the adjustment that may be wrong, and refer to the Rules to analyze which state adjustments mayconflict, repeat or be unnecessary, and output the modified adjustment plan: According to the states adjustmentsin 3...4.1.。[State1]: [Current_value1] ->[Target_value2]4.2.。[State2]: [Current_value1] ->[Target_value2]...5.。According to the analyze and the planing of the verbal command, further analyze the behavior tendencyrequired in the adjustment process (the proportion of Mobile, Offense, Waiting, Supplies, Scouting, first analyzethe reason, and then calculate the percentage)Mobile: [Reason1]: [Percent1]Offense: [Reason2]: [Percent2]Waiting: [Reason3]: [Percent3]Supplies: [Reason4]: [Percent4]Scouting: [Reason5]: [Percent5]6.。Analyze how long the current command needs to be kept (for example, the command of ‘killing the enemy’needs to be kept for a ‘short term’, and the command of ‘pay attention to reconnaissance’ needs to be kept for a‘long term’.。First analyze the reason and then make a judgment).According to the command of the teammate, [Analysis]: The current command needs to be kept by ‘[XX term]’.If you see phrases like [Context] in answer template, replace the entire phrase according to the meaning of theContext, do not repeat the content; make analogy expansion for ‘...’; keep ‘:’; absolutely do not modify others intemplate.Table 16: Chain of thought prompt for GPT4.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationpromptsystembackgroundpromptWe have an agent and a player working together as a teammate in aPUBG game.. \n",
      "the piece 909\n",
      " \n",
      "as possible and as important as possible according to the degree of importance)?According to the teammate’s command:2.1.。[Reason1]: [State1]2.2.。[Reason2]: [State2]...3.。Plan how these key states need to be adjusted (analyze the reason first, and then make adjustments one stateby one state, the state can be changed or remain the same, and must be selected from the value range of thegame state [Choice 1, Choice 2, ...])?According to the teammate’s command:3.1.。[State1]: [Reason1]: [Current_value1] ->[Target_value2]3.2.。[State2]: [Reason2]: [Current_value1] ->[Target_value2]...4.。Modify the adjustment that may be wrong, and refer to the Rules to analyze which state adjustments mayconflict, repeat or be unnecessary, and output the modified adjustment plan: According to the states adjustmentsin 3...4.1.。[State1]: [Current_value1] ->[Target_value2]4.2.。[State2]: [Current_value1] ->[Target_value2]...5.。According to the analyze and the planing of the verbal command, further analyze the behavior tendencyrequired in the adjustment process (the proportion of Mobile, Offense, Waiting, Supplies, Scouting, first analyzethe reason, and then calculate the percentage)Mobile: [Reason1]: [Percent1]Offense: [Reason2]: [Percent2]Waiting: [Reason3]: [Percent3]Supplies: [Reason4]: [Percent4]Scouting: [Reason5]: [Percent5]6.。Analyze how long the current command needs to be kept (for example, the command of ‘killing the enemy’needs to be kept for a ‘short term’, and the command of ‘pay attention to reconnaissance’ needs to be kept for a‘long term’.。First analyze the reason and then make a judgment).According to the command of the teammate, [Analysis]: The current command needs to be kept by ‘[XX term]’.If you see phrases like [Context] in answer template, replace the entire phrase according to the meaning of theContext, do not repeat the content; make analogy expansion for ‘...’; keep ‘:’; absolutely do not modify others intemplate.Table 16: Chain of thought prompt for GPT4.Building Open-Ended Embodied Agent via Language-Policy Bidirectional AdaptationpromptsystembackgroundpromptWe have an agent and a player working together as a teammate in aPUBG game.。We hope you can help the agent plan how the agent’sgame state should change, so as to complete the player’s command andhelp the player win the game.teammatestatepromptThe state of the agent’s teammates can be described as follows:{‘Length of distance moved’: ‘No movement’, ‘Average velocity’: ‘Slow’,‘Horizontal direction of movement’: ‘Southeast’, ‘Horizontaldirection of view’: ‘South’, ‘Pitch direction of view’: ‘Medium’,‘Health level’: ‘Empty’, ‘Whether to restore health’: ‘False’,‘Whether the health is damaged’: ‘False’, ‘Whether rescued teammate’:‘False’, ‘Whether prone position’: ‘False’, ‘Whether crouchposition’: ‘False’, ‘Whether have a gun’: ‘True’, ‘Whether hold agun’: ‘False’, ‘Whether have bullets’: ‘True’, ‘Whether have medicalkits’: ‘True’, ‘Whether be knocked down’: ‘False’, ‘Damage to enemy’:‘Zero’, ‘Whether knock down enemy’: ‘False’, ‘Whether seen enemy’:‘True’, ‘Number of enemies have ever seen’: 5, ‘Whether seen byenemy’: ‘True’, ‘Distance with nearest enemy’: ‘Nearby’, ‘Whethercloser with nearest enemy’: ‘False’, ‘ID of teammate player’: 2}enemystatepromptThe state of the enemy can be described as follows:{‘Horizontaldirection of movement of enemy’: ‘Southwest’, ‘Velocity of enemy’:‘Slow’, ‘Enemy’s position relative to agent’: ‘West’}selfstatepromptThe state of the agent can be described as follows:{‘Damage toenemy’: ‘Zero’, ‘Whether knock down enemy’: ‘False’, ‘Whether killenemy’: ‘False’, ‘Whether seen enemy’: ‘True’, ‘Whether seen byenemy’: ‘True’, ‘Number of enemies have ever seen’: 3, ‘Length ofdistance moved’: ‘Short’, ‘Average velocity’: ‘Fast’, ‘Horizontaldirection of movement’: ‘West’, ‘Horizontal direction of view’:‘NorthEast’, ‘Pitch direction of view’: ‘Medium’, ‘Health level’:‘Full’, ‘Whether to restore health’: ‘False’, ‘Whether the health isdamaged’: ‘False’, ‘Whether rescued teammate’: ‘False’, ‘Whether beknocked down’: ‘False’, ‘Whether prone position’: ‘False’, ‘Whetherhave a gun’: ‘True’, ‘Whether have bullets’: ‘True’, ‘Whether havemedical kits’: ‘True’, ‘Distance with nearest enemy’: ‘Nearby’,‘Whether closer with nearest enemy’: ‘True’, ‘Whether crouchposition’: ‘False’, ‘Whether hold a gun’: ‘True’, ‘Whether seen byteammate’: ‘True’, ‘Length of distance from agent to teammate’:‘Touch’, ‘Teammate’s position relative to agent’: ‘Southwest’,‘Whether follow with the views of teammate’: ‘False’, ‘Whethertarget the same enemy as teammate’: ‘False’, ‘Whether follow withthe movement direction of teammate’: ‘False’}commandpromptTeammate player gives you a verbal command based on the currentgame states:‘You should lie in wait.’questionpromptIn order to complete the command ‘You should lie in wait.’, howthe agent’s game state should change?responsegoalmeta-statepromptWhether prone position:TrueAverage velocity:StaticLength of distance moved:No movementWhether hold a gun:TrueTable 17: Example of prompt and response.. \n"
     ]
    }
   ],
   "source": [
    "reader.main_call()\n",
    "\n",
    "\n",
    "for i,it in enumerate(reader.data):\n",
    "    print(\"the piece \" + str(i))\n",
    "    print(\" \")\n",
    "    print(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = reader.data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
